{"id": "77b90deda6569a7a", "content": "If you are new to Splunk Search, the best way to get acquainted is to start with the Search Tutorial. The Search Tutorial introduces you to the Search and Reporting app and guides you through adding data, searching your data, and building simple reports and dashboards. The Search Tutorial provides a great foundation for understanding Splunk Search.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Get started with Search", "section_heading": "Start Here", "section_id": "id_2ff5be04_90df_461d_a28e_dc5913cc19ac--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/get-started-with-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:14:38.205746+00:00", "version": "10.2"}}
{"id": "9fe048162e8e2bdb", "content": "After you complete the Search Tutorial, you should learn about the types of data you can explore, how Splunk software indexes data, and about Splunk knowledge objects. Here are the resources to look at: Upload data to your Splunk deployment. See the Getting Data In manual. Understand how indexing works. See the Managing Indexers and Clusters of Indexers manual. Understand fields and knowledge objects, such as host, source type, and event type. See the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Get started with Search", "section_heading": "Getting started in your own environment", "section_id": "id_12c0c567_00cb_40c4_bb95_49f0814b1bbe--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/get-started-with-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:14:38.205773+00:00", "version": "10.2"}}
{"id": "e1238026e6a1837a", "content": "And of course you need to learn how to use the Search app effectively, which is the focus of this manual. This manual contains detailed information about how to search your data. Basic Search app skills Navigating Splunk Web Using the Search app Types of searches Types of commands Detailed Search information Retrieving events Specifying time ranges Optimizing searches Creating tables and charts Evaluating and manipulating fields Calculating statistics and advanced statistics Grouping and correlating events Managing search jobs Search command reference For a catalog of search commands and arguments that make up the Splunk SPL, see the Search Reference. Distributed Search If you are using Splunk Enterprise, distributed search provides a way to scale your deployment by separating the search management and presentation layer from the indexing and search retrieval layer. For an introduction to distributed search, see the Distributed Search Manual .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Get started with Search", "section_heading": "Use the Search app effectively", "section_id": "id_69d3f287_83b0_4ad2_9072_cd623409869f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/get-started-with-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:14:38.205780+00:00", "version": "10.2"}}
{"id": "349793bdb2610ae8", "content": "Navigating Splunk Web Using Splunk Search", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Get started with Search", "section_heading": "See also", "section_id": "id_6615ff4b_8401_4d6b_b782_6176617b426b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/get-started-with-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:14:38.205785+00:00", "version": "10.2"}}
{"id": "a5a32a0393bd30d3", "content": "By default, when you search with keywords and phrases, Splunk software retrieves events by matching against the raw event field, _raw , in your data. When you start adding search modifiers, such as fields like _time and tag , you are also matching against pieces of information that have been extracted from the _raw field. When searching for strings, which includes keywords and quoted phrases (or anything that's not a search modifier), Splunk software searches the _raw field for the matching events or results. Some examples of keywords and phrases are: Notice that the search for the quoted phrase \"web error\" is not the same as the search before it. When you search for web error , Splunk software returns events that contain both \"web\" and \"error\". When you search for \"web error\", Splunk software only returns events that contain the phrase \"web error\".", "code_examples": [{"language": "spl", "code": "web"}, {"language": "spl", "code": "error"}, {"language": "spl", "code": "web error"}, {"language": "spl", "code": "\"web error\""}], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Search command primer", "section_heading": "Keywords and phrases", "section_id": "id_119e0aee_93c4_4585_b35b_e0e7010f1a7a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/search-command-primer", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:14:55.628764+00:00", "version": "10.2"}}
{"id": "56304a0bdb396a76", "content": "To search for a file path, such as D:\\Digital\\RTFM , you must escape the backslash characters in the path, for example D:\\\\Digital\\\\RTFM. If the file path contains spaces you must enclose the path in quotation marks. For example: A space is considered a major breaker in data. To learn more about major and minor breakers, see Event segmentation and searching .", "code_examples": [{"language": "spl", "code": "\"D:\\\\Digital\\\\RTFM Backup Folder\""}], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Search command primer", "section_heading": "File paths", "section_id": "eb1187ac_4510_4ad5_ac0e_fd72ec28dcb0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/search-command-primer", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:14:55.628774+00:00", "version": "10.2"}}
{"id": "a76a6539287ea6e7", "content": "Understanding SPL syntax About retrieving events About search time ranges Quick tips for optimization", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Search command primer", "section_heading": "See Also", "section_id": "d2b8823a_9c3b_44a7_bc9e_8be6397b1244--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/search-command-primer", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:14:55.628779+00:00", "version": "10.2"}}
{"id": "9832fdbe81d2a801", "content": "When you run a search, the Splunk software uses the information in the index files to identify which events to retrieve from disk. The smaller the number of events to retrieve from disk, the faster the search runs. How you construct your search has a significant impact on the number of events retrieved from disk. When data is indexed, the data is processed into events based on time. The processed data consists of several files: The raw data in compressed form ( rawdata ) The indexes that point to the raw data ( index files , also referred to as tsidx files ) Some metadata files These files are written to disk and reside in sets of directories, organized by age, called buckets. Use indexes effectively One method to limit the data that is pulled off from disk is to partition data into separate indexes. If you rarely search across more than one type of data at a time, partition different types of data into separate indexes. Then restrict your searches to the specific index. For example, store web access data in one index and firewall data in another. Use separate indexes for sparse data, which might otherwise be buried in a large volume of unrelated data. See Ways to set up multiple indexes in the Managing Indexers and Clusters of Indexers manual See Retrieve events from indexes", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "About search optimization", "section_heading": "Indexes and searches", "section_id": "f001b9e5_25ad_4eb3_a270_4409eb626003--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/about-search-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:15:12.353349+00:00", "version": "10.2"}}
{"id": "2edab9ad23a9afc7", "content": "Some frequently used searches unnecessarily consume a significant amount of system resources. You will learn how optimizing just one search can save significant system resources. A frequently used search One search that is frequently used is a search that contains a lookup and an evaluation, followed by another search. For example: The following diagram shows a simplified, visual representation of this search. When the search is run, the index is accessed and 1 million events are extracted based on the source type. In the next part of the search, the lookup and eval command are run are on all 1 million events. Both the lookup and eval commands add columns to the events, as shown in the following image. Finally, a second search command runs against the columns A, L, and E. For column A, the search looks for values that are equal to 25. For column L, which was added as a result of the lookup command, the search looks for values greater than 100. For column E, which was added as a result of the eval command, the search looks for values that are greater than 50. Events that match the criteria for columns A, L, and E are identified, and 50,000 events that match the search criteria are returned. The following image shows the entire process and the resource costs involved in this inefficient search. An optimized search You can optimize the entire search by moving some of the components from the second search to locations earlier in the search process. Moving the criteria A=25 before the first pipe filters the events earlier and reduces the amount of times that the index is accessed. The number of events extracted is 300,000. This is a reduction of 700,000 compared to the original search. The lookup is performed on 300,000 events instead of 1 million events. Moving the criteria L>100 immediately after the lookup filters the events further, reducing the number of events returned by 100,000. The eval is performed on 200,000 events instead of 1 million events. The criteria E>50 is dependent on the results of the eval command and cannot be moved. The results are the same as the original search. 50,000 events are returned, but with much less impact on resources. This is the optimized search. The following image shows the impact of rearranging the search criteria.", "code_examples": [{"language": "spl", "code": "sourcetype=my_source\n| lookup my_lookup_file D OUTPUTNEW L \n|evalE=L/T \n| search A=25 L>100 E>50"}, {"language": "spl", "code": "sourcetype=my_source A=25\n| lookup my_lookup_file D OUTPUTNEW L\n| search L>100\n|evalE=L/T \n| search  E>50"}], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "About search optimization", "section_heading": "A tale of two searches", "section_id": "f4638502_9244_4372_ad5a_ebd3bdf01ea8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/about-search-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:15:12.353358+00:00", "version": "10.2"}}
{"id": "251b545a248b4942", "content": "Quick tips for optimization Write better searches Built-in optimizations", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "About search optimization", "section_heading": "See also", "section_id": "ce1b88bb_8395_4796_84fe_615e14eec9c1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/about-search-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:15:12.353363+00:00", "version": "10.2"}}
{"id": "c3794b0dd697b98a", "content": "If you are new to Splunk software and searching, start with the Search Tutorial. This tutorial introduces you to the Search & Reporting application. The tutorial guides you through uploading data to your Splunk deployment, searching your data, and building simple charts, reports, and dashboards. After you complete the Search Tutorial, and before you start using Splunk software on your own data you should: Add data to your Splunk instance. See Getting Data In. Understand how indexing works and how data is processed. See Managing Indexers and Clusters of Indexers. Learn about fields and knowledge objects, such as hosts, source types, and event types. See the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Welcome to the Search Reference", "section_heading": "Getting Started", "section_id": "id_7da7bc74_1487_4fd8_8f95_6c3b1e07e276--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/welcome-to-the-search-reference", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:15:28.259984+00:00", "version": "10.2"}}
{"id": "f8cd473b6a6bdab8", "content": "The Search Manual is a companion manual to the Search Reference. The Search Manual contains detailed information about creating and optimizing searches. Types of searches Retrieving events Specifying time ranges Optimizing searches Using subsearches Creating statistical tables and charts Grouping and correlating events Predicting future events Managing jobs", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Welcome to the Search Reference", "section_heading": "Search Manual", "section_id": "id_00976a3e_4a3c_4399_a730_b7ade737215a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/welcome-to-the-search-reference", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:15:28.259992+00:00", "version": "10.2"}}
{"id": "bcf2e2565a7af237", "content": "The Quick Reference Guide contains: Explanations about Splunk features Common search commands Tips on optimizing searches Functions for the eval and stats commands Search examples Regular expressions Formats for converting strings into timestamps SPL commands The Search Processing Language (SPL) includes a wide range of commands. There are two quick reference guides for the commands: The Command quick reference topic contains an alphabetical list of each command, along with a brief description of what the command does and a link to the specific documentation for the command. The Commands by category topic organizes the commands by the type of action that the command performs. This topic contains a brief description of what the command does and a link to the specific documentation for the command. SQL users If you're familiar with SQL, see Splunk SPL for SQL users to see how to use your SQL knowledge to learn SPL.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Welcome to the Search Reference", "section_heading": "Quick Reference Information", "section_id": "id_702885b1_e845_4226_ac94_706e20cff892--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/welcome-to-the-search-reference", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:15:28.259997+00:00", "version": "10.2"}}
{"id": "1b4fad1c43870b1f", "content": "Before you continue, see Understanding SPL syntax for the conventions and rules used in this manual.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Welcome to the Search Reference", "section_heading": "Command syntax", "section_id": "a9e5989a_18f6_4ce2_ace0_54abc612eac4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/welcome-to-the-search-reference", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:15:28.260002+00:00", "version": "10.2"}}
{"id": "e9eb44a2820a5def", "content": "Search commands by category", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Splunk Quick Reference Guide", "section_heading": "See also", "section_id": "id_979c7865_755f_4678_aa47_18ece7b77206--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/splunk-quick-reference-guide", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:15:46.325156+00:00", "version": "10.2"}}
{"id": "718e74f021ffdf6e", "content": "If you cannot find what you are looking for in this search language reference, check out Splunk Answers and see what questions and answers other Splunk users have about the search language.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Splunk Quick Reference Guide", "section_heading": "Splunk Answers", "section_id": "c1d9211d_80e5_4455_a0b9_5dd1f2290df4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/splunk-quick-reference-guide", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:15:46.325163+00:00", "version": "10.2"}}
{"id": "4f4cb85e4c49095c", "content": "See the Supported functions and syntax section for a quick reference list of the evaluation functions.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "Evaluation functions", "section_heading": "Quick reference", "section_id": "c3c99072_82cf_458a_a9e9_ce30ca17ab33--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/evaluation-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:01.280868+00:00", "version": "10.2"}}
{"id": "2dbd6a9efec06fc4", "content": "You can use evaluation functions with the eval , fieldformat , and where commands, and as part of eval expressions with other commands.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "Evaluation functions", "section_heading": "Commands", "section_id": "id_006f8a54_63dc_4726_b8dd_c82a83ae1de9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/evaluation-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:01.280877+00:00", "version": "10.2"}}
{"id": "1abf5b3a48c22337", "content": "All functions that accept strings can accept literal strings or any field. All functions that accept numbers can accept literal numbers or any numeric field. String arguments and fields For most evaluation functions, when a string argument is expected, you can specify either a literal string or a field name. Literal strings must be enclosed in double quotation marks. In other words, when the function syntax specifies a string you can specify any expression that results in a string. For example, you have a field called name that contains the names of your servers. If you want to append the literal string server at the end of the name, you would use dot notation like this in your search: name.\"server\" .â€‹ Nested functions You can specify a function as an argument to another function. In the following example, the cidrmatch function is used as the first argument in the if function. The following example shows how to use the true() function to provide a default to the case function.", "code_examples": [{"language": "spl", "code": "... |evalisLocal=if(cidrmatch(\"123.132.32.0/25\",ip),\"local\",\"not local\")"}, {"language": "spl", "code": "... |evalerror=case(status == 200,\"OK\", status == 404,\"Not found\",true(),\"Other\")"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "Evaluation functions", "section_heading": "Usage", "section_id": "id_965922c9_e545_4e5f_bc01_0c02d497c6ae--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/evaluation-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:01.280884+00:00", "version": "10.2"}}
{"id": "6237279f9acc6e54", "content": "There are two ways that you can see information about the supported evaluation functions: Function list by category Alphabetical list of functions Function list by category The following table is a quick reference of the supported evaluation functions, organized by category. This table provides a brief description for each function. Use the links in the table to learn more about each function and to see examples. Alphabetical list of functions The following table is a quick reference of the supported evaluation functions, organized alphabetically. This table provides a brief description for each function. Use the links in the table to learn more about each function and to see examples.", "code_examples": [], "tables": [{"headers": ["Type of function", "Supported functions and syntax", "Description"], "rows": [["Bitwise functions", "bit_and(<values>)", "Bitwise AND function that takes two or more non-negative integers as arguments and sequentially performs logical bitwise AND on them."], ["bit_or(<values>)", "Bitwise OR function that takes two or more non-negative integers as arguments and sequentially performs bitwise OR on them."], ["bit_not(<value>, <bitmask>)", "Bitwise NOT function that takes a non-negative as an argument and inverts every bit in the binary representation of that number. It also takes an optional second argument that acts as a bitmask."], ["bit_xor(<values>)", "Bitwise XOR function that takes two or more non-negative integers as arguments and sequentially performs bitwise XOR of each of the given arguments."], ["bit_shift_left(<value>, <shift_offset>)", "Logical left shift function that takes two non-negative integers as arguments and shifts the binary representation of the first integer over to the left by the specified shift amount."], ["bit_shift_right(<value>, <shift_offset>)", "Logical right shift function that takes two non-negative integers as arguments and shifts the binary representation of the first integer over to the right by the specified shift amount."], ["Comparison and Conditional functions", "case(<condition>,<value>,...)", "Accepts alternating conditions and values. Returns the first value for which the condition evaluates to TRUE."], ["cidrmatch(<cidr>,<ip>)", "Returns TRUE when an IP address,<ip>, belongs to a particular CIDR subnet,<cidr>."], ["coalesce(<values>)", "Takes one or more values and returns the first value that is not NULL."], ["false()", "Returns FALSE."], ["if(<predicate>,<true_value>,<false_value>)", "If the<predicate>expression evaluates to TRUE, returns the<true_value>, otherwise the function returns the<false_value>."], ["in(<field>,<list>)", "Returns TRUE if one of the values in the list matches a value that you specify."], ["like(<str>,<pattern>)", "Returns TRUE only if<str>matches<pattern>."], ["lookup(<lookup_table>, <json_object>, <json_array>)", "Performs a CSV lookup. Returns the output field or fields in the form of a JSON object.Note:Thelookup()function is available only to Splunk Enterprise users."], ["match(<str>, <regex>)", "Returns TRUE if the regular expression<regex>finds a match against any substring of the string value<str>. Otherwise returns FALSE."], ["null()", "This function takes no arguments and returns NULL."], ["nullif(<field1>,<field2>)", "Compares the values in two fields and returns NULL if the value in<field1>is equal to the value in<field2>. Otherwise returns the value in<field1>."], ["searchmatch(<search_str>)", "Returns TRUE if the event matches the search string."], ["true()", "Returns TRUE."], ["validate(<condition>, <value>,...)", "Takes a list of conditions and values and returns the value that corresponds to the condition that evaluates to FALSE. This function defaults to NULL if all conditions evaluate to TRUE. This function is the opposite of thecasefunction."], ["Conversion functions", "ipmask(<mask>,<ip>)", "Generates a new masked IP address by applying a mask to an IP address using a bitwiseANDoperation."], ["printf(<format>,<arguments>)", "Creates a formatted string based on a format description that you provide."], ["toarray(<value>)", "Converts a value to an array value."], ["tobool(<value>)", "Converts a value to a Boolean value."], ["todouble(<value>, <base>)", "Converts a value to the equivalent double value of the field, if any. The second argument specifies the numeric base used to convert strings."], ["toint(<value>, <base>)", "Converts a value to the equivalent integer value of the field, if any. The second argument specifies the numeric base used to convert strings."], ["tomv(<value>)", "Converts a value to a multivalue."], ["tonumber(<str>,<base>)", "Converts a string to a number."], ["toobject(<value>)", "Converts a value to the equivalent object value of the field, if any."], ["tostring(<value>,<format>)", "Converts the input, such as a number or a Boolean value, to a string."], ["Cryptographic functions", "md5(<str>)", "Computes the md5 hash for the string value."], ["sha1(<str>)", "Computes the sha1 hash for the string value."], ["sha256(<str>)", "Computes the sha256 hash for the string value."], ["sha512(<str>)", "Computes the sha512 hash for the string value."], ["Date and Time functions", "now()", "Returns the time that the search was started when run as an ad-hoc search. If used with a scheduled search, returns the time that the search was scheduled to run, which might not be the time that the scheduled search actual runs."], ["relative_time(<time>,<specifier>)", "Adjusts the time by a relative time specifier."], ["strftime(<time>,<format>)", "Takes a UNIX time and renders it into a human readable format."], ["strptime(<str>,<format>)", "Takes a human readable time and renders it into UNIX time."], ["time()", "The time that eval function was computed. The time will be different for each event, based on when the event was processed."], ["Informational functions", "isarray(<value>)", "Returns TRUE if the field value is an array."], ["isbool(<value>)", "Returns TRUE if the field value is Boolean."], ["isdouble(<value>)", "Returns TRUE if the field value is a double value."], ["isint(<value>)", "Returns TRUE if the field value is an integer."], ["ismv(<value>)", "Returns TRUE if the field value is a multivalue."], ["isnotnull(<value>)", "Returns TRUE if the field value is not NULL."], ["isnull(<value>)", "Returns TRUE if the field value is NULL."], ["isnum(<value>)", "Returns TRUE if the field value is a number."], ["isobject(<value>)", "Returns TRUE if the field value is an object."], ["isstr(<value>)", "Returns TRUE if the field value is a string."], ["typeof(<value>)", "Returns a string that indicates the field type, such as Number, String, Boolean, and so forth"], ["JSON functions", "json_object(<members>)", "Creates a new JSON object from members of key-value pairs."], ["json(<value>)", "Evaluates whether a value can be parsed as JSON. If the value is JSON, the function returns the value. Otherwise, the function returns null."], ["json_append(<json>, <path_value_pairs>)", "Appends values to the ends of indicated arrays within a JSON document."], ["json_array(<values>)", "Creates a JSON array using a list of values."], ["json_array_to_mv(<json_array>, <boolean>)", "Maps the elements of a proper JSON array into a multivalue field."], ["json_delete(<object>,<keys>)", "Removes one or more keys and their corresponding values from the specified JSON object."], ["json_entries(<value>)", "Returns the key-value entries from the top-level key-value pairs in a JSON object. The entries are returned as a JSON array of JSON objects with fieldskeyandvalue."], ["json_extend(<json>, <path_value_pairs>)", "Flattens arrays into their component values and appends those values to the ends of indicated arrays within a valid JSON document."], ["json_extract(<json>, <paths>)", "This function returns a value from a piece JSON and zero or more paths. The value is returned in either a JSON array, or a Splunk software native type value."], ["json_extract_exact(<json>,<keys>)", "Returns Splunk software native type values from a piece of JSON by matching literal strings in the event and extracting them as keys."], ["json_has_key_exact(<object>, <key>)", "Evaluates whether a JSON object contains the specified key and returns either TRUE or FALSE."], ["json_keys(<json>)", "Returns the keys from the key-value pairs in a JSON object as a JSON array."], ["json_set(<json>, <path_value_pairs>)", "Inserts or overwrites values for a JSON node with the values provided and returns an updated JSON object."], ["json_set_exact(<json>,<key_value_pairs>)", "Uses provided key-value pairs to generate or overwrite a JSON object."], ["json_valid(<json>)", "Evaluates whether piece of JSON uses valid JSON syntax and returns either TRUE or FALSE."], ["Mathematical functions", "abs(<num>)", "Returns the absolute value."], ["ceiling(<num>)", "Rounds the value up to the next highest integer."], ["exact(<expression>)", "Returns the result of a numeric eval calculation with a larger amount of precision in the formatted output."], ["exp(<num>)", "Returns the exponential functioneN."], ["floor(<num>)", "Rounds the value down to the next lowest integer."], ["ln(<num>)", "Returns the natural logarithm."], ["log(<num>,<base>)", "Returns the logarithm of <num> using <base> as the base. If <base> is omitted, base 10 is used."], ["pi()", "Returns the constantpito 11 digits of precision."], ["pow(<num>,<exp>)", "Returns <num> to the power of <exp>,<num><exp>."], ["round(<num>,<precision>)", "Returns <num> rounded to the amount of decimal places specified by <precision>. The default is to round to an integer."], ["sigfig(<num>)", "Rounds <num> to the appropriate number of significant figures."], ["sqrt(<num>)", "Returns the square root of the value."], ["sum(<num>,...)", "Returns the sum of numerical values as an integer."], ["Multivalue eval functions", "commands(<value>)", "Returns a multivalued field that contains a list of the commands used in <value>."], ["mvappend(<values>)", "Returns a multivalue result based on all of values specified."], ["mvcount(<mv>)", "Returns the count of the number of values in the specified field."], ["mvdedup(<mv>)", "Removes all of the duplicate values from a multivalue field."], ["mvfilter(<predicate>)", "Filters a multivalue field based on an arbitrary Boolean expression."], ["mvfind(<mv>,<regex>)", "Finds the index of a value in a multivalue field that matches the regular expression."], ["mvindex(<mv>,<start>,<end>)", "Returns a subset of the multivalue field using the start and end index values."], ["mvjoin(<mv>,<delim>)", "Takes all of the values in a multivalue field and appends the values together using a delimiter."], ["mvmap(<mv>,<expression>)", "This function iterates over the values of a multivalue field, performs an operation using the <expression> on each value, and returns a multivalue field with the list of results."], ["mvrange(<start>,<end>,<step>)", "Creates a multivalue field based on a range of specified numbers."], ["mvsort(<mv>)", "Returns the values of a multivalue field sorted lexicographically."], ["mvzip(<mv_left>,<mv_right>,<delim>)", "Combines the values in two multivalue fields. The delimiter is used to specify a delimiting character to join the two values."], ["mv_to_json_array(<field>, <inver_types>)", "Maps the elements of a multivalue field to a JSON array."], ["split(<str>,<delim>)", "Splits the string values on the delimiter and returns the string values as a multivalue field."], ["Statistical eval functions", "avg(<values>)", "Returns the average of numerical values as an integer."], ["max(<values>)", "Returns the maximum of a set of string or numeric values."], ["min(<values>)", "Returns the minimum of a set of string or numeric values."], ["random()", "Returns a pseudo-random integer ranging from zero to 231-1."], ["Text functions", "len(<str>)", "Returns the count of the number of characters, not bytes, in the string."], ["lower(<str>)", "Converts the string to lowercase."], ["ltrim(<str>,<trim_chars>)", "Removes characters from the left side of a string."], ["replace(<str>,<regex>,<replacement>)", "Substitutes the replacement string for every occurrence of the regular expression in the string."], ["rtrim(<str>,<trim_chars>)", "Removes the trim characters from the right side of the string."], ["spath(<value>,<path>)", "Extracts information from the structured data formats XML and JSON."], ["substr(<str>,<start>,<length>)", "Returns a substring of a string, beginning at the start index. The length of the substring specifies the number of character to return."], ["trim(<str>,<trim_chars>)", "Trim characters from both sides of a string."], ["upper(<str>)", "Returns the string in uppercase."], ["urldecode(<url>)", "Replaces URL escaped characters with the original characters."], ["Trigonometry and Hyperbolic functions", "acos(X)", "Computes the arc cosine of X."], ["acosh(X)", "Computes the arc hyperbolic cosine of X."], ["asin(X)", "Computes the arc sine of X."], ["asinh(X)", "Computes the arc hyperbolic sine of X."], ["atan(X)", "Computes the arc tangent of X."], ["atan2(X,Y)", "Computes the arc tangent of X,Y."], ["atanh(X)", "Computes the arc hyperbolic tangent of X."], ["cos(X)", "Computes the cosine of an angle of X radians."], ["cosh(X)", "Computes the hyperbolic cosine of X radians."], ["hypot(X,Y)", "Computes the hypotenuse of a triangle."], ["sin(X)", "Computes the sine of X."], ["sinh(X)", "Computes the hyperbolic sine of X."], ["tan(X)", "Computes the tangent of X."], ["tanh(X)", "Computes the hyperbolic tangent of X."]]}, {"headers": ["Supported functions and syntax", "Description", "Type of function"], "rows": [["abs(<num>)", "Returns the absolute value.", "Mathematical functions"], ["acos(X)", "Computes the arc cosine of X.", "Trigonometry and Hyperbolic functions"], ["acosh(X)", "Computes the arc hyperbolic cosine of X.", "Trigonometry and Hyperbolic functions"], ["asin(X)", "Computes the arc sine of X.", "Trigonometry and Hyperbolic functions"], ["asinh(X)", "Computes the arc hyperbolic sine of X.", "Trigonometry and Hyperbolic functions"], ["atan(X)", "Computes the arc tangent of X.", "Trigonometry and Hyperbolic functions"], ["atan2(X,Y)", "Computes the arc tangent of X,Y.", "Trigonometry and Hyperbolic functions"], ["atanh(X)", "Computes the arc hyperbolic tangent of X.", "Trigonometry and Hyperbolic functions"], ["avg(<values>)", "Returns the average of numerical values as an integer.", "Statistical eval functions"], ["bit_and(<values>)", "Bitwise AND function that takes two or more non-negative integers as arguments and sequentially performs logical bitwise AND on them.", "Bitwise functions"], ["bit_or(<values>)", "Bitwise OR function that takes two or more non-negative integers as arguments and sequentially performs bitwise OR on them.", "Bitwise functions"], ["bit_not(<value>, <bitmask>)", "Bitwise NOT function that takes a non-negative as an argument and inverts every bit in the binary representation of that number. It also takes an optional second argument that acts as a bitmask.", "Bitwise functions"], ["bit_xor(<values>)", "Bitwise XOR function that takes two or more non-negative integers as arguments and sequentially performs bitwise XOR of each of the given arguments.", "Bitwise functions"], ["bit_shift_left(<value>, <shift_offset>)", "Logical left shift function that takes two non-negative integers as arguments and shifts the binary representation of the first integer over to the left by the specified shift amount.", "Bitwise functions"], ["bit_shift_right(<value>, <shift_offset>)", "Logical right shift function that takes two non-negative integers as arguments and shifts the binary representation of the first integer over to the right by the specified shift amount.", "Bitwise functions"], ["case(<condition>,<value,...)", "Accepts alternating conditions and values. Returns the first value for which the condition evaluates to TRUE.", "Comparison and Conditional functions"], ["cidrmatch(<cidr>,<ip>)", "Returns TRUE when an IP address,<ip>, belongs to a particular CIDR subnet,<cidr>.", "Comparison and Conditional functions"], ["ceiling(<num>)", "Rounds the value up to the next highest integer.", "Mathematical functions"], ["coalesce(<values>)", "Takes one or more values and returns the first value that is not NULL.", "Comparison and Conditional functions"], ["commands(<value>)", "Returns a multivalued field that contains a list of the commands used in <value>.", "Multivalue eval functions"], ["cos(X)", "Computes the cosine of an angle of X radians.", "Trigonometry and Hyperbolic functions"], ["cosh(X)", "Computes the hyperbolic cosine of X radians.", "Trigonometry and Hyperbolic functions"], ["exact(<expression>)", "Returns the result of a numeric eval calculation with a larger amount of precision in the formatted output.", "Mathematical functions"], ["exp(<num>)", "Returns the exponential functioneN.", "Mathematical functions"], ["false()", "Returns FALSE.", "Comparison and Conditional functions"], ["floor(<num>)", "Rounds the value down to the next lowest integer.", "Mathematical functions"], ["hypot(X,Y)", "Computes the hypotenuse of a triangle.", "Trigonometry and Hyperbolic functions"], ["if(<predicate>,<true_value>,<false_value>)", "If the<predicate>expression evaluates to TRUE, returns the<true_value>, otherwise the function returns the<false_value>.", "Comparison and Conditional functions"], ["in(<field>,<list>)", "Returns TRUE if one of the values in the list matches a value that you specify.", "Comparison and Conditional functions"], ["ipmask(<mask>,<ip>)", "The function generates a new masked IP address by applying a mask to an IP address using a bitwiseANDoperation.", "Conversion functions"], ["isarray(<value>)", "Returns TRUE if the field value is an array.", "Informational functions"], ["isbool(<value>)", "Returns TRUE if the field value is Boolean.", "Informational functions"], ["isdouble(<value>)", "Returns TRUE if the field value is a double value.", "Informational functions"], ["isint(<value>)", "Returns TRUE if the field value is an integer.", "Informational functions"], ["ismv(<value>)", "Returns TRUE if the field value is a multivalue.", "Informational functions"], ["isnotnull(<value>)", "Returns TRUE if the field value is not NULL.", "Informational functions"], ["isnull(<value>)", "Returns TRUE if the field value is NULL.", "Informational functions"], ["isnum(<value>)", "Returns TRUE if the field value is a number.", "Informational functions"], ["isobject(<value>)", "Returns TRUE if the field value is an object.", "Informational functions"], ["isstr(<value>)", "Returns TRUE if the field value is a string.", "Informational functions"], ["json(<value>)", "Evaluates whether a value can be parsed as JSON. If the value is JSON, the function returns the value. Otherwise, the function returns null.", "JSON functions"], ["json_append(<json>, <path_value_pairs>)", "Appends values to the ends of indicated arrays within a JSON document.", "JSON functions"], ["json_array(<values>)", "Creates a JSON array using a list of values.", "JSON functions"], ["json_array_to_mv(<json_array>, <boolean>)", "Maps the elements of a proper JSON array into a multivalue field.", "JSON functions"], ["json_delete(<object>,<keys>)", "Removes one or more keys and their corresponding values from the specified JSON object.", "JSON functions"], ["json_entries(<value>)", "Returns the key-value entries from the top-level key-value pairs in a JSON object. The entries are returned as a JSON array of JSON objects with fieldskeyandvalue.", "JSON functions"], ["json_extend(<json>, <path_value_pairs>)", "Flattens arrays into their component values and appends those values to the ends of indicated arrays within a valid JSON document.", "JSON functions"], ["json_extract(<json>, <paths>)", "Returns a value from a piece JSON and zero or more paths. The value is returned in either a JSON array, or a Splunk software native type value.", "JSON functions"], ["json_extract_exact(<json>,<keys>)", "Returns Splunk software native type values from a piece of JSON by matching literal strings in the event and extracting them as keys.", "JSON functions"], ["json_has_key_exact(<object>, <key>)", "Returns TRUE if the field value is a JSON key in the provided JSON object.", "JSON functions"], ["json_keys(<json>)", "Returns the keys from the key-value pairs in a JSON object. The keys are returned as a JSON array.", "JSON functions"], ["json_object(<members>)", "Creates a new JSON object from members of key-value pairs.", "JSON functions"], ["json_set(<json>, <path_value_pairs>)", "Inserts or overwrites values for a JSON node with the values provided and returns an updated JSON object.", "JSON functions"], ["json_set_exact(<json>,<key_value_pairs>)", "Uses provided key-value pairs to generate or overwrite a JSON object.", "JSON functions"], ["json_valid(<json>)", "Evaluates whether piece of JSON uses valid JSON syntax and returns either TRUE or FALSE.", "JSON functions"], ["len(X)", "Returns the count of the number of characters (not bytes) in the string.", "Text functions"], ["like(<str>,<pattern>))", "Returns TRUE only if<str>matches<pattern>.", "Comparison and Conditional functions"], ["ln(<num>)", "Returns the natural logarithm.", "Mathematical functions"], ["log(<num>,<base>)", "Returns the logarithm of <num> using <base> as the base. If <base> is omitted, base 10 is used.", "Mathematical functions"], ["lookup(<lookup_table>, <json_object>, <json_array>)", "Performs a CSV lookup. Returns the output field or fields in the form of a JSON object.Note:Thelookup()function is available only to Splunk Enterprise users.", "Comparison and Conditional functions"], ["len(<str>)", "Returns the count of the number of characters, not bytes, in the string.", "Text functions"], ["lower(<str>)", "Converts the string to lowercase.", "Text functions"], ["ltrim(<str>,<trim_chars>)", "Removes characters from the left side of a string.", "Text functions"], ["match(<str>, <regex>)", "Returns TRUE if the regular expression<regex>finds a match against any substring of the string value<str>. Otherwise returns FALSE.", "Comparison and Conditional functions"], ["max(<values>", "Returns the maximum of a set of string or numeric values.", "Statistical eval functions"], ["md5(<str>)", "Computes the md5 hash for the string value.", "Cryptographic functions"], ["min(<values>)", "Returns the minimum of a set of string or numeric values.", "Statistical eval functions"], ["mvappend(<values)", "Returns a multivalue result based on all of values specified.", "Multivalue eval functions"], ["mvcount(<mv>)", "Returns the count of the number of values in the specified field.", "Multivalue eval functions"], ["mvdedup(<mv>)", "Removes all of the duplicate values from a multivalue field.", "Multivalue eval functions"], ["mvfilter(<predicate>)", "Filters a multivalue field based on an arbitrary Boolean expression.", "Multivalue eval functions"], ["mvfind(<mv>,<regex>)", "Finds the index of a value in a multivalue field that matches the regular expression.", "Multivalue eval functions"], ["mvindex(<mv>,<start>,<end>)", "Returns a subset of the multivalue field using the start and end index values.", "Multivalue eval functions"], ["mvjoin(<mv>,<delim>)", "Takes all of the values in a multivalue field and appends the values together using a delimiter.", "Multivalue eval functions"], ["mvmap(<mv>,<expression>)", "This function iterates over the values of a multivalue field, performs an operation using the <expression> on each value, and returns a multivalue field with the list of results.", "Multivalue eval functions"], ["mvrange(<start>,<end>,<step>)", "Creates a multivalue field based on a range of specified numbers.", "Multivalue eval functions"], ["mvsort(<mv>)", "Returns the values of a multivalue field sorted lexicographically.", "Multivalue eval functions"], ["mvzip(<mv_left>,<mv_right>,<delim>)", "Combines the values in two multivalue fields. The delimiter is used to specify a delimiting character to join the two values.", "Multivalue eval functions"], ["mv_to_json_array(<field>, <infer_types>)", "Maps the elements of a multivalue field to a JSON array.", "JSON functions"], ["now()", "Returns the time that the search was started when run as an ad-hoc search. If used with a scheduled search, returns the time that the search was scheduled to run, which might not be the time that the scheduled search actual runs.", "Date and Time functions"], ["null()", "This function takes no arguments and returns NULL.", "Comparison and Conditional functions"], ["nullif(<field1>,<field2>)", "Compares the values in two fields and returns NULL if the value in<field1>is equal to the value in<field2>. Otherwise returns the value in<field1>.", "Comparison and Conditional functions"], ["pi()", "Returns the constantpito 11 digits of precision.", "Mathematical functions"], ["pow(<num>,<exp>)", "Returns <num> to the power of <exp>,<num><exp>.", "Mathematical functions"], ["printf(<format>,<arguments>)", "Creates a formatted string based on a format description that you provide.", "Conversion functions"], ["random()", "Returns a pseudo-random integer ranging from zero to 231-1.", "Statistical eval functions"], ["relative_time(<time>,<specifier>)", "Adjusts the time by a relative time specifier.", "Date and Time functions"], ["replace(<str>,<regex>,<replacement>)", "Substitutes the replacement string for every occurrence of the regular expression in the string.", "Text functions"], ["round(<num>,<precision>)", "Returns <num> rounded to the amount of decimal places specified by <precision>. The default is to round to an integer.", "Mathematical functions"], ["rtrim(<str>,<trim_chars>)", "Removes the trim characters from the right side of the string.", "Text functions"], ["searchmatch(<search_str>)", "Returns TRUE if the event matches the search string.", "Comparison and Conditional functions"], ["sha1(<str>)", "Computes the sha1 hash for the string value.", "Cryptographic functions"], ["sha256(<str>)", "Computes the sha256 hash for the string value.", "Cryptographic functions"], ["sha512(<stri>)", "Computes the sha512 hash for the string value.", "Cryptographic functions"], ["sigfig(<num>)", "Rounds <num> to the appropriate number of significant figures.", "Mathematical functions"], ["sin(X)", "Computes the sine of X.", "Trigonometry and Hyperbolic functions"], ["sinh(X)", "Computes the hyperbolic sine of X.", "Trigonometry and Hyperbolic functions"], ["spath(<value>,<path>)", "Extracts information from the structured data formats XML and JSON.", "Text functions"], ["split(<str>,<delim>)", "Splits the string values on the delimiter and returns the string values as a multivalue field.", "Multivalue eval functions"], ["sqrt(<num>)", "Returns the square root of the value.", "Mathematical functions"], ["strftime(<time>,<format>)", "Takes a UNIX time and renders it into a human readable format.", "Date and Time functions"], ["strptime(<str>,<format>)", "Takes a human readable time and renders it into UNIX time.", "Date and Time functions"], ["substr(<str>,<start>,<length>)", "Returns a substring of a string, beginning at the start index. The length of the substring specifies the number of character to return.", "Text functions"], ["sum(<num>,...)", "Returns the sum of numerical values as an integer.", "Mathematical functions"], ["tan(X)", "Computes the tangent of X.", "Trigonometry and Hyperbolic functions"], ["tanh(X)", "Computes the hyperbolic tangent of X.", "Trigonometry and Hyperbolic functions"], ["time()", "The time that eval function was computed. The time will be different for each event, based on when the event was processed.", "Date and Time functions"], ["toarray(<value>)", "Converts a value to an array value.", "Conversion functions"], ["tobool(<value>)", "Converts a value to a Boolean value.", "Conversion functions"], ["todouble(<field>, <base>)", "Converts a value to the equivalent double value of the field, if any. The second argument specifies the numeric base used to convert strings.", "Conversion functions"], ["toint(<value>, <base>)", "Converts a value to the equivalent integer value of the field, if any. The second argument specifies the numeric base used to convert strings.", "Conversion functions"], ["tomv(<value>)", "Converts a value to a multivalue.", "Conversion functions"], ["tonumber(<str>,<base>)", "Converts a string to a number.", "Conversion functions"], ["toobject(<value>)", "Converts a value to the equivalent object value of the field, if any.", "Conversion functions"], ["tostring(<value>,<format>)", "Converts the input, such as a number or a Boolean value, to a string.", "Conversion functions"], ["trim(<str>,<trim_chars>)", "Trim characters from both sides of a string.", "Text functions"], ["true()", "Returns TRUE.", "Comparison and Conditional functions"], ["typeof(<value>)", "Returns a string that indicates the field type, such as Number, String, Boolean, and so forth.", "Informational functions"], ["upper(<str>)", "Returns the string in uppercase.", "Text functions"], ["urldecode(<url>)", "Replaces URL escaped characters with the original characters.", "Text functions"], ["validate(<condition>, <value>,...)", "Takes a list of conditions and values and returns the value that corresponds to the condition that evaluates to FALSE. This function defaults to NULL if all conditions evaluate to TRUE. This function is the opposite of thecasefunction.", "Comparison and Conditional functions"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "Evaluation functions", "section_heading": "Supported functions and syntax", "section_id": "id_8238c3d2_e1b1_4414_ae0b_d049ec2df62f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/evaluation-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:01.281013+00:00", "version": "10.2"}}
{"id": "9040ace657e16d40", "content": "Topics: Statistical and charting functions Commands: eval fieldformat where", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "Evaluation functions", "section_heading": "See also", "section_id": "id_78566696_9d13_4e90_8c7b_8c39eb74573a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/evaluation-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:01.281020+00:00", "version": "10.2"}}
{"id": "8ec45686c3b1f8c6", "content": "The functions can also be used with related statistical and charting commands. The following table lists the commands supported by the statistical and charting functions and the related command that can also use these functions. Functions that you can use to create sparkline charts are noted in the documentation for each function. Sparkline is a function that applies to only the chart and stats commands, and allows you to call other functions. For more information, see Add sparklines to search results in the Search Manual .", "code_examples": [], "tables": [{"headers": ["Command", "Supported related commands"], "rows": [["chart", "sichart"], ["stats", "eventstatsstreamstatsgeostatssistatsFor thetstatsand themstatscommands, see the documentation for each command for a list of the supported functions."], ["timechart", "sitimechart"]]}], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "Statistical and charting functions", "section_heading": "Support for related commands", "section_id": "ae8b0e75_7e79_45bd_8f0c_6b96579ad2db--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/statistical-and-charting-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:18.048998+00:00", "version": "10.2"}}
{"id": "a62dec78c1d2c73d", "content": "Most of the statistical and charting functions expect the field values to be numbers. All of the values are processed as numbers, and any non-numeric values are ignored. The following functions process the field values as literal string values, even though the values are numbers. For example, you use the distinct_count function and the field contains values such as \"1\", \"1.0\", and \"01\". Each value is considered a distinct string value. The only exceptions are the max and min functions. These functions process values as numbers if possible. For example, the values \"1\", \"1.0\", and \"01\" are processed as the same numeric value.", "code_examples": [], "tables": [{"headers": [], "rows": [["countdistinct_countearliest", "estdcestdc_errorfirst", "latestlastlist", "maxminmodevalues"]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "Statistical and charting functions", "section_heading": "How field values are processed", "section_id": "id_38b4546a_4ddc_403a_8463_2802a706015d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/statistical-and-charting-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:18.049008+00:00", "version": "10.2"}}
{"id": "4f9fa2cf5bf5f151", "content": "There are two ways that you can see information about the supported statistical and charting functions: Function list by category Alphabetical list of functions Function list by category The following table is a quick reference of the supported statistical and charting functions, organized by category. This table provides a brief description for each functions. Use the links in the table to learn more about each function and to see examples. Alphabetical list of functions The following table is a quick reference of the supported statistical and charting functions, organized alphabetically. This table provides a brief description for each function. Use the links in the table to learn more about each function and to see examples.", "code_examples": [], "tables": [{"headers": ["Type of function", "Supported functions and syntax", "Description"], "rows": [["Aggregate functions", "avg(<value>)", "Returns the average of the values in the field specified."], ["count(<value>)", "Returns the number of occurrences where the field that you specify contains any value (is not empty). You can also count the occurrences of a specific value in the field by using theevalcommand with thecountfunction. For example:count( eval(field_name=\"value\"))."], ["distinct_count(<value>)", "Returns the count of distinct values in the field specified."], ["estdc(<value>)", "Returns the estimated count of the distinct values in the field specified."], ["estdc_error(<value>)", "Returns the theoretical error of the estimated count of the distinct values in the field specified. The error represents a ratio of theabsolute_value(estimate_distinct_count - real_distinct_count)/real_distinct_count."], ["exactperc<percentile>(<value>)", "Returns a percentile value of the numeric field specified. Provides the exact value, but is very resource expensive for high cardinality fields. An alternative isperc."], ["max(<value>)", "Returns the maximum value in the field specified. If the field values are non-numeric, the maximum value is found using lexicographical ordering. This function processes field values as numbers if possible, otherwise processes field values as strings."], ["mean(<value>)", "Returns the arithmetic mean of the values in the field specified."], ["median(<value>)", "Returns the middle-most value of the values in the field specified."], ["min(<value>)", "Returns the minimum value in the field specified. If the field values are non-numeric, the minimum value is found using lexicographical ordering."], ["mode(<value>)", "Returns the most frequent value in the field specified."], ["percentile<percentile>(<value>)", "Returns the N-th percentile value of all the values in the numeric field specified.  Valid field values are integers from 1 to 99.Additional percentile functions areupperperc<percentile>(<value>)andexactperc<percentile>(<value>)."], ["range(<value>)", "If the field values are numeric, returns the difference between the maximum and minimum values in the field specified."], ["stdev(<value>)", "Returns the sample standard deviation of the values in the field specified."], ["stdevp(<value>)", "Returns the population standard deviation of the values in the field specified."], ["sum(<value>)", "Returns the sum of the values in the field specified."], ["sumsq(<value>)", "Returns the sum of the squares of the values in the field specified."], ["upperperc<percentile>(<value>)", "Returns an approximate percentile value, based on the requested percentile of the numeric field.When there are more than 1000 values, the upperperc function gives the approximate upper bound for the percentile requested. Otherwise the upperperc function returns the same percentile as thepercfunction."], ["var(<value>)", "Returns the sample variance of the values in the field specified."], ["varp(<value>)", "Returns the population variance of the values in the field specified."], ["Event order functions", "first(<value>", "Returns the first seen value in a field. In general, the first seen value of the field is the most recent instance of this field, relative to the input order of events into the stats command."], ["last(<value>)", "Returns the last seen value in a field. In general, the last seen value of the field is the oldest instance of this field relative to the input order of events into the stats command."], ["Multivalue stats and chart functions", "list(<value>)", "Returns a list of up to 100 values in a field as a multivalue entry. The order of the values reflects the order of input events."], ["values(<value>)", "Returns the list of all distinct values in a field as a multivalue entry. The order of the values is lexicographical."], ["Time functions", "earliest(<value>)", "Returns the chronologically earliest (oldest) seen occurrence of a value in a field."], ["earliest_time(<value>)", "Returns the UNIX time of the earliest (oldest) occurrence of a value of the field. Used in conjunction with theearliest,latest, andlatest_timefunctions to calculate the rate of increase for an accumulating counter."], ["latest(<value>)", "Returns the chronologically latest (most recent) seen occurrence of a value in a field."], ["latest_time(<value>)", "Returns the UNIX time of the latest (most recent) occurrence of a value of the field. Used in conjunction with theearliest,earliest_time, andlatestfunctions to calculate the rate of increase for an accumulating counter."], ["per_day(<value>)", "Returns the values in a field or eval expression for each day."], ["per_hour(<value>)", "Returns the values in a field or eval expression for each hour."], ["per_minute(<value>)", "Returns the values in a field or eval expression for each minute."], ["per_second(<value>)", "Returns the values in a field or eval expression for each second."], ["rate(<value>)", "Returns the per-second rate change of the value of the field. Represents(latest - earliest) / (latest_time - earliest_time)Requires theearliestandlatestvalues of the field to be numerical, and theearliest_timeandlatest_timevalues to be different."], ["rate_avg(<value>)", "Returns the average rates for the time series associated with a specified accumulating counter metric."], ["rate_sum(<value>)", "Returns the summed rates for the time series associated with a specified accumulating counter metric."]]}, {"headers": ["Supported functions and syntax", "Description", "Type of function"], "rows": [["avg(<value>)", "Returns the average of the values in the field specified.", "Aggregate functions"], ["count(<value>)", "Returns the number of occurrences where the field that you specify contains any value (is not empty). You can also count the occurrences of a specific value in the field by using theevalcommand with thecountfunction. For example:count(eval(field_name=\"value\")).", "Aggregate functions"], ["distinct_count(<value)", "Returns the count of distinct values in the field specified.", "Aggregate functions"], ["earliest(<value>)", "Returns the chronologically earliest (oldest) seen occurrence of a value in the field specified.", "Time functions"], ["earliest_time(<value>)", "Returns the UNIX time of the earliest (oldest) occurrence of a value in the field specified. Used in conjunction with theearliest,latest, andlatest_timefunctions to calculate the rate of increase for an accumulating counter.", "Time functions"], ["estdc(<value>)", "Returns the estimated count of the distinct values in the field specified.", "Aggregate functions"], ["estdc_error(<value>)", "Returns the theoretical error of the estimated count of the distinct values in the field specified. The error represents a ratio of theabsolute_value(estimate_distinct_count - real_distinct_count)/real_distinct_count.", "Aggregate functions"], ["exactperc<percentile>(<value>)", "Returns a percentile value for the numeric field specified. Provides the exact value, but is very resource expensive for high cardinality fields. An alternative isperc.", "Aggregate functions"], ["first(<value>)", "Returns the first seen value in a field. In general, the first seen value of the field is the most recent instance of this field, relative to the input order of events into the stats command.", "Event order functions"], ["last(<value>)", "Returns the last seen value in a field. In general, the last seen value of the field is the oldest instance of this field relative to the input order of events into the stats command.", "Event order functions"], ["latest(<value>)", "Returns the chronologically latest (most recent) seen occurrence of a value in a field.", "Time functions"], ["latest_time(<value>)", "Returns the UNIX time of the latest (most recent) occurrence of a value of the field. Used in conjunction with theearliest,earliest_time, andlatestfunctions to calculate the rate of increase for an accumulating counter.", "Time functions"], ["list(<value>)", "Returns a list of up to 100 values in a field as a multivalue entry. The order of the values reflects the order of input events.", "Multivalue stats and chart functions"], ["max(<value>)", "Returns the maximum value in the field specified. If the field values are non-numeric, the maximum value is found using lexicographical ordering. This function processes field values as numbers if possible, otherwise processes field values as strings.", "Aggregate functions"], ["mean(<value>)", "Returns the arithmetic mean of the values in the field specified.", "Aggregate functions"], ["median(<value>)", "Returns the middle-most value of the values in the field specified.", "Aggregate functions"], ["min(<value>)", "Returns the minimum value in the field specified. If the field values are non-numeric, the minimum value is found using lexicographical ordering.", "Aggregate functions"], ["mode(<value>)", "Returns the most frequent value in the field specified.", "Aggregate functions"], ["perc<percentile>(<value>)", "Returns the N-th percentile value of all the values in the numeric field specified.  Valid field values are integers from 1 to 99.Additional percentile functions areupperpercandexactperc.", "Aggregate functions"], ["per_day(<value>)", "Returns the values in a field or eval expression for each day.", "Time functions"], ["per_hour(<value>)", "Returns the values in a field or eval expression for each hour.", "Time functions"], ["per_minute(<value>)", "Returns the values in a field or eval expression for each minute.", "Time functions"], ["per_second(<value>)", "Returns the values in a field or eval expression for each second.", "Time functions"], ["range(<value>)", "If the field values are numeric, returns the difference between the maximum and minimum values in the field specified.", "Aggregate functions"], ["rate(<value>)", "Returns the per-second rate change of the value of the field. Represents(latest - earliest) / (latest_time - earliest_time)Requires theearliestandlatestvalues of the field to be numerical, and theearliest_timeandlatest_timevalues to be different.", "Time functions"], ["rate_avg(<value>)", "Returns the average rates for the time series associated with a specified accumulating counter metric.", "Time functions"], ["rate_sum(<value>)", "Returns the summed rates for the time series associated with a specified accumulating counter metric.", "Time functions"], ["stdev(<value>)", "Returns the sample standard deviation of the values in the field specified.", "Aggregate functions"], ["stdevp(<value>)", "Returns the population standard deviation of the values in the field specified.", "Aggregate functions"], ["sum(<value>)", "Returns the sum of the values in the field specified.", "Aggregate functions"], ["sumsq(<value>)", "Returns the sum of the squares of the values in the field specified.", "Aggregate functions"], ["upperperc<percentile>(<value>)", "Returns an approximate percentile value, based on the requested percentile of the numeric field.When there are more than 1000 values, the upperperc function gives the approximate upper bound for the percentile requested. Otherwise the upperperc function returns the same percentile as thepercfunction.", "Aggregate functions"], ["values(<value>)", "Returns the list of all distinct values in a field as a multivalue entry. The order of the values is lexicographical.", "Multivalue stats and chart functions"], ["var(<value>)", "Returns the sample variance of the values in the field specified.", "Aggregate functions"], ["varp(<value>)", "Returns the population variance of the values in the field specified.", "Aggregate functions"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "Statistical and charting functions", "section_heading": "Supported functions and syntax", "section_id": "id_64c88a09_b6aa_4d11_885f_33cf584a4572--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/statistical-and-charting-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:18.049051+00:00", "version": "10.2"}}
{"id": "3b646873de24c41a", "content": "Commands chart geostats eventstats stats streamstats timechart Functions Evaluation functions", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "Statistical and charting functions", "section_heading": "See also", "section_id": "fbb56429_5479_4097_9b36_d19c25b0f6e0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/statistical-and-charting-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:18.049057+00:00", "version": "10.2"}}
{"id": "2f2c75a5da33ad9d", "content": "Have questions? Visit Splunk Answers and search for a specific function or command.", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "Statistical and charting functions", "section_heading": "Answers", "section_id": "b0cfca04_8421_4c7a_aa82_7018922ffa28--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/statistical-and-charting-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:18.049062+00:00", "version": "10.2"}}
{"id": "735a83a8af985e90", "content": "Produces an abstract, a summary or brief representation, of the text of the search results. The original text is replaced by the summary. The abstract is produced by a scoring mechanism. Events that are larger than the selected maxlines , those with more textual terms and more terms on adjacent lines, are preferred over events with fewer terms. If a line has a search term, its neighboring lines also partially match, and might be returned to provide context. When there are gaps between the selected lines, lines are prefixed with an ellipsis (...). If the text of an event has fewer lines or an equal number of lines as maxlines , no change occurs.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "abstract", "section_heading": "Description", "section_id": "id_1659584e_5002_4ea8_a4b1_fb1c290bd254--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/abstract", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:33.358976+00:00", "version": "10.2"}}
{"id": "84eb5c3e3245c067", "content": "The required syntax is in bold. abstract [maxterms=<int>] [maxlines=<int>] Optional arguments maxterms Syntax: maxterms=<int> Description: The maximum number of terms to match. Accepted values are 1 to 1000. Default: 1000 maxlines Syntax: maxlines=<int> Description: The maximum number of lines to match. Accepted values are 1 to 500. Default: 10", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "abstract", "section_heading": "Syntax", "section_id": "b88b33d4_bfb9_431a_a49e_cef7adebb735--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/abstract", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:33.358984+00:00", "version": "10.2"}}
{"id": "c46a57d90e54c013", "content": "Specify the number of lines to return Show a summary of up to 5 lines for each search result. Specify the number of terms to return Consider the following events: If you specify maxterms=20 the results look like this: The \"terms\" are identified as shown in the following table:", "code_examples": [{"language": "spl", "code": "... | abstract maxlines=5"}], "tables": [{"headers": ["Time", "Event"], "rows": [["1/4/236:22:16.000 PM", "91.205.189.15 - - [04/Jan/2023:18:22:16] \"GET /oldlink?itemId=EST-14&JSESSIONID=SD6SL7FF7ADFF53113 HTTP 1.1\" 200 1665 \"http://www.buttercupgames.com/oldlink?itemId=EST-14\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.46 Safari/536.5\" 159"], ["1/3/2311:08:57.000 PM", "194.146.236.22 - - [03/Jan/2023:23:08:57] \"POST /cart.do?action=addtocart&itemId=EST-15&productId=WC-SH-T02&JSESSIONID=SD4SL1FF2ADFF47548 HTTP 1.1\" 200 1493 \"http://www.buttercupgames.com/product.screen?productId=WC-SH-T02\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.52 Safari/536.5\" 848"]]}, {"headers": ["Time", "Event"], "rows": [["1/4/236:22:16.000 PM", "91.205.189.15 - - [04/Jan/2023:18"], ["1/3/2311:08:57.000 PM", "194.146.236.22  - - [03/Jan/2023:23"]]}, {"headers": ["Number", "Event 1 term", "Event 2 term"], "rows": [["1", "91", "194"], ["2", ".", "."], ["3", "205", "146"], ["4", ".", "."], ["5", "189", "236"], ["6", ".", "."], ["7", "15", "22"], ["8", "", ""], ["9", "-", "-"], ["10", "", ""], ["11", "-", "-"], ["12", "", ""], ["13", "[", "["], ["14", "04", "03"], ["15", "/", "/"], ["16", "Jan", "Jan"], ["17", "/", "/"], ["18", "2023", "2023"], ["19", ":", ":"], ["20", "18", "23"]]}], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "abstract", "section_heading": "Examples", "section_id": "id_5e3369c4_5d30_4274_9b9f_e61e880d3ec6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/abstract", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:33.358999+00:00", "version": "10.2"}}
{"id": "1eb37eb19ef94f5b", "content": "highlight", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "abstract", "section_heading": "See also", "section_id": "id_4919b336_3470_460e_a88c_33df897adcb1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/abstract", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:33.359003+00:00", "version": "10.2"}}
{"id": "d32807fd1e57eb1f", "content": "If you are new to Splunk software and searching, start with the Search Tutorial. This tutorial introduces you to the Search & Reporting application. The tutorial guides you through uploading data to your Splunk deployment, searching your data, and building simple charts, reports, and dashboards. After you complete the Search Tutorial, and before you start using Splunk software on your own data you should: Add data to your Splunk instance. See Getting Data In. Understand how indexing works and how data is processed. See Managing Indexers and Clusters of Indexers. Learn about fields and knowledge objects, such as hosts, source types, and event types. See the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Welcome to the Search Reference", "section_heading": "Getting Started", "section_id": "id_7da7bc74_1487_4fd8_8f95_6c3b1e07e276--en", "url": "https://help.splunk.com/en/?resourceId=Splunk_SearchReference_WhatsInThisManual", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:50.363569+00:00", "version": "10.2"}}
{"id": "0eae4c7057a8d86e", "content": "The Search Manual is a companion manual to the Search Reference. The Search Manual contains detailed information about creating and optimizing searches. Types of searches Retrieving events Specifying time ranges Optimizing searches Using subsearches Creating statistical tables and charts Grouping and correlating events Predicting future events Managing jobs", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Welcome to the Search Reference", "section_heading": "Search Manual", "section_id": "id_00976a3e_4a3c_4399_a730_b7ade737215a--en", "url": "https://help.splunk.com/en/?resourceId=Splunk_SearchReference_WhatsInThisManual", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:50.363576+00:00", "version": "10.2"}}
{"id": "6d93c321ccfbf457", "content": "The Quick Reference Guide contains: Explanations about Splunk features Common search commands Tips on optimizing searches Functions for the eval and stats commands Search examples Regular expressions Formats for converting strings into timestamps SPL commands The Search Processing Language (SPL) includes a wide range of commands. There are two quick reference guides for the commands: The Command quick reference topic contains an alphabetical list of each command, along with a brief description of what the command does and a link to the specific documentation for the command. The Commands by category topic organizes the commands by the type of action that the command performs. This topic contains a brief description of what the command does and a link to the specific documentation for the command. SQL users If you're familiar with SQL, see Splunk SPL for SQL users to see how to use your SQL knowledge to learn SPL.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Welcome to the Search Reference", "section_heading": "Quick Reference Information", "section_id": "id_702885b1_e845_4226_ac94_706e20cff892--en", "url": "https://help.splunk.com/en/?resourceId=Splunk_SearchReference_WhatsInThisManual", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:50.363580+00:00", "version": "10.2"}}
{"id": "d47642ee8573e265", "content": "Before you continue, see Understanding SPL syntax for the conventions and rules used in this manual.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Welcome to the Search Reference", "section_heading": "Command syntax", "section_id": "a9e5989a_18f6_4ce2_ace0_54abc612eac4--en", "url": "https://help.splunk.com/en/?resourceId=Splunk_SearchReference_WhatsInThisManual", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:16:50.363583+00:00", "version": "10.2"}}
{"id": "a1947fbd4483cb5b", "content": "The primary transforming commands are: chart : Creates charts that can display any series of data that you want to plot. You can decide what field is tracked on the x-axis of the chart. timechart : Creates \"trends over time\" reports, which means that _time is always the x-axis. top : Generates charts that display the most common values of a field. rare : Creates charts that display the least common values of a field. stats : Generates a report that display summary statistics. See Transforming commands in the Search Reference to learn more. Note: As you will see in the following examples, you always place your transforming commands after your search commands, linking them with a pipe operator ( | ). The chart , timechart , and stats commands are all designed to work with statistical functions. The list of available statistical functions includes: count, distinct count mean, median, mode min, max, range, percentiles standard deviation, variance sum first occurrence, last occurrence For more information about statistical functions, see Statistical and charting functions in the Search Reference. Some statistical functions only work with the timechart command. Note: All searches with transforming commands generate specific data structures. The different chart types require these data structures to be set up in particular ways. For example, not all searches that enable you to generate bar, column, line, and area charts can be used to generate pie charts. See Data structure requirements for visualizations in the Dashboard and Visualizations manual to learn more.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "About transforming commands and searches", "section_heading": "Transforming commands", "section_id": "eaa79f01_74f9_41d9_a7bd_bef5be3512d4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/about-transforming-commands-and-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:06.737452+00:00", "version": "10.2"}}
{"id": "71ca6409fd15b4cf", "content": "The following examples use transforming commands to create tables, charts, and reports: Create time based charts Create charts that are not (necessarily) time-based Create reports that display summary statistics Build a chart of multiple data series", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "About transforming commands and searches", "section_heading": "Table, chart, and report examples", "section_id": "id_9b0163d0_d830_49ee_b439_acc3a3180e94--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/about-transforming-commands-and-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:06.737460+00:00", "version": "10.2"}}
{"id": "022ef2f045a35ea6", "content": "You can use a real-time search to calculate metrics in real time on large incoming data flows without the use of summary indexing. However, because you are reporting on a live and continuous stream of data, the timeline will update as the events stream in and you can only view the table or chart in preview mode. Also, some search commands will be more applicable (for example, streamstats and rtorder) for use in real-time. See About real-time searches and reports .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "About transforming commands and searches", "section_heading": "Real-time reporting", "section_id": "id_2db12d75_d2b5_4890_a062_3d43b45e106a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/about-transforming-commands-and-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:06.737466+00:00", "version": "10.2"}}
{"id": "3b68819435c08d43", "content": "Types of commands Types of searches", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "About transforming commands and searches", "section_heading": "See also", "section_id": "id_53c11a79_87c8_496f_86be_649c527cf541--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/about-transforming-commands-and-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:06.737470+00:00", "version": "10.2"}}
{"id": "b14295f63d0a7dfc", "content": "The Splunk Home page is your interactive portal to the data and apps in your Splunk deployment. The first time you log into your Splunk deployment, you land on the Splunk Home page. All of the apps that you have access to appear on this page. You can personalize the home page with in-product bookmarks for quick access to guides, manuals, apps, knowledge objects, and so on. Administrators can: Share bookmarks with all users in one operation. Control the domains in which bookmarks can be created. Users can: Access your search history for various apps in a single view, without having to navigate to each app to see the history associated with that app. Filter the Knowledge Object list by App and Owner for quicker access to those objects. There are minor differences between the Splunk Home page for Splunk Enterprise and Splunk Cloud Platform. The following image shows the Home page for a user with administrator access on Splunk Enterprise: Note: Your Splunk account might be configured to start in another view instead of Splunk Home, such as Search or Pivot in the Search & Reporting app. Apps panel The Apps panel lists the apps which are installed on your Splunk instance and that you have permission to use. Select an app from the list to open it. By default the Search & Reporting app, which is often referred to as the Search app, is pinned to the top of the list. For apps that you use frequently, you can pin the apps to move them to the top of the list. Center panel and quick link tabs The center panel contains a set of quick link tabs. The first tab is Bookmarks , where you can set your own bookmarks and see the bookmarks shared with you. The other tabs provide quick access to other information. Quick link tabs Splunk Home has a set of tabs that you can use to gain access to information. The following table describes these tabs:", "code_examples": [], "tables": [{"headers": ["Tab", "Description"], "rows": [["Bookmarks", "Use this tab to access your own bookmarks, bookmarks your organization has defined, and Splunk recommended bookmarks."], ["Dashboard", "Use this tab to access your favorite dashboard faster. You can add a dashboard created either using Simple XML or created through Dashboard Studio as your home dashboard."], ["Search history", "This tab shows the list of searches that you've run recently. You can set how far back to keep the search history. The maximum is the last 90 days."], ["Recently viewed", "This tab shows the list of knowledge objects that you accessed in the last 30 days, including alerts, dashboards, datasets, and reports."], ["Created by you", "This tab shows all of the knowledge objects that you have created, organized by knowledge object type."], ["Shared with you", "This tab shows all the knowledge objects that you have access to, organized by knowledge object type."]]}], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Navigating Splunk Web", "section_heading": "About Splunk Home", "section_id": "id_5adbd8a0_2cd2_47cd_949e_9a448a0b210c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/navigating-splunk-web", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:20.975899+00:00", "version": "10.2"}}
{"id": "4840aa1b4bcb8527", "content": "Use the Splunk bar to navigate Splunk Web. You can use the Splunk bar to switch between apps, add data, manage settings and edit your Splunk configuration, view system-level messages, monitor the activity of your search jobs and alerts, and get help using Splunk software. The Splunk bar in another view, such as the Search view in the Search & Reporting app, also includes the Apps menu next to the Splunk logo. Use the Apps menu to quickly switch between the Splunk applications that you have installed on your Splunk instance. The following image shows the Splunk bar in the Search app in Splunk Enterprise. The Splunk bar in Splunk Cloud Platform has the same elements and menus. Messages menu All system-level error messages are listed on the Messages menu. When you have a new message to review, a numerical notification appears next to the Messages menu. The notification indicates the number of messages that you have. Settings menu The Settings menu lists the configuration pages for knowledge objects, distributed environment settings, system and licensing, data, and authentication settings. If you do not see some of these options, you do not have the permissions to view or edit those options. The following image shows the Settings menu in Splunk Cloud Platform: The Settings menu in Splunk Enterprise contains the same options, however there are several additional large icons to the left of the menu to access the Explore Data and Monitoring Console pages. Activity menu The Activity menu provides shortcuts to the Jobs and Triggered alerts views. Click Jobs to open the search jobs manager window, where you can view and manage currently running searches. Click Triggered Alerts to view scheduled alerts that are triggered. Find search box To search for objects within your Splunk deployment, use the Find search box. The Find search box performs matches that are not case sensitive on the ID, labels, and descriptions in built-in and saved objects. For example, if you type error , it returns the knowledge objects that contain that term. The saved objects, where they exist, are organized into the following categories: Reports , Dashboards , Datasets , and Data models. You can also run a search for the word error in your event and metric data. From the list displayed in the Find search box, select Open error in search to run a search in the Search & Reporting app using the word you specified in the Find search box. User menu Use the user menu to edit your account settings, change preferences, or to log out of the Splunk instance. If you installed Splunk, for example to step through the Search Tutorial , the user menu displays \"Administrator\" because that is the default user name for a new installation. If you are not a Splunk administrator, the name on the user menu is your Splunk user name. Preferences Through the Preferences menu, you can: Set a time zone that is different than the default system time zone. Set a default application other than Splunk Home. Restart background search jobs when the Splunk software is restarted. Change the background to a dark theme. The default is a light theme. Help and Support For assistance, such as to access the Splunk Support portal and Splunk Answers, to file a bug, or to get help on a specific page, use the menu the corresponds to your Splunk instance: Splunk Cloud Platform Use the Support and Services menu on the Splunk bar. Splunk Enterprise Use the Help menu on the Splunk bar. Return to Splunk Home Select the Splunk logo on the Splunk bar to return to Splunk Home from any other view in Splunk Web.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Navigating Splunk Web", "section_heading": "About the Splunk bar", "section_id": "id_4cf96ffc_eff5_4425_8115_5679aa1cbc4d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/navigating-splunk-web", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:20.975907+00:00", "version": "10.2"}}
{"id": "7162ab616ee0727d", "content": "About the Search app", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Navigating Splunk Web", "section_heading": "See also", "section_id": "ad4c86f4_c11b_48f7_8703_61fd05aac92b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/navigating-splunk-web", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:20.975911+00:00", "version": "10.2"}}
{"id": "00413af6fbb57e77", "content": "This section discusses the search commands that enable you to evaluate new fields, manipulate existing fields, enrich events by adding new fields, and parse fields with multiple values. At the core of evaluating new fields is the eval command and its functions. Unlike the stats command, which enables you to calculate statistics based on fields in your events, the eval command enables you to create new fields using existing fields and an arbitrary expression. The eval command has many functions. See Use the eval command and functions. You can easily enrich your data with more information at search time. See Use lookup to add fields from external lookup tables. You can use the Splunk SPL (search processing language) to extract fields in different ways using a variety of search commands. Your events might contain fields with more than one value. There are search commands and functions that work with multivalue fields. See Manipulate and evaluate fields with multiple values .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "dfd272592171d4456892df6ffc505d315--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/about-evaluating-and-manipulating-fields", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:37.778416+00:00", "version": "10.2"}}
{"id": "f03adfaca96e9aab", "content": "There are several ways that you can look at information about your jobs. You can inspect a job or you can manage a job. Search Job Inspector Use the Search Job Inspector to view information about the current job, such as job execution costs and search job properties. See View search job properties. Job Details dashboard The Job Details dashboard provides a clear and concise overview of a search job process. You can access the Job Details dashboard through the Search Job Inspector. See View search job properties. Job Management page Use the Job Management page to view information about recent jobs. If you have the Admin role or a role with an equivalent set of capabilities, you can manage the search jobs run by other users. See Manage search jobs .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "About jobs and job management", "section_heading": "Inspecting jobs and managing jobs", "section_id": "id_3c470f1a_1e07_4dcd_aa9f_bcbeed2a88d3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/about-jobs-and-job-management", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:54.547319+00:00", "version": "10.2"}}
{"id": "8b690310d8579078", "content": "After you run a search or open a report in Splunk Web, you can access and manage information about the search job without leaving the Search page. While the search is running, paused, or finalized, click Job and choose from the available options. Edit the job settings. Select this to open the Job Settings dialog, where you can change the job read permissions, extend the job lifetime, and get a URL for the job. You can use the URL to share the job with others, or to create a bookmark to the job from your web browser. Send the job to the background. Select this if the search job is slow to complete and you want to work on other Splunk activities, including running a new search job. The job continues to run in the background. Inspect the job. Opens a separate window and displays information and metrics for the search job using the Search Job Inspector. Delete the job. Use this to delete a job that is currently running, is paused, or which has finalized. After you delete the job, you can still save the search as a report.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "About jobs and job management", "section_heading": "Job menu", "section_id": "id_564abf15_281f_4e11_92ae_6345bd5b43e6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/about-jobs-and-job-management", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:54.547327+00:00", "version": "10.2"}}
{"id": "7973e8a88c7a6ec4", "content": "You can open the Job Settings dialog when a search job is running, paused, or finalized. Just click Job and select Edit Job Settings. Sharing jobs There are several ways to share a job with other Splunk users. You can change the job permissions or send a link to the job. This can be handy if you want another user to see the results returned by the job. See Sharing and exporting jobs. Job lifetimes When you run a new search, a job is retained in the system for a period of time, called the job lifetime. The default lifetime is 10 minutes. The lifetime starts from the moment the job is run. See Extending job lifetimes in this manual.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "About jobs and job management", "section_heading": "Edit search job settings", "section_id": "id_4ba08955_b669_45ab_ad9e_fef54f10ea4a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/about-jobs-and-job-management", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:54.547331+00:00", "version": "10.2"}}
{"id": "0c2eb23a84225417", "content": "Sometimes a search job runs for a long time. You might want to edit the search to change the search criteria, or you might want to pause the search or run the search in the background. Autopause long-running jobs To handle inadvertently long-running search jobs, you can autopause a job. This feature is enabled by default only for summary dashboard clicks, to deal with the situation where a user mistakenly initiates \"all time\" searches. When autopause is enabled for a particular search view, the search view includes an autopause countdown field during the search. If the search time limit has been reached, an information window will appear to inform the user that the search has been paused. It offers the user the option of resuming or finalizing the search. By default, the limit before autopause is 30 seconds. Managing jobs when a computer goes into sleep mode When a search is run in Splunk Web from a computer that is not a Splunk server and the computer changes to sleep or hibernate mode, the underlying search process is stopped. The Splunk software interprets the change to sleep or hibernate mode as if the browser tab in which the software is running has been closed and is no longer being used. To avoid this issue, use one of the following techniques: Send the job to the background. The job continues to run in the background even when your computer goes into sleep or hibernate mode. From the Job menu, select Send Job to Background. Save and schedule the search. The search runs independently from the computer that was used to create the search. You will need to decide if you want to save and schedule the search as a report, dashboard or an alert. See Saving searches and Scheduling searches. Share the job. The lifetime of the job is automatically extended to 7 days and read permissions are set to Everyone. See Share jobs and export results. Change the settings on the computer to extend the time before the computer goes into sleep or hibernate mode.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "About jobs and job management", "section_heading": "Managing long-running jobs", "section_id": "d6cd85d9_b44c_4712_8750_914c6198eea8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/about-jobs-and-job-management", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:54.547334+00:00", "version": "10.2"}}
{"id": "24e83cd981765561", "content": "Users with the Admin role, or a role with equivalent capabilities, can restrict how many jobs a given user can run, and how much space their job artifacts can take up. You must define a role with the desired restrictions and assign the user to the role. You can apply a high level of granularity by giving unique roles to each user in your system. Edit search restriction settings To edit the search restrictions setting for a role: In Splunk Web, go to Settings and then Roles. Edit an existing role and use the 4. Restrictions tab to limit the scope of search results that are returned when users with the role run searches. See Use roles to limit search results. For more information about roles, see Add and edit roles in the Securing Splunk Enterprise manual. Edit search restrictions using authorize.conf If you're using Splunk Enterprise, you can manually edit search restrictions using the authorize.conf file. Review the contents of the authorize.conf.example file in the Admin Manual. This example explains some the attributes that you might want to use. Create the configuration file. Edit the local authorize.conf file. To restrict the jobs that users can run, add the following information to the file: Add a stanza for the role that you want to create. Use the format [role_<roleName>]. Role names must be in lowercase characters. For example, [role_ninja]. Optional. Add the importRoles attribute. Importing a role also imports the other aspects of that role, such as the indexes that the role is allowed to search. For example, importRoles = user. Add the srchDiskQuota attribute and value. This is the maximum amount of disk space (MB) that search jobs can use, for a user that belongs to this role. The default value is 100MB. For example, srchDiskQuota = 500. Add the srchJobsQuota attribute and value. This is the maximum number of concurrently running searches that a user of this role can have. The default value is 3. srchJobsQuota = 10. Optional. Add the rtsearch attribute to specify if the user is authorized to run real-time searches. If you enable real-time searches for the user, you should also specify the rtSrchJobsQuota attribute. For attribute descriptions and information about the default values, see role_name stanza for the authorize.conf file in the Admin Manual .", "code_examples": [], "tables": [{"headers": ["Scope", "Description"], "rows": [["System-wide", "Create theauthorize.conffile in local directory for the system. The location of the system local directory is$SPLUNK_HOME/etc/system/local."], ["Application-specific", "Create theauthorize.conffile in the local directory for the application. The location of an application local directory is$SPLUNK_HOME/etc/apps/<app_name>/local."]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "About jobs and job management", "section_heading": "Administering jobs", "section_id": "f8e21b38_cfa3_4459_bfb0_693c4d0053f3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/about-jobs-and-job-management", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:54.547340+00:00", "version": "10.2"}}
{"id": "0643df6041da236b", "content": "Create and edit reports in the Reporting Manual", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "About jobs and job management", "section_heading": "See also", "section_id": "id_035c137d_a3fa_47fe_a1b4_4d24437b821d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/about-jobs-and-job-management", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:17:54.547344+00:00", "version": "10.2"}}
{"id": "d40bf4c94a2f1957", "content": "To open the Search app, from Splunk Home click Search & Reporting in the Apps panel. This opens the Search Summary view in the Search & Reporting app.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "About the Search app", "section_heading": "Open the Search app", "section_id": "feb414f4_98a5_4dab_9ee4_5dbe4e68d3f5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/about-the-search-app", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:18:12.482349+00:00", "version": "10.2"}}
{"id": "27ba430f38d814e4", "content": "Before you run a search, the Search summary view displays the following elements: App bar Search bar Time range picker Search icon The Search mode menu The Search History panel The How to Search panel A few additional elements might be available depending on whether you are working in Splunk Enterprise or Splunk Cloud Platform, and depending on whether your Splunk platform deployment supports the Search Processing Language, version 2 (SPL2). SPL2 is available in Splunk Cloud Platform and Splunk Enterprise instances that are installed on Linux machines. Splunk Enterprise only The How to search panel includes an additional option called Data Summary , which shows a summary of the data that is uploaded to the Splunk instance and that you are authorized to view. There is also an additional panel called Analyze Your Data with Table Views. You can use the options in this panel to prepare data without using the Search Processing Language (SPL). Splunk Cloud Platform only Under the Search bar, there is a workload management menu. You can use this menu to specify which pool to run your search in or choose to use a policy-based pool. The policies are defined in the Workload Management app. SPL2 only There are these additional elements: The Modules tab in the App bar The language picker The conversion button The Search, transform, and analyze data using SPL2 panel, which replaces the Analyze Your Data with Table Views panel in Splunk Enterprise. The following image shows the Search Summary view in a Splunk Enterprise instance that supports SPL2. Data summary The Data Summary dialog box displays the following tabs: Hosts Sources Sourcetypes These tabs represent searchable fields in your data. Selecting a host, source, or source type from the Data Summary dialog box is a great way to see how your data is turned into events. Host The host of an event is the host name, IP address, or fully qualified domain name of the network machine from which the event originated. In a distributed environment, you can use the host field to search data from specific machines. Source The source of an event is the file or directory path, network port, or script from which the event originated. Source type The source type of an event tells you what kind of data it is, usually based on how the data is formatted. This classification lets you search for the same type of data across multiple sources and hosts. In this example, source types are: access_combined_wcookie: Apache web server logs secure: Secure server logs vendor_sales: Global sales vendors For information about which source type is assigned to your data, see Why source types matter in the Getting Data In manual.", "code_examples": [], "tables": [{"headers": ["Number", "Element", "Description"], "rows": [["1", "App bar", "Navigate between the different views in the application you are in. For the Search & Reporting app the views are: Search, Datasets, Reports, Alerts, Dashboards, and Modules."], ["2", "Language picker", "Specify whether to search using SPL or SPL2.The setting in the language picker cannot be changed directly after you run your search, or if you open the search by selectingOpen in searchfrom a report. In these scenarios, you can only change the language fromSPLtoSPL2by selectingConvert to SPL2. If you want to change the language fromSPL2toSPL, you must selectCloseand start over with a new search."], ["3", "Conversion button", "Convert a search from SPL to SPL2.This button is available only when the language picker is set toSPLand the Search bar contains a search."], ["4", "Search bar", "Specify your search criteria."], ["5", "Time range picker", "Specify the time period for the search, such as the last 30 minutes or yesterday. The default isLast 24 hours."], ["6", "Search icon", "Run the search specified in the Search bar."], ["7", "Splunk AI Assistant for SPL icon", "Use Splunk AI Assistant for SPL to write, understand, interpret, and optimize SPL searches using natural language.Note:The Splunk AI Assistant for SPL application must be activated before you can use the AI assistant for your searches."], ["8", "Search mode menu", "Use the search mode menu to provide a search experience that fits your needs. The modes are Smart (default), Fast, and Verbose."], ["9", "Search history", "Review a list of the searches that you have run.The search history appears after you run your first search, and only shows previous searches for the selected language. For example, if the language picker is set toSPL2, then the search history shows previous SPL2 searches but not previous SPL searches."], ["10", "How to Search", "Use the links to learn more about how to start searching your data using SPL, as well as get a summary of the data that you have access to."], ["11", "Search, transform, and analyze data using SPL2", "Use the links to learn more about how to start searching your data using SPL2, and to open the SPL2 module editor in a new browser tab."]]}], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "About the Search app", "section_heading": "The Search summary view", "section_id": "id_07c2c6d5_1944_4bf9_be14_bc526e886c67--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/about-the-search-app", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:18:12.482368+00:00", "version": "10.2"}}
{"id": "fa34f8b98558bdb2", "content": "The New Search view opens after you run a search. Many of the elements from the Search summary view, such as the App bar, Search bar, and Time range picker, are still available in this view. Additionally, this view contains many more elements: search action buttons, counts of events, job status bar, and tabs for Events, Patterns, Statistics, and Visualizations. App bar Use the App bar to navigate between the different views in the Search & Reporting app: Search, Pivot, Reports, Alerts, and Dashboards. There are entire manuals devoted to these other capabilities. Alerting manual Dashboards and Visualizations Pivot manual Reporting manual Search bar Use the search bar to specify your search criteria in Splunk Web. Type your search string and press Enter , or click the Search icon which is on the right side of the search bar. Time range picker Time is the single most important search parameter that you can specify. Use the time range picker to retrieve events over a specific time period. For real-time searches you can specify a window over which to retrieve events. For historical searches , you can restrict your search by specifying a relative time range such as 15 minutes ago, Yesterday, and so on. You can also restrict your searches using a specific date and time range. The time range picker has many preset time ranges that you can select from, but you can also type a custom time range. For more information, see About searching with time. Timeline The timeline is a visual representation of the number of events that occur at each point in time in your results. Peaks or valleys in the timeline can indicate spikes in activity or server downtime. The timeline options are located above the timeline. You can zoom in, zoom out, and change the scale of the chart. When you click a point on the timeline or use on of the timeline options, the display of the timeline changes based on the events returned from your search. A new search is not run. Use the Format Timeline drop-down list to hide or change the scale of the timeline. Search actions There are a wide range of search actions you can perform, including working with your search Jobs, saving, sharing, exporting, and printing your search results. For more information, see: Perform actions on running searches About jobs and job management Export search results Search mode You can use the search mode selector to provide a search experience that fits your needs. The modes are Smart (default), Fast, and Verbose. For more information, see Search modes. Fields sidebar To the left of the events list is the Fields sidebar. As events are retrieved that match your search, the Fields sidebar shows the Selected Fields and Interesting Fields in the events. These are the fields that the Splunk software extracts from your data. When you first run a search the Selected Fields list contains the default fields host, source, and sourcetype. The default fields appear in every event. Interesting Fields are fields that appear in at least 20% of the events. Next to the field name is a count of how many distinct values there are in that field. Click on any field name to show more information about that field, such as a count and percentage of events that include each value. Note: If the number of distinct values in a field exceeds 100, the field summary statistics begins discarding some of the statistical information to keep memory usage low. For these fields an approximate distinct count is computed instead of an exact distinct count. This is an intended feature to conserve memory.", "code_examples": [], "tables": [{"headers": ["Number", "Element", "Description"], "rows": [["1", "Save As menu", "Use the Save As menu to save your search results as a report, dashboard, alert, or event type.SPL2 search results cannot be saved as event types."], ["2", "Search action buttons", "Actions that you can perform include working with your search job, and sharing, printing, and exporting your search results."], ["3", "Search results tabs", "The tab that your search results appear on depends on your search. Some searches produce a set of events, which appear on theEventstab. Other searches transform the data in events to produce search results, which appear on theStatisticstab."], ["4", "Timeline", "A visual representation of the number of events that occur at each point in time. Peaks or valleys in the timeline can indicate spikes in activity or server downtime. The timeline options are located above the timeline. You can format the timescale, or zoom in or out of a selected set of events."], ["5", "Fields sidebar", "Displays a list of the fields discovered in the events. The fields are grouped intoSelected FieldsandInteresting Fields."], ["6", "Events viewer", "Displays the events that match your search. By default, the most recent event is listed first. In each event, the matching search terms are highlighted. To change the event view, use theList,Format, andPer Pageoptions."]]}], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "About the Search app", "section_heading": "The New Search view", "section_id": "id_639c4b47_f1b4_4d43_bb35_c3982bec0696--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/about-the-search-app", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:18:12.482377+00:00", "version": "10.2"}}
{"id": "a5d8c392f82841f4", "content": "The techniques to limit the amount of data retrieved from disk range from setting a narrow time window, being as specific as possible, and retrieving the smallest number of events necessary. Narrow the time window One of the most effective ways to limit the data that is pulled off from disk is to limit the time range. Use the time range picker or specify time modifiers in your search to identify the smallest window of time necessary for your search. If you need to see data from only the last hour, do not use the default time range of Last 24 hours. See Select time ranges to apply to your search See Specify time modifiers in your search If you must use a broad time range, such as Last week or All time , then use other techniques to limit the amount of data retrieved from disk. Specify the index, source, or source type Understanding how your data is organized is important to optimizing your searches. Take the time to learn which indexes contain your data, the sources of your data, and the source types. Knowing this information about your data helps you narrow down your searches. Run the following search. This search not optimized, but it does provide an opportunity for you to learn about the data you have access to. In the Selected fields list, click on each type of field and look at the values for host, source, and sourcetype. In the Interesting fields list, click on the index field. Look at the names of the indexes that you have access to. Whenever possible, specify the index, source, or source type in your search. When Splunk software indexes data, it automatically tags each event with a number of fields. The index, source, and source type fields are added automatically to each event as default fields. A default field is an indexed field that the Splunk software recognizes in your event data at search time. The host, source, and source type fields describe where the event originated. Be specific Use the most specific terms in your search that you can. If possible, avoid using wildcard characters. For example, instead of using a wildcard character for a keyword: Use the specific keyword: Here is another example. Instead of using a wildcard character for field values: Specify each value: Combine a source type or an index with one or more field-value pairs. For example: This search retrieves events from only your web access logs. A wildcard character, access_*, is used in the field value to match any Apache web access source type. The source types can be access_common , access_combined , or access_combined_wcookie. Two specific field-value pairs are included in the search, status=200 and action=purchase. Limit the number of events retrieved By default, a Splunk search retrieves all events. However in some situations you might want to retrieve a sample set of events, instead of retrieving the entire event set. Limiting the number of events retrieved is useful in several situations: You are creating a search and want to determine if you are retrieving the correct events You need only a subset or sample set of events for your search You can specify a limit to the number of events retrieved in a couple of ways: Use the head command The head command retrieves only the most recent N events for a historical search, or the first N captured events for a realtime search. For example: Use event sampling Event sampling uses a ratio that you specify to select events. For example, if the sample ratio value is 100, each event has a 1 in 100 chance of being included in the result set. To learn more about event sampling and sampling ratios, see Event sampling. By default, event sampling is not active. You must specify a sampling ratio before you run your search. In Splunk Web, click the Sampling drop-down and choose a sampling ratio. Use the TERM directive to match terms that contain minor breakers The TERM directive is useful when you are searching for a term that contains minor breakers, such as periods or underscores, but does not contain major breakers. The term must be bound by major breakers, such as spaces or commas. For example, the IP address 192.0.2.255 contains the period (. ) minor breaker. If you specify TERM(192.0.2.255) , the Splunk software treats the IP address as a single term, instead of individual numbers. Using the TERM directive to search for terms that contain minor breakers improves search performance. See Event segmentation and searching. Avoid using NOT expressions More resources are used tracking NOT expressions than if you specify what you are looking for. Where ever possible, avoid using NOT expressions. For example, instead of using a string of NOT or != expressions such as: (NOT host=d NOT host=e) or (host!=d AND host!=e) Use the specific terms you are searching for: (host=a OR host=b OR host=c). To learn more, see Difference between NOT and !=. Don't overuse the OR operator While the OR operator can be useful for broadening your search to include multiple terms, overusing it can lead to inefficient searches. Each additional OR condition increases the complexity of the search, which can significantly impact search performance. Instead, try to refine your search criteria to be as specific as possible, using exact matches or other operators that can narrow down the results more effectively. You can also minimize the use of OR operators using other techniques, such as lookups , event types , or regular expressions.", "code_examples": [{"language": "spl", "code": "source=*"}, {"language": "spl", "code": "*error"}, {"language": "spl", "code": "fatal_error"}, {"language": "spl", "code": "status=404 OR status=5*"}, {"language": "spl", "code": "status=404 OR status=500 OR status=503"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase"}, {"language": "spl", "code": "sourcetype=access_* | head 1000 ..."}], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Quick tips for optimization", "section_heading": "Limit the data from disk", "section_id": "id_4ddc2b31_32bd_41c8_9958_c55d61b40544--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/quick-tips-for-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:18:26.762257+00:00", "version": "10.2"}}
{"id": "c230efefee40ffa2", "content": "Filter results as soon as possible before performing calculations. You can use field-value pairs and commands to filter results. Use field-value pairs before the first pipe Field-value pairs are indexed. Specifying field-value pairs before the first pipe is an efficient way to filter out events. For example, in the following search the term status=404 is in a separate search: Move the term status=404 before the first pipe: Here is another example. The second search includes the term clientip=\"10.0.0.0/8\". There is no reason to wait to filter on that term. Move the term clientip=\"10.0.0.0/8\" to filter out all other clientip addresses before the stats command. Use filtering commands before calculating commands Use filtering commands, such as where , before commands that perform calculations, such as eval. For example, this search has a where command after the eval command. The search does not require the results of the eval command before the where command is run. Move the where command to filter the results before the eval command is processed: Filter unnecessary fields from search results You can remove unnecessary fields from the search results by using commands such as fields. Removing extraneous fields from searches make them more efficient because search doesn't have to process superfluous data. Other search optimizations that work on a definite set of fields, such as predicate pushdown and transforming commands, already reduce the number of fields that are propagated and, therefore, can make the fields command less impactful. Use non-streaming commands as late as possible Postpone commands like sort and stats as late as possible in your search. These commands are referred to as non-streaming commands. Before these commands can run, the entire result set must be returned. For example, the results cannot be sorted until all of the results are available. For an explanation about the differences between streaming and non-streaming commands, see Types of commands. For a list of of commands by type, see Command types in the Search Reference .", "code_examples": [{"language": "spl", "code": "ERROR | search status=404"}, {"language": "spl", "code": "ERROR status=404"}, {"language": "spl", "code": "ERROR | stats sum(bytes) as sum by clientip | search sum >1048576 AND clientip=\"10.0.0.0/8\""}, {"language": "spl", "code": "ERROR clientip=\"10.0.0.0/8\"| stats sum(bytes) by clientip | search sum > 1048576"}, {"language": "spl", "code": "field1=value |evalKB=bytes/1024 |wherefield2=field3"}, {"language": "spl", "code": "field1=value |wherefield2=field3 |evalKB=bytes/1024"}], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Quick tips for optimization", "section_heading": "Filter as soon as possible", "section_id": "ec530f87_6164_4f57_bfa6_bcf97e2ec92a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/quick-tips-for-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:18:26.762267+00:00", "version": "10.2"}}
{"id": "83e59c701e120322", "content": "There are a few other techniques that you can use to optimize your searches. Store your apps on a fast, local disk, not on network file system (NFS). Loading apps on NFS can become a performance bottleneck. Use post-process searches in dashboards. See Searches power dashboards and forms in Dashboards and Visualizations. Use summary indexing , report acceleration , and data model acceleration features. Use Fast Mode to increase the speed of searches by reducing the event data that the search returns. See Search modes .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Quick tips for optimization", "section_heading": "Other techniques for search optimization", "section_id": "id_7c63f6b4_1ac4_49eb_b8bc_49e646909c58--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/quick-tips-for-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:18:26.762272+00:00", "version": "10.2"}}
{"id": "6a3cd5731a06bded", "content": "About search optimization Write better searches Built-in optimizations", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Quick tips for optimization", "section_heading": "See also", "section_id": "fc7578d4_5588_4f41_84be_5310c33833bb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/quick-tips-for-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:18:26.762276+00:00", "version": "10.2"}}
{"id": "3d526200e0b5ef71", "content": "Event correlation is finding relationships between seemingly unrelated events in data from multiple sources to answer questions like, \"how far apart in time did a specific set of events occur?\" or \"what's the total amount of time it took for a transaction to complete?\" Splunk software supports event correlations using time and geographic location, transactions, sub-searches, field lookups, and joins. Identify relationships based on the time proximity or geographic location of the events. Use this correlation in any security or operations investigation, where you might need to see all or any subset of events that take place over a given time period or location. Track a series of related events, which may come from separate IT systems and data sources, together as a single transaction. Identify the amount of time it took to complete the transaction and the number of events within a single transaction. Use a sub-search to take the results of one search and use them in another. Create conditional searches, where you see the results of a search only if the sub-search meets certain thresholds. Correlate your data to external sources with lookups. Use SQL-like inner and outer joins to link two completely different data sets together based on one or more common fields. This chapter discusses three methods for correlating or grouping events: Use time to identify relations between events Use subsearch to correlate events Use transactions to identify and group related events You can also use field lookups and other features of the search language. Depending on your search criteria and how you want to define your groupings, you may be able to use a search command, such as append , associate , contingency , join , or stats. Sometimes, there is no single command that you can use. If you're not sure where to start, the following flow chart can help you decide whether to use a lookup, define a transaction, or try another search command to define your event grouping. In most cases, you can accomplish more with the stats command or the transaction command ; and these are recommended over using the join and append commands. You can read more about when to use stats and transaction in the topic \"About transactions\" later in this chapter. You can also read more about the stats commands in the \"Calculate Statistics\" chapter of this manual. Note: The information for this diagram was provided by Nick Mealy .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "df6800ff627dc4d8d8d2b8aaffacdc663--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/group-and-correlate-events/about-event-grouping-and-correlation", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Group and Correlate Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:18:42.996284+00:00", "version": "10.2"}}
{"id": "b60f5d1a9fa4341e", "content": "Raw event searches are searches that just retrieve events from an index or indexes, and are typically used when you want to analyze a problem. Some examples of these searches include: checking error codes, correlating events, investigating security issues, and analyzing failures. These searches do not usually include search commands (except search itself), and the results are typically a list of raw events. Read more about raw event searches starting with the topic About retrieving events .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Types of searches", "section_heading": "Raw event searches", "section_id": "id_7e328d3a_a486_4a8b_bfd0_ed953f3ae55a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:19:00.442569+00:00", "version": "10.2"}}
{"id": "a65af72452ccd094", "content": "Transforming searches are searches that perform some type of statistical calculation against a set of results. These are searches where you first retrieve events from an index and then pass the events into one or more search commands. These searches will always require fields and at least one of a set of statistical commands. Some examples include: getting a daily count of error events, counting the number of times a specific user has logged in, or calculating the 95th percentile of field values. Read more about the structure of a search in About the search processing language syntax. Read more about using subsearches to filter results in About subsearches. Read more about transforming searches and commands starting with the topic About transforming commands and searches .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Types of searches", "section_heading": "Transforming searches", "section_id": "id_1eead236_4a53_41ad_ab11_dc426b7572d3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:19:00.442577+00:00", "version": "10.2"}}
{"id": "0a8cc74472761037", "content": "Whether you are retrieving raw events or building a report, you should also consider whether you are running a search for sparse or dense information: Sparse searches are searches that look for a single event or an event that occurs infrequently within a large set of data. You have probably heard these referred to as 'needle in a haystack' or \"rare term\" searches. Some examples of these searches include: searching for a specific and unique IP address or error code. Dense searches are searches that scan through and report on many events. Some examples of these searches include: counting the number of errors that occurred or finding all events from a specific host. See How search types affect Splunk Enterprise performance in the Capacity Planning Manual .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Types of searches", "section_heading": "Information density", "section_id": "f5268131_ae0e_4ddf_8a4b_faf37ae6975a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:19:00.442582+00:00", "version": "10.2"}}
{"id": "5e6fed5ba9bacea3", "content": "Timestamp processing is a key step in event processing. Splunk software uses timestamps to: Correlate events by time Create the timeline histogram in Splunk Web Set time ranges for searches", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "About searching with time", "section_heading": "How timestamps are used", "section_id": "id_0a6e7626_2095_4f11_972e_70d86f39f241--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/about-searching-with-time", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:19:17.712658+00:00", "version": "10.2"}}
{"id": "d921ba2284e3ccbe", "content": "Regardless of how time is specified in your events, timestamps are converted to UNIX time and stored in the _time field when your data is indexed. If your data does not have timestamps, the time at which your data is indexed is used as the timestamp for your events. UNIX time is the number of seconds that have elapsed since 00:00:00 Coordinated Universal Time (UTC), 1 January 1970. This moment in time is sometimes referred to as epoch time. UNIX time appears as a series of numbers, for example 1518632124. You can use any UNIX time converter to convert the UNIX time to either GMT or your local time. GMT and UTC GMT (Greenwich Mean Time) is sometimes confused with UTC (Coordinated Universal Time). However GMT is a time zone and UTC is a time standard. GMT is a time zone officially used in some European and African countries as their local time. The time is displayed in either the 24-hour format (00:00-23:59) or the 12-hour format (00:00-12:00 AM/PM). UTC is a time standard that is the basis for time and time zones worldwide. No country uses UTC as a local time. Neither GMT nor UTC ever change for Daylight Saving Time (DST). However, some of the countries that use GMT switch to different time zones during their DST period. For example, the United Kingdom uses GMT for most of the year, but switches to British Summer Time (BST) during the summer months. BST is one hour ahead of GMT.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "About searching with time", "section_heading": "Timestamps are stored in UNIX time", "section_id": "id_5f410455_a33d_40ba_b98f_7cdbcf6cba63--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/about-searching-with-time", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:19:17.712667+00:00", "version": "10.2"}}
{"id": "8a439a1a3a23177a", "content": "The _time field appears in a human readable format in Splunk user interfaces. However, the values in the _time field are stored in UNIX time.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "About searching with time", "section_heading": "The _time field", "section_id": "d22ca28b_0648_48fe_b548_c50128fdf130--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/about-searching-with-time", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:19:17.712672+00:00", "version": "10.2"}}
{"id": "0f61500fc2e6a9c5", "content": "Splunk user interfaces use a default time range when you create a search. This range helps to avoid running searches with overly-broad time ranges that waste system resources and produce more results than you really need. Whether you are running a new search, a report, or creating a dashboard, it is important to narrow the time range to only the dates or times that you really need. Time is also crucial for determining what went wrong. You often know when something happened, if not exactly what happened. Looking at events that happened around the same time that something went wrong can help correlate results and find the root cause of the problem.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "About searching with time", "section_heading": "Specify narrow time ranges", "section_id": "id_828c0302_8429_42f7_a195_d25470b4b354--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/about-searching-with-time", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:19:17.712676+00:00", "version": "10.2"}}
{"id": "840a4b6f295d25f0", "content": "Select time ranges to apply to your search Specify time modifiers in your search Specify time ranges for real-time searches Use time to find nearby events How time zones are processed by the Splunk platform Date and time format variables in the Search Reference Time modifiers in the Search Reference", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "About searching with time", "section_heading": "See also", "section_id": "e30915dc_c2a3_4f7a_a1f8_076eddf49f8d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/about-searching-with-time", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:19:17.712680+00:00", "version": "10.2"}}
{"id": "fb4be17d0447b2b1", "content": "Commands for advanced statistics Use the stats command and functions Use stats with eval expressions and functions Add sparklines to report tables", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "About advanced statistics", "section_heading": "See also", "section_id": "id_10183cf1_ad58_4cf1_9e88_58da288bf5ae--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/about-advanced-statistics", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:19:34.006144+00:00", "version": "10.2"}}
{"id": "1db2f1d6e518227a", "content": "The phrase event data refers to your data after it has been added to the Splunk index. Events are a single record of activity or instance of this event data. For example, an event might be a single log entry in a log file. Because the Splunk software separates individual events by their time information, an event is distinguished from other events by a timestamp. Here is a sample event: 172.26.34.223 - - [01/Jul/2005:12:05:27 -0700] \"GET /trade/app?action=logout HTTP/1.1\" 200 2953 Events contain pairs of information, or fields. When you add data and it gets indexed, the Splunk software automatically extracts some useful fields for you, such as the host the event came from and the type of data source it is.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "About retrieving events", "section_heading": "Events, event data, and fields", "section_id": "be10da66_f58d_4254_abd3_7330054d3a81--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/about-retrieving-events", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:19:51.106721+00:00", "version": "10.2"}}
{"id": "3f44168f56313381", "content": "Although it can be easy to get confused by the different categories of commands, having a solid understanding of the differences between types of commands will help you understand the implications for how and where data is processed, and optimize the performance of your searches. For example, suppose you have a search that uses the following commands in this order: The first 4 commands, from the search to eval commands, are distributable streaming commands that can all be processed on the indexers. As a result, when the search is run, the search head pushes the search to the indexers. Since the sort command is not a distributable streaming command and needs all of the events in one place, the events that are returned from the first 4 commands are then sent back to the search head for sorting. As a result, the rest of the search after the sort command must also be processed on the search head. This is true even if the commands that follow sort are distributable streaming commands, like the second where command in the search. Once search processing moves to the search head, it can't be moved back to the indexer. With this in mind, you should put non-streaming commands as late as possible in your searches to make them run efficiently. To find out more about how the types of commands used in searches can affect performance, see Write better searches .", "code_examples": [{"language": "spl", "code": "search... | lookup... |where... |eval... | sort... |where... |..."}], "tables": [], "chunk_index": 0, "total_chunks": 9, "metadata": {"title": "Types of commands", "section_heading": "Why the types of commands matter", "section_id": "cd33b27c_78b8_42a0_89fd_c1bff27e58e5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:07.194944+00:00", "version": "10.2"}}
{"id": "dcff15e8746d41a4", "content": "A streaming command operates on each event as it is returned by a search. Essentially one event in and one (or no) event out. For example, the eval command can create a new field, full_name , to contain the concatenation of the value in the first_name field, a space, and the value in the last_name field. The eval command evaluates each event without considering the other events. A non-streaming command requires the events from all of the indexers before the command can operate on the entire set of events. Many transforming commands are non-streaming commands. There are also several commands that are not transforming commands but are also non-streaming. These non-transforming, non-streaming commands are most often dataset processing commands. For example, before the sort command can begin to sort the events, the entire set of events must be received by the sort command. Other examples of non-streaming commands include dedup (in some modes), stats , and top. Non-streaming commands force the entire set of events to the search head. This requires a lot of data movement and a loss of parallelism. For information on how to mitigate the cost of non-streaming commands, see Write better searches in this manual.", "code_examples": [{"language": "spl", "code": "... |evalfull_name = first_name.\" \".last_name"}], "tables": [], "chunk_index": 1, "total_chunks": 9, "metadata": {"title": "Types of commands", "section_heading": "Streaming and non-streaming commands", "section_id": "id_012254f4_71a7_41f5_a436_df91881a7976--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:07.194951+00:00", "version": "10.2"}}
{"id": "f8622ae1dec072cc", "content": "The following table describes the processing differences between some of the types of commands. When a command is run it outputs either events or results, based on the type of command. For example, when you run the sort command, the input is events and the output is events in the sort order you specify. However, transforming commands do not output events. Transforming commands output results. For example the stats command outputs a table of calculated results. The events used to calculate those results are no longer available. After you run a transforming command, you can't run a command that expects events as an input. Data processing commands are non-streaming commands that require the entire dataset before the command can run. These commands are not transforming, not distributable, not streaming, and not orchestrating. The sort command is an example of a data processing command. See Data processing commands .", "code_examples": [], "tables": [{"headers": ["", "Distributable streaming", "Centralized streaming", "Data processing (non-streaming)", "Transforming"], "rows": [["Can run on indexers", "Y", "N", "N", "N"], ["Can output before final input", "Y", "Y", "N", "N"], ["Outputs events if inputs are events", "Y", "Y", "Y", "N"]]}], "chunk_index": 2, "total_chunks": 9, "metadata": {"title": "Types of commands", "section_heading": "Processing attributes", "section_id": "id_58e0fd97_6cfa_4ba3_a435_5b6b5bbf2cdc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:07.194958+00:00", "version": "10.2"}}
{"id": "0ea72378c7e1ad23", "content": "A streaming command operates on each event returned by a search. For distributable streaming, the order of the events does not matter. A distributable streaming command is a command that can be run on the indexer, which improves processing time. The other commands in a search determine if the distributable streaming command is run on the indexer: If all of the commands before the distributable streaming command can be run on the indexer, the distributable streaming command is run on the indexer. If any one of the commands before the distributable streaming command must be run on the search head, the remaining commands in the search must be run on the search head. When the search processing moves to the search head, it can't be moved back to the indexer. Distributable streaming commands can be applied to subsets of indexed data in a parallel manner. For example, the rex command is streaming. It extracts fields and adds them to events at search time. Some of the common distributable streaming commands are: eval , fields , makemv , rename , regex , replace , strcat , typer , and where. For a complete list of distributable streaming commands, see Streaming commands in the Search Reference .", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 9, "metadata": {"title": "Types of commands", "section_heading": "Distributable streaming", "section_id": "id_754bff98_d859_4809_bdd0_d600b59fa243--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:07.194963+00:00", "version": "10.2"}}
{"id": "d679a5daa20188c4", "content": "For centralized streaming commands, the order of the events matters. A centralized streaming command applies a transformation to each event returned by a search. But unlike distributable streaming commands, a centralized streaming command only works on the search head. You might also hear the term \"stateful streaming\" to describe these commands. Centralized streaming commands include: head , streamstats , some modes of dedup , and some modes of cluster .", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 9, "metadata": {"title": "Types of commands", "section_heading": "Centralized streaming", "section_id": "d9492a9c_e6c1_46b4_85e5_a5f008daeca0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:07.194966+00:00", "version": "10.2"}}
{"id": "4ef61138c3fd82e3", "content": "A transforming command orders the search results into a data table. These commands \"transform\" the specified cell values for each event into numerical values that Splunk software can use for statistical purposes. Transforming commands are not streaming. Also, transforming commands are required to transform search result data into the data structures that are required for visualizations such as column, bar, line, area, and pie charts. Transforming commands include: chart , timechart , stats , top , rare , and addtotals when it is used to calculate column totals (not row totals). For more information about transforming commands and their role in create statistical tables and chart visualizations, see About transforming commands and searches in the this manual. For a complete list of transforming commands, see Transforming commands in the Search Reference .", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 9, "metadata": {"title": "Types of commands", "section_heading": "Transforming", "section_id": "id_4138c4d5_cf7b_420f_97c8_acb2f0702b04--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:07.194970+00:00", "version": "10.2"}}
{"id": "67ce3ece1bfb13ca", "content": "A generating command returns information or generates results. Some generating commands can return information from an index, a data model, a lookup, or a CSV file without any transformations to the information. Other generating commands generate results, usually for testing purposes. Generating commands are either event-generating (distributable or centralized) or report-generating. Most report-generating commands are also centralized. Depending on which type the command is, the results are returned in a list or a table. Generating commands do not expect or require an input. Generating commands are usually invoked at the beginning of the search and with a leading pipe. That is, there cannot be a search piped into a generating command. The exception to this is the search command, because it is implicit at the start of a search and does not need to be invoked. Examples of generating commands include: dbinspect , datamodel , inputcsv , inputlookup , makeresults , metadata , pivot , search , and tstats For a complete list of generating commands, see Generating commands in the Search Reference. Note: A comment inserted before a generating command causes the search to fail. For example, the following search fails because the commented text precedes tstats , which is a generating command: | ```This search returns an error``` | tstats count WHERE host=x BY source .", "code_examples": [], "tables": [], "chunk_index": 6, "total_chunks": 9, "metadata": {"title": "Types of commands", "section_heading": "Generating", "section_id": "f9a96b01_2159_4213_872e_9e60fe56d0ce--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:07.194973+00:00", "version": "10.2"}}
{"id": "fc13e10be4916088", "content": "An orchestrating command is a command that controls some aspect of how the search is processed. It does not directly affect the final result set of the search. For example, you might apply an orchestrating command to a search to enable or turn off search optimization that helps the overall search complete faster. Examples of orchestrating commands include redistribute , noop , and localop. The lookup command also becomes an orchestrating command when you use it with the local=t argument.", "code_examples": [], "tables": [], "chunk_index": 7, "total_chunks": 9, "metadata": {"title": "Types of commands", "section_heading": "Orchestrating", "section_id": "id_992ede74_b9cb_4040_8677_2e7b0ade2d12--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:07.194977+00:00", "version": "10.2"}}
{"id": "09dc9785026b6940", "content": "There are a handful of commands that require the entire dataset before the command can run. These commands are referred to as dataset processing commands. These commands are not transforming, not distributable, not streaming, and not orchestrating. Some of these commands fit into other types in specific situations or when specific arguments are used. Examples of data processing commands include: sort , eventstats , and some modes of cluster , dedup , and fillnull. For a complete list of dataset processing commands, see Dataset processing commands in the Search Reference .", "code_examples": [], "tables": [], "chunk_index": 8, "total_chunks": 9, "metadata": {"title": "Types of commands", "section_heading": "Dataset processing", "section_id": "id_748e05ce_74c2_4ae3_a3ea_1e954ae3cded--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/types-of-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:07.194981+00:00", "version": "10.2"}}
{"id": "870765eba0e8ba7e", "content": "This section discusses how to calculate summary statistics on events. When you think about calculating statistics using the Splunk search processing language (SPL), the stats command is probably what comes to mind first. The stats command generates reports that display summary statistics in a tabular format. Additionally, you can use the chart and timechart commands to create charted visualizations for summary statistics and the geostats command to create map visualizations for summary statistics of events that include geographical location fields. The stats , chart , and timechart commands - and their related commands eventstats , geostats and streamstats - are designed to work in conjunction with statistical functions. For examples of searches using these commands and functions, read Use the stats command and functions. Later topics discuss how to: Use stats with eval expressions and functions to calculate statistics Add sparklines to report tables The Advanced statistics section contains topics on detecting anomalies, finding and removing outliers, detecting patterns, and time series forecasting.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "df4e14e7330ab4008ad768687f41e11a6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/calculate-statistics/about-calculating-statistics", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Calculate Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:23.096727+00:00", "version": "10.2"}}
{"id": "c23351ea46c10ad0", "content": "The order in which the Splunk software evaluates Boolean expressions depends on whether you are using the expression with the search command, the eval command, or the where command. This includes the implied search command at the beginning of the search. The search command evaluates OR before AND operators (XOR is not supported). The eval and where commands evaluate AND before OR operators. The following table describes the order in which the Boolean expressions are evaluated by the commands.", "code_examples": [], "tables": [{"headers": ["Order", "Search command", "Eval and where commands"], "rows": [["1", "Expressions within parentheses", "Expressions within parentheses"], ["2", "NOT clauses", "NOT clauses"], ["3", "OR clauses", "AND clauses"], ["4", "AND clauses", "OR clauses"], ["5", "", "XOR clauses"]]}], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Boolean expressions with logical operators", "section_heading": "Order of evaluation", "section_id": "id_22b97e41_f840_43b8_8f17_5d6addea667d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/boolean-expressions-with-logical-operators", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:37.558731+00:00", "version": "10.2"}}
{"id": "82fdcccb29242b45", "content": "The following examples show how Splunk software processes Boolean expressions using logical operators. Search command example with AND and OR Consider the following search: With the search command, the AND is implied between the expressions. The same results are returned if you omit the AND in the search and specify host=\"www1\" status=200 OR action=\"addtocart\". This search is processed as: This search returns: All of the events where the host is www1 and the status is either 200 or the action is addtocart. With the search command, the OR is processed before the AND. The where command processes this search differently, as shown in the next example. Where command example with AND and OR Consider the following search: This search is processed as: This search returns: All of the events where the host is www1 and the status is 200. All of the events where the action is addtocart. With the where command the AND is processed before the OR. Search command example with NOT Consider the following search: This search returns all host=\"www1\" events that have status codes not equal to 200. Note: Searches that exclude results using the NOT operator are typically less efficient than searches that are more inclusive. As a result, you should avoid using NOT when possible. Search command examples with AND NOT and NOT OR Consider the following search: It produces the same results as the following search: These searches return all host=\"www1\" events that have status codes not equal to 200 or 505. These searches should have fewer results than a search that just excludes events with status equal to 200. Note: Searches that exclude results using the NOT operator are typically less efficient than searches that are more inclusive. As a result, you should avoid using NOT when possible.", "code_examples": [{"language": "spl", "code": "host=\"www1\"AND status=200 OR action=\"addtocart\""}, {"language": "spl", "code": "host=\"www1\"AND (status=200 OR action=\"addtocart\")"}, {"language": "spl", "code": "...|wherehost=\"www1\"AND status=200 OR action=\"addtocart\""}, {"language": "spl", "code": "...|where(host=\"www1\"AND status=200) OR action=\"addtocart\""}, {"language": "spl", "code": "host=\"www1\"NOT status=200"}, {"language": "spl", "code": "host=\"www1\"NOT status=200 AND NOT status=505"}, {"language": "spl", "code": "host=\"www1\"NOT (status=200 OR status=505)"}], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Boolean expressions with logical operators", "section_heading": "Examples", "section_id": "id_5cfb512c_7e91_4922_b53e_63d8cfa02e12--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/boolean-expressions-with-logical-operators", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:37.558742+00:00", "version": "10.2"}}
{"id": "91ba434012c3bb49", "content": "By default, searches are case-insensitive. For example, if you search for Error , any case of that term is returned, such as Error , error , and ERROR. You can use the CASE directive to perform case-sensitive matches for terms and field values. For example, if you search for CASE(error) , your search returns results containing only the specified case of the term, which is error. You can use the CASE directive to search for terms using wildcards. For example, searching for CASE(%ASA-1*) returns events matching values such as %ASA-1-134568 and %ASA-1-12345. Example The following search only matches events that contain localhost in uppercase in the host field.", "code_examples": [{"language": "spl", "code": "host=CASE(LOCALHOST)"}], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Use CASE() and TERM() to match phrases", "section_heading": "When to use CASE", "section_id": "id_80d94dbb_7784_462b_aefe_e2d99897e2e0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/use-case-and-term-to-match-phrases", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:55.279619+00:00", "version": "10.2"}}
{"id": "c579b621c2c58a84", "content": "The TERM directive is useful for more efficiently searching for a term that: Contains minor breakers , such as periods or underscores. Is bound by major breakers , such as spaces or commas. Does not contain major breakers. When data is indexed, characters such as periods and underscores are recognized as minor breakers between terms. Use the TERM directive to ignore the minor breakers and match whatever is inside the parentheses as a single term. For example, the IP address 127.0.0.1 contains the period (. ) minor breaker. If you search for the IP address 127.0.0.1, Splunk software searches for 127 AND 0 AND 1 and returns events that contain those numbers anywhere in the event. If you specify TERM(127.0.0.1) , the search treats the IP address as a single term, instead of individual numbers, and returns all events that contain the IP address 127.0.0.1. The TERM directive only works for terms that are bounded by major or minor breakers, but the term you are searching for cannot contain major breakers. For example, you cannot use TERM to search for Maria Dubois because there is a space between the names. This is discussed in the examples later in this topic. When you use the TERM directive, the Splunk software expects to see the term you specify as a token in the lexicon in the .tsidx file. For more information about how Splunk software breaks events up into searchable segments, see About segmentation in Getting Data In. You can use the TERM directive to search for terms using wildcards. For example, searching for TERM(%ASA-1*) returns events matching values such as %ASA-1-134568 and %ASA-1-12345. See Use the TERM directive to match terms that contain minor breakers. Using TERM() with major and minor breakers Searches that contain the TERM() directive might return unexpected results if the following conditions are met: The term in the TERM directive contains minor breakers, such as a period (. ); and The raw data for the term is bound by major breakers that end in a number or letter; and The search contains other expressions such as host=* , sourcetype=* or myfield=* , OR contains other expressions that include minor breakers, such as \"myfield1.myfield2\", \"myfield1:myfield2\". For example, say your raw data is foo%20www.something.com%20abc 127.1.1 , which includes the major breaker %20. The following searches don't return results: index=main TERM(www.something.com) host=* index=main TERM(www.something.com) \"127.1.1\" However, the following search does return results: index=main TERM(www.something.com) For a full list of minor breakers and multi-character major breakers, see the segmenters.conf file in the Splunk Enterprise Admin Manual. Examples Searching for TERM(127.0.0.1) works for raw data that looks like this: Both 127.0.0.1 and admin are bounded by major breakers, in this case spaces. However, searching for TERM(127.0.0.1) fails for data that looks like this: This is because the equal symbol ( = ) is a minor breaker, not a major breaker. Additionally, the IP address portion of the event is indexed as: ip , 127 , 0 , 1 , and ip=127.0.0.1. You are looking for 127.0.0.1 , which is not an indexed term. If your data looks like this: Searching for TERM(user admin) fails to return results. The space is a major breaker and the phrase \"user admin\" is not indexed as a single term. In this situation, use quotation marks to search for a string that contains a space, for example \"user admin\" .", "code_examples": [{"language": "spl", "code": "127.0.0.1 - admin"}, {"language": "spl", "code": "ip=127.0.0.1 - user=admin"}, {"language": "spl", "code": "ip 127.0.0.1 - user admin"}], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Use CASE() and TERM() to match phrases", "section_heading": "When to use TERM", "section_id": "id_0c0dbb7d_e182_422f_a003_f3ef62f8c0cb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/use-case-and-term-to-match-phrases", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:55.279626+00:00", "version": "10.2"}}
{"id": "fdb9ea8619329202", "content": "Event segmentation and searching", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Use CASE() and TERM() to match phrases", "section_heading": "See also", "section_id": "id_815c3bd4_edea_47b7_9ada_981d20fc3e84--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/use-case-and-term-to-match-phrases", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:20:55.279631+00:00", "version": "10.2"}}
{"id": "847340c194e9fd5c", "content": "To better understand how search commands act on your data, it helps to visualize all your indexed data as a table. Each search command redefines the shape of your table. For example, let's take a look at the following search. The Disk represents all of your indexed data. The Disk is a table of a certain size with columns that represent fields and rows that represent events. The first intermediate results table shows fewer rows--representing the subset of events retrieved from the index that matched the search terms \"sourcetype=syslog ERROR\". The second intermediate results table shows fewer columns, representing the results of the top command, \"top user\", which summarizes the events into a list of the top 10 users and displays the user, count, and percentage. Then, \"fields - percent\" removes the column that shows the percentage, so you are left with a smaller final results table.", "code_examples": [{"language": "spl", "code": "sourcetype=syslog ERROR | top user | fields - percent"}], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Anatomy of a search", "section_heading": "The anatomy of a search", "section_id": "ee5bddc4_67cd_4c77_a2fa_580f59c08725--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/anatomy-of-a-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:21:11.756089+00:00", "version": "10.2"}}
{"id": "ab843ffa9be38e00", "content": "The \"search pipeline\" refers to the structure of a Splunk search, in which consecutive commands are chained together using a pipe character, \"|\". The pipe character tells Splunk software to use the output or result of one command (to the left of the pipe) as the input for the next command (to the right of the pipe). This enables you to refine or enhance the data at each step along the pipeline until you get the results that you want. A Splunk search starts with search terms at the beginning of the pipeline. These search terms are keywords, phrases, Boolean expressions, key/value pairs, etc. that specify which events you want to retrieve from the index(es). See About retrieving events. The retrieved events can then be passed as inputs into a search command using a pipe character. Search commands tell Splunk software what to do to the events after you retrieved them from the index(es). For example, you might use commands to filter unwanted information, extract more information, evaluate new fields, calculate statistics, reorder your results, or create a chart. Some commands have functions and arguments associated with them. These functions and their arguments enable you to specify how the commands act on your results and which fields to act on; for example, how to create a chart, what kind of statistics to calculate, and what fields to evaluate. Some commands also enable you to use clauses to specify how you want to group your search results. For more information about what you can do with search commands, see About the search processing language. In the Search Reference , for a list of search commands, see the Command quick reference and the individual search command reference topics for syntax and usage information.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Anatomy of a search", "section_heading": "About the search pipeline", "section_id": "fc068442_872b_428d_b765_191b3c332f0a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/anatomy-of-a-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:21:11.756096+00:00", "version": "10.2"}}
{"id": "0104523c277a8737", "content": "Generally, you need quotes around phrases and field values that include white spaces, commas, pipes, quotes, or brackets. Quotes must be balanced, an opening quote must be followed by an unescaped closing quote. For example: A search such as error | stats count will find the number of events containing the string error. A search such as ... | search \"error | stats count\" would return the raw events containing the literal string error , a pipe character ( | ) , stats , and count , in that order. Additionally, you want to use quotes around keywords and phrases if you don't want to search for their default meaning, such as Boolean operators and field-value pairs. For example: A search for the keyword AND without meaning the Boolean operator: error \"AND\" A search for this field-value phrase: error \"startswith=Error\" The backslash character ( \\ ) is used to escape quotes, pipes, and itself. Backslash escape sequences are still expanded inside quotes. For example: The sequence \\| as part of a search will send a pipe character to the command, instead of having the pipe split between commands. The sequence \\\" will send a literal quote to the command, for example for searching for a literal quotation mark or inserting a literal quotation mark into a field using rex. The \\\\ sequence will be available as a literal backslash in the command. If Splunk software does not recognize a backslash sequence, it will not alter it. For example, \\s in a search string will be available as \\s to the command, because \\s is not a known escape sequence. However, in the search string, \\\\s will be available as \\s to the command, because \\\\ is a known escape sequence that is converted to \\. Asterisks ( * ) cannot be searched for using a backslash to escape the character. Splunk software treats the asterisk character as a major breaker. Because of this, it will never be in the index. If you want to search for the asterisk character, you have to run a post-filtering regex search on your data: For more information about major breakers, read Overview of event processing in the Getting Data In manual. Examples Example 1: The myfield field is created with the value of 6. Example 2: The myfield field is created with the value of \". Example 3: The myfield field is created with the value of \\. Example 4: This search would produce an error because of unbalanced quotation marks.", "code_examples": [{"language": "spl", "code": "index=_internal | regex\".*\\*.*\""}, {"language": "spl", "code": "... |evalmyfield=\"6\""}, {"language": "spl", "code": "... |evalmyfield=\"\\\"\""}, {"language": "spl", "code": "... |evalmyfield=\"\\\\\""}, {"language": "spl", "code": "... |evalmyfield=\"\\\""}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Anatomy of a search", "section_heading": "Quotes and escaping characters", "section_id": "c080e185_c99a_4d73_a6d1_5b8bc20856b3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/anatomy-of-a-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:21:11.756100+00:00", "version": "10.2"}}
{"id": "e8f5f9aef7f1a08e", "content": "Events and results flowing through the Splunk search pipeline exist as a collection of fields. Fields can fundamentally come from the Splunk index, for example, _time as the time of the event, source as the filename, and so on. Or can be derived from a wide variety of sources at search time, such as eventtypes, tags, regex extractions using the rex command, totals coming from the stats command, and so on. For a given event, a given field name might be present or absent. If present, it might contain a single value or multiple values. Each value is a text string. Values might be of positive length (a string, or text) or zero length (empty strings, or \"\" ). Numbers, for example, are strings that contain the number. For example, a field containing a value of the number 10 contains the characters 1 and 0: \"10\". Commands that take numbers from values automatically convert them internally to numbers for calculations. Null field A null field is not present on a particular result or event. Other events or results in the same search might have values for this field. For example, the fillnull command adds a field and default value to events or results that lack fields present on other events or results in the search. Empty field An empty field is shorthand for a field that contains a single value that is the empty string. Empty value A value that is the empty string, or \"\". You can also describe this as a zero-length string. Multivalue field A field that contains more than one value. For example, events such as email logs often have multivalue fields in the To: and Cc: information. See Manipulate and evaluate fields with multiple values in the Search Manual .", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Anatomy of a search", "section_heading": "Fields", "section_id": "bc7e7591_6dca_4968_8839_dc35cd8891fe--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/anatomy-of-a-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:21:11.756103+00:00", "version": "10.2"}}
{"id": "5e2be3e3b57d599f", "content": "If you search with the != expression: Every event that has a value in the field, where that value does not match the value you specify, is returned. Events that do not have a value in the field are not included in the results. For example, if you search for Location!=\"Calaveras Farms\" , events that do not have Calaveras Farms as the Location are returned. Events that do not have Location value are not included in the results. In this example events with ID 104F5 and 102M1 do not have a Location and are not returned. If you search for a Location that does not exist using the != expression, all of the events that have a Location value are returned.", "code_examples": [{"language": "spl", "code": "source=\"Ponies.csv\"Location!=\"Calaveras Farms\""}], "tables": [{"headers": ["ID", "Name", "Color", "Location"], "rows": [["101M3", "McIntosh", "Chestnut", "Marin Meadows"], ["104M6", "Rutherford", "Dun", "Placer Pastures"], ["101F2", "Rarity", "", "Marin Meadows"], ["101F6", "", "Chestnut", "Marin Meadows"], ["104F4", "Pinkie", "Sorrel", "Placer Pastures"]]}], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Difference betweenÂ != and NOT", "section_heading": "Searching withÂ !=", "section_id": "id_1f82a298_2cc9_496d_89c7_2cc7ce8c5a9d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/difference-between-and-not", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:21:28.310491+00:00", "version": "10.2"}}
{"id": "044fb7d7a6c74bda", "content": "If you search with the NOT operator, every event is returned except the events that contain the value you specify. This includes events that do not have a value in the field. For example, if you search using NOT Location=\"Calaveras Farms\" , every event is returned except the events that contain the value \"Calaveras Farms\". This includes events that do not have a Location value. If you search for a Location that does not exist using NOT operator, all of the events are returned.", "code_examples": [{"language": "spl", "code": "source=\"Ponies.csv\"NOT Location=\"Calaveras Farms\""}], "tables": [{"headers": ["ID", "Name", "Color", "Location"], "rows": [["101M3", "McIntosh", "Chestnut", "Marin Meadows"], ["104F5", "Lyra", "Bay", ""], ["104M6", "Rutherford", "Dun", "Placer Pastures"], ["101F2", "Rarity", "", "Marin Meadows"], ["102M1", "", "Roan", ""], ["101F6", "", "Chestnut", "Marin Meadows"], ["104F4", "Pinkie", "Sorrel", "Placer Pastures"]]}], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Difference betweenÂ != and NOT", "section_heading": "Searching with NOT", "section_id": "ba505aea_cfb9_437c_8a0d_9bd6f0823093--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/difference-between-and-not", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:21:28.310502+00:00", "version": "10.2"}}
{"id": "d6007129abc6aef9", "content": "If you use regular expressions in conjunction with != in searches, see the regex command in the Search Reference .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Difference betweenÂ != and NOT", "section_heading": "UsingÂ != with the regex command", "section_id": "a3793bda_c406_44b7_b273_1ade1512c7d0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/difference-between-and-not", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:21:28.310508+00:00", "version": "10.2"}}
{"id": "c30e574a2a3ed5ee", "content": "Using the != expression or NOT operator to exclude events from your search results is not an efficient method of filtering events. The execution cost for a search is actually less when you explicitly specify the values that you want to include in the search results. For more tips on search optimization, see Quick tips for optimization .", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Difference betweenÂ != and NOT", "section_heading": "Searching withÂ != or NOT is not efficient", "section_id": "abef2cad_481d_4f4e_a903_d6a6c401e356--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/difference-between-and-not", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:21:28.310514+00:00", "version": "10.2"}}
{"id": "f2359decdd04a0d1", "content": "You can use comparison operators to match a specific value or a range of field values. For example, to find events that have a delay field that is greater than 10:", "code_examples": [{"language": "spl", "code": "delay > 10"}], "tables": [{"headers": ["Operator", "Example", "Result"], "rows": [["=", "field=foo", "Multivalued field values that exactly match \"foo\"."], ["!=", "field!=foo", "Multivalued field values that don't exactly match \"foo\"."], ["<", "field<x", "Numerical field values that are less than x."], [">", "field>x", "Numerical field values that are greater than x."], ["<=", "field<=x", "Numerical field values that are less than and equal to x."], [">=", "field>=x", "Numerical field values that are greater than and equal to x."]]}], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Field expressions", "section_heading": "Use comparison operators to match field values", "section_id": "id_9a6354c8_c255_47af_a80d_9830ee45d685--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/field-expressions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:21:44.836623+00:00", "version": "10.2"}}
{"id": "e28b5652fae8b9e8", "content": "When the value you are searching for contains a breaking character, you must enclose the value in quotation marks. Examples of breaking characters are spaces, commas, pipes, square brackets, and equals signs. In addition, to search for reserved keywords such as AND, OR, and NOT you must use quotation marks.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Field expressions", "section_heading": "When quotes are required in field expressions", "section_id": "c9c3901a_5f86_4235_9414_c5fe52cec462--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/field-expressions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:21:44.836631+00:00", "version": "10.2"}}
{"id": "51540fcb15159a03", "content": "There are several field values that match SPL operators or keywords, such as AS, AND, IN, and OR. Here are a few examples: country=IN for India country=AS or state=AS for American Samoa iso=AND for Andorra state=OR for Oregon To search for field values that match operators or keywords, you must enclose the value in quotation marks. For example: country=\"IN\" .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Field expressions", "section_heading": "Field values that match SPL operators or keywords", "section_id": "c9c8740e_e957_4ef4_ab5b_f4e6859038ab--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/field-expressions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:21:44.836637+00:00", "version": "10.2"}}
{"id": "f5ba05b766d8858f", "content": "A pipe character ( | ) is used in regular expressions to specify an OR condition. For example, A | B means A or B. Because pipe characters are used to separate commands in SPL, you must enclose a regular expression that uses the pipe character in quotation marks. The following search shows how to use quotation marks around a pipe character, which is interpreted by SPL as a search for the text \"expression\" OR \"with pipe\"..", "code_examples": [{"language": "spl", "code": "...|regex\"expression | with pipe\""}], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "SPL and regular expressions", "section_heading": "Pipe characters", "section_id": "id_4d25006c_49a1_45ee_ba1b_38a59ffd2003--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/spl-and-regular-expressions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:01.407430+00:00", "version": "10.2"}}
{"id": "2ce52283307b7b07", "content": "The backslash character ( \\ ) is used in regular expressions to escape any special characters that have meaning in regular expressions, such as periods (. ), double quotation marks ( \" ), and backslashes themselves. For example, the period character is used in a regular expression to match any character, except a line break character. If you want to match a period character, you must escape the period character by specifying \\. in your regular expression. In searches that include a regular expression that contains a double backslash, like the file path c:\\\\temp , the search interprets the first backslash as a regular expression escape character. The file path is interpreted as c:\\temp , because one of the backslashes is removed. You must escape both backslash characters in the file path by specifying 4 consecutive backslashes for the root portion of the file path, such as c:\\\\\\\\temp. For a longer file path, such as c:\\\\temp\\example , you can specify c:\\\\\\\\temp\\\\example in your regular expression in the search string. One reason you might need extra escaping backslashes in your searches is that the Splunk platform parses text twice; once for SPL and then again for regular expressions. Each parse applies its own use of backslashes in layers and treats each backslash as a special character that needs an additional backslash to make it literal. As a result, \\\\ in SPL becomes \\ before it is parsed as a regular expression, and \\\\\\\\ in SPL becomes \\\\ before it is parsed as a regular expression. See Backslashes. Avoid extra escaping backslash characters To avoid using extra escaping backslashes in your searches, you can use the octal code \\134 or the hexadecimal code \\x5c in your regular expression. These codes are equivalent to the backslash character and get around the need to double-escape backslashes. For example, consider the following search, which extracts the characters ABC that follow 2 backslashes: The search results look something like this: Instead of using 3 backslashes, you can get the same search results using \\x5c in the regular expression, like this:", "code_examples": [{"language": "spl", "code": "| makeresults \n|evalexample=\"xyz\\\\ABC\"| rex field=example max_match=3\".*\\\\\\(?<extract>.*)\""}, {"language": "spl", "code": "| makeresults \n|evalexample=\"xyz\\\\ABC\"| rex field=example max_match=3\".*\\x5c(?<extract>.*)\""}], "tables": [{"headers": ["time", "example", "extract"], "rows": [["2023-09-20 17:20:59", "xyz\\ABC", "ABC"]]}], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "SPL and regular expressions", "section_heading": "Backslash characters in regular expressions", "section_id": "id_76a4151b_c86b_4d8e_a82c_7ebdc0c61e29--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/spl-and-regular-expressions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:01.407440+00:00", "version": "10.2"}}
{"id": "c5f35b14c6eb18b2", "content": "For more information: See Extract fields using regular expressions See About Splunk regular expressions in the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "SPL and regular expressions", "section_heading": "More about regular expressions", "section_id": "aa802a9b_b9e4_425d_9b7b_0804b2abee6d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/expressions-and-predicates/spl-and-regular-expressions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Expressions and Predicates", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:01.407445+00:00", "version": "10.2"}}
{"id": "4741a1954345c387", "content": "For more information: See SPL and regular expressions See Support for backslash characters ( \\ ) in the fields command in the Search Reference Manual. See Support for backslash characters ( \\ ) in the rename command in the Search Reference Manual. See Quotes and escaping characters in the search command in the Search Reference Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "Backslashes", "section_heading": "More about backslashes", "section_id": "c5dd9937_d7ff_4b64_b9c0_744f8e7fd7df--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/backslashes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:17.537581+00:00", "version": "10.2"}}
{"id": "e378f5dce6d2e628", "content": "If you specify an asterisk with no other criteria, you are asking to match everything. Yes, everything. All events are retrieved, up to the maximum limit. A search to match everything is both inefficient and time consuming. You'll use a lot of system resources, which can prevent others from running their searches. Additionally, you might wait a long time for your search results. To avoid these problems, be as specific as you can in your search criteria. The more specific your search terms are, the more efficient your search is. Sometimes that means not using a wildcard. Searching for a specific word or phrase is more efficient than a search that uses a wildcard. For example, searching for access denied is always better than searching for access* .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "Wildcards", "section_heading": "Be efficient and specific", "section_id": "id_01330d1a_827d_4bac_8fa2_bef9ab993422--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/wildcards", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:34.436718+00:00", "version": "10.2"}}
{"id": "6bc10cb1da64bc94", "content": "The best way to use a wildcard is at the end of a term, such as fail*. Specify a field-value pair whenever possible to avoid searching the raw field, which is the entire event. For example: status=fail*. When using wildcards in searches, carefully consider whether you're getting the results you expect. For example, the following searches containing wildcards do not return any events, as expected: NOT * sourcetype=_json NOT * sourcetype=* NOT * The following searches containing wildcards do return events, as expected: sourcetype=* index=*", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "Wildcards", "section_heading": "Best practices for using wildcards", "section_id": "id_0a72fa38_4b18_46f9_832f_23030a396371--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/wildcards", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:34.436726+00:00", "version": "10.2"}}
{"id": "e9644b9054ed8f47", "content": "If you want the inverse of a wildcard when using index= and NOT * , you must use (NOT *) to produce the most accurate search results. For example, the following searches do not return events, as expected. index=<index name> (NOT *) index=* (NOT *) The following searches incorrectly return events, because NOT * is not enclosed in parentheses. index=<index name> NOT * index=* NOT *", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "Wildcards", "section_heading": "Using wildcards with index= and NOT", "section_id": "id_157cf9ed_a98f_47b4_a34d_dc7f59bb998c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/wildcards", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:34.436730+00:00", "version": "10.2"}}
{"id": "b235fa99ed396ed5", "content": "There are several situations in which you should avoid using wildcard characters. Avoid using wildcards in the middle of a string Certain searches with wildcard characters in the middle of a word or string might cause inconsistent results. This is especially true if the string contains punctuation, such as an underscore _ or dash - character. If your search displays a warning message indicating that a term contains a wildcard with punctuation in the middle of a word or string, rewrite your search. For example, if your search contains the following string: Modify your search to use the following string instead: A search that uses a wildcard in the middle of the term returns inconsistent results because of the way in which data that contains punctuation is indexed and searched. For example, suppose you have the following list of product IDs. Say you create a search that looks for all of the product IDs that begin with the letter S and end in G01, like this: Your search will fail because of the way that events are indexed. When the events with the product IDs are indexed, the product IDs are broken up into segments. For example, the product ID SC-MG-G01 has these segments: SC , MG , G01. There is no segment that starts with an S and ends with G01, which is what the search productID=S*G01 specifies. Because there are no segments that match your search, no results are found. The solution to this problem? If the number of product IDs is small, specify the exact product IDs in your search rather than using a wildcard. For example: If the number of product IDs is large, use a lookup instead of a wildcard. To learn more about how punctuation can impact using wildcards, see Event segmentation and searching. Avoid using wildcards to match punctuation Punctuation are characters that are not numbers or letters. If you want to match part of a string that includes punctuation, specify each string with the punctuation that you are searching for. For example, you have the following values in the uri_path field in your events. You want to match every uri_path that starts with /cart. The problem is that the paths contain a forward slash ( / ) character and period (. ) character. Instead of specifying a wildcard character for the punctuation such as /cart* , specify the punctuation directly in your search criteria. For example, Avoid using wildcards as prefixes When you use a wildcard character at the beginning of a string, the search must look at every string to determine if the end of the string matches what you specify after the asterisk. Using a prefix wildcard is almost like using a wildcard by itself. Prefix wildcards might cause performance issues. Avoid using wildcards at the beginning of search terms.", "code_examples": [{"language": "spl", "code": "(host=sh-*splunkcloud* OR host=si-*splunkcloud*)"}, {"language": "spl", "code": "( ( host=sh-* OR host=si-*) host=*.splunkcloud* )"}, {"language": "spl", "code": "DB-SG-G01\nDC-SG-G02\nMB-AG-G07\nMB-AG-T01\nSC-MG-G01\nSF-BVS-G01\nSG-SH-G05\nWC-SH-A02\nWC-SH-G04"}, {"language": "spl", "code": "productID=S*G01"}, {"language": "spl", "code": "productID=SC-MG-G01 OR productID=SF-BVS-G01"}, {"language": "spl", "code": "/cart.do\t\n/cart/error.do\t\n/cart/success.do\n/category.screen\t\t\n/oldlink\n/product.screen\t\n/productscreen.html\n/show.do\t\n/stuff/logo.ico"}, {"language": "spl", "code": "...uri_path=/cart.do OR  uri_path=/cart/error.do OR  uri_path=/cart/success.do"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "Wildcards", "section_heading": "When to avoid wildcard characters", "section_id": "id_301f2af8_3f9f_46e4_aba3_b3d69738eecd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/wildcards", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:34.436735+00:00", "version": "10.2"}}
{"id": "46c6634fa2a73546", "content": "You can't search for the asterisk ( * ) character directly because the character is reserved as a wildcard. However, you can search for a term without the asterisk and then use either the where or regex command to filter the results. For example, to search for a term that contains an asterisk such as *78 , use these steps: First search for 78 without the asterisk, which returns all events that contain the number. Follow that with | regex _raw=\\*78 to return only those events that contain *78. The backslash ( \\ ) is used in the regular expression to not interpret, or escape, the asterisk character. See the regex command.", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "Wildcards", "section_heading": "Searching for the asterisk character", "section_id": "id_549c347a_88b1_4f87_b889_e3ab55f0155c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/wildcards", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:34.436740+00:00", "version": "10.2"}}
{"id": "51fac83368d70d05", "content": "The LIKE function supports using other wildcards for pattern matching. The percent ( % ) symbol is used as a wildcard for matching multiple characters. The underscore ( _ ) character is used to match a single character. These wildcards are only applicable to the LIKE function. See like (TEXT, PATTERN) in the list of Comparison and Conditional functions. You can use the percent ( % ) wildcard anywhere in the PATTERN.", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "Wildcards", "section_heading": "Other supported wildcards", "section_id": "eefd7762_6b4f_4234_81cc_8a09f43d1689--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/wildcards", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:34.436744+00:00", "version": "10.2"}}
{"id": "284a5aaabc5e5a93", "content": "Event segmentation occurs at index-time and at search-time. Index-time segmentation Index-time segmentation affects indexing and search speed, storage size, and the ability to use typeahead functionality in the Search bar. Search-time segmentation Search-time segmentation affects search speed and the ability to create searches by selecting items from the search results. For more information about the distinction between index-time segmentation and search-time segmentation see Index time versus search time in the Splunk Enterprise Managing Indexers and Clusters of Indexers manual. In this topic we are going to focus on search-time segmentation and how major and minor breakers impact searching.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "Event segmentation and searching", "section_heading": "Event segmentation at index-time and at search-time", "section_id": "f904a9a4_8f47_49c8_b632_750d0b5feb3f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/event-segmentation-and-searching", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:51.791226+00:00", "version": "10.2"}}
{"id": "e6959da5aa31158e", "content": "If your field values contain punctuation symbols such as quotation marks, periods, and colons, follow these best practices in your searches: To match a punctuation symbol, don't use a wildcard to match the symbol. Specify the actual symbol. Avoid using wildcards in the middle of a term. Wild cards used in the middle of a term might slow down search performance and might return inconsistent results if the term contains punctuation. Avoid using wildcards at the beginning of a value. Wild cards used as the first character in a value will slow down search performance. See Wildcards for more information about using wildcards in your search criteria. Punctuation symbols and segment tokens Many punctuation symbols are interpreted as major or minor breakers in event data. These breakers are used to parse the data into small segments. To search for values that contain punctuation, enclose the data in quotation marks. For example, an IP address such as 91.205.189.15 is broken into segment tokens based on the period character. 91 205 189 15 91.205 91.205.189 91.205.189.15 205.189 and so forth To search for this IP address, you must use quotation marks. The quotation marks tell the search to find the complete string \"91.205.189.15\". Other examples are a bit more complicated. Suppose you have data that sometimes appears with quotations and sometimes does not, such as app=\"uat_staging-mgr\" and app=uat_staging-mgr. The quotation mark ( \" ) is a major breaker. The equal sign ( = ) is a minor breaker. The data gets parsed into segments as shown in this table: The quotations around the data make a difference for the major tokens. For app=\"uat_staging-mgr\" , the quote is a major breaker and so you end up with these 2 segments: app= uat_staging-mgr Where as with app=uat_staging-mgmr , which does not have any part enclosed in quotations, there is no major breaker and the entire term is 1 segment.", "code_examples": [], "tables": [{"headers": ["Data", "Segments from major breakers", "Segments from minor breakers"], "rows": [["app=\"uat_staging-mgr\"", "app=uat_staging-mgr", "appuatstagingmgr"], ["app=uat_staging-mgr", "app=uat_staging-mgr", "appuatstagingmgr"]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "Event segmentation and searching", "section_heading": "Searching and punctuation symbols", "section_id": "e871b47b_3113_467c_9c3f_2b9bfd7231d7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/event-segmentation-and-searching", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:51.791236+00:00", "version": "10.2"}}
{"id": "1a43994fa7a6c0e4", "content": "Major breakers are a set of characters that are used to divide words, phrases, or terms in the event data into large tokens. Examples of major breakers are: A space A newline A tab Angle brackets < > Square brackets [ ] Parenthesis ( ) Curly brackets { } An exclamation point ! A question mark ? A semicolon ; A comma , Single and double quotation marks ' \" The ampersand sign & There are also multiple major breakers that use percent-encoding, primarily for reserved characters. These major breakers begin with a percent symbol followed by a code. For example, %21 is the code for the exclamation point ( ! ) character and %2526 is a double encoded ampersand ( && ). For a complete list of segmenters, see segmenters.conf file in the Splunk Enterprise Admin Manual. Here is an example of part of an event: This partial example gets segmented on the major breakers into the following tokens:", "code_examples": [{"language": "spl", "code": "91.205.189.15 - - [13/Jun/2025:18:22:16]\"GET /oldlink?itemId=EST-14&JSESSIONID=SD6SL7FF7ADFF53113 HTTP 1.1\""}, {"language": "spl", "code": "91.205.189.15\n-\n-\n13/Jun/2025:18:22:16\nGET\n/oldlink\nitemId=EST-14\nJSESSIONID=SD6SL7FF7ADFF53113\nHTTP\n1.1"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "Event segmentation and searching", "section_heading": "Major breakers", "section_id": "f7c03c70_5945_4c22_a15b_7e3add1d6381--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/event-segmentation-and-searching", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:51.791242+00:00", "version": "10.2"}}
{"id": "e8c08e0d526771f6", "content": "Minor breakers are a set of characters that are used to further divide large tokens into smaller tokens. Examples of minor breakers are: A period. A forward slash / A double backslash \\\\ A colon : The equal sign = The AT sign @ The pound sign # The dollar sign $ The percent sign % The dash sign - The underscore sign _ Note: A double backslash ( \\\\ ) is primarily used for Windows path segmentation. For a complete list of segmenters, see segmenters.conf file in the Splunk Enterprise Admin Manual .", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "Event segmentation and searching", "section_heading": "Minor breakers", "section_id": "id_8d676b8f_cbd8_49e1_8046_0ad5d2b3df01--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/event-segmentation-and-searching", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:51.791247+00:00", "version": "10.2"}}
{"id": "762b3ac04bcfc0e8", "content": "About event segmentation in the Getting Data In manual segmenters.conf file in the Splunk Enterprise Admin Manual", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "Event segmentation and searching", "section_heading": "See also", "section_id": "id_94ce273f_af12_4521_b925_fdb50551acd6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-primer/event-segmentation-and-searching", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Primer", "manual": "search-manual", "scraped_at": "2026-01-23T13:22:51.791251+00:00", "version": "10.2"}}
{"id": "5c2086ba0f65a216", "content": "Predicates act as filters to remove unnecessary events as your search is processed. The earlier in the search pipeline that a predicate is applied, the more efficient is the use of system resources. If a predicate is applied early, when events are fetched, only the events that match the predicate are fetched. If the same predicate is applied later in the search, then resources are wasted fetching more events than are necessary. There are several built-in optimizations that focus on predicates: Predicate merge Predicate pushdown Predicate splitting Types of predicates There are two types of predicates, simple predicates and complex predicates. A simple predicate is a single conditional of the form field = value. A complex predicate is a combination of several conjunctions (AND and OR) and disjunctions (NOT). For example, Field1 = Value1 OR Field2 = Value2 AND Field3 = Value3. A complex predicate can be merged or split apart to optimize a search. In Splunk SPL, there are two commands that perform predicate-based filtering, where and search. An example of using the where command for filtering is: An example of using the search command for filtering is: Search-based predicates are a subset of where-based predicates. In other words, search-based predicates can be replaced by where-based predicates. However, where-based predicates cannot be replaced by search-based predicates.", "code_examples": [{"language": "spl", "code": "index=\"_internal\"|wherebytes  > 10"}, {"language": "spl", "code": "index=\"_internal\"| search bytes  > 10"}], "tables": [], "chunk_index": 0, "total_chunks": 10, "metadata": {"title": "Built-in optimization", "section_heading": "Predicate optimization", "section_id": "id_694665d2_2445_42dd_8a31_1f40a12e16d8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/built-in-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:08.829636+00:00", "version": "10.2"}}
{"id": "db315fec16050f4d", "content": "The predicate merge optimization takes two predicates and merges the predicates into one predicate. For example, consider the following search: There are two search commands in this example. The search command before the pipe, which is implied at the beginning of every search, and the explicit search command after the first pipe. With the predicate merge optimization, the predicates in the second search command are merged with the predicates in the first search command. For example: In some cases, predicates used with the where command can also be merged. For example, consider the following search: With the predicate merge optimization, the predicates specified with the where command are merged with the predicates specified with the search command. Issues with field extractions and predicate merge An inline field extraction requires special handling if the regular expression pattern extracts a subtoken. The field must be set to indexed=false in the fields.conf file. See Example inline field extraction configurations in the Knowledge Manager Manual. Consider the following sample event: You create an extracted field called country that uses the following regular expression: SUCCESS matches the characters SUCCESS literally and is case sensitive. \\s matches any whitespace character (space, tab, new line). +? is a quantifier that matches between one and unlimited times, the fewest needed to match the pattern. \\S matches any non-whitespace character. {3} is a quantifier that matches exactly 3 non-whitespace characters For the sample event, the following regular expression extracts the first 3 characters of the word FRANCE or FRA. The extraction FRA is a subtoken of the indexed term FRANCE. When you search using the extracted field, for example: The search is optimized, using the predicate merge optimizer: However, the search returns no results because FRA is not part of the index. FRANCE is the indexed term. To overcome this issue, you must add the following setting to the fields.conf file: Alternatively, you can disable the built-in optimizations. See Optimization settings .", "code_examples": [{"language": "spl", "code": "index=main |  search a > 10  AND  fieldA =\"New\""}, {"language": "spl", "code": "index=main AND a > 10  AND fieldA =\"New\""}, {"language": "spl", "code": "index=main |wherefieldA =\"New\""}, {"language": "spl", "code": "index=main AND fieldA =\"New\""}, {"language": "spl", "code": "Mon Apr 17 16:08:16 2017 host=10.10.1.1  Login  name=John  SUCCESS FRANCE"}, {"language": "spl", "code": "SUCCESS\\s+?\\S{3}"}, {"language": "spl", "code": "index=main | search country=FRA"}, {"language": "spl", "code": "index=main country=FRA"}, {"language": "spl", "code": "[country]\nindexed=false"}], "tables": [], "chunk_index": 1, "total_chunks": 10, "metadata": {"title": "Built-in optimization", "section_heading": "Predicate merge", "section_id": "id_3c99429f_963c_4d53_a1b0_990873cb7e44--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/built-in-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:08.829644+00:00", "version": "10.2"}}
{"id": "0c13a1ff7ceb25ba", "content": "The predicate pushdown optimization analyzes a search and reorders the search processing so that predicates are processed as early as possible. Example of a simple predicate pushdown You perform the following search: The sourcetype=access* (status=401 OR status=403) portion of the search retrieves 50,000 events. The lookup is performed on all 50,000 events. Then the where command is applied and filters out events that are not src_category=\"email_server\". The result is that 45,000 events are discarded and 5,000 events are returned in the search results. If the search criteria used with the where command is applied before the lookup , more events are filtered out of the results. Only 5,000 events are retrieved from disk before the lookup is performed. With predicate pushdown, the search is reordered for processing. The where command is eliminated by moving the search criteria src_category=\"email_server\" before the first pipe. Example of a complex predicate pushdown Consider the following search fragment: In this situation, the eval command is assigning a default value when a field is null or empty. This is a common pattern in Common Information Model (CIM) data models. The built-in optimization reorganizes the search criteria before processing the search. The where command is moved before the eval command.", "code_examples": [{"language": "spl", "code": "sourcetype=access* (status=401 OR status=403) | lookup usertogroup user OUTPUT group |wheresrc_category=\"email_server\""}, {"language": "spl", "code": "sourcetype=access* (status=401 OR status=403) src_category=\"email_server\"| lookup usertogroup user OUTPUT group"}, {"language": "spl", "code": "... |evalx=if(isnull(x) OR x==\"\",\"missing\", x) |wherex =\"hello\""}, {"language": "spl", "code": "... |wherex =\"hello\"|evalx=if(isnull(x) OR x==\"\",\"missing\", x)"}], "tables": [], "chunk_index": 2, "total_chunks": 10, "metadata": {"title": "Built-in optimization", "section_heading": "Predicate pushdown", "section_id": "b9536bae_defb_4ea7_917c_6bc9dbff143c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/built-in-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:08.829649+00:00", "version": "10.2"}}
{"id": "31f47baff5a738a0", "content": "Predicate splitting is the action of taking a predicate and dividing, or splitting, the predicate into smaller predicates. The predicate splitting optimizer can then move the smaller predicates, when possible, to an earlier place in the search. Consider the following search: Because field a is generated as part of the eval command, it cannot be processed earlier in the search. However, field x exists in the events and can be processed earlier. The predicate splitting optimization separates the components of the where portion of the search. The search is reordered to process eligible components earlier. The portion of the where command x < 10 is moved to before the eval command. This move reduces the number of results that the eval command must process.", "code_examples": [{"language": "spl", "code": "index=\"_internal\"component =\"SearchProcess\"|evala = (x + y) |wherea > 200 AND x < 10"}, {"language": "spl", "code": "index=\"_internal\"component =\"SearchProcess\"|wherex < 10 |evala = (x + y) |wherea > 200"}], "tables": [], "chunk_index": 3, "total_chunks": 10, "metadata": {"title": "Built-in optimization", "section_heading": "Predicate splitting", "section_id": "id_490620df_ca6c_4cda_b581_bc0d2e88c608--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/built-in-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:08.829653+00:00", "version": "10.2"}}
{"id": "89b4ac2174300deb", "content": "This optimization analyzes your search and determines if any of the generated fields specified in the search are not used to produce the search results. If generated fields are identified that can be eliminated, an optimized version of the search is run. Your search syntax remains unchanged. For example, consider the following search: The c field that is generated by eval c = x * y / z is not used in the stats calculation. The c field can be eliminated from the search. Your search syntax remains: But the optimized search that is run is: Here is another example: For buckets whose data models are not built, this expands to the following fallback search: This search can be optimized to this syntax: Eliminating unnecessary generated fields, or projections, leads to better search performance.", "code_examples": [{"language": "spl", "code": "index=_internal |evalc = x * y / z | stats count BY a, b"}, {"language": "spl", "code": "index=_internal |evalc = x * y / z | stats count BY a, b"}, {"language": "spl", "code": "index=_internal | stats count BY a, b"}, {"language": "spl", "code": "| tstats count FROM datamodel=internal_audit_logs"}, {"language": "spl", "code": "| search ((index=* OR index=_*) index=_audit) \n|evalnodename =\"Audit\"|evalis_searches=if(searchmatch(\"(action=search NOT dmauditsearch)\"),1,0), \n  is_not_searches=1-is_searches, is_modify=if(searchmatch(\"(action=edit_user \n  OR action=edit_roles OR action=update)\"),1,0), is_not_modify=1-is_modify \n|evalnodename =if(nodename ==\"Audit\"AND searchmatch(\"(action=search NOT dmauditsearch)\"), mvappend(nodename,\"Audit.searches\"), nodename) \n|evalis_realtime=case(is_realtime == 0,\"false\", is_realtime == 1,\"true\", \n  is_realtime ==\"N/A\",\"false\"), search_id=replace(search_id,\"'\",\"\"), \n  search=replace(search,\"'\",\"\"), search_type=case((id LIKE\"DM_%\"OR savedsearch_name LIKE\"_ACCELERATE_DM%\"),\"dm_acceleration\", \n  search_id LIKE\"scheduler%\",\"scheduled\", search_id LIKE\"rt%\",\"realtime\", \n  search_id LIKE\"subsearch%\",\"subsearch\", (search_id LIKE\"SummaryDirector%\"OR search_id LIKE\"summarize_SummaryDirector%\"),\"summary_director\", \n  1=1,\"adhoc\") \n| rename is_realtime AS Audit.searches.is_realtime \n  search_id AS Audit.searches.search_id \n  search AS Audit.searches.search \n  search_type AS Audit.searches.search_type \n|evalnodename =if(nodename ==\"Audit\"AND searchmatch(\"(action=edit_user \n  OR action=edit_roles OR action=update)\"), mvappend(nodename,\"Audit.modify\"), nodename) \n| rename action AS Audit.action info AS Audit.info object AS Audit.object \n  operation AS Audit.operation path AS Audit.path user AS Audit.user \n  exec_time AS Audit.exec_time result_count AS Audit.result_count \n  savedsearch_name AS Audit.savedsearch_name \n  scan_count AS Audit.scan_count total_run_time AS Audit.total_run_time \n  is_searches AS Audit.is_searches is_not_searches AS Audit.is_not_searches \n  is_modify AS Audit.is_modify is_not_modify AS Audit.is_not_modify \n| addinfotype=count label=prereport_events \n| stats count"}, {"language": "spl", "code": "| search ((index=* OR index=_*) index=_audit) | stats count"}], "tables": [], "chunk_index": 4, "total_chunks": 10, "metadata": {"title": "Built-in optimization", "section_heading": "Projection elimination", "section_id": "id_59ff266a_8ee9_4727_95a7_8c335494a3d2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/built-in-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:08.829656+00:00", "version": "10.2"}}
{"id": "548d2f122c33f762", "content": "This optimization converts qualifying stats , chart , or timechart searches into tstats searches. When the search processor applies this optimization to qualifying stats , chart , or timechart searches that aggregate large amounts of data, such searches can complete far faster than they otherwise would. This increase in efficiency happens because tstats performs statistical searches on indexed fields in tsidx index summary files rather than the raw data. For example, consider this search: The stats / chart / timechart to tstats optimizer optimizes that search to something like this: How searches qualify for stats/chart/timechart to tstats optimization The search optimizer applies stats / chart / timechart to tstats optimization only to stats , chart , and timechart searches that meet an array of conditions. Turning off the stats/chart/timechart to tstats optimizer on specific searches The stats / chart / timechart to tstats optimizer can cause certain kinds of searches to run slower. For example, this optimizer does not cooperate with searches that populate summary indexes , because it does not respect the fields created by summary indexing commands. This optimizer may cause searches that perform stats , chart , or timechart operations on summary indexes to perform extra work on a fallback search, thus slowing their performance. You can turn off this optimization for individual searches. If you want to run a stats search without the optimization, append | noop search_optimization.replace_stats_cmds_with_tstats=f to the search string. If you want to run a chart or timechart search without the optimization, append | noop search_optimization.replace_chart_cmds_with_tstats=f to the search string.", "code_examples": [{"language": "spl", "code": "index=_internal sourcetype=splunkd  | stats count"}, {"language": "spl", "code": "| tstats prestats=truesummariesonly=falseallow_old_summaries=falseinclude_reduced_buckets=truecount WHERE (index=_internal sourcetype=splunkd) | stats count"}, {"language": "spl", "code": "index=_internal sourcetype=splunkd | stats count"}], "tables": [{"headers": ["Qualification for stats/chart/timechart to tstats search optimization", "More information"], "rows": [["The search must have an explicit or implicitsearchcommand followed by astats,chart, ortimechartcommand.", "Here is an example of a search that fits this pattern:index=_internal sourcetype=splunkd | stats count"], ["Thesearchportion of the search string and thestats,chart, ortimechartportion of the search string reference onlyindexed fields.", "Thetstatscommand can perform searches only on indexed fields.For more information about indexed fields and search-time fields, seeWhen Splunk software extracts fieldsin theKnowledge Manager Manual."], ["The search does not havesearch-timefield extractions that overwrite the indexed fields in thesearchand thestats,chart, ortimechartportions of the search.", "For example, a search with a search-time field extraction for a calculated field that overwrites thehostfield would not be optimized.At run time, the optimizer reviews all possible configurations to which the user has access. If any of these configurations might lead to overwriting of indexed fields with search-time fields, the optimizer disqualifies the search fortstatsoptimization.SeeWhen Splunk software extracts fieldsin theKnowledge Manager Manual."], ["The search uses only statistical or charting functions that are supported by thetstatscommand.", "For example, theearliest,latest,earliest_time, andlatest_timefunctions are not supported bytstats. If the search uses those functions in conjunction with thestats,chart, ortimechartcommands, it is disqualified fortstatsoptimization.See thetstatstopic."], ["The search filters out source types that invokemultikvto extract values from table-formatted events.", "These are source types that are configured withKV_MODE=multiin theAdvancedtab of the Edit Source Types dialog box.If a search does not explicitly filter out source types, the optimizer then checks all source types available to the search. If the optimizer finds any source types that are configured withKV_MODE=multi, it does not applystats/chart/timecharttotstatsoptimization to the search.SeeManage source typesin theGetting Data Inmanual."], ["The search does not invokeverbose mode.", "SeeSearch modes."], ["The search is not areal-time search.", "SeeAbout real-time searches and reports."], ["The search does not require subsecond timestamp resolution.", "Searches that require result timestamps to have millisecond or smaller time resolution are disqualified.SeeAbout searching with time."], ["The search does not use event sampling.", "SeeEvent sampling."], ["The search does not includesubsearches.", "SeeAbout subsearches."], ["The search is not running in an environment that has been configured to rename source types at search time.", "SeeRename source typesin theGetting Data Inmanual."], ["The search is not running in an environment wherestatus_buckets > 0.", "A positive value for thestatus_bucketssetting causes timelines to be generated bystatssearches, which disqualifies them fortstatsoptimization. Thestatus_bucketssetting can be set through a POST operation to the search/jobs REST API endpoint, or through a direct change to thestatus_bucketssetting inlimits.conf.See theSearch endpoint descriptionsin theREST API Reference Manual."]]}], "chunk_index": 5, "total_chunks": 10, "metadata": {"title": "Built-in optimization", "section_heading": "Stats/chart/timechart to tstats optimization", "section_id": "id_2c580aa5_95e7_48dc_a744_9a192e490db8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/built-in-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:08.829664+00:00", "version": "10.2"}}
{"id": "372873657c8c7583", "content": "Event typing and tagging can take a significant amount of search processing time. This is especially true with Splunk Enterprise Security where there are many event types and tags defined. The more event types and tags that are defined in the system, the greater the annotation costs. The built-in event types and tags optimization applies to transforming searches and data model acceleration searches. Transforming searches When a transforming search is run without the built-in optimization, all of the event types and tags are uploaded from the configuration system to the search processor. This upload happens whether the search requires the event types and tags or not. The search processing attempts to apply the event types and tags to each event. For example, consider the following search: This search needs only the tag foo. But without the optimization, all of the event types and tags are uploaded. The built-in optimization solves this problem by analyzing the search and only uploading the event types and tags that are required. If no event types or tags are needed, none are uploaded which improves search performance. For transforming searches, this optimization is controlled by several settings in the limits.conf file, under the [search_optimization::required_field_values] stanza. These settings are turned on by default. There is no need to change these settings, unless you need to troubleshoot an issue. Data model acceleration searches Another problem that can be solved by the event type and tags optimization is specifically targeted at data model acceleration. With a configuration setting in the datamodels.conf file, you can specify a list of tags that you want to use with the data model acceleration searches. You specify the list under the [dm.name] stanza with the tags_whitelist setting. For detailed information about the tags_whitelist setting, including usage examples, see Set a tag whitelist for better data model performance in the Knowledge Manager Manual .", "code_examples": [{"language": "spl", "code": "index=_internal | search tag=foo | stats count by host"}], "tables": [], "chunk_index": 6, "total_chunks": 10, "metadata": {"title": "Built-in optimization", "section_heading": "Event types and tags optimization", "section_id": "eeaad0ea_9f18_47c6_8dc0_c7f6ac9a8f14--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/built-in-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:08.829667+00:00", "version": "10.2"}}
{"id": "86032c11cafbbae7", "content": "You can analyze the impact of the built-in optimizations by using the Job Inspector. Determine search processing time You can use the Job Inspector to determine whether the built-in search optimizations are helping your search to complete faster. Run your search. From the search action buttons, select Job > Inspect job. In the Job Inspector window, review the message at the top of the window. The message is similar to \"The search has completed and returned X results by scanning X events in X seconds.\" Make note of the number of seconds to complete the search. Close the Job inspector window. In the Search bar, add |noop search_optimization=false to the end of your search. This turns off the built-in optimizations for this search. Run the search. Inspect the job and compare the message at the top of the Job Inspector window with the previous message. This message specifies how many seconds the search processing took without the built-in optimizations. View the optimized search You can compare your original search with the optimized search that is created when the built-in optimizations are turned on. Run your search. From the search action buttons, select Job , Inspect job. In the Job Inspector window, expand the Search job properties section and scroll to the normalizedSearch property. This property shows the internal search that is created based on your original search. Scroll to the optimizedSearch property. This property shows the search that is created, based on the normalizedSearch , when the built-in optimizations are applied.", "code_examples": [], "tables": [], "chunk_index": 7, "total_chunks": 10, "metadata": {"title": "Built-in optimization", "section_heading": "Analyze search optimizations", "section_id": "d436cf55_5f4d_47ea_9145_1f56955b0531--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/built-in-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:08.829671+00:00", "version": "10.2"}}
{"id": "9a04d20d6a55b1d3", "content": "By default, the built-in optimizations are turned on. Turn off optimization for a specific search In some very limited situations, the optimization that is built into the search processor might not optimize a search correctly. In these situations, you can troubleshoot the problem by turning off the search optimization for that specific search. Turning off the search optimization enables you to determine if the cause of unexpected or limited search results is the search optimization. You turn off the built-in optimizations for a specific search by using the noop command. You add noop search_optimization=false at the end of your search. For example: Turn off all optimizations You can turn off all of the built-in optimizations for all users. Splunk Cloud Platform To turn off the built-in optimizations for all users, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Enterprise To turn off the built-in optimizations for all users, follow these steps. Prerequisites Only users with file system access, such as system administrators, can turn off the built-in optimizations using a configuration file. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. You can have configuration files with the same name in your default, local, and app directories. Read Where you can place (or find) your modified configuration files in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Steps Open or create a local limits.conf file for the Search app at $SPLUNK_HOME/etc/apps/search/local. Under the [search_optimization] stanza, set enabled=false .", "code_examples": [{"language": "spl", "code": "| datamodel Authentication Successful_Authentication search |whereAuthentication.user =\"fred\"| noop search_optimization=false"}], "tables": [], "chunk_index": 8, "total_chunks": 10, "metadata": {"title": "Built-in optimization", "section_heading": "Optimization settings", "section_id": "id_1524ff90_1c9c_48ab_b436_4a79e4477e22--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/built-in-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:08.829674+00:00", "version": "10.2"}}
{"id": "3ceb1d41b1c50ef8", "content": "In this manual: About optimization Quick tips for optimization Write better searches In the Search Reference : The noop command", "code_examples": [], "tables": [], "chunk_index": 9, "total_chunks": 10, "metadata": {"title": "Built-in optimization", "section_heading": "See also", "section_id": "id_128efd12_5ce7_4f0d_92df_680faca1bb8f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/built-in-optimization", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:08.829678+00:00", "version": "10.2"}}
{"id": "5f1f924649f528a5", "content": "The recommendations for optimizing searches depend on the type of search that you run and the characteristics of the data you are searching. Searches fall into two types, that are based on the goal you want to accomplish. Either a search is designed to retrieve events or a search is designed to generate a report that summarizes or organizes the data. See Types of searches. Searches that retrieve events Raw event searches retrieve events from a Splunk index without any additional processing of the events that are retrieved. When retrieving events from the index, be specific about the events that you want to retrieve. You can do this with keywords and field-value pairs that are unique to the events. If the events you want to retrieve occur frequently in the dataset, the search is called a dense search. If the events you want to retrieve are rare in the dataset, the search is called a sparse search. Sparse searches that run against large volumes of data take longer than dense searches against the same data set. See Use fields to retrieve events. Searches that generate reports Report-generating searches, or transforming searches, perform additional processing on events after the events are retrieved from an index. This processing can include filtering, transforming, and other operations using one or more statistical functions against the set of results. Because this processing occurs in memory, the more restrictive and specific you are retrieving the events, the faster the search will run. See About transformating commands and searches .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 7, "metadata": {"title": "Write better searches", "section_heading": "Know your type of search", "section_id": "a7eacf8c_4354_44d7_84a3_0171ec2db862--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/write-better-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:25.644488+00:00", "version": "10.2"}}
{"id": "e432fc9ee0029477", "content": "Some commands process events in a stream. There is one event in and one, or no, event out. These are referred to as streaming commands. Examples of streaming commands are where , eval , lookup , and search. Other commands require all of the events from all of the indexers before the command can finish. These are referred to as non-streaming commands. Examples of non-streaming commands are stats , sort , dedup , top , and append. Non-streaming commands can run only when all of the data is available. To process non-streaming commands, all of the search results from the indexers are sent to the search head. When this happens, all further processing must be performed by the search head, rather than in parallel on the indexers. Parallel processing example Non-streaming commands that are early in your search reduce parallel processing. For example, the following image shows a search that a user has run. The search starts with the search command, which is implied as the first command in the Search bar. The search continues with the lookup , where , and eval commands. The search then contains a sort , based on the Name field, followed by another where command. The search is sent to the search head and distributed to the indexers to process as much of the search as possible on the indexers. For the events that are on each indexer, the indexer processes the search until the indexer encounters a non-streaming command. In this example, the indexers process the search through the eval command. To perform the sort , all of the results must be sent to the search head for processing. However, the results that are on each indexer can be sorted by the indexer. This is referred to as a presort. In this example the sort is on the Name field. The following image shows that the first indexer returns the names Alex and Maria. The second indexer returns the name Wei. The third indexer returns the names Claudia, David, and Eduardo. To return the full list of results sorted by name, all of the events that match the search criteria must be sent to the search head. When all of the results are on the search head, the rest of the search must be processed on the search head. In this example the sort and any remaining commands are processed on the search head. The following image shows that each indexer has presorted the results, based on the Name field. The results are sent to the search head, and are appended together. The search head then sorts the entire list into the correct order. The search head processes the remaining commands in the search to produce the final results. In this example, that includes the second where command. The final results are returned to the user. When part or all of a search is run on the indexers, the search processes in parallel and search performance is much quicker. To optimize your searches, place non-streaming commands as late as possible in your search string.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 7, "metadata": {"title": "Write better searches", "section_heading": "Command types and parallel processing", "section_id": "de5da3b5_f64a_477d_8ef4_d58cbb9f1061--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/write-better-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:25.644496+00:00", "version": "10.2"}}
{"id": "7551385eead28476", "content": "In most cases, your search is slow because of the complexity of your query to retrieve events from index. For example, if your search contains extremely large OR lists, complex subsearches (which break down into OR lists), and types of phrase searches, it takes longer to process. This section discusses some tips for tuning your searches so that they are more efficient. Performing statistics with a BY clause on a set of field values that have a high cardinality, lots of uncommon or unique values, requires a lot of memory. One possible remedy is to decrease the value for the chunk_size setting used with the tstats command. Additionally, reducing the number of distinct values that the BY clause must process can also be beneficial. Restrict searches to the specific index If you rarely search across more than one type of data at a time, partition your different types of data into separate indexes. Then restrict your searches to the specific index. For example, store Web access data in one index and firewall data in another. Use separate indexes for sparse data, which might otherwise be buried in a large volume of unrelated data. See Create custom indexes in Managing Indexers and Clusters of Indexers and Retrieve events from indexes in this manual.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 7, "metadata": {"title": "Write better searches", "section_heading": "Tips for tuning your searches", "section_id": "id_459f4564_830f_4def_8726_29a10d58078f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/write-better-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:25.644500+00:00", "version": "10.2"}}
{"id": "c8515ba8bf60930a", "content": "Searches with fields are faster when they use fields that have already been extracted (indexed fields) instead of fields extracted at search time. For more information about indexed fields and default fields, see About fields in the Knowledge Manager Manual. Use indexed and default fields Use indexed and default fields whenever you can to help search or filter your data efficiently. At index time, Splunk software extracts a set of default fields that are common to each event. These fields include host , source , and sourcetype. Use these fields to filter your data as early as possible in the search so that processing is done on a minimum amount of data. For example, if you're building a report on web access errors, search for those specific errors before the reporting command: Specify indexed fields with <field>::<value> You can also run efficient searches for fields that have been indexed from structured data such as CSV files and JSON data sources. When you do this, replace the equal sign with double colons, like this: <field>::<value>. This syntax works best in searches for fields that have been indexed from structured data, though it can be used to search for default and custom indexed fields as well. You cannot use it to search on search-time fields. Turn off field discovery to improve search performance If you don't need additional fields in your search, set Search Mode to a setting that turns off field discovery to improve search performance in the timeline view or use the fields command to specify only the fields that you want to see in your results. However, disabling field discovery prevents automatic field extraction , except for fields that are required to fulfill your search, such as fields that you are specifically searching on and default fields such as _time , host , source , and sourcetype. The search runs faster because Splunk software is no longer trying to extract every field possible from your events. Search mode is set to Smart by default. Set the search mode to Verbose if you are running searches with reporting commands, you don't know what fields exist in your data, and think you might need the fields to help you narrow down your search. See Set search modes. Also see the topic about the fields command in the Search Reference .", "code_examples": [{"language": "spl", "code": "sourcetype=access_* (status=4* OR status=5*) | stats count by status"}], "tables": [], "chunk_index": 3, "total_chunks": 7, "metadata": {"title": "Write better searches", "section_heading": "Use fields effectively", "section_id": "fdd28b52_7d52_4a3b_9b12_b77edca35505--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/write-better-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:25.644505+00:00", "version": "10.2"}}
{"id": "21a8984b596f9256", "content": "It can take a lot of time to search through very large data sets. If you regularly generate reports on large volumes of data, use summary indexing to pre-calculate the values that you use most often in your reports. Schedule saved searches to collect metrics on a regular basis, and report on the summarized data instead of on raw data. See Use summary indexing for increased reporting efficiency .", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 7, "metadata": {"title": "Write better searches", "section_heading": "Summarize your data", "section_id": "id_96ac5670_48f9_41d2_bb6f_8bf314e13284--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/write-better-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:25.644508+00:00", "version": "10.2"}}
{"id": "3aac83d372f18583", "content": "The Search Job Inspector is a tool you can use both to troubleshoot the performance of a search and to determine which phase of the search takes the greatest amounts of time. It dissects the behavior of your searches to help you understand the execution costs of knowledge objects such as event types, tags, lookups, search commands, and other components within the search. See View search job properties in this manual.", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 7, "metadata": {"title": "Write better searches", "section_heading": "Use the Search Job Inspector", "section_id": "id_14bad1c8_77e2_451e_962b_281405d166f0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/write-better-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:25.644512+00:00", "version": "10.2"}}
{"id": "dc90101462bb6031", "content": "About search optimization Quick tips for optimization Built-in optimizations", "code_examples": [], "tables": [], "chunk_index": 6, "total_chunks": 7, "metadata": {"title": "Write better searches", "section_heading": "See also", "section_id": "d624d09b_e4c1_4d0e_92f9_56a00385aba7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/optimize-searches/write-better-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Optimize Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:25.644515+00:00", "version": "10.2"}}
{"id": "7b8f5f9fb352b358", "content": "The Splunk administrator can set the default indexes that a user searches. Based on the roles and permissions, the user might have access to one or many indexes. For example the user might be able to only search main or all public indexes. The user can then specify a subset of these indexes, either an individual index or multiple indexes, to search. For more information about setting up users and roles, see \"About users and roles\" in Securing Splunk Enterprise. For more information about managing your indexes and setting up multiple indexes, see the \"About managing indexes\" in the Managing Indexers and Clusters manual. Control index access using Splunk Web 1. Navigate to Settings > Roles. 2. Click the role that the User has been assigned to. 3. Click on \"3. Indexes\". 4. Control the indexes that particular role has access to, as well as the default search indexes. Syntax You can specify different indexes to search in the same way that you specify field names and values. In this case, the field name is index and the field value is the name of a particular index: Specify groups of indexes using wildcards You can use a wildcard ( * ) to specify groups of indexes. For example, if you want to search both \"mail\" and \"main\" indexes, search for: To match internal indexes using a wildcard, use _* in your search, like this: You can use a wildcard to to match all of the non-internal indexes or all of the internal indexes. But, you can't use a wildcard to match both types of indexes at the same time. Partition different searches using parentheses You can also use parentheses to partition different searches to certain indexes. See Example 3 for details. Note: When you type \"index=\" into the search bar, typeahead indicates all the indexes that you can search, based on your roles and permissions settings. Examples Example 1: Search across all public indexes. Example 2: Search across all indexes, public and internal. Example 3: Partition different searches to different indexes; in this example, you're searching three different indexes: main, _internal, and mail. You want to see events that match \"error\" in all three indexes; but also, errors that match \"warn\" in main or \"failed\" in mail. Example 4: Search across multiple indexes on different distributed Splunk servers.", "code_examples": [{"language": "spl", "code": "index=<indexname>"}, {"language": "spl", "code": "index=mai*"}, {"language": "spl", "code": "index=_*"}, {"language": "spl", "code": "index=*"}, {"language": "spl", "code": "index=* OR index=_*"}, {"language": "spl", "code": "(index=main (error OR warn)) OR (index=_internal error) OR (index=mail (error OR failed))"}, {"language": "spl", "code": "(splunk_server=localindex=main 404 ip=10.0.0.0/16) OR (splunk_server=remote index=mail user=admin)"}], "tables": [], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Retrieve events from indexes", "section_heading": "Specify one or multiple indexes to search", "section_id": "id_9d195bf2_0043_4f62_974f_e9bbf22181a3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/retrieve-events-from-indexes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:42.905720+00:00", "version": "10.2"}}
{"id": "303a2da936850ee2", "content": "When you add an input, the input gets added relative to the app you're in. Some apps write input data to their own specific index (for example, the Splunk App for Unix and Linux uses the 'os' index).", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Retrieve events from indexes", "section_heading": "Not finding the events you're looking for?", "section_id": "b4222bfb_c477_4fe5_87e3_d6de8993910f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/retrieve-events-from-indexes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:23:42.905728+00:00", "version": "10.2"}}
{"id": "8df0b89d3cbe943f", "content": "SPL commands consist of required and optional arguments. Required arguments are shown in angle brackets < >. Optional arguments are enclosed in square brackets [ ]. Consider this command syntax: bin [<bins-options>...] <field> [AS <newfield>] The required argument is <field>. To use this command, at a minimum you must specify bin <field>. The optional arguments are [<bins-options>...] and [AS <newfield>] .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "Required and optional arguments", "section_id": "a6da4467_f717_4d84_bd17_a50b9c0f0355--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414288+00:00", "version": "10.2"}}
{"id": "350cd92e57b70e15", "content": "Consider this command syntax: replace (<wc-string> WITH <wc-string>)... [IN <field-list>] The user input arguments are: <wc-string> and <field-list>. The argument <wc-string> is an abbreviation for <wildcard-string> and indicates that the argument accepts a wildcard character in the string that you provide. See Wildcards in the Search Reference .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "User input  arguments", "section_id": "id_384ad6f7_d840_4d3c_a81e_f159f9f7e57a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414296+00:00", "version": "10.2"}}
{"id": "5e95d4b0775e38f0", "content": "Some arguments can be specified multiple times. The syntax displays ellipsis ... to specify which part of an argument can be repeated. The ellipsis always appear immediately after the part of the syntax that you can repeat. Consider this command: convert [timeformat=string] (<convert-function> [AS <field>])... The required argument is <convert-function> , with an option to specify a field with the [AS <field>] clause. Notice the ellipsis at the end of the syntax, just after the close parenthesis. In this example, the syntax that is inside the parenthesis can be repeated <convert-function> [AS <field>]. In the following syntax, you can repeat the <bins-options>... bin [<bins-options>...] <field> [AS <newfield>]", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "Repeating arguments", "section_id": "id_11700507_f89a_40ea_91db_2c1cc9da492b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414301+00:00", "version": "10.2"}}
{"id": "7320685000520aed", "content": "Sometimes the syntax must display arguments as a group to show that the set of arguments are used together. Parenthesis ( ) are used to group arguments. For example in this syntax: replace (<wc-string> WITH <wc-string>)... [IN <field-list>] The grouped argument is (<wc-string> WITH <wc-string>)... This is a required set of arguments that you can repeat multiple times.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "Grouped arguments", "section_id": "id_7960295b_f3a8_45f5_9c2c_858791861266--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414305+00:00", "version": "10.2"}}
{"id": "61b9807b557b2e16", "content": "Many commands use keywords with some of the arguments or options. Examples of keywords include: AS BY OVER WHERE You can specify these keywords in uppercase or lowercase in your search. However, for readability, the syntax in the Splunk documentation uses uppercase on all keywords.", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "Keywords", "section_id": "a11e087a_e259_46ef_8cdd_c99ca9a156fb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414310+00:00", "version": "10.2"}}
{"id": "3cb1848c98800187", "content": "If an element is in quotation marks, you must include that element in your search. The most common quoted elements are parenthesis. Consider the syntax for the chart command: There are quotation marks on the parenthesis surrounding the <eval-expression>. This means that you must enclose the <eval-expression> in parenthesis in your search. In the following search example, the <eval-expression> is avg(size)/max(delay) and is enclosed in parenthesis.", "code_examples": [{"language": "spl", "code": "chart [<chart-options>] [agg=<stats-agg-term>]\n( <stats-agg-term> | <sparkline-agg-term> |\"(\"<eval-expression>\")\")...\n[ BY <row-split> <column-split> ] | [ OVER <row-split> ] [BY <column-split>] ]"}, {"language": "spl", "code": "... | charteval(avg(size)/max(delay)) AS ratio BY host user"}], "tables": [], "chunk_index": 5, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "Quoted elements", "section_id": "id_950b8e29_cea3_4de4_bd6f_48fa4776585a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414315+00:00", "version": "10.2"}}
{"id": "68810272b088a953", "content": "In the command syntax, the command arguments are presented in the order in which the arguments are meant to be used. In the descriptions of the arguments, the Required arguments and Optional argument sections, the arguments are listed alphabetically. For each argument, there is a Syntax and Description. Additionally, for Optional arguments, there might be a Default.", "code_examples": [], "tables": [], "chunk_index": 6, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "Argument order", "section_id": "d6bc980d_fb4f_4609_9094_b8f619cafbf9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414320+00:00", "version": "10.2"}}
{"id": "ec209f2c825ac8ad", "content": "The nomenclature used for the data types in SPL syntax are described in the following table.", "code_examples": [], "tables": [{"headers": ["Syntax", "Data type", "Notes"], "rows": [["<bool>", "boolean", "Usetrueorfalse. Other variations are accepted. For example, fortrueyou can also use 't', 'T', 'TRUE', 'yes', or the number one ( 1 ).  Forfalseyou can also specify 'no', the number zero ( 0 ), and variations of the wordfalse, similar to the variations of the wordtrue."], ["<field>", "A field name. You cannot specify a wild card for the field name.", "See <wc-field>."], ["<int> or <integer>", "An integer that can be a positive or negative value.", "Sometimes referred to as a \"signed\" integer.  See <unsigned int>."], ["<string>", "string", "See <wc-string>."], ["<unsigned int>", "unsigned integer", "An unsigned integer must be positive value. Unsigned integers can be larger numbers than signed integers."], ["<wc-field>", "A field name or a partial name with a wildcard character to specify multiple, similarly named fields.", "Use the asterisk ( * ) character as the wildcard character."], ["<wc-string>", "A string value or partial string value with a wildcard character.", "Use the asterisk ( * ) character as the wildcard character."]]}], "chunk_index": 7, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "Data types", "section_id": "id_45a0400f_0c33_4ca1_bd0f_fd02d8e86e77--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414330+00:00", "version": "10.2"}}
{"id": "4c50d8e4b4962fcb", "content": "When a logical operator is included in the syntax of a command, you must always specify the operator in uppercase. Logical operators include: AND OR NOT XOR The search command evaluates operates logical operators in a different order of precedence than the eval and where commands.. To learn more about the order in which boolean expressions are evaluated, along with some examples, see Boolean expressions with logical operators in the Search Manual. To learn more about the the NOT operator, see Difference between NOT and != in the Search Manual .", "code_examples": [], "tables": [], "chunk_index": 8, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "Logical operators", "section_id": "id_2ea2c318_6e2e_4e07_9cba_39a269eaba1f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414334+00:00", "version": "10.2"}}
{"id": "f956d8f2a5ddba0d", "content": "A <by-clause> and a <split-by-clause> are not the same argument. When you use a <by-clause>, one row is returned for each distinct value <by-clause> field. A <by-clause> displays each unique item in a separate row. Think of the <by-clause> as a grouping. The <split-by-clause> displays each unique item in a separate column. Think of the <split-by-clause> as a splitting or dividing. Wildcard characters ( * ) are not accepted in BY clauses.", "code_examples": [], "tables": [], "chunk_index": 9, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "BY clauses", "section_id": "id_4ade76eb_90e2_44b1_85f6_ee2b22404598--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414339+00:00", "version": "10.2"}}
{"id": "42a9bbeabcf1e340", "content": "When the syntax contains <field> you specify a field name from your events. Consider this syntax: bin [<bins-options>...] <field> [AS <newfield>] The <field> argument is required. You can specify that the field displays a different name in the search results by using the [AS <newfield>] argument. This argument is optional. For example, if the field is categoryId and you want the field to be named CategoryID in the output, you would specify: categoryId AS CategoryID The <wc-field> argument indicates that you can use wild card characters when specifying field names. For example, if you have a set of fields that end with \"log\" you can specify *log to return all of those fields. If you use a wild card character in the middle of a value, especially as a wild card for punctuation, the results might be unpredictable.", "code_examples": [], "tables": [], "chunk_index": 10, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "Fields and wildcard fields", "section_id": "fbef51a7_fe60_441a_bb86_50df7334cde8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414343+00:00", "version": "10.2"}}
{"id": "517bc0721ee9b380", "content": "With many commands you can specify multiple expressions. Some commands use a space between expressions, while other commands use a comma between expressions. In the syntax for a command you will see something like <field-list to indicate that you can specify one or more expressions. For example, the stats command includes a <field-list argument. The list of fields must be separated by commas: With the outlier command, the <field-list argument expects the field names to be space-separated:", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | stats count BY status, host"}, {"language": "spl", "code": "...| outlier bytes clientip"}], "tables": [], "chunk_index": 11, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "Repeating expressions", "section_id": "id_641d31c0_4c5a_4709_a955_f6ccf303a800--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414347+00:00", "version": "10.2"}}
{"id": "8654de3e9b392ea5", "content": "In the Search Manual : Anatomy of a search Wildcards Field expressions Quotes and escaping characters", "code_examples": [], "tables": [], "chunk_index": 12, "total_chunks": 13, "metadata": {"title": "Understanding SPL syntax", "section_heading": "See also", "section_id": "id_43025301_b0f3_4430_bc1b_7c8a749a8e4a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/introduction/understanding-spl-syntax", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Introduction", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:23:59.414352+00:00", "version": "10.2"}}
{"id": "a2465494ea77aa89", "content": "These commands can be used to build correlation searches.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["append", "Appends subsearch results to current results."], ["appendcols", "Appends the fields of the subsearch results to current results, first results to first result, second to second, etc."], ["appendpipe", "Appends the result of the subpipeline applied to the current result set to results."], ["arules", "Finds association rules between field values."], ["associate", "Identifies correlations between fields."], ["contingency, counttable, ctable", "Builds a contingency table for two fields."], ["correlate", "Calculates the correlation between different fields."], ["diff", "Returns the difference between two search results."], ["join", "Combines the results from the main results pipeline with the results from a subsearch."], ["lookup", "Explicitly invokes field value lookups."], ["selfjoin", "Joins results with itself."], ["set", "Performs set operations (union, diff, intersect) on subsearches."], ["stats", "Provides statistics, grouped optionally by fields. SeeStatistical and charting functions."], ["transaction", "Groups search results into transactions."]]}], "chunk_index": 0, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Correlation", "section_id": "daf5e1b3_2a4f_466c_91ac_a670320d5ddd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141599+00:00", "version": "10.2"}}
{"id": "521d15e27de74768", "content": "These commands can be used to learn more about your data, add and delete data sources, or manage the data in your summary indexes. View data These commands return information about the data you have in your indexes. They do not modify your data or indexes in any way. Manage data These are some commands you can use to add data sources to or delete specific data from your indexes. Manage summary indexes These commands are used to create and manage your summary indexes.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["datamodel", "Return information about a data model or data model object."], ["dbinspect", "Returns information about the specified index."], ["eventcount", "Returns the number of events in an index."], ["metadata", "Returns a list of source, sourcetypes, or hosts from a specified index or distributed search peer."], ["typeahead", "Returns typeahead information on a specified prefix."]]}, {"headers": ["Command", "Description"], "rows": [["delete", "Delete specific events or search results."]]}, {"headers": ["Command", "Description"], "rows": [["collect, stash", "Puts search results into a summary index."], ["overlap", "Finds events in a summary index that overlap in time or have missed events."], ["sichart", "Summary indexing version of chart. Computes the necessary information for you to later run a chart search on the summary index."], ["sirare", "Summary indexing version of rare. Computes the necessary information for you to later run a rare search on the summary index."], ["sistats", "Summary indexing version of stats. Computes the necessary information for you to later run a stats search on the summary index."], ["sitimechart", "Summary indexing version of timechart. Computes the necessary information for you to later run a timechart search on the summary index."], ["sitop", "Summary indexing version of top. Computes the necessary information for you to later run a top search on the summary index."]]}], "chunk_index": 1, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Data and indexes", "section_id": "id_8c05a469_d981_4587_8413_16aef66d18bb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141614+00:00", "version": "10.2"}}
{"id": "9cc09c72b31a1218", "content": "These are commands you can use to add, extract, and modify fields or field values. The most useful command for manipulating fields is eval and its statistical and charting functions. Add fields Use these commands to add new fields. Extract fields These commands provide different ways to extract new fields from search results. Modify fields and field values Use these commands to modify fields or their values.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["accum", "Keeps a running total of the specified numeric field."], ["addinfo", "Add fields that contain common information about the current search."], ["addtotals", "Computes the sum of all numeric fields for each result."], ["delta", "Computes the difference in field value between nearby results."], ["eval", "Calculates an expression and puts the value into a field. See also,evaluation functions."], ["iplocation", "Adds location information, such as city, country, latitude, longitude, and so on, based on IP addresses."], ["lookup", "For configured lookup tables, explicitly invokes the field value lookup and adds fields from the lookup table to the events."], ["multikv", "Extracts field-values from table-formatted events."], ["rangemap", "Sets RANGE field to the name of the ranges that match."], ["strcat", "Concatenates string values and saves the result to a specified field."]]}, {"headers": ["Command", "Description"], "rows": [["erex", "Allows you to specify example or counter example values to automatically extract fields that have similar values."], ["extract, kv", "Extracts field-value pairs from search results."], ["kvform", "Extracts values from search results, using a form template."], ["rex", "Specify a Perl regular expression named groups to extract fields while you search."], ["spath", "Provides a straightforward means for extracting fields from structured data formats, XML and JSON."], ["xmlkv", "Extracts XML key-value pairs."]]}, {"headers": ["Command", "Description"], "rows": [["convert", "Converts field values into numerical values."], ["filldown", "Replaces NULL values with the last non-NULL value."], ["fillnull", "Replaces null values with a specified value."], ["makemv", "Change a specified field into a multivalue field during a search."], ["nomv", "Changes a specified multivalue field into a single-value field at search time."], ["reltime", "Converts the difference between 'now' and '_time' to a human-readable value and adds adds this value to the field, 'reltime', in your search results."], ["rename", "Renames a specified field. Use wildcards to specify multiple fields."], ["replace", "Replaces values of specified fields with a specified new value."]]}], "chunk_index": 2, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Fields", "section_id": "d2e7fe8e_376b_41b6_84b3_a5f1297a73aa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141632+00:00", "version": "10.2"}}
{"id": "2d545d8d3ba14026", "content": "These commands are used to find anomalies in your data. Either search for uncommon or outlying events and fields or cluster similar events together.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["analyzefields, af", "Analyze numerical fields for their ability to predict another discrete field."], ["anomalies", "Computes an \"unexpectedness\" score for an event."], ["anomalousvalue", "Finds and summarizes irregular, or uncommon, search results."], ["anomalydetection", "Identifies anomalous events by computing a probability for each event and then detecting unusually small probabilities."], ["cluster", "Clusters similar events together."], ["kmeans", "Performs k-means clustering on selected fields."], ["outlier", "Removes outlying numerical values."], ["rare", "Displays the least common values of a field."]]}], "chunk_index": 3, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Find anomalies", "section_id": "id_0afc4fc5_fa96_435a_ae98_5e23c596899f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141641+00:00", "version": "10.2"}}
{"id": "a5acfc4d5f84f816", "content": "These commands add geographical information to your search results.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["iplocation", "Returns location information, such as city, country, latitude, longitude, and so on, based on IP addresses."], ["geom", "Adds a field, named \"geom\", to each event. This field contains geographic data structures for polygon geometry in JSON and is used for choropleth map visualization. This command requires an external lookup withexternal_type=geoto be installed."], ["geomfilter", "Accepts two points that specify a bounding box for clipping choropleth maps. Points that fall outside of the bounding box are filtered out."], ["geostats", "Generate statistics which are clustered into geographical bins to be rendered on a world map."]]}], "chunk_index": 4, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Geographic and location", "section_id": "a8477e95_be4c_4f16_ac31_671a6d9af3d1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141647+00:00", "version": "10.2"}}
{"id": "404cac33baf2b829", "content": "These commands work with metrics data.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["mcollect", "Converts events into metric data points and inserts the data points into a metric index on the search head."], ["meventcollect", "Converts events into metric data points and inserts the data points into a metric index on indexer tier."], ["mpreview, msearch", "Provides samples of the raw metric data points in the metric time series in your metrics indexes. Helps you troubleshoot your metrics data."], ["mstats", "Calculates visualization-ready statistics for themeasurement,metric_name, anddimensionfields in metric indexes."]]}], "chunk_index": 5, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Metrics", "section_id": "id_306522ab_7fcc_4dfa_bb06_e5112129a1ec--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141654+00:00", "version": "10.2"}}
{"id": "4b6315b8804149d2", "content": "These commands predict future values and calculate trendlines that can be used to create visualizations.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["predict", "Enables you to use time series algorithms to predict future values of fields."], ["trendline", "Computes moving averages of fields."], ["x11", "Enables you to determine the trend in your data by removing the seasonal pattern."]]}], "chunk_index": 6, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Prediction and trending", "section_id": "e020512b_e34e_4e00_98f6_64a586a3ee6a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141660+00:00", "version": "10.2"}}
{"id": "4c01d5cebed942e8", "content": "These commands are used to build transforming searches. These commands return statistical data tables that are required for charts and other kinds of data visualizations.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["addtotals", "Computes the sum of all numeric fields for each result."], ["autoregress", "Prepares your events for calculating the autoregression, or moving average, based on a field that you specify."], ["bin, discretize", "Puts continuous numerical values into discrete sets."], ["chart", "Returns results in a tabular output for charting. See also,Statistical and charting functions."], ["contingency, counttable, ctable", "Builds a contingency table for two fields."], ["correlate", "Calculates the correlation between different fields."], ["eventcount", "Returns the number of events in an index."], ["eventstats", "Adds summary statistics to all search results."], ["gauge", "Transforms results into a format suitable for display by the Gauge chart types."], ["makecontinuous", "Makes a field that is supposed to be the x-axis continuous (invoked bychart/timechart)"], ["mstats", "Calculates statistics for the measurement, metric_name, and dimension fields in metric indexes."], ["outlier", "Removes outlying numerical values."], ["rare", "Displays the least common values of a field."], ["stats", "Provides statistics, grouped optionally by fields. See also,Statistical and charting functions."], ["streamstats", "Adds summary statistics to all search results in a streaming manner."], ["timechart", "Create a time series chart and corresponding table of statistics. See also,Statistical and charting functions."], ["top", "Displays the most common values of a field."], ["trendline", "Computes moving averages of fields."], ["tstats", "Performs statistical queries on indexed fields intsidxfiles."], ["untable", "Converts results from a tabular format to a format similar tostatsoutput. Inverse ofxyseriesandmaketable."], ["xyseries", "Converts results into a format suitable for graphing."]]}], "chunk_index": 7, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Reports", "section_id": "id_35bc5836_6c85_4695_8c4d_ca1076e7a071--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141673+00:00", "version": "10.2"}}
{"id": "7455620fdff1b466", "content": "These commands can be used to manage search results. For example, you can append one set of results with another, filter more events from the results, reformat the results, and so on. Alerting Use this command to email the results of a search. Appending Use these commands to append one set of results with another set or to itself. Filtering Use these commands to remove more events or fields from your current results. Formatting Use these commands to reformat your current results. Generating Use these commands to generate or return events. Grouping Use these commands to group or classify the current results. Reordering Use these commands to change the order of the current search results. Reading Use these commands to read in results from external files or previous searches. Writing Use these commands to define how to output current search results.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["sendemail", "Emails search results, either inline or as an attachment, to one or more specified email addresses."]]}, {"headers": ["Command", "Description"], "rows": [["append", "Appends subsearch results to current results."], ["appendcols", "Appends the fields of the subsearch results to current results, first results to first result, second to second, and so on."], ["join", "SQL-like joining of results from the main results pipeline with the results from the subpipeline."], ["selfjoin", "Joins results with itself."]]}, {"headers": ["Command", "Description"], "rows": [["dedup", "Removes subsequent results that match a specified criteria."], ["fields", "Removes fields from search results."], ["from", "Retrieves data from a dataset, such as a data model dataset, a CSV lookup, a KV Store lookup, a saved search, or a table dataset."], ["mvcombine", "Combines events in search results that have a single differing field value into one result with a multivalue field of the differing field."], ["regex", "Removes results that do not match the specified regular expression."], ["searchtxn", "Finds transaction events within specified search constraints."], ["table", "Creates a table using the specified fields."], ["uniq", "Removes any search that is an exact duplicate with a previous result."], ["where", "Performs arbitrary filtering on your data. See also,Evaluation functions."]]}, {"headers": ["Command", "Description"], "rows": [["fieldformat", "Usesevalexpressions to change the format of field values when they are rendered without changing their underlying values. Does not apply to exported data."], ["transpose", "Reformats rows of search results as columns. Useful for fixing X- and Y-axis display issues with charts, or for turning sets of data into a series to produce a chart."], ["untable", "Converts results from a tabular format to a format similar tostatsoutput. Inverse ofxyseriesandmaketable."], ["xyseries", "Converts results into a format suitable for graphing."]]}, {"headers": ["Command", "Description"], "rows": [["gentimes", "Returns results that match a time-range."], ["loadjob", "Loads events or results of a previously completed search job."], ["makeresults", "Creates a specified number of empty search results."], ["mvexpand", "Expands the values of a multivalue field into separate events for each value of the multivalue field."], ["savedsearch", "Returns the search results of a saved search."], ["search", "Searches indexes for matching events. This command is implicit at the start of every search pipeline that does not begin with another generating command."]]}, {"headers": ["Command", "Description"], "rows": [["cluster", "Clusters similar events together."], ["kmeans", "Performs k-means clustering on selected fields."], ["mvexpand", "Expands the values of a multivalue field into separate events for each value of the multivalue field."], ["transaction", "Groups search results into transactions."], ["typelearner", "Generates suggested eventtypes."], ["typer", "Calculates the eventtypes for the search results."]]}, {"headers": ["Command", "Description"], "rows": [["head", "Returns the first number n of specified results."], ["reverse", "Reverses the order of the results."], ["sort", "Sorts search results by the specified fields."], ["tail", "Returns the last number N of specified results"]]}, {"headers": ["Command", "Description"], "rows": [["inputcsv", "Loads search results from the specified CSV file."], ["inputlookup", "Loads search results from a specified static lookup table."], ["loadjob", "Loads events or results of a previously completed search job."]]}, {"headers": ["Command", "Description"], "rows": [["collect, stash", "Puts search results into a summary index."], ["meventcollect", "Converts events into metric data points and inserts the data points into a metric index on indexer tier."], ["mcollect", "Converts events into metric data points and inserts the data points into a metric index on the search head."], ["outputcsv", "Outputs search results to a specified CSV file."], ["outputlookup", "Writes search results to the specified static lookup table."], ["outputtext", "Ouputs the raw text field (_raw) of results into the_xmlfield."], ["sendemail", "Emails search results, either inline or as an attachment, to one or more specified email addresses."]]}], "chunk_index": 8, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Results", "section_id": "id_10ccd27a_b2cb_4c4d_beb1_55a4f27b6bab--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141700+00:00", "version": "10.2"}}
{"id": "1abcce73bbb1c143", "content": "", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["localop", "Run subsequent commands, that is all commands following this, locally and not on a remote peer."], ["map", "A looping operator, performs a search over each search result."], ["redistribute", "Invokes parallel reduce search processing to shorten the search runtime of a set of supported SPL commands."], ["search", "Searches indexes for matching events. This command is implicit at the start of every search pipeline that does not begin with another generating command."], ["sendalert", "invokes a custom alert action."], ["sendemail", "Emails search results, either inline or as an attachment, to one or more specified email addresses."]]}], "chunk_index": 9, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Search", "section_id": "b5f8660b_5901_4f28_add4_ef1122cf2f18--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141708+00:00", "version": "10.2"}}
{"id": "e88e38e1db9a945f", "content": "These are commands that you can use with subsearches .", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["append", "Appends subsearch results to current results."], ["appendcols", "Appends the fields of the subsearch results to current results, first subsearch results to first current results, second to second, and so on."], ["appendpipe", "Appends the result of the subpipeline applied to the current result set to results."], ["foreach", "Runs a templated streaming subsearch for each field in a wildcarded field list."], ["format", "Takes the results of a subsearch and formats them into a single result."], ["join", "Combines the results of a subsearch with the results of a main search."], ["multisearch", "Runs multiple streaming subsearches at the same time."], ["return", "Specifies the values to return from a subsearch."], ["set", "Performs set operations (union, diff, intersect) on subsearches."]]}], "chunk_index": 10, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Subsearch", "section_id": "cf13fc60_7d44_476d_97d2_0840272a1586--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141716+00:00", "version": "10.2"}}
{"id": "0513dd0ad0490342", "content": "Use these commands to search based on time ranges or add time information to your events.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["gentimes", "Returns results that match a time-range."], ["localize", "Returns a list of the time ranges in which the search results were found."], ["reltime", "Converts the difference between 'now' and '_time' to a human-readable value and adds adds this value to the field, 'reltime', in your search results."]]}], "chunk_index": 11, "total_chunks": 12, "metadata": {"title": "Commands by category", "section_heading": "Time", "section_id": "f0889724_1a92_4dbd_963d_b6b75220a3ca--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/commands-by-category", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:17.141720+00:00", "version": "10.2"}}
{"id": "3289f9af654f6fc5", "content": "The table below lists all of the search commands in alphabetical order. There is a short description of the command and links to related commands. For the complete syntax, usage, and detailed examples, click the command name to display the specific topic for that command. Some of these commands share functions. For a list of the functions with descriptions and examples, see Evaluation functions and Statistical and charting functions. If you don't find a command in the table, that command might be part of a third-party app or add-on. For information about commands contributed by apps and add-ons, see the documentation on Splunkbase .", "code_examples": [], "tables": [{"headers": ["Command", "Description", "Related commands"], "rows": [["abstract", "Produces a summary of each search result.", "highlight"], ["accum", "Keeps a running total of the specified numeric field.", "autoregress, delta, trendline, streamstats"], ["addcoltotals", "Computes an event that contains sum of all numeric fields for previous events.", "addtotals, stats"], ["addinfo", "Add fields that contain common information about the current search.", "search"], ["addtotals", "Computes the sum of all numeric fields for each result.", "addcoltotals,stats"], ["analyzefields", "Analyze numerical fields for their ability to predict another discrete field.", "anomalousvalue"], ["anomalies", "Computes an \"unexpectedness\" score for an event.", "anomalousvalue,cluster,kmeans,outlier"], ["anomalousvalue", "Finds and summarizes irregular, or uncommon, search results.", "analyzefields,anomalies,cluster,kmeans,outlier"], ["anomalydetection", "Identifies anomalous events by computing a probability for each event and then detecting unusually small probabilities.", "analyzefields,anomalies,anomalousvalue,cluster,kmeans,outlier"], ["append", "Appends subsearch results to current results.", "appendcols,appendcsv,appendlookup,join,set"], ["appendcols", "Appends the fields of the subsearch results to current results, first results to first result, second to second, etc.", "append,appendcsv,join,set"], ["appendpipe", "Appends the result of the subpipeline applied to the current result set to results.", "append,appendcols,join,set"], ["arules", "Finds association rules between field values.", "associate,correlate"], ["associate", "Identifies correlations between fields.", "correlate,contingency"], ["autoregress", "Sets up data for calculating the moving average.", "accum,autoregress,delta,trendline,streamstats"], ["bin(bucket)", "Puts continuous numerical values into discrete sets.", "chart,timechart"], ["bucketdir", "Replaces a field value with higher-level grouping, such as replacing filenames with directories.", "cluster,dedup"], ["chart", "Returns results in a tabular output for charting. See also,Statistical and charting functions.", "bin,sichart,timechart"], ["cluster", "Clusters similar events together.", "anomalies,anomalousvalue,cluster,kmeans,outlier"], ["cofilter", "Finds how many times field1 and field2 values occurred together.", "associate,correlate"], ["collect", "Puts search results into a summary index.", "overlap"], ["concurrency", "Uses a duration field to find the number of \"concurrent\" events for each event.", "timechart"], ["contingency", "Builds a contingency table for two fields.", "associate,correlate"], ["convert", "Converts field values into numerical values.", "eval"], ["correlate", "Calculates the correlation between different fields.", "associate,contingency"], ["datamodel", "Examine data model or data model dataset and search a data model dataset.", "pivot"], ["dbinspect", "Returns information about the specified index.", ""], ["dedup", "Removes subsequent results that match a specified criteria.", "uniq"], ["delete", "Delete specific events or search results.", ""], ["delta", "Computes the difference in field value between nearby results.", "accum,autoregress,trendline,streamstats"], ["diff", "Returns the difference between two search results.", ""], ["erex", "Allows you to specify example or counter example values to automatically extract fields that have similar values.", "extract,kvform,multikv,regex,rex,xmlkv"], ["eval", "Calculates an expression and puts the value into a field. See also,Evaluation functions.", "where"], ["eventcount", "Returns the number of events in an index.", "dbinspect"], ["eventstats", "Adds summary statistics to all search results.", "stats"], ["extract(kv)", "Extracts field-value pairs from search results.", "kvform,multikv,xmlkv,rex"], ["fieldformat", "Expresses how to render a field at output time without changing the underlying value.", "eval,where"], ["fields", "Keeps or removes fields from search results based on the field list criteria.", ""], ["fieldsummary", "Generates summary information for all or a subset of the fields.", "analyzefields,anomalies,anomalousvalue,stats"], ["filldown", "Replaces NULL values with the last non-NULL value.", "fillnull"], ["fillnull", "Replaces null values with a specified value.", ""], ["findtypes", "Generates a list of suggested event types.", "typer"], ["folderize", "Creates a higher-level grouping, such as replacing filenames with directories.", ""], ["foreach", "Run a templatized streaming subsearch for each field in a wildcarded field list.", "eval"], ["format", "Takes the results of a subsearch and formats them into a single result.", ""], ["from", "Retrieves data from a dataset, such as a data model dataset, a CSV lookup, a KV Store lookup, a saved search, or a table dataset.", ""], ["gauge", "Transforms results into a format suitable for display by the Gauge chart types.", ""], ["gentimes", "Generates time-range results.", ""], ["geom", "Adds a field, namedgeom, to each event. This field contains geographic data structures for polygon geometry in JSON and is used for the choropleth map visualization.", "geomfilter"], ["geomfilter", "Accepts two points that specify a bounding box for clipping a choropleth map. Points that fall outside of the bounding box are filtered out.", "geom"], ["geostats", "Generate statistics which are clustered into geographical bins to be rendered on a world map.", "stats, xyseries"], ["head", "Returns the first numbernof specified results.", "reverse, tail"], ["highlight", "Highlights the specified terms.", "iconify"], ["history", "Returns a history of searches formatted as an events list or as a table.", "search"], ["iconify", "Displays a unique icon for each different value in the list of fields that you specify.", "highlight"], ["inputcsv", "Loads search results from the specified CSV file.", "loadjob, outputcsv"], ["inputlookup", "Loads search results from a specified static lookup table.", "inputcsv,join,lookup,outputlookup"], ["iplocation", "Extracts location information from IP addresses.", ""], ["join", "Combine the results of a subsearch with the results of a main search.", "appendcols, lookup, selfjoin"], ["kmeans", "Performs k-means clustering on selected fields.", "anomalies, anomalousvalue, cluster, outlier"], ["kvform", "Extracts values from search results, using a form template.", "extract, kvform, multikv, xmlkv, rex"], ["loadjob", "Loads events or results of a previously completed search job.", "inputcsv"], ["localize", "Returns a list of the time ranges in which the search results were found.", "map, transaction"], ["localop", "Run subsequent commands, that is all commands following this, locally and not on remote peers.", ""], ["lookup", "Explicitly invokes field value lookups.", ""], ["makecontinuous", "Makes a field that is supposed to be the x-axis continuous (invoked by chart/timechart)", "chart, timechart"], ["makemv", "Change a specified field into a multivalued field during a search.", "mvcombine, mvexpand, nomv"], ["makeresults", "Creates a specified number of empty search results.", ""], ["map", "A looping operator, performs a search over each search result.", ""], ["mcollect", "Converts search results into metric data and inserts the data into a metric index on the search head.", "collect,meventcollect"], ["metadata", "Returns a list of source, sourcetypes, or hosts from a specified index or distributed search peer.", "dbinspect"], ["metasearch", "Retrieves event metadata from indexes based on terms in the logical expression.", "metadata,search"], ["meventcollect", "Converts search results into metric data and inserts the data into a metric index on the indexers.", "collect,mcollect"], ["mpreview", "Returns a preview of the rawmetric data pointsin a specified metric index that match a provided filter.", "mcatalog,mstats,msearch"], ["msearch", "Alias for thempreviewcommand.", "mcatalog,mstats,mpreview"], ["mstats", "Calculates statistics for the measurement, metric_name, and dimension fields in metric indexes.", "stats,tstats"], ["multikv", "Extracts field-values from table-formatted events.", ""], ["multisearch", "Run multiplestreaming searchesat the same time.", "append, join"], ["mvcombine", "Combines events in search results that have a single differing field value into one result with a multivalue field of the differing field.", "mvexpand, makemv, nomv"], ["mvexpand", "Expands the values of a multivalue field into separate events for each value of the multivalue field.", "mvcombine, makemv, nomv"], ["nomv", "Changes a specified multivalued field into a single-value field at search time.", "makemv, mvcombine, mvexpand"], ["outlier", "Removes outlying numerical values.", "anomalies,anomalousvalue,cluster,kmeans"], ["outputcsv", "Outputs search results to a specified CSV file.", "inputcsv,outputtext"], ["outputlookup", "Writes search results to the specified static lookup table.", "inputlookup,lookup,outputcsv"], ["outputtext", "Outputs the raw text field (_raw) of results into the_xmlfield.", "outputcsv"], ["overlap", "Finds events in a summary index that overlap in time or have missed events.", "collect"], ["pivot", "Run pivot searches against a particular data model dataset.", "datamodel"], ["predict", "Enables you to use time series algorithms to predict future values of fields.", "x11"], ["rangemap", "Sets RANGE field to the name of the ranges that match.", ""], ["rare", "Displays the least common values of a field.", "sirare, stats, top"], ["redistribute", "Implements parallel reduce search processing to shorten the search runtime of high-cardinality dataset searches.", ""], ["regex", "Removes results that do not match the specified regular expression.", "rex,search"], ["reltime", "Converts the difference between 'now' and '_time' to a human-readable value and adds adds this value to the field, 'reltime', in your search results.", "convert"], ["rename", "Renames a specified field; wildcards can be used to specify multiple fields.", ""], ["replace", "Replaces values of specified fields with a specified new value.", ""], ["require", "Causes a search to fail if the queries and commands that precede it in the search string return zero events or results.", ""], ["rest", "Access a REST endpoint and display the returned entities as search results.", ""], ["return", "Specify the values to return from a subsearch.", "format, search"], ["reverse", "Reverses the order of the results.", "head, sort, tail"], ["rex", "Specify a Perl regular expression named groups to extract fields while you search.", "extract, kvform, multikv, xmlkv, regex"], ["rtorder", "Buffers events from real-time search to emit them in ascending time order when possible.", ""], ["savedsearch", "Returns the search results of a saved search.", ""], ["script(run)", "Runs an external Perl or Python script as part of your search.", ""], ["scrub", "Anonymizes the search results.", ""], ["search", "Searches indexes for matching events.", ""], ["searchtxn", "Finds transaction events within specified search constraints.", "transaction"], ["selfjoin", "Joins results with itself.", "join"], ["sendalert", "invokes a custom alert action.", ""], ["sendemail", "Emails search results to a specified email address.", ""], ["set", "Performs set operations (union, diff, intersect) on subsearches.", "append, appendcols, join, diff"], ["setfields", "Sets the field values for all results to a common value.", "eval,fillnull,rename"], ["sichart", "Summary indexing version of the chart command.", "chart, sitimechart, timechart"], ["sirare", "Summary indexing version of the rare command.", "rare"], ["sistats", "Summary indexing version of the stats command.", "stats"], ["sitimechart", "Summary indexing version of the timechart command.", "chart, sichart, timechart"], ["sitop", "Summary indexing version of the top command.", "top"], ["sort", "Sorts search results by the specified fields.", "reverse"], ["spath", "Provides a straightforward means for extracting fields from structured data formats, XML and JSON.", "xpath"], ["stats", "Provides statistics, grouped optionally by fields. See also,Statistical and charting functions.", "eventstats, top, rare"], ["strcat", "Concatenates string values.", ""], ["streamstats", "Adds summary statistics to all search results in a streaming manner.", "eventstats, stats"], ["table", "Creates a table using the specified fields.", "fields"], ["tags", "Annotates specified fields in your search results with tags.", "eval"], ["tail", "Returns the last number n of specified results.", "head, reverse"], ["timechart", "Create a time series chart and corresponding table of statistics. See also,Statistical and charting functions.", "chart, bucket"], ["timewrap", "Displays, or wraps, the output of thetimechart commandso that everytimewrap-spanrange of time is a different series.", "timechart"], ["tojson", "Converts events into JSON objects.", ""], ["top", "Displays the most common values of a field.", "rare, stats"], ["transaction", "Groups search results into transactions.", ""], ["transpose", "Reformats rows of search results as columns.", ""], ["trendline", "Computes moving averages of fields.", "timechart"], ["tscollect", "Writes results into tsidx file(s) for later use by the tstats command.", "collect, stats, tstats"], ["tstats", "Calculates statistics over tsidx files created with the tscollect command.", "stats, tscollect"], ["typeahead", "Returns typeahead information on a specified prefix.", ""], ["typelearner", "Deprecated. Usefindtypesinstead. Generates suggested eventtypes.", "typer"], ["typer", "Calculates the eventtypes for the search results.", "findtypes"], ["union", "Merges the results from two or more datasets into one dataset.", ""], ["uniq", "Removes any search that is an exact duplicate with a previous result.", "dedup"], ["untable", "Converts results from a tabular format to a format similar tostatsoutput. Inverse ofxyseriesandmaketable.", ""], ["walklex", "Generates a list of terms or indexed fields from each bucket of event indexes.", "metadata,tstats"], ["where", "Performs arbitrary filtering on your data. See also,Evaluations functions.", "eval"], ["x11", "Enables you to determine the trend in your data by removing the seasonal pattern.", "predict"], ["xmlkv", "Extracts XML key-value pairs.", "extract, kvform, multikv, rex"], ["xmlunescape", "Unescapes XML.", ""], ["xpath", "Redefines the XML path.", ""], ["xyseries", "Converts results into a format suitable for graphing.", ""]]}], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "dea1673fb46084c8180b8cf3556870a68--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/command-quick-reference", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:33.674919+00:00", "version": "10.2"}}
{"id": "410a6ba69c22010f", "content": "The Splunk platform does not store data in a conventional database. Rather, it stores data in a distributed, non-relational, semi-structured database with an implicit time dimension. Relational databases require that all table columns be defined up-front and they do not automatically scale by just plugging in new hardware. However, there are analogues to many of the concepts in the database world.", "code_examples": [], "tables": [{"headers": ["Database Concept", "Splunk Concept", "Notes"], "rows": [["SQL query", "Splunk search", "A Splunk search retrieves indexed data and can perform transforming and reporting operations. Results from one search can be \"piped\", or transferred, from command to command, to filter, modify, reorder, and group your results."], ["table/view", "search results", "Search results can be thought of as a database view, a dynamically generated table of rows, with columns."], ["index", "index", "All values and fields are indexed by Splunk software, so there is no need to manually add, update, drop, or even think about indexing columns. Everything can be quickly retrieved automatically."], ["row", "result/event", "A result in a Splunk search is a list of fields (i.e., column) values, corresponding to a table row. An event is a result that has a timestamp and raw text. Typically an event is a record from a log file, such as:173.26.34.223 - - [01/Jul/2009:12:05:27 -0700] \"GET /trade/app?action=logout HTTP/1.1\" 200 2953"], ["column", "field", "Fields are returned dynamically from a search, meaning that one search might return a set of fields, while another search might return another set. After teaching Splunk software how to extract more fields from the raw underlying data, the same search will return more fields than it previously did. Fields are not tied to a datatype."], ["database/schema", "index/app", "A Splunk index is a collection of data, somewhat like a database has a collection of tables. Domain knowledge of that data, how to extract it, what reports to run, etc, are stored in a Splunk application."]]}], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Splunk SPL for SQL users", "section_heading": "Concepts", "section_id": "id_695f1519_74ab_45bd_9106_8dd86f80b0a0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/splunk-spl-for-sql-users", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:50.488916+00:00", "version": "10.2"}}
{"id": "713bd353f85c2941", "content": "SQL is designed to search relational database tables which are comprised of columns. SPL is designed to search events, which are comprised of fields. In SQL, you often see examples that use \"mytable\" and \"mycolumn\". In SPL, you will see examples that refer to \"fields\". In these examples, the \"source\" field is used as a proxy for \"table\". In Splunk software, \"source\" is the name of the file, stream, or other input from which a particular piece of data originates, for example /var/log/messages or UDP:514. Note: When translating from any language to another, often the translation is longer because of idioms in the original language. Some of the Splunk search examples shown below could be more concise and more efficient, but for parallelism and clarity, the SPL table and field names are kept the same as the SQL example. SPL searches rarely need the FIELDS command to filter out columns because the user interface provides a more convenient method for filtering. The FIELDS command is used in the SPL examples for parallelism. With SPL, you never have to use the AND operator in Boolean searches, because AND is implied between terms. However when you use the AND or OR operators, they must be specified in uppercase. SPL commands do not need to be specified in uppercase. In the these SPL examples, the commands are specified in uppercase for easier identification and clarity. Although some SPL commands loosely correspond to specific SQL commands as shown in the following table, your SPL searches might not produce the desired results if you \"think in SQL.\" For this reason, avoid directly translating from SQL to SPL when you design your searches. See About the search language in the Search Manual for an overview of SPL.", "code_examples": [{"language": "spl", "code": "SELECT *\nFROM mytable"}, {"language": "spl", "code": "source=mytable"}, {"language": "spl", "code": "SELECT *\nFROM mytable\nWHERE mycolumn=5"}, {"language": "spl", "code": "source=mytable mycolumn=5"}, {"language": "spl", "code": "SELECT mycolumn1, mycolumn2\nFROM mytable"}, {"language": "spl", "code": "source=mytable\t\n| FIELDS mycolumn1, mycolumn2"}, {"language": "spl", "code": "SELECT *\nFROM mytable\nWHERE (mycolumn1=\"true\"OR mycolumn2=\"red\") \nAND mycolumn3=\"blue\""}, {"language": "spl", "code": "source=mytable\nAND (mycolumn1=\"true\"OR mycolumn2=\"red\")\nAND mycolumn3=\"blue\""}, {"language": "spl", "code": "source=mytable\n(mycolumn1=\"true\"OR mycolumn2=\"red\")\nmycolumn3=\"blue\""}, {"language": "spl", "code": "SELECT mycolumn AS column_alias\nFROM mytable"}, {"language": "spl", "code": "source=mytable\n| RENAME mycolumn as column_alias\n| FIELDS column_alias"}, {"language": "spl", "code": "SELECT *\nFROM mytable\nWHERE mycolumn\nBETWEEN 1 AND 5"}, {"language": "spl", "code": "source=mytable \n  mycolumn>=1 mycolumn<=5"}, {"language": "spl", "code": "SELECT mycolumn, avg(mycolumn)\nFROM mytable\nWHERE mycolumn=value\nGROUP BY mycolumn"}, {"language": "spl", "code": "source=mytable mycolumn=value\n| STATS avg(mycolumn) BY mycolumn\n| FIELDS mycolumn, avg(mycolumn)"}, {"language": "spl", "code": "SELECT mycolumn, avg(mycolumn)\nFROM mytable\nWHERE mycolumn=value\nGROUP BY mycolumn\nHAVING avg(mycolumn)=value"}, {"language": "spl", "code": "source=mytable mycolumn=value\n| STATS avg(mycolumn) BY mycolumn\n| SEARCH avg(mycolumn)=value\n| FIELDS mycolumn, avg(mycolumn)"}, {"language": "spl", "code": "SELECT *\nFROM mytable\nWHERE mycolumn LIKE\"%some text%\""}, {"language": "spl", "code": "source=mytable \n  mycolumn=\"*some text*\""}, {"language": "spl", "code": "source=mytable\"some text\""}, {"language": "spl", "code": "SELECT *\nFROM mytable\nORDER BY mycolumn desc"}, {"language": "spl", "code": "source=mytable\n| SORT -mycolumn"}, {"language": "spl", "code": "SELECT DISTINCT \n  mycolumn1, mycolumn2\nFROM mytable"}, {"language": "spl", "code": "source=mytable\n| DEDUP mycolumn1, mycolumn2\n| FIELDS mycolumn1, mycolumn2"}, {"language": "spl", "code": "SELECT TOP(5) \nmycolum1, \nmycolum2\nFROM mytable1\nWHERE mycolum3 =\"bar\"ORDER BY mycolum1 mycolum2"}, {"language": "spl", "code": "Source=mytable1 mycolum3=\"bar\"| FIELDS mycolum1 mycolum2\n| SORT mycolum1 mycolum2\n| HEAD 5"}, {"language": "spl", "code": "SELECT *\nFROM mytable1\nINNER JOIN mytable2\nON mytable1.mycolumn= \n  mytable2.mycolumn"}, {"language": "spl", "code": "index=myIndex1 OR index=myIndex2\n| stats values(*) AS * BY myField"}, {"language": "spl", "code": "... | LOOKUP myvaluelookup \n  mycolumn \n  OUTPUT myoutputcolumn"}, {"language": "spl", "code": "source=mytable1\n  [SEARCHsource=mytable2 \n    mycolumn2=myvalue\n    | FIELDS mycolumn2]"}, {"language": "spl", "code": "source=mytable1 \n| JOINtype=inner mycolumn \n  [ SEARCHsource=mytable2 \n    | RENAME mycolumn2 \n    AS mycolumn]"}, {"language": "spl", "code": "index=myIndex1 OR index=myIndex2\n| rename myfield1 as myField\n| stats values(*) AS * BY myField"}, {"language": "spl", "code": "SELECT *\nFROM mytable1\nLEFT JOIN mytable2\nON mytable1.mycolumn=\n  mytable2.mycolumn"}, {"language": "spl", "code": "source=mytable1\n| JOINtype=left mycolumn \n  [SEARCHsource=mytable2]"}, {"language": "spl", "code": "SELECT *\nINTO new_mytable IN mydb2\nFROM old_mytable"}, {"language": "spl", "code": "source=old_mytable\n| EVALsource=new_mytable\n| COLLECT index=mydb2"}, {"language": "spl", "code": "TRUNCATE TABLE mytable"}, {"language": "spl", "code": "source=mytable\n| DELETE"}, {"language": "spl", "code": "INSERT INTO mytable\nVALUES (value1, value2, value3,....)"}, {"language": "spl", "code": "SELECT mycolumn\nFROM mytable1\nUNION\nSELECT mycolumn FROM mytable2"}, {"language": "spl", "code": "source=mytable1\n| APPEND \n  [SEARCHsource=mytable2]\n| DEDUP mycolumn"}, {"language": "spl", "code": "SELECT *\nFROM mytable1\nUNION ALL\nSELECT * FROM mytable2"}, {"language": "spl", "code": "source=mytable1\n| APPEND \n  [SEARCHsource=mytable2]"}, {"language": "spl", "code": "DELETE FROM mytable\nWHERE mycolumn=5"}, {"language": "spl", "code": "source=mytable1 mycolumn=5\n| DELETE"}, {"language": "spl", "code": "UPDATE mytable\nSET column1=value, \n  column2=value,...\nWHERE some_column=some_value"}], "tables": [{"headers": ["SQL command", "SQL example", "Splunk SPL example"], "rows": [["SELECT *", "SELECT *\nFROM mytable", "source=mytable"], ["WHERE", "SELECT *\nFROM mytable\nWHERE mycolumn=5", "source=mytable mycolumn=5"], ["SELECT", "SELECT mycolumn1, mycolumn2\nFROM mytable", "source=mytable\t\n| FIELDS mycolumn1, mycolumn2"], ["AND/OR", "SELECT *\nFROM mytable\nWHERE (mycolumn1=\"true\"OR mycolumn2=\"red\") \nAND mycolumn3=\"blue\"", "source=mytable\nAND (mycolumn1=\"true\"OR mycolumn2=\"red\")\nAND mycolumn3=\"blue\"Note:The AND operator is implied in SPL and does not need to be specified. For this example you could also use:source=mytable\n(mycolumn1=\"true\"OR mycolumn2=\"red\")\nmycolumn3=\"blue\""], ["AS (alias)", "SELECT mycolumn AS column_alias\nFROM mytable", "source=mytable\n| RENAME mycolumn as column_alias\n| FIELDS column_alias"], ["BETWEEN", "SELECT *\nFROM mytable\nWHERE mycolumn\nBETWEEN 1 AND 5", "source=mytable \n  mycolumn>=1 mycolumn<=5"], ["GROUP BY", "SELECT mycolumn, avg(mycolumn)\nFROM mytable\nWHERE mycolumn=value\nGROUP BY mycolumn", "source=mytable mycolumn=value\n| STATS avg(mycolumn) BY mycolumn\n| FIELDS mycolumn, avg(mycolumn)Several commands use aby-clauseto group information, includingchart,rare,sort,stats, andtimechart."], ["HAVING", "SELECT mycolumn, avg(mycolumn)\nFROM mytable\nWHERE mycolumn=value\nGROUP BY mycolumn\nHAVING avg(mycolumn)=value", "source=mytable mycolumn=value\n| STATS avg(mycolumn) BY mycolumn\n| SEARCH avg(mycolumn)=value\n| FIELDS mycolumn, avg(mycolumn)"], ["LIKE", "SELECT *\nFROM mytable\nWHERE mycolumn LIKE\"%some text%\"", "source=mytable \n  mycolumn=\"*some text*\"Note:The most common search in Splunk SPL is nearly impossible in SQL - to search all fields for a substring. The following SPL search returns all rows that contain \"some text\" anywhere:source=mytable\"some text\""], ["ORDER BY", "SELECT *\nFROM mytable\nORDER BY mycolumn desc", "source=mytable\n| SORT -mycolumnIn SPL you use a negative sign ( - ) in front of a field name to sort in descending order."], ["SELECT DISTINCT", "SELECT DISTINCT \n  mycolumn1, mycolumn2\nFROM mytable", "source=mytable\n| DEDUP mycolumn1, mycolumn2\n| FIELDS mycolumn1, mycolumn2"], ["SELECT TOP", "SELECT TOP(5) \nmycolum1, \nmycolum2\nFROM mytable1\nWHERE mycolum3 =\"bar\"ORDER BY mycolum1 mycolum2", "Source=mytable1 mycolum3=\"bar\"| FIELDS mycolum1 mycolum2\n| SORT mycolum1 mycolum2\n| HEAD 5"], ["INNER JOIN", "SELECT *\nFROM mytable1\nINNER JOIN mytable2\nON mytable1.mycolumn= \n  mytable2.mycolumn", "index=myIndex1 OR index=myIndex2\n| stats values(*) AS * BY myFieldNote:There are two other methods to join tables:Use thelookupcommand to add fields from an external table:... | LOOKUP myvaluelookup \n  mycolumn \n  OUTPUT myoutputcolumnUse a subsearch:source=mytable1\n  [SEARCHsource=mytable2 \n    mycolumn2=myvalue\n    | FIELDS mycolumn2]If the columns that you want to join on have different names, use therenamecommand to rename one of the columns.  For example, to rename the column in mytable2:source=mytable1 \n| JOINtype=inner mycolumn \n  [ SEARCHsource=mytable2 \n    | RENAME mycolumn2 \n    AS mycolumn]To rename the column in myindex1:index=myIndex1 OR index=myIndex2\n| rename myfield1 as myField\n| stats values(*) AS * BY myFieldYou can rename a column regardless of whether you use the search command, a lookup, or a subsearch."], ["LEFT (OUTER) JOIN", "SELECT *\nFROM mytable1\nLEFT JOIN mytable2\nON mytable1.mycolumn=\n  mytable2.mycolumn", "source=mytable1\n| JOINtype=left mycolumn \n  [SEARCHsource=mytable2]"], ["SELECT INTO", "SELECT *\nINTO new_mytable IN mydb2\nFROM old_mytable", "source=old_mytable\n| EVALsource=new_mytable\n| COLLECT index=mydb2Note:COLLECT is typically used to store expensively calculated fields back into your Splunk deployment so that future access is much faster. This current example is atypical but shown for comparison to the SQL command. The source will be renamed orig_source"], ["TRUNCATE TABLE", "TRUNCATE TABLE mytable", "source=mytable\n| DELETE"], ["INSERT INTO", "INSERT INTO mytable\nVALUES (value1, value2, value3,....)", "Note:See SELECT INTO. Individual records are not added via the search language, but can be added via the API if need be."], ["UNION", "SELECT mycolumn\nFROM mytable1\nUNION\nSELECT mycolumn FROM mytable2", "source=mytable1\n| APPEND \n  [SEARCHsource=mytable2]\n| DEDUP mycolumn"], ["UNION ALL", "SELECT *\nFROM mytable1\nUNION ALL\nSELECT * FROM mytable2", "source=mytable1\n| APPEND \n  [SEARCHsource=mytable2]"], ["DELETE", "DELETE FROM mytable\nWHERE mycolumn=5", "source=mytable1 mycolumn=5\n| DELETE"], ["UPDATE", "UPDATE mytable\nSET column1=value, \n  column2=value,...\nWHERE some_column=some_value", "Note:There are a few things to think about when updating records inSplunk Enterprise. First, you can just add the new values to your Splunk deployment  (see INSERT INTO) and not worry about deleting the old values, because Splunk software always returns the most recent results first. Second, on retrieval, you can always de-duplicate the results to ensure only the latest values are used (see SELECT DISTINCT). Finally, you can actually delete the old records (see DELETE)."]]}], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Splunk SPL for SQL users", "section_heading": "From SQL to Splunk SPL", "section_id": "id_7e728fa9_ff57_402b_9270_4fab64e8947e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/splunk-spl-for-sql-users", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:50.488946+00:00", "version": "10.2"}}
{"id": "add630193018fa0b", "content": "Understanding SPL syntax", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Splunk SPL for SQL users", "section_heading": "See also", "section_id": "id_4ccd1654_bf0a_42cb_9ceb_313a6ae42607--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/splunk-spl-for-sql-users", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:24:50.488951+00:00", "version": "10.2"}}
{"id": "fc2f37294865ef14", "content": "Description This function returns a count of the UTF-8 code points in a string. While the character length and number of code points are identical for some strings in English, the count is not the same for all strings, including strings in other languages. Note: If your strings contain non-ASCII characters that aren't in UTF-8 format, you must perform a code conversion before using the len function in searches. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. When specifying this function, you can use either len or length for the function name. This function is not supported on multivalue fields. Basic example Suppose you have a set of results that looks something like this: You can determine the length of the values in the names field using the len function: The results show a count of the character length of the values in the names field:", "code_examples": [{"language": "spl", "code": "... |evallength=len(names)"}], "tables": [{"headers": ["_time", "names"], "rows": [["2020-01-09 16:35:14", "buttercup"], ["2020-01-09 16:35:14", "rarity"], ["2020-01-09 16:35:14", "tenderhoof"], ["2020-01-09 16:35:14", "dash"], ["2020-01-09 16:35:14", "mistmane"]]}, {"headers": ["_time", "length", "names"], "rows": [["2020-01-09 16:35:14", "9", "buttercup"], ["2020-01-09 16:35:14", "6", "rarity"], ["2020-01-09 16:35:14", "10", "tenderhoof"], ["2020-01-09 16:35:14", "4", "dash"], ["2020-01-09 16:35:14", "8", "mistmane"]]}], "chunk_index": 0, "total_chunks": 11, "metadata": {"title": "Text functions", "section_heading": "len(<str>)", "section_id": "id_4689a0c3_c8ce_4833_ad84_98fbac9cf6af--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/text-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:08.628856+00:00", "version": "10.2"}}
{"id": "4044979e64f25b10", "content": "Description This function takes one string argument and returns the string in lowercase. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You can use this function on multivalue fields. Basic example The following example returns the value provided by the field username in lowercase.", "code_examples": [{"language": "spl", "code": "... |evalusername=lower(username)"}], "tables": [], "chunk_index": 1, "total_chunks": 11, "metadata": {"title": "Text functions", "section_heading": "lower(<str>)", "section_id": "id_1f8b5367_39cb_4e10_b53d_37cb03d6bfda--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/text-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:08.628866+00:00", "version": "10.2"}}
{"id": "483814339d0892c9", "content": "Description This function removes characters from the left side of a string. Usage The <str> argument can be the name of a string field or a string literal. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The <trim_chars> argument is optional. If not specified, spaces and tabs are removed from the left side of the string. This function is not supported on multivalue fields. Basic example The following example trims the leading spaces and all of the occurrences of the letter Z from the left side of the string. The value that is returned is x=\"abcZZ \". The following example removes the dollar sign ( $ ) from the results for the NET_COST field.", "code_examples": [{"language": "spl", "code": "... |evalx=ltrim(\"   ZZZZabcZZ \",\" Z\")"}, {"language": "spl", "code": "... |evalcost=ltrim(NET_COST,\"$\")"}], "tables": [], "chunk_index": 2, "total_chunks": 11, "metadata": {"title": "Text functions", "section_heading": "ltrim(<str>,<trim_chars>)", "section_id": "df115d0b_7596_4cb7_840d_6304da0c6c88--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/text-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:08.628874+00:00", "version": "10.2"}}
{"id": "0394706db9d26882", "content": "Description This function substitutes the replacement string for every occurrence of the regular expression in the string. Usage The <str> argument can be the name of a string field or a string literal. The <replacement> argument can also reference groups that are matched in the <regex> using perl-compatible regular expressions (PCRE) syntax. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. This function is not supported on multivalue fields. To replace a backslash ( \\ ) character, you must escape the backslash twice. This is because the replace function occurs inside an eval expression. The eval expression performs one level of escaping before passing the regular expression to PCRE. Then PCRE performs its own escaping. See SPL and regular expressions. Basic example The following example returns the values in the date field, with the month and day numbers switched. If the input is 1/14/2023 the return value would be 14/1/2023.", "code_examples": [{"language": "spl", "code": "... |evaln=replace(date,\"^(\\d{1,2})/(\\d{1,2})/\",\"\\2/\\1/\")"}], "tables": [], "chunk_index": 3, "total_chunks": 11, "metadata": {"title": "Text functions", "section_heading": "replace(<str>,<regex>,<replacement>)", "section_id": "c63248fc_bde4_420a_a2f9_dbbcb4e72252--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/text-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:08.628880+00:00", "version": "10.2"}}
{"id": "f2643df40fb82c1f", "content": "Description This function removes the trim characters from the right side of the string. Usage The <str> argument can be the name of a string field or a string literal. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The <trim_chars> argument is optional. If not specified, spaces and tabs are removed from the right side of the string. This function is not supported on multivalue fields. Basic example The following example trims the leading spaces and all of the occurrences of the letter Z from the right side of the string. The value returned is ZZZZabc .", "code_examples": [{"language": "spl", "code": "... |evaln=rtrim(\"   ZZZZabcZZ \",\" Z\")"}], "tables": [], "chunk_index": 4, "total_chunks": 11, "metadata": {"title": "Text functions", "section_heading": "rtrim(<str>,<trim_chars>)", "section_id": "c12b4ab5_30e2_48a3_8b01_66d678856531--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/text-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:08.628886+00:00", "version": "10.2"}}
{"id": "9388d1c1c1640dfd", "content": "Description Use this function to extract information from the structured data formats XML and JSON. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The <value> is an input source field. The <path> is an spath expression for the location path to the value that you want to extract from. If <path> is a literal string, you need to enclose the string in double quotation marks. If <path> is a field name, with values that are the location paths, the field name doesn't need quotation marks. Using a field name for <path> might result in a multivalue field. This function is not supported on multivalue fields. Basic example The following example returns the values of locDesc elements. The following example returns the hashtags from a twitter event. index=twitter | eval output=spath(_raw, \"entities.hashtags\")", "code_examples": [{"language": "spl", "code": "... |evallocDesc=spath(_raw,\"vendorProductSet.product.desc.locDesc\")"}], "tables": [], "chunk_index": 5, "total_chunks": 11, "metadata": {"title": "Text functions", "section_heading": "spath(<value>,<path>)", "section_id": "ccdbf2f5_ab78_49aa_9c59_da4e52977128--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/text-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:08.628892+00:00", "version": "10.2"}}
{"id": "df2feaa831581d52", "content": "Description This function returns a substring of a string, beginning at the start index. The length of the substring specifies the number of character to return. Usage The <str> argument can be the name of a string field or a string literal. The indexes follow SQLite semantics; they start at 1. Negative indexes can be used to indicate a start from the end of the string. The <length> is optional, and if not specified returns the rest of the string. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. This function is not supported on multivalue fields. Basic example The following example concatenates \"str\" and \"ing\" together, returning \"string\":", "code_examples": [{"language": "spl", "code": "... |evaln=substr(\"string\", 1, 3) + substr(\"string\", -3)"}], "tables": [], "chunk_index": 6, "total_chunks": 11, "metadata": {"title": "Text functions", "section_heading": "substr(<str>,<start>,<length>)", "section_id": "id_3867fdc9_7449_4e3a_a4df_5b4858b8ee43--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/text-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:08.628897+00:00", "version": "10.2"}}
{"id": "a315d7cf0e1c0bb5", "content": "Description This function removes the trim characters from both sides of the string. Usage The <str> argument can be the name of a string field or a string literal. The <trim_chars> argument is optional. If not specified, spaces and tabs are removed from both sides of the string. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. This function is not supported on multivalue fields. Basic example The following example trims the leading spaces and all of the occurrences of the letter Z from the left and right sides of the string. The value returned is abc .", "code_examples": [{"language": "spl", "code": "... |evaln=trim(\"   ZZZZabcZZ \",\" Z\")"}], "tables": [], "chunk_index": 7, "total_chunks": 11, "metadata": {"title": "Text functions", "section_heading": "trim(<str>,<trim_chars>)", "section_id": "id_908166e3_2539_4b22_8b48_c64b509aa356--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/text-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:08.628902+00:00", "version": "10.2"}}
{"id": "95cc4cebe7eae1f5", "content": "Description This function returns a string in uppercase. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You can use this function on multivalue fields. Basic example The following example returns the value provided by the field username in uppercase.", "code_examples": [{"language": "spl", "code": "... |evaln=upper(username)"}], "tables": [], "chunk_index": 8, "total_chunks": 11, "metadata": {"title": "Text functions", "section_heading": "upper(<str>)", "section_id": "ab29e1ec_9ecc_49e5_a206_2450e8799e32--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/text-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:08.628907+00:00", "version": "10.2"}}
{"id": "c4f5e6f7007c1638", "content": "Description This function takes one URL string argument X and returns the unescaped or decoded URL string. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. This function is not supported on multivalue fields. Basic example The following example returns \"http://www.splunk.com/download?r=header\".", "code_examples": [{"language": "spl", "code": "... |evaln=urldecode(\"http%3A%2F%2Fwww.splunk.com%2Fdownload%3Fr%3Dheader\")"}], "tables": [], "chunk_index": 9, "total_chunks": 11, "metadata": {"title": "Text functions", "section_heading": "urldecode(<url>)", "section_id": "id_0ddf159f_52f2_4d2f_88c5_9877eb80fd09--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/text-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:08.628912+00:00", "version": "10.2"}}
{"id": "0513691025e85284", "content": "Related functions tostring", "code_examples": [], "tables": [], "chunk_index": 10, "total_chunks": 11, "metadata": {"title": "Text functions", "section_heading": "See also", "section_id": "eb672b7e_2dc8_42a5_b10b_66b5cf1ead54--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/text-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:08.628917+00:00", "version": "10.2"}}
{"id": "b4d0023cdcbfc4ee", "content": "Description This function computes the arc cosine of X, in the interval [0,pi] radians. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples This example returns 1.5707963267948966. The following example calculates 180 divided by pi and multiplies the result by the arc cosine of 0. This example returns 90.0000000003 .", "code_examples": [{"language": "spl", "code": "... |evaln=acos(0)"}, {"language": "spl", "code": "... |evaldegrees=acos(0)*180/pi()"}], "tables": [], "chunk_index": 0, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "acos(X)", "section_id": "id_022a52cd_0bc4_4847_82ef_7835fe05ef43--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.516961+00:00", "version": "10.2"}}
{"id": "01491a7bb328b350", "content": "Description This function computes the arc hyperbolic cosine of X radians. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example This example returns 1.3169578969248166 .", "code_examples": [{"language": "spl", "code": "... |evaln=acosh(2)"}], "tables": [], "chunk_index": 1, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "acosh(X)", "section_id": "id_7904a0b6_303f_4363_a251_8a688c61b214--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.516968+00:00", "version": "10.2"}}
{"id": "f92ff540b1383dcc", "content": "Description This function computes the arc sine of X, in the interval [-pi/2,+pi/2] radians. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example This example returns 1.5707963267948966. The following example calculates 180 divided by pi and multiplies that by the arc sine of 1.", "code_examples": [{"language": "spl", "code": "... |evaln=asin(1)"}, {"language": "spl", "code": "... |evaldegrees=asin(1)*180/pi()"}], "tables": [], "chunk_index": 2, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "asin(X)", "section_id": "id_0c0d67da_927a_4d2b_b30e_1bde1eb86176--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.516973+00:00", "version": "10.2"}}
{"id": "713df1219bf9885f", "content": "Description This function computes the arc hyperbolic sine of X radians. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example This example returns 0.881373587019543 .", "code_examples": [{"language": "spl", "code": "... |evaln=asinh(1)"}], "tables": [], "chunk_index": 3, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "asinh(X)", "section_id": "id_6ebbe5dd_d293_4056_98de_4c5796f1b12e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.516977+00:00", "version": "10.2"}}
{"id": "83728d838276620b", "content": "Description This function computes the arc tangent of X, in the interval [-pi/2,+pi/2] radians. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example This example returns 0.46 .", "code_examples": [{"language": "spl", "code": "... |evaln=atan(0.50)"}], "tables": [], "chunk_index": 4, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "atan(X)", "section_id": "id_8d8f38bc_5d52_4e33_9ac5_588b05dd2121--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.516980+00:00", "version": "10.2"}}
{"id": "9dee65ef5bba8489", "content": "Description This function computes the arc tangent of Y, X in the interval [-pi,+pi] radians. Y is a value that represents the proportion of the y-coordinate. X is the value that represents the proportion of the x-coordinate. To compute the value, the function takes into account the sign of both arguments to determine the quadrant. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example This example returns 0.59 .", "code_examples": [{"language": "spl", "code": "... |evaln=atan2(0.50, 0.75)"}], "tables": [], "chunk_index": 5, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "atan2(Y, X)", "section_id": "f1d3824a_241b_497d_97a7_550ab96aff07--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.516985+00:00", "version": "10.2"}}
{"id": "2664ee62e247c70e", "content": "Description This function computes the arc hyperbolic tangent of X radians. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example This example returns 0.549 .", "code_examples": [{"language": "spl", "code": "... |evaln=atanh(0.500)"}], "tables": [], "chunk_index": 6, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "atanh(X)", "section_id": "e71dea00_a844_4f1d_a52c_f16a7b00ab28--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.516988+00:00", "version": "10.2"}}
{"id": "be245e73cb00ea09", "content": "Description This function computes the cosine of an angle of X radians. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples This example returns 0.5403023058681398. The following example calculates the cosine of pi and returns -1.00000000000 .", "code_examples": [{"language": "spl", "code": "... |evaln=cos(-1)"}, {"language": "spl", "code": "... |evaln=cos(pi())"}], "tables": [], "chunk_index": 7, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "cos(X)", "section_id": "e1a0c967_ed7b_4be2_a463_228b2a0ed001--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.516993+00:00", "version": "10.2"}}
{"id": "b708f0c4f14bfa55", "content": "Description This function computes the hyperbolic cosine of X radians. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example This example returns 1.5430806348152437 .", "code_examples": [{"language": "spl", "code": "... |evaln=cosh(1)"}], "tables": [], "chunk_index": 8, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "cosh(X)", "section_id": "id_4cf6c908_a843_4ee0_8a29_f80e8b80b519--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.516997+00:00", "version": "10.2"}}
{"id": "73c191776e43ff6f", "content": "Description This function computes the hypotenuse of a right-angled triangle whose legs are X and Y. The function returns the square root of the sum of the squares of X and Y, as described in the Pythagorean theorem. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example Creates a field called n and returns n=5 , which is the hypotenuse of a triangle whose legs are 3 and 4.", "code_examples": [{"language": "spl", "code": "... |evaln=hypot(3,4)"}], "tables": [], "chunk_index": 9, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "hypot(X,Y)", "section_id": "id_137cb1f3_4503_492c_9224_78d21b6d019b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.517002+00:00", "version": "10.2"}}
{"id": "1c1c7b2fcbf93294", "content": "Description This function computes the sine of X. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples This example returns 0.8414709848078965. The following example calculates the sine of pi divided by 180 and then multiplied by 90.", "code_examples": [{"language": "spl", "code": "... |evaln=sin(1)"}, {"language": "spl", "code": "... |evaln=sin(90 * pi()/180)"}], "tables": [], "chunk_index": 10, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "sin(X)", "section_id": "c281b549_dc9a_4a73_8093_79d7c7b09202--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.517006+00:00", "version": "10.2"}}
{"id": "5fd66be63619247a", "content": "Description This function computes the hyperbolic sine of X radians. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example This example returns 1.1752011936438014 .", "code_examples": [{"language": "spl", "code": "... |evaln=sinh(1)"}], "tables": [], "chunk_index": 11, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "sinh(X)", "section_id": "id_2dac7cda_76bc_47a5_ad43_c7e4c83ac611--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.517010+00:00", "version": "10.2"}}
{"id": "81c8a2146ced48a2", "content": "Description This function computes the tangent of X radians. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples This example returns 1.5574077246549023 This example returns -0.08871575677006045", "code_examples": [{"language": "spl", "code": "... |evaln=tan(1)"}, {"language": "spl", "code": "... |evaln=tan(135)"}], "tables": [], "chunk_index": 12, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "tan(X)", "section_id": "id_2f996c19_6bf5_495c_a3f5_e6ad17994a10--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.517014+00:00", "version": "10.2"}}
{"id": "33bf59060c0c386b", "content": "Description This function computes the hyperbolic tangent of X radians. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example This example returns 0.7615941559557649", "code_examples": [{"language": "spl", "code": "... |evaln=tanh(1)"}], "tables": [], "chunk_index": 13, "total_chunks": 14, "metadata": {"title": "Trig and Hyperbolic functions", "section_heading": "tanh(X)", "section_id": "id_25899add_c9c6_4ad9_92ea_9770ffaa0e3d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/trig-and-hyperbolic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:25.517018+00:00", "version": "10.2"}}
{"id": "075377643ec0ac1c", "content": "The where command uses eval-expressions to filter search results. These eval-expressions must be Boolean expressions, where the expression returns either true or false. The where command returns only the results for which the eval expression returns true.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "where", "section_heading": "Description", "section_id": "id_40836dfd_a3d3_4bf9_bc5a_989748b75d3d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/where", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:41.992323+00:00", "version": "10.2"}}
{"id": "30806c72cfb09f66", "content": "where <eval-expression> Required arguments eval-expression Syntax: <eval-mathematical-expression> | <eval-concatenate-expression> | <eval-comparison-expression> | <eval-boolean-expression> | <eval-function-call> Description: A combination of values, variables, operators, and functions that represent the value of your destination field. See Usage. The <eval-expression> is case-sensitive. The syntax of the eval expression is checked before running the search, and an exception is thrown for an invalid expression. The following table describes characteristics of eval expressions that require special handling.", "code_examples": [], "tables": [{"headers": ["Expression characteristics", "Description", "Example"], "rows": [["Field names starting with numeric characters", "If the expression references a field name that starts with a numeric character, the field name must be surrounded by single quotation marks.", "'5minutes'=\"late\"This expression is a field name equal to a string value. Because the field starts with a numeric it must be enclosed in single quotations. Because the value is a string, it must be enclosed in double quotations."], ["Field names with non-alphanumeric characters", "If the expression references a field name that contains non-alphanumeric characters, the field name must be surrounded by single quotation marks.", "new=count+'server-1'This expression could be interpreted as a mathematical equation, where the dash is interpreted as a minus sign. To avoid this, you must enclose the field nameserver-1in single quotation marks."], ["Literal strings", "If the expression references a literal string, the literal string must be surrounded by double quotation marks.", "new=\"server-\"+countThere are two issues with this example. First,server-could be interpreted as a field name or as part of a mathematical equation, that uses a minus sign and a plus sign. To ensure thatserver-is interpreted as a literal string, enclose the string in double quotation marks."]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "where", "section_heading": "Syntax", "section_id": "id_18deb409_934b_420b_a27b_241c034165fa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/where", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:41.992333+00:00", "version": "10.2"}}
{"id": "9a063a8d4a96334d", "content": "The where command is a distributable streaming command. See Command types. The <eval-expression> is case-sensitive. The where command uses the same expression syntax as the eval command. Also, both commands interpret quoted strings as literals. If the string is not quoted, it is treated as a field name. Because of this, you can use the where command to compare two different fields, which you cannot use the search command to do. Boolean expressions The order in which Boolean expressions are evaluated with the where command is: Expressions within parentheses NOT clauses AND clauses OR clauses XOR clauses This evaluation order is different than the order used with the search command, which evaluates OR before AND clauses, and doesn't support XOR. See Boolean expressions with logical operators in the Splunk platform Search Manual. Using a wildcard with the where command You can only specify a wildcard by using the like function with the where command. The percent ( % ) symbol is the wildcard that you use with the like function. See the like() evaluation function. Supported functions You can use a wide range of evaluation functions with the where command. For general information about using functions, see Evaluation functions. For a list of functions by category, see Function list by category. For an alphabetical list of functions, see Alphabetical list of functions .", "code_examples": [{"language": "spl", "code": "... |whereipaddress=clientip"}, {"language": "spl", "code": "| search host=www2"}, {"language": "spl", "code": "... |wherehost=\"www2\""}], "tables": [{"headers": ["Command", "Example", "Description"], "rows": [["Where", "... |whereipaddress=clientip", "This search looks for events where the fieldipaddressis equal to the fieldclientip."], ["Search", "| search host=www2", "This search looks for events where the fieldhostcontains the string valuewww2."], ["Where", "... |wherehost=\"www2\"", "This search looks for events where the value in the fieldhostis the string valuewww2."]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "where", "section_heading": "Usage", "section_id": "id_71951eb2_71d0_4364_9070_f3679a959fbc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/where", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:41.992339+00:00", "version": "10.2"}}
{"id": "16b732c0f210c19a", "content": "1. Specify a wildcard with the where command You can only specify a wildcard with the where command by using the like function. The percent ( % ) symbol is the wildcard you must use with the like function. The where command returns like=TRUE if the ipaddress field starts with the value 198. 2. Match IP addresses or a subnet using the where command Return \"CheckPoint\" events that match the IP or is in the specified subnet. 3. Specify a calculation in the where command expression Return \"physicsjobs\" events with a speed is greater than 100.", "code_examples": [{"language": "spl", "code": "... |wherelike(ipaddress,\"198.%\")"}, {"language": "spl", "code": "host=\"CheckPoint\"|wherelike(src,\"10.9.165.%\") OR cidrmatch(\"10.9.165.0/25\", dst)"}, {"language": "spl", "code": "sourcetype=physicsjobs |wheredistance/time > 100"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "where", "section_heading": "Examples", "section_id": "ed68ac25_566b_4a2d_9515_8f971c23f0b1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/where", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:41.992344+00:00", "version": "10.2"}}
{"id": "de93962bac6363ee", "content": "eval , search , regex", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "where", "section_heading": "See also", "section_id": "b92f1336_b823_4ffa_ba24_8e4ddbf5392d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/where", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:41.992348+00:00", "version": "10.2"}}
{"id": "15bcab9f32167c64", "content": "Description This function takes a search string, or field that contains a search string, and returns a multivalued field containing a list of the commands used in <value>. Usage This function is generally not recommended for use except for analysis of audit.log events. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example The following example returns a multivalued field called x, that contains the commands search , stats , and sort which are the commands used in the search string specified.", "code_examples": [{"language": "spl", "code": "... |evalx=commands(\"search foo | stats count | sort count\")"}], "tables": [], "chunk_index": 0, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "commands(<value>)", "section_id": "id_9bf98eec_3fff_460a_bd1a_b3f252a63a80--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675322+00:00", "version": "10.2"}}
{"id": "1274435c38d7098c", "content": "Description This function takes one or more values and returns a single multivalue result that contains all of the values. The values can be strings, multivalue fields, or single value fields. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples This example shows how to append two values, localhost is a literal string value and srcip is a field name. The following example shows how to use nested mvappend functions. The inner mvappend function contains two values: localhost is a literal string value and srcip is a field name. The outer mvappend function contains three values: the inner mvappend function, destip is a field name, and 192.168.1.1 which is a literal IP address. The results are placed in a new field called ipaddresses , which contains the array [\"localhost\", <values_in_scrip>, <values_in_destip>, \"192.168.1.1\"]. Note that the previous example generates the same results as the following example, which does not use a nested mvappend function: If the first value in the srcip field is 203.0.113.0 and the first value in the destip field is 203.0.113.255, the results look something like this:", "code_examples": [{"language": "spl", "code": "... |evalfullName=mvappend(\"localhost\", srcip)"}, {"language": "spl", "code": "... |evalipaddresses=mvappend(mvappend(\"localhost\", srcip), destip,\"192.168.1.1\")"}, {"language": "spl", "code": "| makeresults |evalipaddresses=mvappend(\"localhost\", srcip, destip,\"192.168.1.1\")"}], "tables": [{"headers": ["time", "ipaddresses"], "rows": [["2024-11-19 16:43:31", "localhost203.0.113.0203.0.113.255192.168.1.1"]]}], "chunk_index": 1, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvappend(<values>)", "section_id": "aab085fc_ffda_42b4_b3d3_247d19acc8f4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675333+00:00", "version": "10.2"}}
{"id": "35c66fec1505895b", "content": "Description This function takes a field and returns a count of the values in that field for each result. If the field is a multivalue field, this function returns the number of values in that field. If the field contains a single value, this function returns 1. If the field has no values, this function returns NULL. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example Extended example In the following example, the mvcount() function returns the number of email addresses in the To , From , and Cc fields and saves the addresses in the specified \"_count\" fields. This search takes the values in the To field and uses the split function to separate the email address on the @ symbol. The split function is also used on the Cc field for the same purpose. If only a single email address exists in the From field, as you would expect, mvcount(From) returns 1. If there is no Cc address, the Cc field might not exist for the event. In that situation mvcount(cc) returns NULL.", "code_examples": [{"language": "spl", "code": "... |evaln=mvcount(multifield)"}, {"language": "spl", "code": "eventtype=\"sendmail\"|evalTo_count=mvcount(split(To,\"@\"))-1 \n|evalFrom_count=mvcount(From) \n|evalCc_count= mvcount(split(Cc,\"@\"))-1"}], "tables": [], "chunk_index": 2, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvcount(<mv>)", "section_id": "c75b8df5_bcd3_424f_bde4_7c8c1337b7c6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675340+00:00", "version": "10.2"}}
{"id": "5cc371eeb7cb3bce", "content": "Description This function takes a multivalue field and returns a multivalue field with its duplicate values removed. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example", "code_examples": [{"language": "spl", "code": "... |evals=mvdedup(mvfield)"}], "tables": [], "chunk_index": 3, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvdedup(<mv>)", "section_id": "cc75fce9_a3ad_4645_b167_b13c0f693a77--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675345+00:00", "version": "10.2"}}
{"id": "90a6073c39f3143f", "content": "Description This function filters a multivalue field based on an arbitrary Boolean expression. The Boolean expression can reference ONLY ONE field at a time. Usage This function will return NULL values of the field as well. If you do not want the NULL values, use one of the following expressions: mvfilter(!isnull(<value>)) mvfilter(isnotnull(<value>)) You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example The following example returns all of the values in field email that end in .net or .org .", "code_examples": [{"language": "spl", "code": "... |evaln=mvfilter(match(email,\"\\.net$\") OR match(email,\"\\.org$\"))"}], "tables": [], "chunk_index": 4, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvfilter(<predicate>)", "section_id": "e8f82913_5e50_424b_81b1_9d84f562bba5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675350+00:00", "version": "10.2"}}
{"id": "1ddcde5d36e9f2c5", "content": "Description This function tries to find a value in the multivalue field that matches the regular expression. If a match exists, the index of the first matching value is returned (beginning with zero). If no values match, NULL is returned. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example", "code_examples": [{"language": "spl", "code": "... |evaln=mvfind(mymvfield,\"err\\d+\")"}], "tables": [], "chunk_index": 5, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvfind(<mv>,<regex>)", "section_id": "id_98728f4c_5d49_4217_974d_77dbdde4658e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675355+00:00", "version": "10.2"}}
{"id": "8649f4598cee3c35", "content": "Description This function returns a subset of the multivalue field using the start and end index values. Usage The <mv> argument must be a multivalue field. The <start> and <end> indexes must be numbers. The <mv> and <start> arguments are required. The <end> argument is optional. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Specifying the start and end indexes Indexes start at zero. If you have 5 values in the multivalue field, the first value has an index of 0. The second value has an index of 1, and so on. If only the <start> argument is specified, only that value is included in the results. When the <end> argument is specified, the range of values from <start> to <end> are included in the results. Both the <start> and <end> arguments can be negative. An index of -1 is used to specify the last value in the list. If the indexes are out of range or invalid, the result is NULL. Examples Consider the following values in a multivalue field called names : Because indexes start at zero, the following example returns the value claudia : To return a range of values, specify both a <start> and <end> value. For example, the following search returns the first 4 values in the field. The start value is 0 and the end value is 3. The results look like this: Extended examples Consider the following values in a multivalue field: To return a value from the end of the list of values, the index numbers start with -1. The negative symbol indicates that the indexing starts from the last value. For example: To return the last value in the list, you specify -1 , which indicates to start at the end of the list and return only one value. For example: The results look like this: To return the 3rd value from the end, you would specify the index number -3. For example: The results look like this: To return a range of values, specify both a <start> and <end> value. For example, the following search returns the last 3 values in the field. The start value is -3 and the end value is -1. The results look like this:", "code_examples": [{"language": "spl", "code": "... |evalmy_names=mvindex(names,2)"}, {"language": "spl", "code": "... |evalmy_names=mvindex(names,0,3)"}, {"language": "spl", "code": "... |evalmy_ponies=mvindex(ponies,-1)"}, {"language": "spl", "code": "... |evalmy_ponies=mvindex(ponies,-3)"}, {"language": "spl", "code": "... |evalmy_ponies=mvindex(ponies, -3, -1)"}], "tables": [{"headers": ["Name", "alex", "celestino", "claudia", "david", "ikraam", "nyah", "rutherford", "wei"], "rows": [["index number", "0", "1", "2", "3", "4", "5", "6", "7"]]}, {"headers": ["my_names"], "rows": [["alex,celestino,claudia,david"]]}, {"headers": ["ponies"], "rows": [["buttercup, dash, flutter, honey, ivory, minty, pinky, rarity"]]}, {"headers": ["Pony name", "buttercup", "dash", "flutter", "honey", "ivory", "minty", "pinky", "rarity"], "rows": [["index number", "-8", "-7", "-6", "-5", "-4", "-3", "-2", "-1"]]}, {"headers": ["my_ponies"], "rows": [["rarity"]]}, {"headers": ["my_ponies"], "rows": [["minty"]]}, {"headers": ["my_ponies"], "rows": [["minty,pinky,rarity"]]}], "chunk_index": 6, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvindex(<mv>,<start>,<end>)", "section_id": "id_15ffe888_e873_4c37_8d3a_2b837b8a4d38--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675371+00:00", "version": "10.2"}}
{"id": "2fd2c1eb9269fc22", "content": "Description This function takes two arguments, a multivalue field and a string delimiter. The function concatenates the individual values within <mv> using the value of <delim> as a separator. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples You have a multivalue field called \"base\" that contains the values \"1\" \"2\" \"3\" \"4\" \"5\". The values are separated by a space. You want to create a single value field instead, with OR as the delimiter. For example \"1 OR 2 OR 3 OR 4 OR 5\". The following search creates the base field with the values. The search then creates the joined field by using the result of the mvjoin function. The following example joins together the individual values of \"myfield\" using a semicolon as the delimiter:", "code_examples": [{"language": "spl", "code": "... |evalbase=mvrange(1,6), joined=mvjoin('base',\" OR \")"}, {"language": "spl", "code": "... |evaln=mvjoin(myfield,\";\")"}], "tables": [], "chunk_index": 7, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvjoin(<mv>,<delim>)", "section_id": "id_50128fbd_f911_4dbc_b5a3_39039c42809a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675376+00:00", "version": "10.2"}}
{"id": "c62ca279dbf3d20a", "content": "Description This function iterates over the values of a multivalue field, performs an operation using the <expression> on each value, and returns a multivalue field with the list of results. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example multiplies each value in the results field by 10. The following example multiplies each value in the results field by threshold , where threshold is a single-valued field. The following example multiplies the 2nd and 3rd values in the results field by threshold , where threshold is a single-valued field. This example uses the mvindex function to identify specific values in the results field.", "code_examples": [{"language": "spl", "code": "... |evaln=mvmap(results, results*10)"}, {"language": "spl", "code": "... |evaln=mvmap(results, results*threshold)"}, {"language": "spl", "code": "... |evaln=mvmap(mvindex(results, 1,2), results*threshold)"}], "tables": [], "chunk_index": 8, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvmap(<mv>,<expression>)", "section_id": "id_200cbf75_6fa7_409b_ab06_730df11445c4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675381+00:00", "version": "10.2"}}
{"id": "e107f8e78cb025f1", "content": "Description This function creates a multivalue field for a range of numbers. This function can contain up to three arguments: a starting number, an ending number (which is excluded from the field), and an optional step increment. If the increment is a timespan such as 7d , the starting and ending numbers are treated as UNIX time. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The step increment is optional. If the <step> increment is a timespan such as 7d, the starting and ending numbers are treated as UNIX time. The <end> number is not included from the multivalue field that is created. Basic examples The following example returns a multivalue field with the values 1, 3, 5, 7, 9. The following example takes the UNIX timestamp for 1/1/2018 as the start date and the UNIX timestamp for 4/19/2018 as an end date and uses the increment of 7 days. This example returns a multivalue field with the UNIX timestamps. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "... |evalmv=mvrange(1,11,2)"}, {"language": "spl", "code": "| makeresults |evalmv=mvrange(1514834731,1524134919,\"7d\")"}], "tables": [{"headers": ["_time", "mv"], "rows": [["2018-04-10 12:31:03", "1514834731151543953115160443311516649131151725393115178587311518463531151906833115196731311520277931152087913115214839311522088731152269353115232983311523903131"]]}], "chunk_index": 9, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvrange(<start>,<end>,<step>)", "section_id": "id_38aa2311_81e7_4a42_9d76_8a64a7349058--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675388+00:00", "version": "10.2"}}
{"id": "d96f29db0c14bf9d", "content": "Description The mvreverse command reverses the order of the values in a multivalue field. Usage You must first construct a multivalue field in order to use the mvreverse function on that field. The following two examples show how to build a valid multivalue field using the split and mvappend eval functions. Examples The following example reverses the order of the values in the multivalue field myfield. The following example reverses the order of the values in the multivalue field myfield by changing \"1\", \"2\", \"3\" to \"3\", \"2\", \"1\". The following example reverses the order of the values in multivalue field myfield from \"1\",\"2\",\"3\" to \"3\", \"2\", \"1\" in multivalue field new_myfield .", "code_examples": [{"language": "spl", "code": "| makeresults |evala=mvreverse(split(\"1,2,3\",\",\"))"}, {"language": "spl", "code": "| makeresults |evalb = mvappend(\"1\",\"2\",\"3\"), a=mvreverse(b)"}, {"language": "spl", "code": "| makeresults\n|evalmyfield =\"one,two,three\"| makemv tokenizer =\"([^,]+),?\"myfield\n|evalnew_myfield = mvreverse(myfield)"}, {"language": "spl", "code": "| makeresults \n|evalmyfield = mvreverse(mvappend(\"1\",\"2\",\"3\"))"}, {"language": "spl", "code": "| makeresults \n|evalmyfield = mvappend(\"1\",\"2\",\"3\"), new_myfield=mvreverse(myfield)"}], "tables": [], "chunk_index": 10, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvreverse(<value>)", "section_id": "cb11b97c_130e_4ec3_9ccd_8a2e5071447e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675393+00:00", "version": "10.2"}}
{"id": "0cd9d2fc8a15423c", "content": "Description This function uses a multivalue field and returns a multivalue field with the values sorted lexicographically. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Lexicographical order sorts items based on the values used to encode the items in computer memory. In Splunk software, this is almost always UTF-8 encoding, which is a superset of ASCII. Numbers are sorted before letters. Numbers are sorted based on the first digit. For example, the numbers 10, 9, 70, 100 are sorted lexicographically as 10, 100, 70, 9. Uppercase letters are sorted before lowercase letters. Symbols are not standard. Some symbols are sorted before numeric values. Other symbols are sorted before or after letters. Basic example", "code_examples": [{"language": "spl", "code": "... |evals=mvsort(mvfield)"}], "tables": [], "chunk_index": 11, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvsort(<mv>)", "section_id": "c46f9328_ce1d_4bc7_a2c0_91392e98884a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675398+00:00", "version": "10.2"}}
{"id": "ff353f92adc28c21", "content": "Description This function combines the values in two multivalue fields. The delimiter is used to specify a delimiting character to join the two values. Usage This is similar to the Python zip command. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The values are stitched together combining the first value of <mv_left> with the first value of field <mv_right>, then the second with the second, and so on. The delimiter is optional, but when specified must be enclosed in quotation marks. The default delimiter is a comma ( , ). Basic example Extended example You can nest several mvzip functions together to create a single multivalued field three_fields from three separate fields. The pipe ( | ) character is used as the separator between the field values. (Thanks to Splunk user cmerriman for this example.)", "code_examples": [{"language": "spl", "code": "... |evalnserver=mvzip(hosts,ports)"}, {"language": "spl", "code": "...|evalthree_fields=mvzip(mvzip(field1,field2,\"|\"),field3,\"|\")"}], "tables": [], "chunk_index": 12, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mvzip(<mv_left>,<mv_right>,<delim>)", "section_id": "b40115fb_8d40_49f5_b5e6_8dab78ea04b6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675403+00:00", "version": "10.2"}}
{"id": "dc5e6cd84f31cf19", "content": "This function maps the elements of a multivalue field to a JSON array. Usage You can use this function with the eval and where commands, and as part of evaluation expressions with other commands. Because the elements of JSON arrays can have many data types (such as string, numeric, Boolean, and null), the mv_to_json_array function lets you specify how it should map the contents of multivalue fields into JSON arrays. You can have the field values simply written to arrays as string data types, or you can have the function infer different JSON data types. Use the <infer_types> input to specify that the mv_to_json_array function should attempt to infer JSON data types when it converts field values into array elements. The <infer_types> input defaults to false. Example This example shows you how the mv_to_json_array function can validate JSON as it generates JSON arrays. This search creates a multivalue field named ponies. The array that is created from these values depends on the <infer_types> input. Without inferring data types When <infer_types> is set to false or omitted, the mv_to_json_array function converts the field values into array elements without changing the values. The resulting array looks like this: [\"\\\"Buttercup\\\"\",\"\\\"Fluttershy\\\"\",\"\\\"Rarity\\\"\",\"true\",\"null\"] With inferring data types When you run this search with infer_values set to true() , the mv_to_json_array function removes the extra quote and backslash escape characters from the field values when the values are converted into array elements. The resulting array looks like this: [\"Buttercup\",\"Fluttershy\",\"Rarity\",true,null]", "code_examples": [{"language": "spl", "code": "... |evalponies = mvappend(\"\\\"Buttercup\\\"\",\"\\\"Fluttershy\\\"\",\"\\\"Rarity\\\"\",\"true\",\"null\"),"}, {"language": "spl", "code": "... |evalmy_sweet_ponies = mv_to_json_array(ponies,false())"}, {"language": "spl", "code": "... |evalmy_sweet_ponies = mv_to_json_array(ponies,true())"}], "tables": [{"headers": ["Syntax", "Description"], "rows": [["mv_to_json_array(<field>, false())ormv_to_json_array(<field>)", "By default, or when you explicitly set it tofalse(), themv_to_json_arrayfunction maps all values in the multivalued field to the JSON array as string data types, whether they are numeric, strings, Boolean values, or any other JSON data type. Themv_to_json_arrayfunction effectively splits the multivalue field on the comma and writes each quote-enclosed value to the array as an element with the string data type."], ["mv_to_json_array(<field>, true())", "When you set themv_to_json_arrayfunction totrue(), the function removes one set of bracketing quote characters from each value it transfers into the JSON array. If the function does not recognize the resulting array element as a proper JSON data type (such as string, numeric, Boolean, or null), the function turns the element into a null data type."]]}], "chunk_index": 13, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "mv_to_json_array(<field>, <infer_types>)", "section_id": "id_3eacc26c_4c89_47a9_9601_2f12e8905cf6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675410+00:00", "version": "10.2"}}
{"id": "600b6cb30388b029", "content": "Description This function splits the string values on the delimiter and returns the string values as a multivalue field. Note: The split function doesn't have a maximum character limit for input strings or delimiter, provided enough memory is available for searches. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Use an empty string (\"\") to split the original string into one value per character. For example, the following search splits the string into a , b , c , and d. Basic example To illustrate how the split function works, the following search creates an event with a test field that contains a list of string values separated by semicolon characters ( ; ). The results look like this: To split up each of the names in the event into a multivalue field using the semicolon delimiter, you could run a search like this: Now each of the pony names in the test event is a field in a multivalue field. The results look something like this: You can also use a string of contiguous characters in your search like this, which splits the string on \"def\". The results look something like this. Extended example The following search is useful for building equivalents to string functions like Oracle INSTR. The results look something like this. The length of the first entry (mvindex=0) is the position of the \"::\" string, plus or minus one.", "code_examples": [{"language": "spl", "code": "|makeresults \n|evaltest=\"abcd\"|evalresults=split(test,\"\")"}, {"language": "spl", "code": "| makeresults\n|evaltest=\"buttercup;rarity;tenderhoof;dash;mcintosh;fleetfoot;mistmane\""}, {"language": "spl", "code": "| makeresults\n|evaltest=\"buttercup;rarity;tenderhoof;dash;mcintosh;fleetfoot;mistmane\"|evalponies=split(test,\";\")"}, {"language": "spl", "code": "|makeresults \n|evaltest=\"1a2b3c4def567890\"|evalresults=split(test,\"def\")"}, {"language": "spl", "code": "| makeresults\n|evaltest=\"name::value\"|evalresults=split(test,\"::\")"}], "tables": [{"headers": ["_time", "test"], "rows": [["2022-09-20 17:39:56", "buttercup;rarity;tenderhoof;dash;mcintosh;fleetfoot;mistmane"]]}, {"headers": ["_time", "ponies", "test"], "rows": [["2022-09-20 18:22:03", "buttercupraritytenderhoofdashmcintoshfleetfootmistmane", "buttercup;rarity;tenderhoof;dash;mcintosh;fleetfoot;mistmane"]]}, {"headers": ["_time", "results", "test"], "rows": [["2023-01-23 12:18:11", "1a2b3c4567890", "1a2b3c4def567890"]]}, {"headers": ["_time", "results", "test"], "rows": [["2023-01-23 12:18:11", "namevalue", "name::value"]]}], "chunk_index": 14, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "split(<str>,<delim>)", "section_id": "id_1ea6b248_ae0a_4cb8_8551_da6909b88523--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675420+00:00", "version": "10.2"}}
{"id": "02fedaa99047c800", "content": "See the following multivalue commands: makemv mvcombine mvexpand nomv", "code_examples": [], "tables": [], "chunk_index": 15, "total_chunks": 16, "metadata": {"title": "Multivalue eval functions", "section_heading": "See also", "section_id": "f35f4bae_9cdc_4d47_8e63_1e81ab8eadfb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/multivalue-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:25:56.675424+00:00", "version": "10.2"}}
{"id": "206fca7190d6a869", "content": "Creates a new JSON object from members of key-value pairs. Usage If you specify a string for a <key> or <value> , you must enclose the string in double quotation marks. A <key> must be a string. A <value> can be a string, number, Boolean, null, multivalue field, array, or another JSON object. You can use this function with the eval and where commands, and as part of evaluation expressions with other commands. Examples These examples show different ways to use the json_object function to create JSON objects in your events. 1. Create a basic JSON object The following example creates a basic JSON object { \"name\": \"maria\" }. 2. Create a JSON object using a multivalue field The following example creates a multivalue field called firstnames that uses the key name and contains the values \"maria\" and \"arun\". The JSON object created is { \"name\": [\"maria\", \"arun\"] }. 3. Create a JSON object using a JSON array The following example creates a JSON object that uses a JSON array for the values. The result is the JSON object { \"cities\": [\"London\", \"Sydney\", \"Berlin\", \"Santiago\"] }. 4. Create a nested JSON object The following example creates a nested JSON object that uses other JSON objects and a multivalue or JSON array field called gamelist. The result is this JSON object:", "code_examples": [{"language": "spl", "code": "... |evalname = json_object(\"name\",\"maria\")"}, {"language": "spl", "code": "... |evalfirstnames = json_object(\"name\", json_array(\"maria\",\"arun\"))"}, {"language": "spl", "code": "... |evallocations = json_object(\"cities\", json_array(\"London\",\"Sydney\",\"Berlin\",\"Santiago\"))"}, {"language": "spl", "code": "...|evalgamelist = json_array(\"Pandemic\",\"Forbidden Island\",\"Castle Panic\"), games = json_object(\"category\", json_object(\"boardgames\", json_object(\"cooperative\", gamelist)))"}, {"language": "spl", "code": "{\"games\": {\"category\": {\"boardgames\": {\"cooperative\": [\"Pandemic\",\"Forbidden Island\",\"Castle Panic\"]\n      }\n    }\n  }\n}"}], "tables": [], "chunk_index": 0, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_object(<members>)", "section_id": "id_6a12a2ac_7d61_424a_ad9d_a9f7c6438160--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495837+00:00", "version": "10.2"}}
{"id": "3c057bb50feb713e", "content": "Evaluates whether a value can be parsed as JSON. If the value is in a valid JSON format, the function returns the value. Otherwise, the function returns null. Usage A <value> can be any kind of value such as string, number, Boolean, null, or JSON array or object. Examples 1. Identify a JSON value This example shows how you can use the json function to confirm that a value is JSON. The following search verifies that {\"animal\" : \"pony\"} is a JSON value by returning its value, {\"animal\":\"pony\"}. The search results look something like this: 2. Compare multiple results to identify JSON values The following example shows how to use the json function to determine if the values in a field are JSON arrays or objects. Consider the following search results: When you add the json evaluation function to the following search, the results in the bridgesAsJson field identifies which values in the bridges field are JSON values: When the value is JSON, the value is returned in the bridgesAsJson field. When the value is not JSON, the function returns null. The results look like something like this:", "code_examples": [{"language": "spl", "code": "... |evalanimals = json_object(\"animal\",\"pony\"), result = json(animals)"}, {"language": "spl", "code": "... |evalbridgesAsJson = json(bridges)"}], "tables": [{"headers": ["_time", "animals", "result"], "rows": [["2023-02-22 14:39:50", "{\"animal\": \"pony\"}", "{\"animal\": \"pony\"}"]]}, {"headers": ["_time", "bridges", "city"], "rows": [["2023-04-26 21:10:45", "[\"bridges\",{\"name\":\"Tower Bridge\"},{\"length\":\"801\"},{\"name\":\"Millennium Bridge\"},{\"length\":\"1066\"}]", "London"], ["2023-04-26 21:10:45", "[\"bridges\",{\"name\":\"Rialto Bridge\"},{\"length\":\"157\"},{\"name\":\"Bridge of Sighs\"},{\"length\":\"36\"},{\"name\":\"Ponte della Paglia\"}]", "Venice"], ["2023-04-26 21:10:45", "Golden Gate Bridge", "San Francisco"]]}, {"headers": ["_time", "bridges", "bridgesAsJson", "city"], "rows": [["2023-04-26 21:10:45", "[\"bridges\",{\"name\":\"Tower Bridge\"},{\"length\":\"801\"},{\"name\":\"Millennium Bridge\"},{\"length\":\"1066\"}]", "[{\"name\":\"Tower Bridge\",\"length\":801}, {\"name\":\"Millennium Bridge\",\"length\":1066}]", "London"], ["2023-04-26 21:10:45", "[{\"name\":\"Rialto Bridge\",\"length\":157}, {\"name\":\"Bridge of Sighs\",\"length\":36}, {\"name\":\"Ponte della Paglia\"}]", "[{\"name\":\"Rialto Bridge\",\"length\":157}, {\"name\":\"Bridge of Sighs\",\"length\":36}, {\"name\":\"Ponte della Paglia\"}]", "Venice"], ["2023-04-26 21:10:45", "Golden Gate Bridge", "", "San Francisco"]]}], "chunk_index": 1, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json(<value>)", "section_id": "cdda4959_8fc6_4edb_a374_82308777cb4d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495856+00:00", "version": "10.2"}}
{"id": "23d8a4d662b68b8c", "content": "This function appends values to the ends of indicated arrays within a JSON document. This function provides a JSON eval function equivalent to the multivalue mvappend function. Usage The json_append function always has at least three function inputs: <json> (the name of a valid JSON document such as a JSON object), and at least one <path> and <value> pair. If <json> does not reference a valid JSON document, such as a JSON object, the function outputs nothing. The json_append function evaluates <path_value_pairs> from left to right. When a path-value pair is evaluated, the function updates the <json> document. The function then evaluates the next path-value pair against the updated document. You can use this function with the eval and where commands, and as part of evaluation expressions with other commands. Use <path> to designate a JSON document value Each <path> designates an array or value within the <json> document. The json_append function adds the corresponding <value> to the end of the value designated by the <path>. The following table explains what json_append does depending on what the <path> specifies. The json_append function ignores path-value pairs for which the <path> does not identify any valid value in the JSON document. Append arrays as single elements When the new <value> is an array, json_append appends the array as a single element. For example, if a json_array <path> leads to the array [\"a\", \"b\", \"c\"] and its <value> is the array [\"d\", \"e\", \"f\"] , the result is [\"a\", \"b\", \"c\", [\"d\", \"e\", \"f\"]]. Appending arrays as single elements separates json_append from json_extend , a similar function that flattens arrays and objects into separate elements as it appends them. When json_extend takes the example in the preceding paragraph, it returns [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]. Examples The following examples show how you can use json_append to append values to arrays within a JSON document. 1. Add a string to an array Say you have an object named ponies that contains an array named ponylist : [\"Minty\", \"Rarity\", \"Buttercup\"]. This is the search you would run to append \"Fluttershy\" to ponylist. The output of that eval statement is {\"ponylist\": [\"Minty\", \"Rarity\", \"Buttercup\", \"Fluttershy\"]}. 2. Append a string to a nested object This example has a <path> with the value Fluttershy.ponySkills. Fluttershy.ponySkills references an array of an object that is nested within ponyDetails , the source object. The query uses json_append to add a string to the nested object array. The output of this eval statement is ponyDetailsUpdated = {\"Fluttershy\":{\"ponySkills\":[\"running\",\"jumping\",\"codebreaking\"]}}", "code_examples": [{"language": "spl", "code": "... |evalponies = json_object(\"ponylist\", json_array(\"Minty\",\"Rarity\",\"Buttercup\")), \nupdatePonies = json_append(ponies,\"ponylist\",\"Fluttershy\")"}, {"language": "spl", "code": "... |evalponyDetails = json_object(\"Fluttershy\", json_object(\"ponySkills\", json_array(\"running\",\"jumping\"))), ponyDetailsUpdated = json_append(ponyDetails,\"Fluttershy.ponySkills\",\"codebreaking\")"}], "tables": [{"headers": ["If<path>specifies...", "...This is whatjson_appenddoes with the corresponding<value>"], "rows": [["An array with one or more values.", "json_appendadds the corresponding<value>to the end of that array."], ["An empty array", "json_appendadds the corresponding<value>to that array, creating an array with a single value."], ["A scalar or object value", "json_appendautowraps the scalar or object value within an array and adds the corresponding<value>to the end of that array."]]}], "chunk_index": 2, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_append(<json>, <path_value_pairs>)", "section_id": "e088ab24_8a73_4549_afaa_649b643a88c2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495867+00:00", "version": "10.2"}}
{"id": "3b1a187839f5665a", "content": "Creates a JSON array using a list of values. Usage A <value> can be any kind of value such as string, number, or Boolean. You can also use the json_object function to specify values. You can use this function with the eval and where commands, and as part of evaluation expressions with other commands. Examples These examples show different ways to use the json_array function to create JSON arrays in your events. 1. Create a basic JSON array The following example creates a simple array [\"buttercup\", \"fluttershy\", \"rarity\"]. 2. Create an JSON array from a string and a JSON object The following example uses a string dubois and the json_object function for the array values. The result is the JSON array [ \"dubois\", {\"name\": \"patel}\" ] .", "code_examples": [{"language": "spl", "code": "... |evalponies = json_array(\"buttercup\",\"fluttershy\",\"rarity\")"}, {"language": "spl", "code": "... |evalsurname = json_array(\"dubois\", json_object(\"name\",\"patel\"))"}], "tables": [], "chunk_index": 3, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_array(<values>)", "section_id": "id_48fc8828_6579_4b85_8360_fab5453a1eb7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495874+00:00", "version": "10.2"}}
{"id": "17163df9aa7c54c9", "content": "This function maps the elements of a proper JSON array into a multivalue field. Usage You can use this function with the eval and where commands, and as part of evaluation expressions with other commands. If the <json array> input to the function is not a valid JSON array, the function outputs nothing. Use the <boolean> input to specify that the json_array_to_mv function should preserve bracketing quotes on JSON-formatted strings. The <boolean> input defaults to false(). Example This example demonstrates usage of the json_array_to_mv function to create simple multivalue fields out of JSON data. The following example creates a simple array: [\"Buttercup\", \"Fluttershy\", \"Rarity\"]. Then it maps that array into a multivalue field named my_little_ponies with the values Buttercup , Fluttershy , and Rarity. The function removes the quote characters when it converts the array elements into field values. If you change this search so it has my_sweet_ponies = json_array_to_mv(ponies,true()) , you get an array with the values \"Buttercup\" , \"Fluttershy\" , and \"Rarity\". Setting the function to true causes the function to preserve the quote characters when it converts the array elements into field values.", "code_examples": [{"language": "spl", "code": "... |evalponies = json_array(\"Buttercup\",\"Fluttershy\",\"Rarity\"), my_sweet_ponies = json_array_to_mv(ponies)"}], "tables": [{"headers": ["Syntax", "Description"], "rows": [["json_array_to_mv(<json_array>, false())orjson_array_to_mv(<json_array>)", "By default (or when you explicitly set it tofalse()), thejson_array_to_mvfunction removes bracketing quotes from JSON string data types when it converts an array into a multivalue field."], ["json_array_to_mv(<json_array>, true())", "When set totrue(), thejson_array_to_mvfunction preserves bracketing quotes on JSON string data types when it converts an array into a multivalue field."]]}], "chunk_index": 4, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_array_to_mv(<json_array>, <boolean>)", "section_id": "id_328e7783_4b6a_4a11_bdd2_acc7a24e7c13--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495882+00:00", "version": "10.2"}}
{"id": "b1a6f3766f6c1b33", "content": "Use json_delete to remove one or more keys and their corresponding values from the specified JSON object. The original JSON object is not modified. Instead, a new object is returned. Usage The json_delete function uses two arguments: The <object> argument identifies the JSON object from which you want to delete key-value pairs. The <keys> argument identifies one or more keys that you want delete. The corresponding values are also deleted. Array indexing is not supported. You can use this function with the eval and where commands, and as part of evaluation expressions with other commands. See the eval and where commands. Specifying keys You can specify the <keys> in 2 ways, as shown in the following table: Nested keys You can delete key-value pairs from nested keys. However, deleting key names that contain the dot character (. ) is not supported. For example, suppose you have the key student.name , which has the value Claudia. Using json_delete(obj, \"student.name\") looks for the nested object name under the key student , which doesn't exist. Examples 1. Delete key-value pairs in an object The following search deletes several key-value pairs from a JSON object. A new JSON object is returned in a field called sales_account. This search uses the eval command to create a JSON object literal in a field called object. Another eval command is used with the json_delete function to remove several key-value pairs from the JSON object literal, including the values in an array. The results look like this: Note: You don't have to use 2 separate eval commands for this search example. You can specify multiple eval command operations separated by commas. For example: ...| eval object = {\"name\":\"Wei Zhang\", \"SSN\":\"123-45-6789\", \"city\":\"Seattle\", \"accounts\":[\"Hagal Quartz\", \"Caladan Water\", \"Arrakis Spices\"]}, sales_account = json_delete(object, \"SSN\", \"accounts\") 2. Delete a key-value pair in a nested object The following search removes a key-value pair from the addresses nested object. A new JSON object is returned in a field called result. This search uses the eval command to create a JSON object literal in a field called employee. The search then uses another eval command with the json_delete function to remove the email key-value pair from the addresses nested object. The results look like this: 3. Delete a key-value pair in a nested object in a pipeline Consider the following JSON object, which contains Buttercup Games supplier information including a nested object with address information. The following pipeline uses the eval command with the json_delete function to remove the email key-value pair from the addresses nested object. A new JSON object is returned in a field called cleaned .", "code_examples": [{"language": "spl", "code": "...|evalobject = {\"name\":\"Wei Zhang\",\"SSN\":\"123-45-6789\",\"city\":\"Seattle\",\"accounts\":[\"Hagal Quartz\",\"Caladan Water\",\"Arrakis Spices\"]}\n|evalsales_account = json_delete(object,\"SSN\",\"accounts\")"}, {"language": "spl", "code": "...|evalemployee = {\"name\":\"Celestino Paulo\",\"company\":\"Isthmus Pastimes\",\"addresses\": {\"email\":\"celestino@sample.com\",\"office\":\"edificio 890 Avenida Demetrio Panama City Panama\"}}\n|evalresult = json_delete(employee,\"addresses.email\")"}, {"language": "spl", "code": "{\"name\":\"Celestino Paulo\",\"company\":\"Isthmus Pastimes\",\"addresses\": {\"email\":\"celestino@sample.com\",\"office\":\"edificio 890 Avenida Demetrio \n                 Panama City Panama\"}\"name\":\"David Mayer\",\"company\":\"Euro Games\",\"addresses\": {\"email\":\"david@sample.com\",\"office\":\"567 Pariser Platz 2 10117 \n                Berlin Germany \"}\"name\":\"Wei Zhang\",\"company\":\"Tiger Fun\",\"addresses\": {\"email\":\"wei@sample.com\",\"office\":\"678 Chome-10-5 Akasaka \n                 Minato City Tokyo 107-8420 Japan\"}\"name\":\"Rutherford Sullivan\",\"company\":\"Blarney Games\",\"addresses\": {\"email\":\"rutherford@sample.com\",\"office\":\"789 Market St Sleveen \n                 Kinsale Co. Cork P17 E068 Ireland\"}\n}"}, {"language": "spl", "code": "$pipeline= from$source|evalcleaned = json_delete(employee, [\"addresses.email\"]) | into$destination"}], "tables": [{"headers": ["Method", "Example"], "rows": [["A comma-separated list", "json_delete(object, \"SSN\", \"accounts\")"], ["An array", "json_delete(object, [\"SSN\", \"accounts\"]"]]}, {"headers": ["object", "sales_account"], "rows": [["{\"name\":\"Wei Zhang\", \"SSN\":\"123-45-6789\", \"city\":\"Seattle\", \"accounts\":[\"Hagal Quartz\", \"Caladan Water\", \"Arrakis Spices\"]}", "{\"name\":\"Wei Zhang\", \"city\":\"Seattle\"}"]]}, {"headers": ["employee", "results"], "rows": [["{\"name\":\"Celestino Paulo\", \"company\":\"Isthmus Pastimes\", \"addresses\": {\"email\":\"celestino@sample.com\", \"office\":\"edificio 890 Avenida Demetrio  Panama City Panama\"}}", "{\"name\":\"Celestino Paulo\", \"company\":\"Isthmus Pastimes\", \"addresses\": {\"office\":\"edificio 890 Avenida Demetrio Panama City Panama\"}}"]]}], "chunk_index": 5, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_delete(<object>,<keys>)", "section_id": "e1d0bff6_e301_4880_8e8d_daf710234e07--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495894+00:00", "version": "10.2"}}
{"id": "c305bda44e404af1", "content": "Description |Returns the key-value entries from the top-level key-value pairs in a JSON object. The entries are returned as a JSON array of JSON objects with fields key and value. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The <value> argument can be a valid JSON object or the name of a field that contains a valid JSON object. Use this function in type tests to confirm that the format of an object is what is required and expected. Basic example The following example returns a field named entries containing the array [{\"key\":\"a\",\"value\":1},{\"key\":\"b\",\"value\":2}]. Extended example The employee_record field in these events contains JSON objects. The following eval command returns the top-level members of the objects from the employee_record field and stores them in a field named array_format : The results look like this:", "code_examples": [{"language": "spl", "code": "| makeresults\n|evalentries=json_entries(\"{\\\"a\\\": 1, \\\"b\\\": 2}\")"}, {"language": "spl", "code": "... |evalarray_format = json_entries(employee_record)"}], "tables": [{"headers": ["_time", "employee_record"], "rows": [["2024-12-17 21:22:43", "{\"name\":\"maria\",\"age\":25,\"status\":\"full-time\"}"], ["2024-12-17 21:22:43", "{\"name\":\"charlie\",\"age\":21,\"status\":\"part-time\"}"]]}, {"headers": ["_time", "array_format", "employee_record"], "rows": [["2024-12-17 21:22:43", "[{\"key\":\"name\",\"value\":\"maria\"},{\"key\":\"age\",\"value\":25},{\"key\":\"status\",\"value\":\"full-time\"}]", "{\"name\":\"maria\",\"age\":25,\"status\":\"full-time\"}"], ["2024-12-17 21:22:43", "[{\"key\":\"name\",\"value\":\"charlie\"},{\"key\":\"age\",\"value\":21},{\"key\":\"status\",\"value\":\"part-time\"}]", "{\"name\":\"charlie\",\"age\":21,\"status\":\"part-time\"}"]]}], "chunk_index": 6, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_entries(<value>)", "section_id": "dd813aae_9396_485b_ba7f_bd6dea10ca04--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495905+00:00", "version": "10.2"}}
{"id": "9048bcb10c9079a8", "content": "Use json_extend when you want to append multiple values at once to an array. json_extend flattens arrays into their component values and appends those values to the ends of indicated arrays within a valid JSON document. Usage The json_extend function always has at least three function inputs: <json> (the name of a valid JSON document such as a JSON object), and at least one <path> and <value> pair. The <value> must be an array. When given valid inputs, json_extend always outputs an array. If <json> does not reference a valid JSON document, such as a JSON object, the function outputs nothing. json_extend evaluates <path_value_pairs> from left to right. When json_extend evaluates a path-value pair, it updates the <json> document. json_extend then evaluates the next path-value pair against the updated document. You can use json_extend with the eval and where commands, and as part of evaluation expressions with other commands. Use <path> to designate a JSON document value Each <path> designates an array or value within the <json> document. The json_extend function adds the values of the corresponding <array> after the last value of the array designated by the <path>. The following table explains what json_extend does depending on what the <path> specifies. json_extend ignores path-value pairs for which the <path> does not identify any valid value in the JSON document. How json_extend flattens arrays before it appends them The json_extend function flattens arrays as it appends them to the specified value. \"Flattening\" refers to the act of breaking the array down into its component values. For example, if a json_extend <path> leads to the array [\"a\", \"b\", \"c\"] and its <value> is the array [\"d\", \"e\", \"f\"] , the result is [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]. Appending arrays as individual values separates json_extend from json_append , a similar function that appends the <value> as a single element. When json_append takes the example in the preceding paragraph, it returns [\"a\", \"b\", \"c\", [\"d\", \"e\", \"f\"]]. Examples The following examples show how you can use json_extend to append multiple values at once to arrays within a JSON document. 1. Extend an array with a set of string values You start with an object named fakeBandsInMovies that contains an array named fakeMovieBandList : [\"The Blues Brothers\", \"Spinal Tap\", \"Wyld Stallyns\"]. This is the search you would run to extend that list with three more names of fake bands from movies. The output of this eval statement is: 2. Extend an array with an object This example has an object named dndChars that contains an array named characterClasses. You want to update this array with an object from a secondary array. Here is a search you could run to achieve that goal. The output of this eval statement is: Note that when json_extend flattens array2 , it removes the object from the array. Otherwise the output would be:", "code_examples": [{"language": "spl", "code": "... |evalfakeBandsInMovies = json_object(\"fakeMovieBandList\", json_array(\"The Blues Brothers\",\"Spinal Tap\",\"Wyld Stallyns\")), updateBandList = json_extend(fakeBandsInMovies,\"fakeMovieBandList\", json_array(\"The Soggy Bottom Boys\",\"The Weird Sisters\",\"The Barden Bellas\"))"}, {"language": "spl", "code": "{\"fakeMovieBandList\": [\"The Blues Brothers\",\"Spinal Tap\",\"Wyld Stallyns\",\"The Soggy Bottom Boys\",\"The Weird Sisters\",\"The Barden Bellas\"]\n}"}, {"language": "spl", "code": "... |evaldndChars = json_object(\"characterClasses\", json_array(\"wizard\",\"rogue\",\"barbarian\")), array2 = json_array(json_object(\"artifact\",\"deck of many things\")), updatedParty = json_extend(dndChars,\"characterClasses\", array2)"}, {"language": "spl", "code": "{\"updatedParty\": [\"wizard\",\"rogue\",\"barbarian\",\n    {\"artifact\":\"deck of many things\"}\n  ]\n}"}, {"language": "spl", "code": "{\"updatedParty\": [\"wizard\",\"rogue\",\"barbarian\",\n    {\"artifact\":\"deck of many things\"}\n  ]\n}"}], "tables": [{"headers": ["If<path>specifies...", "...This is whatjson_extenddoes with the corresponding array values"], "rows": [["An array with one or more values.", "json_extendadds the corresponding array values to the end of that array."], ["An empty array", "json_extendadds the corresponding array values to that array."], ["A scalar or object value", "json_extendautowraps the scalar or object value within an array and adds the corresponding array values to the end of that array."]]}], "chunk_index": 7, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_extend(<json>, <path_value_pairs>)", "section_id": "id_8a7ea9a6_f6c8_4b9b_baa8_1898458ceb43--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495914+00:00", "version": "10.2"}}
{"id": "d27fde27a8d9fcf1", "content": "This function returns a value from a piece of JSON and zero or more paths. The value is returned in either a JSON array, or a Splunk software native type value. Note: If a JSON object contains a value with a special character, such as a period, json_extract can't access it. Use the json_extract_exact function for those situations. See json_extract_exact. Usage What is converted or extracted depends on whether you specify a piece of JSON, or JSON and one or more paths. You can use this function with the eval and where commands, and as part of evaluation expressions with other commands. Examples These examples use this JSON object, which is in a field called cities in an event: 1. Extract the entire JSON object in a field The following example returns the entire JSON object from the cities field. The cities field contains only one object. The key is the entire object. This extraction can return any type of value. Here are the results of the search: 2. Extract the first nested JSON object in a field The following example extracts the information about the city of London from the JSON object. This extraction can return any type of value. The {<num>} indexing demonstrated in this example search only works when the <path> maps to a JSON array. In this case the {0} maps to the \"0\" item in the array, which is London. If the example used {1} it would select Venice from the array. Here are the results of the search: 3. Extract the third nested JSON object in a field The following example extracts the information about the city of San Francisco from the JSON object. This extraction can return any type of value. Here are the results of the search: 4. Extract a specific key from each nested JSON object in a field The following example extracts the names of the cities from the JSON object. This extraction can return any type of value. Here are the results of the search: 5. Extract a specific set of key-value pairs from each nested JSON object in a field The following example extracts the information about each bridge from every city from the JSON object. This extraction can return any type of value. Here are the results of the search: 6. Extract a specific value from each nested JSON object in a field The following example extracts the names of the bridges from all of the cities from the JSON object. This extraction can return any type of value. Here are the results of the search: 7. Extract a specific key-value pair from a specific nested JSON object in a field The following example extracts the name and length of the first bridge from the third city from the JSON object. This extraction can return any type of value. Here are the results of the search: 8. Extract a specific value from a specific nested JSON object in a field The following example extracts the length of the first bridge from the third city from the JSON object. This extraction can return any type of value. Here are the results of the search:", "code_examples": [{"language": "spl", "code": "{\"cities\": [\n    {\"name\":\"London\",\"Bridges\": [\n        {\"name\":\"Tower Bridge\",\"length\": 801 },\n        {\"name\":\"Millennium Bridge\",\"length\": 1066 }\n      ]\n    },\n    {\"name\":\"Venice\",\"Bridges\": [\n        {\"name\":\"Rialto Bridge\",\"length\": 157 },\n        {\"name\":\"Bridge of Sighs\",\"length\": 36 },\n        {\"name\":\"Ponte della Paglia\"}\n      ]\n    },\n    {\"name\":\"San Francisco\",\"Bridges\": [\n        {\"name\":\"Golden Gate Bridge\",\"length\": 8981 },\n        {\"name\":\"Bay Bridge\",\"length\": 23556 }\n      ]\n    }\n  ]\n}"}, {"language": "spl", "code": "... |evalextracted_cities = json_extract(cities,\"{}\")"}, {"language": "spl", "code": "... |evalLondon=json_extract(cities,\"{0}\")"}, {"language": "spl", "code": "... |evalSan_Francisco=json_extract(cities,\"{2}\")"}, {"language": "spl", "code": "... |evalmy_cities=json_extract(cities,\"{}.name\")"}, {"language": "spl", "code": "... |evalBridges=json_extract(cities,\"{}.Bridges{}\")"}, {"language": "spl", "code": "... |evalBridge_names=json_extract(cities,\"{}.Bridges{}.name\")"}, {"language": "spl", "code": "... |evalGG_Bridge=json_extract(cities,\"{2}.Bridges{0}\")"}, {"language": "spl", "code": "... |evalGG_Bridge_length=json_extract(cities,\"{2}.Bridges{0}.length\")"}], "tables": [{"headers": ["Syntax", "Description"], "rows": [["json_extract(<json>)", "Converts a JSON field to the Splunk software native type. For example:Converts a JSON string to a stringConverts a JSON Boolean to a BooleanConverts a JSON null to a null"], ["json_extract(<json>, <path>)", "Extracts the value specified by<path>from<json>, and converts the value to the native type. This can be a JSON array if the path leads to an array."], ["json_extract(<json>, <path>, <path>, ...)", "Extracts all of the paths from<json>and returns it as a JSON array."]]}, {"headers": ["Field", "Results"], "rows": [["extract_cities", "{\"cities\":[{\"name\":\"London\",\"Bridges\":[{\"name\":\"Tower Bridge\",\"length\":801},{\"name\":\"Millennium Bridge\",\"length\":1066}]},{\"name\":\"Venice\",\"Bridges\":[{\"name\":\"Rialto Bridge\",\"length\":157},{\"name\":\"Bridge of Sighs\",\"length\":36},{\"name\":\"Ponte della Paglia\"}]},{\"name\":\"San Francisco\",\"Bridges\":[{\"name\":\"Golden Gate Bridge\",\"length\":8981},{\"name\":\"Bay Bridge\",\"length\":23556}]}]}"]]}, {"headers": ["Field", "Results"], "rows": [["London", "{\"name\":\"London\",\"Bridges\":[{\"name\":\"Tower Bridge\",\"length\":801},{\"name\":\"Millennium Bridge\",\"length\":1066}]}"]]}, {"headers": ["Field", "Results"], "rows": [["San_Francisco", "{\"name\":\"San Francisco\",\"Bridges\":[{\"name\":\"Golden Gate Bridge\",\"length\":8981},{\"name\":\"Bay Bridge\",\"length\":23556}]}"]]}, {"headers": ["Field", "Results"], "rows": [["my_cities", "[\"London\",\"Venice\",\"San Francisco\"]"]]}, {"headers": ["Field", "Results"], "rows": [["Bridges", "[{\"name\":\"Tower Bridge\",\"length\":801},{\"name\":\"Millennium Bridge\",\"length\":1066},{\"name\":\"Rialto Bridge\",\"length\":157},{\"name\":\"Bridge of Sighs\",\"length\":36},{\"name\":\"Ponte della Paglia\"},{\"name\":\"Golden Gate Bridge\",\"length\":8981},{\"name\":\"Bay Bridge\",\"length\":23556}]"]]}, {"headers": ["Field", "Results"], "rows": [["Bridge_names", "[\"Tower Bridge\",\"Millennium Bridge\",\"Rialto Bridge\",\"Bridge of Sighs\",\"Ponte della Paglia\",\"Golden Gate Bridge\",\"Bay Bridge\"]"]]}, {"headers": ["Field", "Results"], "rows": [["GG_Bridge", "{\"name\":\"Golden Gate Bridge\",\"length\":8981}"]]}, {"headers": ["Field", "Results"], "rows": [["GG_Bridge_length", "8981"]]}], "chunk_index": 8, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_extract(<json>, <paths>)", "section_id": "id_716ba25f_315e_4095_8639_24c8539e8aa2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495937+00:00", "version": "10.2"}}
{"id": "d7879a8913ce4f81", "content": "Like the json_extract function, this function returns a Splunk software native type value from a piece of JSON. The main difference between these functions is that the json_extract_exact function does not use paths to locate and extract values, but instead matches literal strings in the event and extracts those strings as keys. See json_extract. Usage The json_extract_exact function treats strings for key extraction literally. This means that the function does not support explicitly nested paths. You can set paths with nested json_array / json_object function calls. You can use this function with the eval and where commands, and as part of evaluation expressions with other commands. Example Suppose you have a JSON event that looks like this: {\"system.splunk.path\":\"/opt/splunk/\"} If you want to extract system.splunk.path from that event, you can't use the json_extract function because of the period characters. Instead, you would use json_extract_exact , as shown in the following search:", "code_examples": [{"language": "spl", "code": "... |evalextracted_path=json_extract_exact(splunk_path,\"system.splunk.path\")"}], "tables": [{"headers": ["Syntax", "Description"], "rows": [["json_extract_exact(<json>)", "Converts a JSON field to the Splunk software native type. For example:Converts a JSON string to a stringConverts a JSON Boolean to a BooleanConverts a JSON null to a null"], ["json_extract_exact(<json>, <string>)", "Extracts the key specified by<string>from<json>, and converts the key to the Splunk software native type. This can be a JSON array if the path leads to an array."], ["json_extract_exact(<json>, <string>, <string>, ...)", "Extracts all of the strings from<json>and returns them as a JSON array of keys."]]}], "chunk_index": 9, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_extract_exact(<json>, <keys>)", "section_id": "id_97a72a59_d0a3_4ebb_b0f1_e4a4dbc0a4da--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495946+00:00", "version": "10.2"}}
{"id": "ccb16a9be49d654f", "content": "Description This function evaluates whether a JSON object contains the specified key and returns either TRUE or FALSE. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The <object> argument identifies the JSON object that you want to check for a specific key. This argument can be a valid JSON object or the name of a field that contains a valid JSON object. The <key> argument identifies the key that you want to check for in the JSON object. This argument can be a string or the name of a field that contains a string. You can use this function directly with the where command in searches, but the eval command can't directly accept a Boolean value. You must specify the function inside another function, such as the if function, which can accept a Boolean value as an input. Basic examples The following example returns has key , indicating that the name key exists in the provided JSON object. The following example returns has key , indicating that the \"charlie.garcia\" key exists in the provided JSON object. The function treats the dot character (. ) in \"charlie.garcia\" as a string literal, not as a nested object in JSON format. Extended example The employee_record field in these events contains JSON objects. The following eval command checks whether the objects in the employee_record field contain the status key, and stores the results in a field named test_results. The results look like this:", "code_examples": [{"language": "spl", "code": "... |evaltest=if(json_has_key_exact(json_object(\"name\",\"charlie\"),\"name\"),\"has key\",\"doesn't have key\")"}, {"language": "spl", "code": "| makeresults\n|evaltest=if(json_has_key_exact(\"{\\\"charlie.garcia\\\":10}\",\"charlie.garcia\"),\"has key\",\"doesn't have key\")"}, {"language": "spl", "code": "... |evaltest_results=if(json_has_key_exact(employee_record,\"status\"),\"has key\",\"doesn't have key\")"}], "tables": [{"headers": ["_time", "employee_record"], "rows": [["2024-12-17 21:22:43", "{\"name\":\"maria\",\"age\":25,\"status\":\"full-time\"}"], ["2024-12-17 21:22:43", "{\"name\":\"charlie\",\"age\":21,\"region\":\"US\"}"]]}, {"headers": ["_time", "employee_record", "test_results"], "rows": [["2024-12-17 21:22:43", "{\"name\":\"maria\",\"age\":25,\"status\":\"full-time\"}", "has key"], ["2024-12-17 21:22:43", "{\"name\":\"charlie\",\"age\":21,\"region\":\"US\"}", "doesn't have key"]]}], "chunk_index": 10, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_has_key_exact(<object>, <key>)", "section_id": "cba30966_dc57_4e5e_af7c_b7da9a4944c7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495955+00:00", "version": "10.2"}}
{"id": "690d4976e6da944f", "content": "Returns the keys from the key-value pairs in a JSON object. The keys are returned as a JSON array. Usage You can use this function with the eval and where commands, and as part of evaluation expressions with other commands. The json_keys function cannot be used on JSON arrays. Examples 1. Return a list of keys from a JSON object Consider the following JSON object, which is in the bridges field: This example extracts the keys from the JSON object in the bridges field: Here are the results of the search: 2. Return a list of keys from multiple JSON objects Consider the following JSON objects, which are in separate rows in the bridges field: This example extracts the keys from the JSON objects in the bridges field: Here are the results of the search:", "code_examples": [{"language": "spl", "code": "... |evalbridge_keys = json_keys(bridges)"}, {"language": "spl", "code": "... |evalbridge_keys = json_keys(bridges)"}], "tables": [{"headers": ["bridges"], "rows": [["{\"name\": \"Clifton Suspension Bridge\", \"length\": 1352, \"city\": \"Bristol\", \"country\": \"England\"}"]]}, {"headers": ["bridge_keys"], "rows": [["[\"name\", \"length\", \"city\", \"country\"]"]]}, {"headers": ["bridges"], "rows": [["{\"name\": \"Clifton Suspension Bridge\", \"length\": 1352, \"city\": \"Bristol\", \"country\": \"England\"}"], ["{\"name\":\"Rialto Bridge\",\"length\":157, \"city\": \"Venice\", \"region\": \"Veneto\", \"country\": \"Italy\"}"], ["{\"name\": \"Helix Bridge\", \"length\": 918, \"city\": \"Singapore\", \"country\": \"Singapore\"}"], ["{\"name\": \"Tilikum Crossing\", \"length\": 1700, \"city\": \"Portland\", \"state\": \"Oregon\", \"country\": \"United States\"}"]]}, {"headers": ["bridge_keys"], "rows": [["[\"name\", \"length\", \"city\", \"country\"]"], ["[\"name\", \"length\", \"city\", \"region\", \"country\"]"], ["[\"name\", \"length\", \"city\", \"country\"]"], ["[\"name\", \"length\", \"city\", \"state\", \"country\"]"]]}], "chunk_index": 11, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_keys(<json>)", "section_id": "a9be9c2f_1b53_445c_abec_5720614cdaa9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495970+00:00", "version": "10.2"}}
{"id": "b7b31c2860b4910c", "content": "Inserts or overwrites values for a JSON node with the values provided and returns an updated JSON object. Similar to the json_set_exact function. See json_set_exact Usage You can use this function with the eval and where commands, and as part of evaluation expressions with other commands. If the path contains a list of keys, all of the keys in the chain are created if the keys don't exist. If there's a mismatch between the JSON object and the path, the update is skipped and doesn't generate an error. For example, for object {\"a\": \"b\"}, json_set(.., \"a.c\", \"d\") produces no results since \"a\" has a string value and \"a.c\" implies a nested object. If the value already exists and is of a matching non-value type, the json_set function overwrites the value by default. A value type match isn't enforced. For example, you can overwrite a number with a string, Boolean, null, and so on. Examples These examples use this JSON object, which is in a field called games in an event: 1. Overwrite a value in an existing JSON array The following example overwrites the value \"Castle Panic\" in the path [category.boardgames.cooperative] in the JSON object. The value is replaced with \"name\":\"Sherlock Holmes: Consulting Detective\". The results are placed into a new field called my_games. The position count starts with 0. The third position is 2, which is why the example specifies {2} in the path. Here are the results of the search: 2. Insert a list of values in an existing JSON object The following example inserts a list of popular games [\"name\":\"Settlers of Catan\", \"name\":\"Terraforming Mars\", \"name\":\"Ticket to Ride\"] into the path [category.boardgames.competitive] in the JSON object. Because the key competitive doesn't exist in the path, the key is created. The json_array function is used to append the value list to the boardgames JSON object. Here are the results of the search: The JSON object now looks like this: 3. Insert a set of key-value pairs in an existing JSON object The following example inserts a set of key-value pairs that specify if the game is available using a Boolean value. These pairs are inserted into the path [category.boardgames.competitive] in the JSON object. The json_array function is used to append the key-value pairs list to the boardgames JSON object. Here are the results of the search: The JSON object now looks like this: If the Settlers of Catan game is out of stock, you can overwrite the value for the available key with the value false(). For example: Here are the results of the search: The JSON object now looks like this:", "code_examples": [{"language": "spl", "code": "{\"category\": {\"boardgames\": {\"cooperative\": [\n        {\"name\":\"Pandemic\"},\n        {\"name\":\"Forbidden Island\"},\n        {\"name\":\"Castle Panic\"}\n      ]\n    }\n  }\n}"}, {"language": "spl", "code": "... |evalmy_games = json_set(games,\"category.boardgames.cooperative{2}\",\"name\":\"Sherlock Holmes: Consulting Detective\")"}, {"language": "spl", "code": "...|evalmy_games = json_set(games,\"category.boardgames.competitive\", json_array(json_object(\"name\",\"Settlers of Catan\"), json_object(\"name\",\"Terraforming Mars\"), json_object(\"name\",\"Ticket to Ride\")))"}, {"language": "spl", "code": "{\"category\": {\"boardgames\": {\"cooperative\": [\n        {\"name\":\"Pandemic\"},\n        {\"name\":\"Forbidden Island\"},\n        {\"name\":\"Castle Panic\"}\n      ]\n    },\"competitive\": [\n      {\"name\":\"Settlers of Catan\"},\n      {\"name\":\"Terraforming Mars\"},\n      {\"name\":\"Ticket to Ride\"}\n    ]\n  }\n}"}, {"language": "spl", "code": "...|evalmy_games = json_set(games,\"category.boardgames.competitive{}.available\",true())"}, {"language": "spl", "code": "{\"category\": {\"boardgames\": {\"cooperative\": [\n        {\"name\":\"Pandemic\"},\n        {\"name\":\"Forbidden Island\"},\n        {\"name\":\"Castle Panic\"}\n      ]\n    },\"competitive\": [\n      {\"name\":\"Settlers of Catan\",\"available\":true},\n      {\"name\":\"Terraforming Mars\",\"available\":true},\n      {\"name\":\"Ticket to Ride\",\"available\":true}\n    ]\n  }\n}"}, {"language": "spl", "code": "... |evalmy_games = json_set(games,\"category.boardgames.competitive{0}.available\",false())"}, {"language": "spl", "code": "{\"category\": {\"boardgames\": {\"cooperative\": [\n        {\"name\":\"Pandemic\"},\n        {\"name\":\"Forbidden Island\"},\n        {\"name\":\"Castle Panic\"}\n      ]\n    },\"competitive\": [\n      {\"name\":\"Settlers of Catan\",\"available\":false},\n      {\"name\":\"Terraforming Mars\",\"available\":true},\n      {\"name\":\"Ticket to Ride\",\"available\":true}\n    ]\n  }\n}"}], "tables": [{"headers": ["Field", "Results"], "rows": [["my_games", "{\"category\":{\"boardgames\":{\"cooperative\":[\"name\":\"Pandemic\", \"name\":\"Forbidden Island\", \"name\":\"Sherlock Holmes: Consulting Detective\"]}}}"]]}, {"headers": ["Field", "Results"], "rows": [["my_games", "{\"category\":{\"boardgames\":{\"cooperative\":[\"name\":\"Pandemic\", \"name\":\"Forbidden Island\", \"name\":\"Sherlock Holmes: Consulting Detective\"],\"competitive\": [\"name\":\"Settlers of Catan\", \"name\":\"Terraforming Mars\", \"name\":\"Ticket to Ride\"]}}}"]]}, {"headers": ["Field", "Results"], "rows": [["my_games", "{\"category\":{\"boardgames\":{\"cooperative\":[\"name\":\"Pandemic\", \"name\":\"Forbidden Island\", \"name\":\"Sherlock Holmes: Consulting Detective\"],\"competitive\": [\"name\":\"Settlers of Catan\", \"available\":true, \"name\":\"Terraforming Mars\", \"available\":true, \"name\":\"Ticket to Ride\", \"available\":true]}}}"]]}, {"headers": ["Field", "Results"], "rows": [["my_games", "{\"category\":{\"boardgames\":{\"cooperative\":[\"name\":\"Pandemic\", \"name\":\"Forbidden Island\", \"name\":\"Sherlock Holmes: Consulting Detective\"],\"competitive\": [\"name\":\"Settlers of Catan\", \"available\":false, \"name\":\"Terraforming Mars\", \"available\":true, \"name\":\"Ticket to Ride\", \"available\":true]}}}"]]}], "chunk_index": 12, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_set(<json>, <path_value_pairs>)", "section_id": "id_4dda6c77_4316_4297_8561_fe49e8a9fe35--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495981+00:00", "version": "10.2"}}
{"id": "83436a55262e3a5a", "content": "Generates or overwrites a JSON object using the key-value pairs that you specify. Similar to the json_set function. See json_set Usage You can use the json_set_exact function with the eval and where commands, and as part of evaluation expressions with other commands. The json_set_exact function interprets the keys as literal strings, including special characters. This function does not interpret strings separated by period characters as keys for nested objects. If you supply multiple key-value pairs to json_set_exact , the function outputs an array. The json_set_exact function does not support or expect paths. You can set paths with nested json_array or json_object function calls. Example Suppose you want to have a JSON object that looks like this: To generate this object, you can use the makeresults command and the json_set_exact function as shown in the following search: You use json_set_exact for this instead of json_set because the json_set function interprets the period characters in {\"system.splunk.path\"} as nested objects. If you use json_set in the preceding search you get this JSON object: Instead of this object:", "code_examples": [{"language": "spl", "code": "{\"system.splunk.path\":\"/opt/splunk\"}"}, {"language": "spl", "code": "| makeresults |evalmy_object=json_object(), splunk_path=json_set_exact(my_object,\"system.splunk.path\",\"/opt/splunk\")"}, {"language": "spl", "code": "{\"system\":{\"splunk\":{\"path\":\"/opt/splunk\"}}}"}, {"language": "spl", "code": "{\"system.splunk.path\":\"/opt/splunk\"}"}], "tables": [], "chunk_index": 13, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_set_exact(<json>, <key_value_pairs>)", "section_id": "id_2207f3c6_ed02_4a42_b4b0_05229ba759ad--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495989+00:00", "version": "10.2"}}
{"id": "104839406dae1721", "content": "Evaluates whether a piece of JSON uses valid JSON syntax and returns either TRUE or FALSE. Usage You can use this function with the eval and where commands, and as part of evaluation expressions with other commands. Example The following example validates a JSON object { \"names\": [\"maria\", \"arun\"] } in the firstnames field. Because fields cannot hold Boolean values, the if function is used with the json_valid function to place the string value equivalents of the Boolean values into the isValid field.", "code_examples": [{"language": "spl", "code": "... |evalIsValid =if(json_valid(firstnames),\"true\",\"false\")"}], "tables": [], "chunk_index": 14, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "json_valid(<json>)", "section_id": "id_9a335a37_c74b_47cb_b80f_d4c5abea4670--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495993+00:00", "version": "10.2"}}
{"id": "4732aeabf436687b", "content": "Function information Evaluation functions quick reference Related functions mv_to_json_array function Related commands tojson fromjson", "code_examples": [], "tables": [], "chunk_index": 15, "total_chunks": 16, "metadata": {"title": "JSON functions", "section_heading": "See also", "section_id": "ea701038_27d0_4a0d_a906_13c811ec86d0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/json-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:11.495998+00:00", "version": "10.2"}}
{"id": "fb7b0d3bbb8987f0", "content": "Description This function takes one argument and evaluates whether the value is an array data type. The function returns TRUE if the value is an array. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You can use this function directly with the where command in searches, but the eval command can't directly accept a Boolean value. You must specify the function inside another function, such as the if function, which can accept a Boolean value as an input. Basic examples The following search returns True because [1, 2, 3] is an array. The result of the following search is False because 1 is not an array.", "code_examples": [{"language": "spl", "code": "| makeresults\n|evalresult =if(isarray(\"[1, 2, 3]\"),\"True\",\"False\")"}, {"language": "spl", "code": "| makeresults\n|evalresult =if(isarray(1),\"True\",\"False\")"}], "tables": [], "chunk_index": 0, "total_chunks": 11, "metadata": {"title": "Informational functions", "section_heading": "isarray(<value>)", "section_id": "af9a3c80_e99f_4b58_b073_782b068a50bf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/informational-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:28.851713+00:00", "version": "10.2"}}
{"id": "a256ab7cf9f96b2c", "content": "Description This function takes one argument and evaluates whether the value is a Boolean data type. The function returns TRUE if the value is Boolean. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Use this function with other functions that return Boolean data types, such as cidrmatch and mvfind. This function cannot be used to determine if field values are \"true\" or \"false\" because field values are either string or number data types. Instead, use syntax such as <fieldname>=true OR <fieldname>=false to determine field values. You can use this function directly with the where command in searches, but the eval command can't directly accept a Boolean value. You must specify the function inside another function, such as the if function, which can accept a Boolean value as an input. Basic examples The following search returns True because 1==2 is Boolean. The following search returns False because the value a is not Boolean.", "code_examples": [{"language": "spl", "code": "| makeresults\n|evalresult =if(isbool(1==2),\"True\",\"False\")"}, {"language": "spl", "code": "| makeresults\n|evalresult =if(isbool(a),\"True\",\"False\")"}], "tables": [], "chunk_index": 1, "total_chunks": 11, "metadata": {"title": "Informational functions", "section_heading": "isbool(<value>)", "section_id": "a6fd42d1_98fa_4858_a666_c3074dcf0e6a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/informational-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:28.851721+00:00", "version": "10.2"}}
{"id": "b3aab64f05192b66", "content": "Description This function takes one argument and evaluates whether the value is a double data type. The function returns TRUE if the value is a double value. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You can use this function directly with the where command in searches, but the eval command can't directly accept a Boolean value. You must specify the function inside another function, such as the if function, which can accept a Boolean value as an input. Basic examples The following search returns True because the value 3.546 is a double. The following example returns False because 1000000 is not a double.", "code_examples": [{"language": "spl", "code": "| makeresults\n|evalresult =if(isdouble(3.546),\"True\",\"False\")"}, {"language": "spl", "code": "... |evalresult =if(isdouble(1000000),\"True\",\"False\")"}], "tables": [], "chunk_index": 2, "total_chunks": 11, "metadata": {"title": "Informational functions", "section_heading": "isdouble(<value>)", "section_id": "a310481c_6517_49a2_a313_68dbaa10ba3c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/informational-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:28.851728+00:00", "version": "10.2"}}
{"id": "ba8a06b36349982d", "content": "Description This function takes one argument and returns TRUE if the value is an integer. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You can use this function directly with the where command in searches, but the eval command can't directly accept a Boolean value. You must specify the function inside another function, such as the if function, which can accept a Boolean value as an input. Basic examples The following example uses the isint function with the if function. A field, \"n\", is added to each result with a value of \"int\" or \"not int\", depending on the result of the isint function. If the value of \"field\" is a number, the isint function returns TRUE and the value adds the value \"int\" to the \"n\" field. The following example shows how to use the isint function with the where command.", "code_examples": [{"language": "spl", "code": "... |evaln=if(isint(field),\"int\",\"not int\")"}, {"language": "spl", "code": "... |whereisint(field)"}], "tables": [], "chunk_index": 3, "total_chunks": 11, "metadata": {"title": "Informational functions", "section_heading": "isint(<value>)", "section_id": "id_7c9ff4db_f87c_4e44_9ac8_548ddeb31632--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/informational-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:28.851732+00:00", "version": "10.2"}}
{"id": "420346781c495aac", "content": "Description This function takes one argument and evaluates whether the field is a multivalue data type. The function returns TRUE if the field is a multivalue. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You can use this function directly with the where command in searches, but the eval command can't directly accept a Boolean value. You must specify the function inside another function, such as the if function, which can accept a Boolean value as an input. Basic examples The following example returns True because the value in the number_list field is a multivalue. The result looks like this:", "code_examples": [{"language": "spl", "code": "... |evalnumber_list=split(\"1, 2, 3\",\",\")\n|evalresult=if(ismv(number_list),\"True\",\"False\")"}], "tables": [{"headers": ["_time", "number_list", "result"], "rows": [["2024-12-11 00:49:31", "123", "True"]]}], "chunk_index": 4, "total_chunks": 11, "metadata": {"title": "Informational functions", "section_heading": "ismv(<value>)", "section_id": "id_2af26cc6_c454_4030_802a_74bc66d1fd8c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/informational-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:28.851738+00:00", "version": "10.2"}}
{"id": "c9df2a255750b59b", "content": "Description This function takes one argument and returns TRUE if the value is not NULL. Usage This function is useful for checking for whether or not a field contains a value. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You can use this function directly with the where command in searches, but the eval command can't directly accept a Boolean value. You must specify the function inside another function, such as the if function, which can accept a Boolean value as an input. Basic examples The following example uses the isnotnull function with the if function. A field, \"n\", is added to each result with a value of \"yes\" or \"no\", depending on the result of the isnotnull function. If the value of \"field\" is a number, the isnotnull function returns TRUE and the value adds the value \"yes\" to the \"n\" field. The following example shows how to use the isnotnull function with the where command.", "code_examples": [{"language": "spl", "code": "... |evaln=if(isnotnull(field),\"yes\",\"no\")"}, {"language": "spl", "code": "... |whereisnotnull(field)"}], "tables": [], "chunk_index": 5, "total_chunks": 11, "metadata": {"title": "Informational functions", "section_heading": "isnotnull(<value>)", "section_id": "ee557a13_80a7_448d_9771_38d2a5285c2d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/informational-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:28.851743+00:00", "version": "10.2"}}
{"id": "06e10ed7c9764713", "content": "Description This function takes one argument and returns TRUE if the value is NULL.. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You can use this function directly with the where command in searches, but the eval command can't directly accept a Boolean value. You must specify the function inside another function, such as the if function, which can accept a Boolean value as an input. Basic examples The following example uses the isnull function with the if function. A field, \"n\", is added to each result with a value of \"yes\" or \"no\", depending on the result of the isnull function. If there is no value for \"field\" in a result, the isnull function returns TRUE and adds the value \"yes\" to the \"n\" field. The following example shows how to use the isnull function with the where command.", "code_examples": [{"language": "spl", "code": "... |evaln=if(isnull(field),\"yes\",\"no\")"}, {"language": "spl", "code": "... |whereisnull(field)"}], "tables": [], "chunk_index": 6, "total_chunks": 11, "metadata": {"title": "Informational functions", "section_heading": "isnull(<value>)", "section_id": "id_967b2b12_0a19_4a92_86a5_8e77c006d07e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/informational-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:28.851748+00:00", "version": "10.2"}}
{"id": "c1c37cb6044e9175", "content": "Description This function takes one argument and returns TRUE if the value is a number. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Using isnum in searches with NaN In eval functions, fields can be either a string or a number. Working with NaN (Not a Number) values in the Splunk platform can be challenging because Splunk fields contain values that can be processed as either strings or numeric values based on their context. This can create confusion between a numeric NaN value, and the string representation of that value, \"NaN\". For example, depending on the kind of value that is needed to satisfy the current calculation, a NaN can be a numeric NaN , or a string \"NaN\" that can be interpreted as a numeric value or treated as a string. If your data contains \"NaN\" , you should proceed with caution when using searches that require NaN handling in SPL because NaN values can behave in unexpected ways. If you're using \"NaN\" in your searches with the isnum command, it's important to distinguish between a literal string \"NaN\" and a field containing the value \"NaN\". When a field contains a \"NaN\" string, the \"NaN\" behaves as both a string and a number. However, when a \"NaN\" value is present as a literal string in an evaluator expression, it is considered a string, not a number. For example, because \"NaN\" is just a collection of characters, it is considered a string and returns false in a search like isnum(\"NaN\"). However, if the same value is stored as \"NaN\" in a field, it is parsed as a numeric type. For example, say you run the following search. Your results look like this. Notice that literalNanIsNumeric is false because the isnum command interprets \"NaN\" as a string, not a number. It can be difficult to determine whether a value stored in a numeric field is a NaN or a literal value. A reliable test for NaN in the Splunk platform to confirm that a value is a real numeric NaN is to include the following search string in your search: For example, if the value you're testing is \"NaN\" , the search returns isnan is True , like the following search: See Numeric calculations. Basic examples The following example uses the isnum function with the if function. A field, \"n\", is added to each result with a value of \"yes\" or \"no\", depending on the result of the isnum function. If the value of \"field\" is a number, the isnum function returns TRUE and the value adds the value \"yes\" to the \"n\" field. The following example shows how to use the isnum function with the where command.", "code_examples": [{"language": "spl", "code": "| makeresults \n|evalstrval=\"NaN\", numval=strvalÂ % 1 \n| fields - _time\n|evalliteralIsNumeric=if(isnum(\"anystring\"),\"true\",\"false\")\n|evalliteralNanIsNumeric=if(isnum(\"NaN\"),\"true\",\"false\")\n|evalnumvalIsNumeric=if(isnum(numval),\"true\",\"false\")\n|evalstrvalIsNumeric=if(isnum(strval),\"true\",\"false\")\n| transpose"}, {"language": "spl", "code": "|evalisnan=if(isnum(numval), match(numval,\"NaN\"),false)"}, {"language": "spl", "code": "| makeresults \n|evalstrval=\"NaN\", numval=strvalÂ % 1\n|evalisnan=if(isnum(numval), match(numval,\"NaN\"),false)\n| transpose"}, {"language": "spl", "code": "... |evaln=if(isnum(field),\"yes\",\"no\")"}, {"language": "spl", "code": "... |whereisnum(field)"}], "tables": [{"headers": ["column", "row"], "rows": [["literalIsNumeric", "false"], ["literalNanIsNumeric", "false"], ["numval", "NaN"], ["numvalIsNumeric", "true"], ["strval", "NaN"], ["strvalIsNumeric", "true"]]}], "chunk_index": 7, "total_chunks": 11, "metadata": {"title": "Informational functions", "section_heading": "isnum(<value>)", "section_id": "id_20b3653a_7a10_458b_87d8_739e8eea7c44--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/informational-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:28.851759+00:00", "version": "10.2"}}
{"id": "45ba5de0bc408726", "content": "Description This function takes one argument and evaluates whether the value is an object. The function returns TRUE if a string is a valid JSON object. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You can use this function directly with the where command in searches, but the eval command can't directly accept a Boolean value. You must specify the function inside another function, such as the if function, which can accept a Boolean value as an input. Basic examples The following example returns False because the value in the games field isn't a valid JSON object. The following example returns True because the value in the games field is a valid JSON object. Say you run the following search. Your results look like this.", "code_examples": [{"language": "spl", "code": "... |evalgames =\"Ticket to Ride, Settlers of Catan\"|evalresult =if(isobject(\"games\"),\"True\",\"False\")"}, {"language": "spl", "code": "... |evalgames =\"{\\\"type\\\": \\\"competitive\\\", \\\"name\\\": \\\"Ticket to Ride\\\"}\"|evalresult =if(isobject(games),\"True\",\"False\")"}, {"language": "spl", "code": "| makeresults\n|evalis_an_object =if(isobject(\"{cities: \\\"3\\\"}\"),\"is object\",\"is not object\")"}], "tables": [{"headers": ["_time", "is_an_object"], "rows": [["2024-12-19 21:49:04", "is not object"]]}], "chunk_index": 8, "total_chunks": 11, "metadata": {"title": "Informational functions", "section_heading": "isobject(<value>)", "section_id": "id_34f6ec30_cf11_4315_bbb5_0aae8cf66437--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/informational-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:28.851765+00:00", "version": "10.2"}}
{"id": "bd2fbf5cb83d665e", "content": "Description This function takes one argument and returns TRUE if the value is a string. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You can use this function directly with the where command in searches, but the eval command can't directly accept a Boolean value. You must specify the function inside another function, such as the if function, which can accept a Boolean value as an input. Using isstr in searches with NaN In eval functions, fields can be either a string or a number. Working with NaN (Not a Number) values in the Splunk platform can be challenging because Splunk fields contain values that can be processed as either strings or numeric values based on their context. This can create confusion between a numeric NaN value, and the string representation of that value, \"NaN\". For example, depending on the kind of value that is needed to satisfy the current calculation, a NaN can be a numeric NaN , or a string \"NaN\" that can be interpreted as a numeric value or treated as a string. If your data contains \"NaN\" , you should proceed with caution when using searches that require NaN handling in SPL because NaN values can behave in unexpected ways. If you're using \"NaN\" in your searches with the isstr command, the distinction between a literal string \"NaN\" and a field containing the value \"NaN\" is not as important as it is with the isnum command. When a \"NaN\" string is contained in a field, the \"NaN\" behaves as both a string and a number. But, when a \"NaN\" value is present as a literal string in an evaluator expression, it is considered a string, not a number. In both cases, the isstr command parses the \"NaN\" value as a string. For example, say you run the following search. Your results look like this. Notice that, as expected, each isstr test identifies \"NaN\" as a string, regardless of whether the \"NaN\" is a string or numeric value. See Numeric calculations. Basic examples The following example uses the isstr function with the if function. A field, \"n\", is added to each result with a value of \"yes\" or \"no\", depending on the result of the isstr function. If the value of \"field\" is a string, the isstr function returns TRUE and the value adds the value \"yes\" to the \"n\" field. The following example shows how to use the isstr function with the where command.", "code_examples": [{"language": "spl", "code": "| makeresults \n|evalstrval=\"NaN\", numval=strvalÂ % 1 \n| fields - _time\n|evalliteralIsStr=if(isstr(\"anystring\"),\"true\",\"false\")\n|evalliteralNanIsStr=if(isstr(\"NaN\"),\"true\",\"false\")\n|evalnumvalIsStr=if(isstr(numval),\"true\",\"false\")\n|evalstrvalIsStr=if(isstr(strval),\"true\",\"false\")\n| transpose"}, {"language": "spl", "code": "... |evaln=if(isstr(field),\"yes\",\"no\")"}, {"language": "spl", "code": "... |whereisstr(field)"}], "tables": [{"headers": ["column", "row"], "rows": [["literalIsStr", "true"], ["literalNanIsStr", "true"], ["numval", "NaN"], ["numvalIsStr", "true"], ["strval", "NaN"], ["strvalIsStr", "true"]]}], "chunk_index": 9, "total_chunks": 11, "metadata": {"title": "Informational functions", "section_heading": "isstr(<value>)", "section_id": "e8882870_c30f_47b8_9815_25da00c17c55--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/informational-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:28.851772+00:00", "version": "10.2"}}
{"id": "cdf03185c593103b", "content": "Description This function takes one argument and returns the data type of the argument. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example takes one argument and returns a string representation of its type. This example returns \"NumberStringBoolInvalid\" The following example creates a single result using the makeresults command. For example: To determine the data type of the _time field, use the eval command with the typeof function. For example: The results are:", "code_examples": [{"language": "spl", "code": "... |evaln=typeof(12) + typeof(\"string\") + typeof(1==2) + typeof(badfield)"}, {"language": "spl", "code": "| makeresults"}, {"language": "spl", "code": "| makeresults |evalt=typeof(_time)"}], "tables": [{"headers": ["_time"], "rows": [["2018-08-14 14:00:15"]]}, {"headers": ["_time", "t"], "rows": [["2018-08-14 14:00:15", "Number"]]}], "chunk_index": 10, "total_chunks": 11, "metadata": {"title": "Informational functions", "section_heading": "typeof(<value>)", "section_id": "id_2f87cd5b_3fe9_42d7_a58e_3d0fb959e57e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/informational-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:28.851778+00:00", "version": "10.2"}}
{"id": "6be6ac4c034ca2f3", "content": "Description This function computes and returns the MD5 hash of a string value. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Examples The following example returns a new field n with a message-digest (MD5) 128-bit hash value for the phrase \"Hello World\". The results look like this: The following example creates a large random string. The makeresults command creates 32768 results with timestamps. The eval command creates a new field called message : The random function returns a random numeric field value for each of the 32768 results. The \"\". makes the numeric number generated by the random function into a string value. The md5 function creates a 128-bit hash value from the string value. The results of the md5 function are placed into the message field created by the eval command. The stats command with the values function is used to convert the individual random values into one multivalue result. The eval command with the mvjoin function is used to combine the multivalue entry into a single value.", "code_examples": [{"language": "spl", "code": "... |evaln=md5(\"Hello World\")"}, {"language": "spl", "code": "| makeresults count=32768 \n|evalmessage=md5(\"\". random()) \n| stats values(message) as message \n|evalmessage = mvjoin(message,\"\")"}], "tables": [{"headers": ["_time", "n"], "rows": [["2025-01-02 09:23:00", "b10a8db164e0754105b7a99be72e3fe5"]]}], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Cryptographic functions", "section_heading": "md5(str)", "section_id": "id_182cc337_b6e2_4012_8a90_143ab16a2a1f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/cryptographic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:43.963257+00:00", "version": "10.2"}}
{"id": "dbc6ad5b966d6740", "content": "Description This function computes and returns the secure hash of a string value based on the FIPS compliant SHA-1 hash function. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Examples The following example creates a secure hash value for the string phrase: The following data shows a set of string values in the ID field: You can use the sha1 cryptographic function to create secure hash values for the values in the ID field: The results look like this:", "code_examples": [{"language": "spl", "code": "... |evaln=sha1(\"You are a wonderful person.\")"}, {"language": "spl", "code": "... |evalhashID=sha1(ID)"}], "tables": [{"headers": ["_time", "name", "ID"], "rows": [["2025-01-02 09:09:03", "Charlie Garcia", "222-333-4444"], ["2025-01-02 09:09:03", "Taylor Zhang", "444-11-8888"], ["2025-01-02 09:09:03", "Sasha Patel", "555-22-9999"], ["2025-01-02 09:09:03", "Nyah Aamadu", "777-88-9999"]]}, {"headers": ["_time", "name", "ID", "hashID"], "rows": [["2025-01-02 09:09:03", "Charlie Garcia", "222-333-4444", "274ab92ea358c9b31f615290809085a58578b057"], ["2025-01-02 09:09:03", "Taylor Zhang", "444-11-8888", "ff8f94405a9089d0d0749ce9f729921c4f7f31fd"], ["2025-01-02 09:09:03", "Sasha Patel", "555-22-9999", "b67e7f2e0ad4e744e5f7b6b148249bad13c794ce"], ["2025-01-02 09:09:03", "Nyah Aamadu", "777-88-9999", "205c1cd079019f46003947a12662b3d5a17f0d5f"]]}], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Cryptographic functions", "section_heading": "sha1(str)", "section_id": "b887c0c6_f55c_48e8_a0b8_c8dd2e16055c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/cryptographic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:43.963270+00:00", "version": "10.2"}}
{"id": "54a9bf0b22517ac7", "content": "Description This function computes and returns the secure hash of a string value based on the FIPS compliant SHA-256 (SHA-2 family) hash function. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Example", "code_examples": [{"language": "spl", "code": "... |evaln=sha256(\"Can you SPL?\")"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Cryptographic functions", "section_heading": "sha256(str)", "section_id": "d2e4c33f_d7a6_41a4_a40d_c8191b839495--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/cryptographic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:43.963276+00:00", "version": "10.2"}}
{"id": "24454241c7e0d36c", "content": "Description This function computes and returns the secure hash of a string value based on the FIPS compliant SHA-512 (SHA-2 family) hash function. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Example", "code_examples": [{"language": "spl", "code": "... |evaln=sha512(\"You bet your sweet SaaS.\")"}], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Cryptographic functions", "section_heading": "sha512(str)", "section_id": "id_165d45f1_bee6_440c_836e_e6c160725c72--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/cryptographic-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:26:43.963280+00:00", "version": "10.2"}}
{"id": "3745cb24baa29295", "content": "Description Accepts alternating conditions and values. Returns the first value for which the condition evaluates to TRUE. The <condition> arguments are Boolean expressions that are evaluated from first to last. When the first <condition> expression is encountered that evaluates to TRUE, the corresponding <value> argument is returned. The function defaults to NULL if none of the <condition> arguments are true. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example The following example returns descriptions for the corresponding http status code. The results appear on the Statistics tab and look like this: For an example of how to display a default value when that status does not match one of the values specified, see the True function. Extended example This example shows you how to use the case function in two different ways, to create categories and to create a custom sort order. You want classify earthquakes based on depth. Shallow-focus earthquakes occur at depths less than 70 km. Mid-focus earthquakes occur at depths between 70 and 300 km. Deep-focus earthquakes occur at depths greater than 300 km. We'll use Low, Mid, and Deep for the category names. The eval command is used to create a field called Description , which takes the value of \"Low\", \"Mid\", or \"Deep\" based on the Depth of the earthquake. The case() function is used to specify which ranges of the depth fits each description. For example, if the depth is less than 70 km, the earthquake is characterized as a shallow-focus quake; and the resulting Description is Low. The search also pipes the results of the eval command into the stats command to count the number of earthquakes and display the minimum and maximum magnitudes for each Description. The results appear on the Statistics tab and look like this: You can sort the results in the Description column by clicking the sort icon in Splunk Web. However in this example the order would be alphabetical returning results in Deep, Low, Mid or Mid, Low, Deep order. You can also use the case function to sort the results in a custom order, such as Low, Mid, Deep. You create the custom sort order by giving the values a numerical ranking and then sorting based on that ranking. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "sourcetype=access_* |evaldescription=case(status==200,\"OK\", status==404,\"Not found\", status==500,\"Internal Server Error\") | table status description"}, {"language": "spl", "code": "source=all_month.csv \n|evalDescription=case(depth<=70,\"Low\", depth>70 AND depth<=300,\"Mid\", \n  depth>300,\"Deep\") \n| stats count min(mag) max(mag) by Description"}, {"language": "spl", "code": "source=all_month.csv \n|evalDescription=case(depth<=70,\"Low\", depth>70 AND depth<=300,\"Mid\", \n  depth>300,\"Deep\") \n| stats count min(mag) max(mag) by Description\n|evalsort_field=case(Description=\"Low\", 1, Description=\"Mid\", 2, Description=\"Deep\",3) \n| sort sort_field"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial, but should work with any format of Apache Web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use theYesterdaytime range when you run the search."]]}, {"headers": ["status", "description"], "rows": [["200", "OK"], ["200", "OK"], ["408", ""], ["200", "OK"], ["404", "Not found"], ["200", "OK"], ["406", ""], ["500", "Internal Server Error"], ["200", "OK"]]}, {"headers": [], "rows": [["This example uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), and so forth, for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance if you want follow along with this example."]]}, {"headers": ["Description", "count", "min(Mag)", "max(Mag)"], "rows": [["Deep", "35", "4.1", "6.7"], ["Low", "6236", "-0.60", "7.70"], ["Mid", "635", "0.8", "6.3"]]}, {"headers": ["Description", "count", "min(Mag)", "max(Mag)"], "rows": [["Low", "6236", "-0.60", "7.70"], ["Mid", "635", "0.8", "6.3"], ["Deep", "35", "4.1", "6.7"]]}], "chunk_index": 0, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "case(<condition>,<value>,...)", "section_id": "id_71d2bc88_ab3e_4ab9_92a5_6f938276a30e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033717+00:00", "version": "10.2"}}
{"id": "a3006b226cde72bd", "content": "Description This function returns TRUE when an IP address, <ip> , belongs to a particular CIDR subnet, <cidr>. Both <cidr> and <ip> are string arguments. If you specify a literal string value, instead of a field name, that value must be enclosed in double quotation marks. The cidrmatch function supports IPv4 and IPv6 addresses and subnets that use CIDR notation. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example uses the cidrmatch and if functions to set a field, isLocal , to \"local\" if the field ip matches the subnet. If the ip field does not match the subnet, the isLocal field is set to \"not local\". The following example uses the cidrmatch function as a filter to remove events that do not match the ip address: Extended examples for IPv4 addresses You can use the cidrmatch function to identify CIDR IP addresses by subnet. The following example uses cidrmatch with the eval command to compare an IPv4 address with a subnet that uses CIDR notation to determine whether the IP address is a member of the subnet. If there is a match, the search returns true in a new field called result. The IP address is not in the subnet, so search displays false in the result field. The search results look something like this. In the following example, cidrmatch evaluates the IPv4 address 192.0.2.56 to find out if it is in the subnet. This time, instead of using the eval command with the cidrmatch function, we're using the where command, which eliminates any IP addresses that aren't within the subnet. This search compares the CIDR IP address with the subnet and filters the search results by returning the IP address only if it is true. The IP address is located within the subnet, so it is displayed in the search results, which look like this. Note that you can get the same results when using the search command, as shown in this example. The results of the search look like this. Extended examples for IPv6 addresses The following example uses cidrmatch with the eval command to compare an IPv6 address with a subnet that uses CIDR notation to determine whether the IP address is a member of the subnet. If there is a match, search returns true in a new field called result. The IP address is located within the subnet, so search displays true in the result field. The search results look something like this. The following example is another way to use cidrmatch to identify which IP addresses are in a subnet. This time, instead of using the eval command with the cidrmatch function, we're using the where command. This search compares the CIDR IPv6 addresses with the specified subnet and filters the search results by returning only the IP addresses that are in the subnet. The search results look something like this. See also Commands iplocation lookup search", "code_examples": [{"language": "spl", "code": "... |evalisLocal=if(cidrmatch(\"123.132.32.0/25\",ip),\"local\",\"not local\")"}, {"language": "spl", "code": "... |wherecidrmatch(\"123.132.32.0/25\", ip)"}, {"language": "spl", "code": "| makeresults \n|evalsubnet=\"192.0.2.0/24\", ip=\"192.0.3.0\"|evalresult=if(cidrmatch(subnet, ip),\"true\",\"false\")"}, {"language": "spl", "code": "| makeresults \n|evalip=\"192.0.2.56\"|wherecidrmatch(\"192.0.2.0/24\", ip)"}, {"language": "spl", "code": "| makeresults \n|evalip=\"192.0.2.56\"| search ip=\"192.0.2.0/24\""}, {"language": "spl", "code": "| makeresults \n|evalsubnet=\"2001:0db8:ffff:ffff:ffff:ffff:ffff:ff00/120\", ip=\"2001:0db8:ffff:ffff:ffff:ffff:ffff:ff99\"|evalresult =if(cidrmatch(subnet, ip),\"true\",\"false\")"}, {"language": "spl", "code": "| makeresults \n|evalip=\"2001:0db8:ffff:ffff:ffff:ffff:ffff:ff99\"|wherecidrmatch(\"2001:0db8:ffff:ffff:ffff:ffff:ffff:ff00/120\", ip)"}], "tables": [{"headers": ["time", "ip", "result", "subnet"], "rows": [["2020-11-19 16:43:31", "192.0.3.0", "false", "192.0.2.0/24"]]}, {"headers": ["time", "ip"], "rows": [["2020-11-19 16:43:31", "192.0.2.56"]]}, {"headers": ["time", "ip"], "rows": [["2020-11-19 16:43:31", "192.0.2.56"]]}, {"headers": ["time", "ip", "result", "subnet"], "rows": [["2020-11-19 16:43:31", "2001:0db8:ffff:ffff:ffff:ffff:ffff:ff99", "true", "2001:0db8:ffff:ffff:ffff:ffff:ffff:ff00/120"]]}, {"headers": ["time", "ip"], "rows": [["2020-11-19 16:43:31", "2001:0db8:ffff:ffff:ffff:ffff:ffff:ff99"]]}], "chunk_index": 1, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "cidrmatch(<cidr>,<ip>)", "section_id": "a2266981_13f5_4731_b40a_44941e7a7074--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033736+00:00", "version": "10.2"}}
{"id": "ddce152ee8cb2896", "content": "Description This function takes one or more values and returns the first value that is not NULL. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples You have a set of events where the IP address is extracted to either clientip or ipaddress. This example defines a new field called ip , that takes the value of either the clientip field or ipaddress field, depending on which field is not NULL (does not exist in that event). If both the clientip and ipaddress field exist in the event, this function returns the first argument, the clientip field.", "code_examples": [{"language": "spl", "code": "... |evalip=coalesce(clientip,ipaddress)"}], "tables": [], "chunk_index": 2, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "coalesce(<values>)", "section_id": "id_57674d3b_2a85_4636_9f5c_531dadfae18c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033744+00:00", "version": "10.2"}}
{"id": "0c3cc5f678f2765e", "content": "Description Use this function to return FALSE. This function enables you to specify a conditional that is obviously false, for example 1==0. You do not specify a field with this function. Usage This function is often used as an argument with other functions. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "false()", "section_id": "f304bc45_0967_4f85_a8c9_89ed6220bd2a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033749+00:00", "version": "10.2"}}
{"id": "6012ff22d8643db2", "content": "Description If the <predicate> expression evaluates to TRUE, returns the <true_value> , otherwise the function returns the <false_value>. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The if function is frequently used in combination with other functions. Basic examples The following example looks at the values of the field error. If error=200 , the function returns err=OK. Otherwise the function returns err=Error. The following example uses the cidrmatch and if functions to set a field, isLocal , to \"local\" if the field ip matches the subnet. If the ip field does not match the subnet, the isLocal field is set to \"not local\".", "code_examples": [{"language": "spl", "code": "... |evalerr=if(error == 200,\"OK\",\"Error\")"}, {"language": "spl", "code": "... |evalisLocal=if(cidrmatch(\"123.132.32.0/25\",ip),\"local\",\"not local\")"}], "tables": [], "chunk_index": 4, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "if(<predicate>,<true_value>,<false_value>)", "section_id": "cdbeba6b_1166_4ab6_8e5b_421946847e29--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033755+00:00", "version": "10.2"}}
{"id": "b2dae74f94898ad4", "content": "Description The function returns TRUE if one of the values in the list matches a value that you specify. This function takes a list of comma-separated values. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions with other commands. The following syntax is supported: ...| where in(field,\"value1\",\"value2\", ...) ...| where field in(\"value1\",\"value2\", ...) ...| eval new_field=if(in(field,\"value1\",\"value2\", ...), \"value-if_true\",\"value-if-false\") Note: The eval command cannot accept a Boolean value. You must specify the in function inside a function that can accept a Boolean value as input. Those functions are: case , if , and validate. The string values must be enclosed in quotation marks. You cannot specify wildcard characters with the values to specify a group of similar values, such as HTTP error codes or CIDR IP address ranges. Use the IN operator instead. The IN operator is similar to the in function. You can use the IN operator with the search and tstats commands. You can use wildcard characters in the VALUE-LIST with these commands. Basic examples The following example uses the where command to return in=TRUE if one of the values in the status field matches one of the values in the list. The following example uses the in function as the first parameter for the if function. The evaluation expression returns TRUE if the value in the status field matches one of the values in the list. The following example uses the where command to return in=TRUE if the value 203.0.113.255 appears in either the ipaddress or clientip fields. Extended example The following example combines the in function with the if function to evaluate the status field. The value of true is placed in the new field error if the status field contains one of the values 404, 500, or 503. Then a count is performed of the values in the error field. See also Blogs Smooth operator | Searching for multiple field values", "code_examples": [{"language": "spl", "code": "... |wherestatusin(\"400\",\"401\",\"403\",\"404\")"}, {"language": "spl", "code": "... |evalerror=if(in(status,\"error\",\"failure\",\"severe\"),\"true\",\"false\")"}, {"language": "spl", "code": "... |where\"203.0.113.255\"in(ipaddress, clientip)"}, {"language": "spl", "code": "... |evalerror=if(in(status,\"404\",\"500\",\"503\"),\"true\",\"false\") | stats count by error"}], "tables": [], "chunk_index": 5, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "in(<field>,<list>)", "section_id": "id_460becca_09c2_499a_b741_eebe6d40bc6f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033762+00:00", "version": "10.2"}}
{"id": "fecf9a1443f800ac", "content": "Description This function returns TRUE only if <str> matches <pattern>. The match can be an exact match or a match using a wildcard: Use the percent ( % ) symbol as a wildcard for matching multiple characters Use the underscore ( _ ) character as a wildcard to match a single character The <str> can be a field name or a string value. The <pattern> must be a string expression enclosed in double quotation marks. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The following syntax is supported: ...|eval new_field=if(like(<str>, <pattern>) ...| where like(<str>, <pattern>) ...| where <str> LIKE <pattern> Note: The eval command cannot accept a Boolean value. You must specify the like function inside a function that can accept a Boolean value as input. Those functions are: case , if , and validate. Basic examples The following example returns like=TRUE if the field value starts with foo: The following example uses the where command to return like=TRUE if the ipaddress field starts with the value 198. The percent ( % ) symbol is a wildcard with the like function:", "code_examples": [{"language": "spl", "code": "... |evalis_a_foo=if(like(field,\"foo%\"),\"yes a foo\",\"not a foo\")"}, {"language": "spl", "code": "... |wherelike(ipaddress,\"198.%\")"}], "tables": [], "chunk_index": 6, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "like(<str>,<pattern>)", "section_id": "f09f5236_0794_4832_b50e_033c1401a4ef--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033768+00:00", "version": "10.2"}}
{"id": "5e151a4a3a7647fc", "content": "Description This function performs a CSV lookup. It returns the output field or fields in the form of a JSON object. Note: The lookup() function is available only to Splunk Enterprise users. Syntax lookup(\"<lookup_table>\", json_object(\"<input_field>\", <match_field>,...), json_array(\"<output_field>\",...)) Usage You can use the lookup() function with the eval , fieldformat , and where commands, and as part of eval expressions. The lookup() function takes an <input_field> from a CSV <lookup_table> , finds events in the search result that have the <match_field> , and then identifies other field-value pairs from from the CSV table that correspond to the input_field and adds them to the matched events in the form of a JSON object. The lookup() requires a <lookup_table>. You can provide this either a CSV lookup file or CSV lookup definition, enclosed within quotation marks. To provide a file, give the full filename of a CSV lookup file that is stored in the global lookups directory ( $SPLUNK_HOME/etc/system/lookups/ ) or in a lookup directory that matches your current app context, such as $SPLUNK_HOME/etc/users/<user>/<app>/lookups/. If the first quoted string does not end in \".csv\", the eval processor assumes it is the name of a CSV lookup definition. Specified CSV lookup definitions must be shared globally. CSV lookup definitions cannot be private or shared to a specific app. Note: Specify a lookup definition if you want the various settings associated with the definition to apply, such as limits on matches, case-sensitive match options, and so on. A lookup() function can use multiple <input_field> / <match_field> pairs to identify events, and multiple <output_field> values can be applied to those events. Here is an example of valid lookup() syntax with multiple inputs, matches, and outputs. For more information about uploading CSV lookup files and creating CSV lookup definitions, see Define a CSV lookup in Splunk Web in the Knowledge Manager Manual. The lookup() function uses two JSON functions for eval : json_object and json_array. JSON functions allow the eval processor to efficiently group things together. For more information, see JSON functions in the Search Reference. Examples These examples show different ways to use the lookup() function. 1. Simple example that returns a JSON object with an array This simple makeresults example returns an array that illustrates what status_description values are paired in the http_status.csv lookup table with a status_type of Successful. This search returns: output={\"status_description\":[\"OK\",\"Created\",\"Accepted\",\"Non-Authoritative Information\",\"No Content\",\"Reset Content\",\"Partial Content\"]} 2. Example of a search with multiple input and match field pairs This search uses multiple input and match field pairs to show that an event with type=\"Successful\" and status=\"200\" matches a status_description of OK in the http_status.csv lookup table. This search returns: output={\"status_description\":\"OK\"} 3. Get counts of HTTP status description and type pairs This example matches values of a status field in a http_status.csv lookup file with values of status fields in the returned events. It then generates JSON objects as values of a status_details field, with the corresponding status_description and status_type field-value pairs, and adds them to the events. Finally, it provides counts of the JSON objects, broken out by object. Here is an example of a JSON object returned by this search: status_details=JSON:{\"status_description\":\"Created\",\"status_type\":\"Successful\"} 4. Get counts of the HTTP status description values that have been applied to your events by a HTTP status eval lookup This example shows how you can nest a lookup function inside another eval function. In this case it is the json_extract JSON function. This extracts status_description field-value pairs from the json_array objects and applies them to corresponding events. The search then returns a count of events with status_description fields, broken out by status_description value. Here is an example of an extracted status_description value returned by this search. Compare it to the result returned by the third example: status_details=Created", "code_examples": [{"language": "spl", "code": "... |eval<string>=lookup(\"<lookup_table>\", json_object(\"<input_field1>\", <match_field1>,\"<input_field2>\", <match_field2>), json_array(\"<output_field1>\",\"<output_field2>\",\"<output_field3>\")"}, {"language": "spl", "code": "| makeresults \n|evaltype=\"Successful\"|evaloutput=lookup(\"http_status.csv\", json_object(\"status_type\",type), json_array(\"status_description\"))"}, {"language": "spl", "code": "| makeresults \n|evaltype=\"Successful\", status=\"200\"|evaloutput=lookup(\"http_status.csv\", json_object(\"status_type\",type,\"status\", status), json_array(\"status_description\"))"}, {"language": "spl", "code": "index=_internal \n|evaloutput=lookup(\"http_status.csv\", json_object(\"status\", status), json_array(\"status_description\",\"status_type\")), status_details=\"JSON:\".output \n| stats count by status_details"}, {"language": "spl", "code": "index=_internal \n|evalstatus_details=json_extract(lookup(\"http_status.csv\", json_object(\"status\", status), json_array(\"status_description\")),\"status_description\") \n| stats count by status_details"}], "tables": [], "chunk_index": 7, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "lookup(<lookup_table>,<json_object>,<json_array>)", "section_id": "id_0f3c8829_2aa8_43fc_a652_eee4430fd809--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033775+00:00", "version": "10.2"}}
{"id": "c264de25eba98af7", "content": "Description This function returns TRUE if the regular expression <regex> finds a match against any substring of the string value <str>. Otherwise returns FALSE. Usage The match function is regular expression based. For example use the backslash ( \\ ) character to escape a special character, such as a quotation mark. Use the pipe ( | ) character to specify an OR condition. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example returns TRUE if, and only if, field matches the basic pattern of an IP address. This examples uses the caret ( ^ ) character and the dollar ( $ ) symbol to perform a full match. The following example uses the match function in an <eval-expression>. The <str> is a calculated field called test. The <regex> is the string yes. If the value is stored with quotation marks, you must use the backslash ( \\ ) character to escape the embedded quotation marks. For example:", "code_examples": [{"language": "spl", "code": "... |evaln=if(match(field,\"^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\"), 1, 0)"}, {"language": "spl", "code": "...  |evalmatches =if(match(test,\"yes\"), 1, 0)"}, {"language": "spl", "code": "| makeresults |evaltest=\"\\\"yes\\\"\"|evalmatches =if(match(test,\"\\\"yes\\\"\"), 1, 0)"}], "tables": [], "chunk_index": 8, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "match(<str>, <regex>)", "section_id": "b41cc9a8_fa15_4423_b7d1_a060659aca0b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033781+00:00", "version": "10.2"}}
{"id": "64a1f66303e8569a", "content": "Description This function takes no arguments and returns NULL. The evaluation engine uses NULL to represent \"no value\". Setting a field value to NULL clears the field value. Usage NULL values are field values that are missing in a some results but present in another results. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples Suppose you want to calculate the average of the values in a field, but several of the values are zero. If the zeros are placeholders for no value, the zeros will interfere with creating an accurate average. You can use the null function to remove the zeros. See also You can use the fillnull command to replace NULL values with a specified value. You can use the nullif(X,Y) function to compare two fields and return NULL if X = Y.", "code_examples": [], "tables": [], "chunk_index": 9, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "null()", "section_id": "id_84479397_fcaa_47bb_8e40_c6115e7b272f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033786+00:00", "version": "10.2"}}
{"id": "cad7466edc222ede", "content": "Description This function compares the values in two fields and returns NULL if the value in <field1> is equal to the value in <field2>. Otherwise the function returns the value in <field1>. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example Using the makeresults command, the following search creates a field called names. Another field called ponies is created based on the names field. The if function is used to change the name buttercup to mistmane in the ponies field. The results look like this: Using the nullif function, you can compare the values in the names and ponies fields. If the values are different, the value from the first field specified are displayed in the compare field. If the values are the same, no value is returned. The results look like this:", "code_examples": [{"language": "spl", "code": "| makeresults\n|evalnames=\"buttercup rarity tenderhoof dash\"| makemv delim=\" \"names\n| mvexpand names\n|evalponies =if(names=\"buttercup\",\"mistmane\", names)"}, {"language": "spl", "code": "...evalcompare = nullif(names, ponies)"}], "tables": [{"headers": ["_time", "names", "ponies"], "rows": [["2022-10-17 14:57:12", "buttercup", "mistmane"], ["2022-10-17 14:57:12", "rarity", "rarity"], ["2022-10-17 14:57:12", "tenderhoof", "tenderhoof"], ["2022-10-17 14:57:12", "dash", "dash"]]}, {"headers": ["_time", "compare", "names", "ponies"], "rows": [["2022-10-17 14:57:12", "buttercup", "buttercup", "mistmane"], ["2022-10-17 14:57:12", "", "rarity", "rarity"], ["2022-10-17 14:57:12", "", "tenderhoof", "tenderhoof"], ["2022-10-17 14:57:12", "", "dash", "dash"]]}], "chunk_index": 10, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "nullif(<field1>, <field2>)", "section_id": "d43c9b6c_066b_4903_a431_2d54542f751e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033800+00:00", "version": "10.2"}}
{"id": "07466778a9f2bf66", "content": "Description This function returns TRUE if the event matches the search string. Usage To use the searchmatch function with the eval command, you must use the searchmatch function inside the if function. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example uses the makeresults command to create some simple results. The searchmatch function is used to determine if any of the results match the search string \"x=hi y=*\". The result of the if function is yes ; the results match the search string specified with the searchmatch function.", "code_examples": [{"language": "spl", "code": "| makeresults 1 \n|eval_raw =\"x=hi y=bye\"|evalx=\"hi\"|evaly=\"bye\"|evaltest=if(searchmatch(\"x=hi y=*\"),\"yes\",\"no\") \n| table _rawtestx y"}], "tables": [], "chunk_index": 11, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "searchmatch(<search_str>)", "section_id": "id_4b651d80_64b9_416d_8c2d_1f73f48f29ef--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033805+00:00", "version": "10.2"}}
{"id": "333b3344a72faa45", "content": "Description Use this function to return TRUE. This function enables you to specify a condition that is obviously true, for example 1==1. You do not specify a field with this function. Usage This function is often used as an argument with other functions. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example shows how to use the true() function to provide a default value to the case function. If the values in the status field are not 200, or 404, the value used is Other. The results appear on the Statistics tab and look like this:", "code_examples": [{"language": "spl", "code": "sourcetype=access_* |evaldescription=case(status==200,\"OK\", status==404,\"Not found\",true(),\"Other\") | table status description"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial, but should work with any format of Apache Web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use theYesterdaytime range when you run the search."]]}, {"headers": ["status", "description"], "rows": [["200", "OK"], ["200", "OK"], ["408", "Other"], ["200", "OK"], ["404", "Not found"], ["200", "OK"], ["200", "OK"], ["406", "Other"], ["200", "OK"]]}], "chunk_index": 12, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "true()", "section_id": "d2244121_2330_4b87_b376_fb2c8cf98650--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033819+00:00", "version": "10.2"}}
{"id": "054e186da4e95144", "content": "Description This function takes a list of conditions and values and returns the value that corresponds to the condition that evaluates to FALSE. This function defaults to NULL if all conditions evaluate to TRUE. This function is the opposite of the case function. Usage The <condition> arguments must be expressions. The <value> arguments must be strings. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example runs a simple check for valid ports.", "code_examples": [{"language": "spl", "code": "... |evaln=validate(isint(port),\"ERROR: Port is not an integer\", port >= 1 AND port <= 65535,\"ERROR: Port is out of range\")"}], "tables": [], "chunk_index": 13, "total_chunks": 14, "metadata": {"title": "Comparison and Conditional functions", "section_heading": "validate(<condition>, <value>,...)", "section_id": "fe28f901_b10b_4393_9770_606df4e94aab--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/comparison-and-conditional-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:02.033825+00:00", "version": "10.2"}}
{"id": "5a02c830b20f43c1", "content": "The eval command calculates an expression and puts the resulting value into a search results field. If the field name that you specify does not match a field in the output, a new field is added to the search results. If the field name that you specify matches a field name that already exists in the search results, the results of the eval expression overwrite the values in that field. The eval command evaluates mathematical, string, and boolean expressions. You can chain multiple eval expressions in one search using a comma to separate subsequent expressions. The search processes multiple eval expressions left-to-right and lets you reference previously evaluated fields in subsequent expressions. Difference between eval and stats commands The stats command calculates statistics based on fields in your events. The eval command creates new fields in your events by using existing fields and an arbitrary expression.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "eval", "section_heading": "Description", "section_id": "id_03e1c95e_ddb8_4d87_961b_6a2fefb804a4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eval", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:19.875355+00:00", "version": "10.2"}}
{"id": "f8e611197939eb63", "content": "eval <field>=<expression>[\",\" <field>=<expression>]... Required arguments field Syntax: <string> Description: A destination field name for the resulting calculated value. If the field name already exists in your events, eval overwrites the value. expression Syntax: <string> Description: A combination of values, variables, operators, and functions that will be executed to determine the value to place in your destination field. The eval expression is case-sensitive. The syntax of the eval expression is checked before running the search, and an exception is thrown for an invalid expression. â€¢ The result of an eval expression cannot be a Boolean. â€¢ If the expression references a field name that contains non-alphanumeric characters, other than the underscore ( _ ) character, the field name needs to be surrounded by single quotation marks. For example, if the field name is server-1 you specify the field name like this new=count+'server-1'. â€¢ If the expression references a literal string , that string needs to be surrounded by double quotation marks. For example, if the string you want to use is server- you specify the string like this new=\"server-\".host .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "eval", "section_heading": "Syntax", "section_id": "de503c78_444f_465e_8319_c45d58b91a08--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eval", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:19.875364+00:00", "version": "10.2"}}
{"id": "152a321dfb0eea06", "content": "The eval command is a distributable streaming command. See Command types. General You must specify a field name for the results that are returned from your eval command expression. You can specify a name for a new field or for an existing field. CAUTION: If the field name that you specify matches an existing field name, the values in the existing field are replaced by the results of the eval expression. Numbers and strings can be assigned to fields, while booleans cannot be assigned. However you can convert booleans and nulls to strings using the tostring() function, which can be assigned to fields. If you are using a search as an argument to the eval command and functions, you cannot use a saved search name; you must pass a literal search string or a field that contains a literal search string (like the 'search' field extracted from index=_audit events). Numeric calculations During calculations, numbers are treated as double-precision floating-point numbers, subject to all the usual behaviors of floating point numbers. If the calculation results in the floating-point special value NaN (Not a Number), it is represented as \"nan\" in your results. The special values for positive and negative infinity are represented in your results as \"inf\" and \"-inf\" respectively. Division by zero results in a null field. See the isnum and the isstr functions in Informational functions. Rounding Results are rounded to a precision appropriate to the precision of the input results. The precision of the results can be no greater than the precision of the least-precise input. For example, the following search has different precision for 0.2 in each of the calculations based on the number of zeros following the number 2: The results look like this: If you want to return an arbitrary number of digits of precision, use the exact function, as shown in the last calculation in the search. See the exact evaluation function. Long numbers There are situations where the results of a calculation contain more digits than can be represented by a floating- point number. In those situations precision might be lost on the least significant digits. The limit to precision is 17 significant digits, or -2 53 +1 to 2 53 -1. Significant digits If a result returns a long number with more digits than you want to use, you can specify the number of digits to return using the sigfig function. See Example 2 under the basic examples for the sigfig(X) function. Supported functions You can use a wide range of functions with the eval command. For general information about using functions, see Evaluation functions. For a list of functions by category, see Function list by category. For an alphabetical list of functions, see Alphabetical list of functions. Operators The following table lists the basic operations you can perform with the eval command. For these evaluations to work, the values need to be valid for the type of operation. For example, with the exception of addition, arithmetic operations might not produce valid results if the values are not numerical. When concatenating values, Splunk software reads the values as strings, regardless of the value. Operators that produce numbers The plus ( + ) operator accepts two numbers for addition, or two strings for concatenation. The subtraction ( - ), multiplication ( * ), division ( / ), and modulus ( % ) operators accept two numbers. Operators that produce strings The period (. ) operator concatenates both strings and number. Numbers are concatenated in their string represented form. Operators that produce booleans The AND, OR, and XOR operators accept two Boolean values. The < , > , <= , >= , != , = , and == operators accept two numbers or two strings. In expressions, the single equal sign ( = ) is a synonym for the double equal sign ( == ). The LIKE operator accepts two strings. This is a pattern match similar to what is used in SQL. For example string LIKE pattern. The pattern operator supports literal text, a percent ( % ) character for a wildcard, and an underscore ( _ ) character for a single character match. For example, field LIKE \"a%b_\" matches any string starting with a , followed by anything, followed by b , followed by one character. The = and == operators In expressions, the single equal sign ( = ) and the double equal sign ( == ) are synonymous. Although you can use these operators interchangeably in your searches to make assignments or comparisons, keep the following conventions in mind when you create your searches: The = operator means either \"is equal to\" or \"is assigned to\", depending on the context. This operator is typically used to assign a value to a field. The == operator means \"is equal to\". This operator is typically used to compare 2 values. For example, in the following search, = is used to assign the description to the case expression, and == is used to indicate status is equal to specific error codes. Boolean expressions The order in which Boolean expressions are evaluated with the eval command is: Expressions within parentheses NOT clauses AND clauses OR clauses XOR clauses This evaluation order is different than the order used with the search command, which evaluates OR before AND clauses, and doesn't support XOR. See Boolean expressions with logical operators in the Splunk platform Search Manual. Field names To specify a field name with multiple words, you can either concatenate the words, or use single quotation marks when you specify the name. For example, to specify the field name Account ID you can specify AccountID or 'Account ID'. To specify a field name with special characters, such as a period, use single quotation marks. For example, to specify the field name Last.Name use 'Last.Name'. When assigning the value of a field to the value of another field, do not use leading spaces in the field name. However, you can use spaces at the end of the field name. For example, a search like this that assigns the same value to fields called first and second produces valid results, even though first has a trailing space: However, the following search does not produce valid results because first has a leading space: Dynamic field name creation You can use the value of one field as the name of another field by using curly braces ( { } ), which dynamically creates a field name on the left side of an eval expression. For example, if you have an event with the aName=counter and aValue=1234 fields, use | eval {aName}=aValue to return counter=1234. Searches that dynamically create field names generate errors and do not complete if there are unbounded recursive replacements. For example, the following search doesn't produce results because the right side of the eval expression generates bracketed field names that are recursive. In this case, {p} replaces the value of p in the fieldname, so v_{p} becomes v_{p} over and over again, in a recursive loop. The following search produces valid results because the value of one field is not recursively used as the name of another field. The search results look something like this: Calculated fields You can use eval statements to define calculated fields by defining the eval statement in props.conf. If you are using Splunk Cloud Platform, you can define calculated fields using Splunk Web, by choosing Settings > Fields > Calculated Fields. When you run a search, Splunk software evaluates the statements and creates fields in a manner similar to that of search time field extraction. Setting up calculated fields means that you no longer need to define the eval statement in a search string. Instead, you can search on the resulting calculated field directly. You can use calculated fields to move your commonly used eval statements out of your search string and into props.conf , where they will be processed behind the scenes at search time. With calculated fields, you can change the search from: to this search: In this example, the three eval statements that were in the search--that defined the accountname , from_user , and from_domain fields--are now computed behind the scenes when the search is run for any event that contains the extracted field mailfrom field. You can also search on those fields independently once they're set up as calculated fields in props.conf. You could search on from_domain=email.com , for example. For more information about calculated fields, see About calculated fields in the Knowledge Manager Manual. Search event tokens If you are using the eval command in search event tokens, some of the evaluation functions might be unavailable or have a different behavior. See Custom logic for search tokens in Dashboards and Visualizations for information about the evaluation functions that you can use with search event tokens.", "code_examples": [{"language": "spl", "code": "|makeresults \n|evaldecimal1=8.250 * 0.2, decimal2=8.250 * 0.20, decimal3=8.250 * 0.200, \n  exact=8.250 * exact(0.2)"}, {"language": "spl", "code": "|evaldescription=case(status==200,\"OK\", status==404,\"Not found\", status==500,\"Internal Server Error\")"}, {"language": "spl", "code": "| makeresults \n|eval\"first  \"= 123 |evalsecond='first  '"}, {"language": "spl", "code": "| makeresults \n|eval\" first\"= 123 |evalsecond=' first'"}, {"language": "spl", "code": "| makeresults\n|evalp=\"{p}\", v_{p} = p"}, {"language": "spl", "code": "| makeresults \n|evalfield1=\"counter\", {field1}=\"1234\""}, {"language": "spl", "code": "sourcetype=\"cisco_esa\"mailfrom=* |evalaccountname=split(mailfrom,\"@\"), from_user=mvindex(accountname,0), from_domain=mvindex(accountname,-1) | table mailfrom, from_user, from_domain"}, {"language": "spl", "code": "sourcetype=\"cisco_esa\"mailfrom=* | table mailfrom, from_user, from_domain"}], "tables": [{"headers": ["_time", "decimal1", "decimal2", "decimal3", "exact"], "rows": [["2022-09-02 21:53:30", "2", "1.7", "1.65", "1.650"]]}, {"headers": ["Type", "Operators"], "rows": [["Arithmetic", "+ - * /Â %"], ["Concatenation", "."], ["Boolean", "AND OR NOT XOR < > <= >=Â != = == LIKE"]]}, {"headers": ["_time", "counter", "field1"], "rows": [["2023-05-10 21:15:49", "1234", "counter"]]}], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "eval", "section_heading": "Usage", "section_id": "id_13ff3aa2_5902_4937_989f_2e46ff409422--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eval", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:19.875384+00:00", "version": "10.2"}}
{"id": "777368c47c665238", "content": "1. Create a new field that contains the result of a calculation Create a new field called velocity in each event. Calculate the velocity by dividing the values in the distance field by the values in the time field. 2. Use the if function to analyze field values Create a field called error in each event. Using the if function, set the value in the error field to OK if the status value is 200. Otherwise set the error field value to Problem. 3. Convert values to lowercase Create a new field in each event called low-user. Using the lower function, populate the field with the lowercase version of the values in the username field. 4. Use the value of one field as the name for a new field In this example, use each value of the field counter to make a new field name. Assign to the new field the value of the Value field. See Field names under the Usage section. 5. Set sum_of_areas to be the sum of the areas of two circles 6. Set status to some simple http error codes 7. Concatenate values from two fields Use the period (. ) character to concatenate the values in first_name field with the values in the last_name field. Quotation marks are used to insert a space character between the two names. When concatenating, the values are read as strings, regardless of the actual value. 8. Separate multiple eval operations with a comma You can specify multiple eval operations by using a comma to separate the operations. In the following search the full_name evaluation uses the period (. ) character to concatenate the values in the first_name field with the values in the last_name field. The low_name evaluation uses the lower function to convert the full_name evaluation into lowercase. 9. Convert a numeric field value to a string with commas and 2 decimals If the original value of x is 1000000.1278, the following search returns x as 1,000,000.13. The tostring function returns only two decimal places with the decimals rounded up or down depending on the values. To include a currency symbol at the beginning of the string: This returns x as $1,000,000.13 10. Rounding with values outside of the range of supported values The range of values supported in Splunk searches is 0 to 2 53 -1. This example demonstrates the differences in results you get when you use values in eval expressions that fall outside of the range of supported values. The results look like this: As you can see, because val1 and val2 are beyond the range of supported values, the results are the same even though they should be different. This is because of rounding on those values that are outside of the supported range of values.", "code_examples": [{"language": "spl", "code": "... |evalvelocity=distance/time"}, {"language": "spl", "code": "... |evalerror =if(status == 200,\"OK\",\"Problem\")"}, {"language": "spl", "code": "... |evallow-user = lower(username)"}, {"language": "spl", "code": "index=perfmon sourcetype=Perfmon* counter=* Value=* |eval{counter} = Value"}, {"language": "spl", "code": "... |evalsum_of_areas = pi() * pow(radius_a, 2) + pi() * pow(radius_b, 2)"}, {"language": "spl", "code": "... |evalerror_msg =case(error == 404,\"Not found\", error == 500,\"Internal Server Error\", error == 200,\"OK\")"}, {"language": "spl", "code": "... |evalfull_name = first_name.\" \".last_name"}, {"language": "spl", "code": "... |evalfull_name = first_name.\" \".last_name, low_name = lower(full_name)"}, {"language": "spl", "code": "... |evalx=tostring(x,\"commas\")"}, {"language": "spl", "code": "... |evalx=\"$\".tostring(x,\"commas\")"}, {"language": "spl", "code": "| makeresults |evalmax_supported_val = pow(2, 53)-1, val1 = max_supported_val + 1, val2 = max_supported_val + 2"}], "tables": [{"headers": ["_time", "max_supported_val", "val1", "val2"], "rows": [["2022-09-04  10:22:11", "9007199254740991", "9007199254740992", "9007199254740992"]]}], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "eval", "section_heading": "Basic Examples", "section_id": "id_56693988_c128_41cc_9732_8dbbacb3c627--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eval", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:19.875398+00:00", "version": "10.2"}}
{"id": "7ee2e5273f4b0681", "content": "1. Coalesce a field from two different source types, create a transaction of events This example shows how you might coalesce a field from two different source types and use that to create a transaction of events. sourcetype=A has a field called number , and sourcetype=B has the same information in a field called subscriberNumber. The eval command is used to add a common field, called phone , to each of the events whether they are from sourcetype=A or sourcetype=B. The value of phone is defined, using the coalesce() function, as the values of number and subscriberNumber. The coalesce() function takes the value of the first non-NULL field (that means, it exists in the event). Now, you're able to group events from either source type A or B if they share the same phone value. 2. Separate events into categories, count and display minimum and maximum values Earthquakes occurring at a depth of less than 70 km are classified as shallow-focus earthquakes, while those with a focal-depth between 70 and 300 km are commonly termed mid-focus earthquakes. In subduction zones, deep-focus earthquakes may occur at much greater depths (ranging from 300 up to 700 kilometers). To classify recent earthquakes based on their depth, you use the following search. The eval command is used to create a field called Description , which takes the value of \"Shallow\", \"Mid\", or \"Deep\" based on the Depth of the earthquake. The case() function is used to specify which ranges of the depth fits each description. For example, if the depth is less than 70 km, the earthquake is characterized as a shallow-focus quake; and the resulting Description is Shallow. The search also pipes the results of the eval command into the stats command to count the number of earthquakes and display the minimum and maximum magnitudes for each Description. The results appear on the Statistics tab and look something like this: 3. Find IP addresses and categorize by network using eval functions cidrmatch and if In this search, you're finding IP addresses and classifying the network they belong to. This example uses the cidrmatch() function to compare the IP addresses in the clientip field to a subnet range. The search also uses the if() function, which says that if the value of clientip falls in the subnet range, then the network field value is local. Otherwise, network=other. The eval command does not do any special formatting to your results. The command creates a new field based on the eval expression you specify. In the fields sidebar, click on the network field. In the popup, next to Selected click Yes and close the popup. Now you can see, inline with your search results, which IP addresses are part of your local network and which are not. Your events list looks something like this: Another option for formatting your results is to pipe the results of eval to the table command to display only the fields of interest to you. Note: This example just illustrates how to use the cidrmatch function. If you want to classify your events and quickly search for those events, the better approach is to use event types. Read more about event types in the Knowledge manager manual. 4. Extract information from an event into a separate field, create a multivalue field Use the email address field to extract the name and domain. The eval command in this search contains multiple expressions, separated by commas. The split() function is used to break the mailfrom field into a multivalue field called accountname. The first value of accountname is everything before the \"@\" symbol, and the second value is everything after. The mvindex() function is used to set from_user to the first value in accountname and to set from_domain to the second value in accountname. The results of the eval expressions are then piped into the table command. You can see the the original mailfrom values and the new from_user and from_domain values in the results table. The results appear on the Statistics tab and look something like this: Note: This example was written to demonstrate how to use an eval function to identify the individual values of a multivalue fields. Because this particular set of email data did not have any multivalue fields, the example creates a multivalue filed, accountname , from a single value field, mailfrom. 5. Categorize events using the match function This example classifies where an email came from based on the email address domain. The .com, .net, and .org addresses are considered local , while anything else is considered abroad. There are many domain names. Of course, domains that are not .com, .net, or .org are not necessarily from abroad. This is just an example. The eval command in this search contains multiple expressions, separated by commas. The first half of this search is similar to previous example. The split() function is used to break up the email address in the mailfrom field. The mvindex function defines the from_domain as the portion of the mailfrom field after the @ symbol. Then, the if() and match() functions are used. If the from_domain value ends with a .com, .net., or .org , the location field is assigned the value local. If from_domain does not match, location is assigned the value abroad. The eval results are then piped into the stats command to count the number of results for each location value. The results appear on the Statistics tab and look something like this: Note: This example merely illustrates using the match() function. If you want to classify your events and quickly search for those events, the better approach is to use event types. Read more about event types in the Knowledge manager manual. 6. Convert the duration of transactions into more readable string formats When you use the transaction command, as shown in the following search, it calculates the length of time for the transaction. A new field, called duration , is automatically added to the results. The duration is the time between the first and last events in the transaction. In the Interesting fields list, click on the duration field to see the top 10 values for duration. The values are displayed in seconds. Click Yes to add the field to the Selected fields list. You can use the eval command to reformat a numeric field into a more readable string format. The following search uses the tostring() function with the \"duration\" option to convert the values in the duration field into a string formatted as HH:MM:SS. The search defines a new field, durationstr , for the reformatted duration values. In the Interesting fields list, click on the durationstr field and select Yes to add the field to the Selected fields list. The values for the fields now appear in the set of fields below each transaction. The following image shows how your search results should look:", "code_examples": [{"language": "spl", "code": "sourcetype=A OR sourcetype=B |evalphone=coalesce(number,subscriberNumber) | transaction phone maxspan=2m"}, {"language": "spl", "code": "source=all_month.csv |evalDescription=case(depth<=70,\"Shallow\", depth>70 AND depth<=300,\"Mid\", depth>300,\"Deep\") | stats count min(mag) max(mag) by Description"}, {"language": "spl", "code": "sourcetype=access_* |evalnetwork=if(cidrmatch(\"182.236.164.11/16\", clientip),\"local\",\"other\")"}, {"language": "spl", "code": "sourcetype=\"cisco:esa\"mailfrom=* |evalaccountname=split(mailfrom,\"@\"), from_user=mvindex(accountname,0), from_domain=mvindex(accountname,-1) | table mailfrom, from_user, from_domain"}, {"language": "spl", "code": "sourcetype=\"cisco:esa\"mailfrom=*|evalaccountname=split(mailfrom,\"@\"),  from_domain=mvindex(accountname,-1), location=if(match(from_domain,\"[^\\n\\r\\s]+\\.(com|net|org)\"),\"local\",\"abroad\") | stats count BY location"}, {"language": "spl", "code": "sourcetype=access_* | transaction clientip maxspan=10m"}, {"language": "spl", "code": "sourcetype=access_* | transaction clientip maxspan=10m |evaldurationstr=tostring(duration,\"duration\")"}], "tables": [{"headers": [], "rows": [["This example uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), and so forth, for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance if you want follow along with this example."]]}, {"headers": ["Description", "count", "min(Mag)", "max(Mag)"], "rows": [["Deep", "35", "4.1", "6.7"], ["Mid", "635", "0.8", "6.3"], ["Shallow", "6236", "-0.60", "7.70"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": [], "rows": [["This example uses sample email data. You should be able to run this search on any email data by replacing thesourcetype=cisco:esawith thesourcetypevalue and themailfromfield with email address field name in your data. For example, the email might beTo,From, orCc)."]]}, {"headers": ["mailfrom", "from_user", "from_domain"], "rows": [["na.lui@sample.net", "na.lui", "sample.net"], ["MAILER-DAEMON@hcp2mailsec.sample.net", "MAILER-DAEMON", "hcp2mailsec.sample.net"], ["M&MService@example.com", "M&MService", "example.com"], ["AlexMartin@oursample.de", "AlexMartin", "oursample.de"], ["Exit_Desk@sample.net", "Exit_Desk", "sample.net"], ["buttercup-forum+SEMAG8PUC4RETTUB@groups.com", "buttercup-forum+SEMAG8PUC4RETTUB", "groups.com"], ["eduardo.rodriguez@sample.net", "eduardo.rodriguez", "sample.net"], ["VC00110489@techexamples.com", "VC00110489", "techexamples.com"]]}, {"headers": [], "rows": [["This example uses sample email data. You should be able to run this search on any email data by replacing thesourcetype=cisco:esawith thesourcetypevalue and themailfromfield with email address field name in your data. For example, the email might beTo,From, orCc)."]]}, {"headers": ["location", "count"], "rows": [["abroad", "3543"], ["local", "14136"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "eval", "section_heading": "Extended Examples", "section_id": "e15a696a_fddb_46d6_aa6b_55304acf2ce2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eval", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:19.875428+00:00", "version": "10.2"}}
{"id": "8a9e72a8ce4b12d0", "content": "Functions Evaluation functions Commands where", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "eval", "section_heading": "See also", "section_id": "ee34c060_e642_42e8_a166_6786c49d9732--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eval", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:19.875433+00:00", "version": "10.2"}}
{"id": "07971bc2c1ec49d3", "content": "Description This function takes two or more nonnegative integers as arguments and sequentially performs logical bitwise AND operations on them. Each argument must be in the range of 0 to 2 53 -1. Usage This function takes an arbitrary number of comma-separated arguments and returns the result of a logical AND operation on each pair of corresponding bits. For example, the result of the following search is 8. This is because the result of the bitwise AND operation is 1000, which is the number 8. See the following table for more information about the sequence of operations in this bitwise function.", "code_examples": [{"language": "spl", "code": "| makeresults |evalresult = bit_and(12, 9)"}], "tables": [{"headers": ["Operation", "Binary string", "Result"], "rows": [["12", "1100", ""], ["9", "1001", ""], ["bit_and(12, 9)", "1000", "8"]]}], "chunk_index": 0, "total_chunks": 7, "metadata": {"title": "Bitwise functions", "section_heading": "bit_and(<values>)", "section_id": "da7b35be_353d_4faf_b24c_b8ffb716cb37--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/bitwise-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:34.359833+00:00", "version": "10.2"}}
{"id": "be33a35d92bf0235", "content": "Description This function takes two or more nonnegative integers as arguments and sequentially performs bitwise OR operations on them. Each argument must be in the range of 0 to 2 53 -1. Usage This function takes an arbitrary number of comma-separated arguments and returns the result of a logical OR operation on each pair of corresponding bits. For example, the result of the following search is 6. This is because the result of the bitwise OR operation is 0110, which is the number 6. See the following table for more information about the sequence of operations in this bitwise function.", "code_examples": [{"language": "spl", "code": "| makeresults |evalresult = bit_or(4, 2)"}], "tables": [{"headers": ["Operation", "Binary string", "Result"], "rows": [["4", "0100", ""], ["2", "0010", ""], ["bit_or(4, 2)", "0110", "6"]]}], "chunk_index": 1, "total_chunks": 7, "metadata": {"title": "Bitwise functions", "section_heading": "bit_or(<values>)", "section_id": "id_7d84160f_a846_4266_8823_f9e4d47be26c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/bitwise-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:34.359844+00:00", "version": "10.2"}}
{"id": "02ef066422da5373", "content": "Description This function takes a nonnegative integer as an argument and inverts every bit in the binary representation of that number. This function also takes an optional second argument with a default value of 2 53 -1 that acts as a bitmask that is used in an AND operation with the result of the first operation. You can think of the bitmask as the value up to which you want to print the result. Both arguments must be in the range of 0 to 2 53 -1 or the operation returns null. Usage You might want to use an optional integer bitmask as the second argument in a bit_not function to limit the number of leading 1s in the result. The bitmask truncates the result by performing a silent bitwise AND on the result of a bitwise NOT operation. The number of bits specified as the bitmask indicates the number of bits you want to see in the results. For example, if the bitmask argument is \"1111\" , the first four bits of the binary results are displayed. Consider the following search, which specifies a four-bit bitmask: The results look something like this: The short_binary result is 110 instead of a long string of bits like the long_binary results. You might wonder why the result is only three bits, and not four bits, since you specified \"1111\" in the bitmask. The binary result is actually 0110 , but the leading 0 is not displayed. You can take a closer look at what is happening in the sequence of bitwise operations using the following table as a guide. First, the bitmask is set to the bitwise NOT of 0, by default. Then, the result of bit_not(9) is \"bitwise ANDed\" with 15, which is the bitmask resulting from tonumber(\"1111\", 2). The result of bit_not(9,15) is 110 in binary and 6 in decimal.", "code_examples": [{"language": "spl", "code": "| makeresults |evallong_result = bit_not(9), short_result = bit_not(9, tonumber(\"1111\", 2)) |evallong_binary = tostring(long_result,\"binary\"), short_binary = tostring(short_result,\"binary\")"}], "tables": [{"headers": ["_time", "long_binary", "long_result", "short_binary", "short_result"], "rows": [["2022-09-05  11:12:53", "11111111111111111111111111111111111111111111111110110", "9007199254740982", "110", "6"]]}, {"headers": ["Operation", "Binary string", "Result"], "rows": [["0", "00000 00000000 00000000 00000000 00000000 00000000 00000000", ""], ["bit_not(0)", "11111 11111111 11111111 11111111 11111111 11111111 11111111", ""], ["9", "00000 00000000 00000000 00000000 00000000 00000000 00001001", ""], ["bit_not(9)", "11111 11111111 11111111 11111111 11111111 11111111 11110110", ""], ["15", "00000 00000000 00000000 00000000 00000000 00000000 00001111", ""], ["bit_not(9, 15)", "00000 00000000 00000000 00000000 00000000 00000000 00000110", "110 in binary6 in decimal"]]}], "chunk_index": 2, "total_chunks": 7, "metadata": {"title": "Bitwise functions", "section_heading": "bit_not(<value>, <bitmask>)", "section_id": "ffa0a3d7_1170_446a_b81e_f35de4535ba6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/bitwise-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:34.359855+00:00", "version": "10.2"}}
{"id": "daa1587c9ba10f70", "content": "Description This function takes two or more nonnegative integers as arguments and sequentially performs bitwise XOR operations on each of the given arguments. Each argument must be in the range of 0 to 2 53 -1. Usage This function takes an arbitrary number of comma-separated arguments and returns the result of a logical XOR operation on each pair of corresponding bits. For example, the result of the following function is 1. This is because the result of the bitwise XOR operation is 0001, which is the number 1. See the following table for more information about the sequence of operations in this bitwise function.", "code_examples": [{"language": "spl", "code": "| makeresults |evalresult = bit_xor(3, 2)"}], "tables": [{"headers": ["Operation", "Binary string", "Result"], "rows": [["3", "0011", ""], ["2", "0010", ""], ["bit_xor(3, 2)", "0001", "1"]]}], "chunk_index": 3, "total_chunks": 7, "metadata": {"title": "Bitwise functions", "section_heading": "bit_xor(<values>)", "section_id": "c3e6a6d5_fc1a_43ec_95f2_1f0802e2434b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/bitwise-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:34.359864+00:00", "version": "10.2"}}
{"id": "03b6081e790d3d8e", "content": "Description This logical left shift function takes two valid nonnegative integers as arguments and shifts the binary representation of the first integer over to the left by the specified shift offset amount. Shifting left drops the 53rd bit and appends a 0 to the binary representation of the input. Both arguments must be in the range of 0 to 2 53 -1 or the operation returns null. All results are masked to stay below the 2 53 -1 limit in case of overflows. Usage The shift offset is an integer that specifies the number of times the given integer is shifted to the left. When the bits in a binary digit are shifted to the left, the most-significant bit on the left side is lost and a 0 bit is inserted on the right side of the value. For example, the result of the following search is 4. This is because the decimal value of 0100 is 4. See the following table for more information about the sequence of operations in this bitwise function. Because only nonnegative integers in the range of 0 to 2 53 -1 are supported, if values in bit-shift functions are negative or greater than 2 53 -1, such as 2 53 , the function returns null. Also, if the shift offset is greater than 53 bits, the function returns 0. For example, consider the following search. The search results look something like this: Since the operation results in 54 bits, the 1 on the left drops off and is replaced with 53 zeros, and the value is truncated to 0 when displayed as a search result. After the most significant bit is lost, it is not possible to reverse the operation and recover that bit. The following table provides examples of left-shift functions to help you understand the results. For the bit_shift_left(3, 52) function, notice that the result is truncated because the bit on the left that is in bold in the binary string is dropped.", "code_examples": [{"language": "spl", "code": "| makeresults |evalresult = bit_shift_left(2, 1)"}, {"language": "spl", "code": "| makeresults |evalresult = bit_shift_left(1, 53)"}], "tables": [{"headers": ["Operation", "Binary string", "Result"], "rows": [["2", "0010", ""], ["bit_shift_left(2, 1)", "0100", "4"]]}, {"headers": ["_time", "result"], "rows": [["2022-08-22  13:39:24", "0"]]}, {"headers": ["Operation", "Binary string", "Result"], "rows": [["3", "00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000011", ""], ["bit_shift_left(3, 1)", "00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000110", "6"], ["bit_shift_left(3, 52)", "110000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000", "4503599627370496"], ["bit_shift_left(3, 57)", "00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000", "0"], ["bit_shift_left(3, 253-1)", "00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000", "0"], ["bit_shift_left(3, 253)", "", "null"], ["67", "00000000 00000000 00000000 00000000 00000000 00000000 00000000 01000011", ""], ["bit_shift_left(67, 1)", "00000000 00000000 00000000 00000000 00000000 00000000 00000000 10000110", "134"], ["bit_shift_left(253, 1)", "", "null"], ["bit_shift_left(3, -2)", "", "null"], ["bit_shift_right(-3, 1)", "", "null"]]}], "chunk_index": 4, "total_chunks": 7, "metadata": {"title": "Bitwise functions", "section_heading": "bit_shift_left(<value>, <shift_offset>)", "section_id": "id_5a01407e_f7e7_497d_87a0_a88ebec05802--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/bitwise-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:34.359881+00:00", "version": "10.2"}}
{"id": "280e35bc5ac7046f", "content": "Description This logical right shift function takes two valid nonnegative integers as arguments and shifts the binary representation of the first integer over to the right by the specified shift offset amount. Shifting right drops the rightmost bit and prepends a 0 to the binary representation of the input. Both arguments must be in the range of 0 to 2 53 -1 or the operation returns null. All results are masked to stay below the 2 53 -1 limit in case of overflows. Usage The shift offset is an integer that specifies the number of times the given integer is shifted to the right. For example, the result of the following function is 2. This is because the binary representation of the decimal number 4 is 0100, which is 0010 when shifted left by 1. The decimal value of 0010 is 2. Like the bit_shift_left function, the bit_shift_right function supports only nonnegative integers in the range of 0 to 2 53 -1. As a result, values in bit-shift operations that are negative or greater than 53 bits or 2 53 return null.", "code_examples": [{"language": "spl", "code": "| makeresults |evalresult = bit_shift_right(4, 1)"}], "tables": [], "chunk_index": 5, "total_chunks": 7, "metadata": {"title": "Bitwise functions", "section_heading": "bit_shift_right(<value>, <shift_offset>)", "section_id": "id_0a389beb_4b2d_42bf_8d66_ae44c0269ee1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/bitwise-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:34.359887+00:00", "version": "10.2"}}
{"id": "f98fb856bf6f2b40", "content": "1. Compare the results of bitwise functions Consider the following search, which includes sequential bitwise AND, bitwise OR, and bitwise XOR functions. The results look something like this: 2. Compare the results of bit shift functions Consider the following search, which includes sequential logical left shift and logical right shift functions. The results look something like this: 3. Get the binary representation of a value from another function If you want to see your results in binary, you can use the tostring function to convert an argument to a binary string. See tostring(X,Y). The following search converts the result of the bit_shift_left(2,1) to its binary representation, which is 100. The results look something like this: 4. Apply a bitmask to limit the binary output Consider this search, which uses the bit_not function to invert the bits in the result. The results look something like this: The binary result in the string field has a lot of leading 1's that are in the way. To limit the output, add a bitmask that specifies that you want to see only three bits in the final result, like this: The results look something like this: The result is the three-bit binary string 110 instead of a lot of 1s and 0s. 5. Using values that are outside the range of supported values Look at what happens when you use a value as input in a function that falls outside of the range of supported values, which is 0 to 2 53 -1. In the following search, the input to the function is 2 53 -1, the max_supported_value , plus one. The results look something like this: The search returns only the value for max_supported_val and the value for the result field is not displayed. This is because result is outside of the supported range of values, so the function returns null. The function also returns null if you use negative values as input. 6. Update a binary string to set flags You can run the following search to set multiple flags using a variable called flags. The results look something like this: You can see that the first, fourth, and fifth bits in 11011 are set. 7. Check whether a specific flag in a binary string is set You can run the following search to check whether the third flag in a variable called flags is set. The results look something like this: You can see that the third bit in the flag 00010 is set to 0. 8. Determine the matching bits in two binary strings This search gives you the matching bits in two binary strings. Every bit that matches is displayed as 1 in the search results. The results look something like this: You can see that the matching bits in 10011 and 10001 are the first, third, fourth, and last bits. 9. Append a bit flag to a binary string The following search uses bit_shift_left and bit_or to add a bit flag of 1 to the binary string 10001. The results look something like this: You appended 10001 , so now the result is 100011. 10. Use nested bitwise operations You can nest multiple bitwise operations in a single search. For example, say you have a field called StreamId=0x12da3b7514f19ce7. If you want to perform StreamId >> 8 & 0xFFFFFFFF , which right-shifts the StreamID by 8 and then performs a bitwise AND operation with 0xFFFFFFFF, you can run the following search: The results look something like this:", "code_examples": [{"language": "spl", "code": "| makeresults \n|evalAND_result = bit_and(4, 6), OR_result = bit_or(4, 6), XOR_result = bit_xor(4, 6)"}, {"language": "spl", "code": "| makeresults \n|evalLEFT_result = bit_shift_left(2,1), RIGHT_result = bit_shift_right(2,1)"}, {"language": "spl", "code": "| makeresults \n|evalLEFT_result = bit_shift_left(2,1) \n|evalstring = tostring(LEFT_result,\"binary\")"}, {"language": "spl", "code": "| makeresults \n|evalNOT_result = bit_not(9) \n|evalstring = tostring(NOT_result,\"binary\")"}, {"language": "spl", "code": "| makeresults \n|evalNOT_result = bit_not(9, tonumber(\"111\",2)) \n|evalstring = tostring(NOT_result,\"binary\")"}, {"language": "spl", "code": "| makeresults \n|evalmax_supported_val = pow(2, 53)-1 \n|evalresult = bit_and(max_supported_val + 1, 1)"}, {"language": "spl", "code": "| makeresults \n|evalflags =\"00010\", result = tonumber(flags, 2) \n|evalresult = bit_or(result, tonumber(\"11001\", 2)) \n|evalresult = tostring(result,\"binary\")"}, {"language": "spl", "code": "| makeresults \n|evalflags =\"00010\", result = tonumber(flags, 2) \n|evalresult = bit_and(result, 4) \n|evalresult =if(result==0,\"false\",\"true\")"}, {"language": "spl", "code": "| makeresults \n|evalbin_number1 =\"10011\", bin_number2 =\"10001\", number1 = tonumber(bin_number1, 2), number2 = tonumber(bin_number2, 2) \n|evalmatching_bits = bit_xor(number1, number2) \n|evalmatching_bits = bit_not(matching_bits, tonumber(\"11111\",2)) \n|evalmatching_bits = tostring(matching_bits,\"binary\")"}, {"language": "spl", "code": "| makeresults \n|evalflags =\"10001\", result = tonumber(flags, 2), flag_bool = 1 \n|evalresult = bit_shift_left(result, 1) \n|evalresult = bit_or(result, flag_bool) \n|evalresult = tostring(result,\"binary\")"}, {"language": "spl", "code": "| makeresults \n|evalstreamId = tonumber(\"0xa3b7514f19ce7\", 16), result = bit_and(bit_shift_right(streamId,8), tonumber(\"0xFFFFFFFF\",16))"}], "tables": [{"headers": ["_time", "AND_result", "OR_result", "XOR_result"], "rows": [["2022-09-02  13:44:21", "4", "6", "2"]]}, {"headers": ["_time", "LEFT_result", "RIGHT_result"], "rows": [["2022-09-02  13:44:21", "4", "1"]]}, {"headers": ["_time", "LEFT_result", "string"], "rows": [["2022-09-02  11:23:43", "4", "100"]]}, {"headers": ["_time", "NOT_result", "string"], "rows": [["2022-09-02  09:52:12", "9007199254740982", "11111111111111111111111111111111111111111111111110110"]]}, {"headers": ["_time", "NOT_result", "string"], "rows": [["2022-09-02  09:52:12", "6", "110"]]}, {"headers": ["_time", "max_supported_val"], "rows": [["2022-09-04  11:15:19", "9007199254740991"]]}, {"headers": ["_time", "flags", "result"], "rows": [["2022-09-05  12:19:03", "00010", "11011"]]}, {"headers": ["_time", "flags", "result"], "rows": [["2022-09-05  09:13:22", "00010", "0"]]}, {"headers": ["_time", "bin_number1", "bin_number2", "matching_bits", "number1", "number2"], "rows": [["2022-09-05  10:15:26", "10011", "10001", "11101", "19", "17"]]}, {"headers": ["_time", "flags", "flag_bool", "result"], "rows": [["2022-09-05  12:19:20", "10001", "1", "100011"]]}, {"headers": ["_time", "result", "StreamId"], "rows": [["2022-09-05  11:21:02", "1964306844", "2880123815697639"]]}], "chunk_index": 6, "total_chunks": 7, "metadata": {"title": "Bitwise functions", "section_heading": "Examples", "section_id": "e0af4855_657c_482b_8e12_4304f6ee593c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/bitwise-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:34.359904+00:00", "version": "10.2"}}
{"id": "b884bccba77e5de1", "content": "Description This function generates a new masked IP address by applying a mask to an IP address through a bitwise AND operation. You can use this function to simplify the isolation of an IPv4 address octet without splitting the IP address. Usage The mask must be a valid IPv4 address. The IP must be a valid IPv4 address or a field name where the field value is a valid IPv4 address. A valid IPv4 address is a quad-dotted notation of four decimal integers, each ranging from 0 to 255. For the mask argument, you can specify one of the default subnet masks such as 255.255.255.0. You can use this function with the eval command, and as part of eval expressions. Basic examples The following example shows how to use the ipmask function with the eval command: The output of this example is 10.20.30.0. The following example shows how to use the ipmask function in the SELECT clause of the from command: This search masks every IP address in the clientip field and returns the results in an aliased field called maskedip. The following example shows how to use the ipmask function in the WHERE clause of the from command to filter the events on a specific mask value: In this example, the masked value is 0.20.0.96. The following example shows how to use the ipmask function in a pipeline to create a new field with the masked values:", "code_examples": [{"language": "spl", "code": "... |evalmaskedIP = ipmask(\"255.255.255.0\",\"10.20.30.120\")"}, {"language": "spl", "code": "... |evalmaskedIP = ipmask(\"0.255.0.244\", clientip) AS maskedip"}, {"language": "spl", "code": "...|whereipmask(\"0.255.0.224\", clientip)=\"10.20.30.120\""}, {"language": "spl", "code": "$pipeline= from$source|evalmaskedIP = ipmask(\"255.0.255.0\", clientip) | fields -clientip | into$destination"}], "tables": [], "chunk_index": 0, "total_chunks": 11, "metadata": {"title": "Conversion functions", "section_heading": "ipmask(<mask>,<ip>)", "section_id": "id_974eae02_10bd_486b_8f87_f38e7ba9c009--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/conversion-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:51.207484+00:00", "version": "10.2"}}
{"id": "ebe044c8ea6ec8e8", "content": "Description This function builds a string value, based on a string format and the values specified. You can specify zero or more values. The values can be strings, numbers, computations, or fields. The SPL printf function is similar to the C sprintf() function and similar functions in other languages such as Python, Perl, and Ruby. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. format Description: The format is a character string that can include one or more format conversion specifiers. Each conversion specifier can include optional components such as flag characters, width specifications, and precision specifications. The format must be enclosed in quotation marks. Syntax: \"(%[flags][width][.precision]<conversion_specifier>)...\" arguments Description: The arguments are optional and can include the width, precision, and the value to format. The value can be a string, number, or field name. Syntax: [width][.precision][value] Supported conversion specifiers The following table describes the supported conversion specifiers. Flag characters The following table describes the supported flag characters. Specifying field width You can use an asterisk ( * ) with the printf function to return the field width or precision from an argument. Examples The following example returns the positive or negative integer values, including any signs specified with those values. printf(\"%*d\", 5, 123) which returns 123 The following example returns the floating point number with 1 decimal point. printf(\"%.*f\", 1, 1.23) which returns 1.2 The following example returns the value of pi() in exponential format with 2 decimal points. printf(\"%*.*e\", 9, 2, pi()) which returns 3.14e+00 The field width can be expressed using a number or an argument denoted with an asterisk ( * ) character. Specifying precision Unsupported conversion specifiers There are a few conversion specifiers from the C sprintf() function that are not supported, including: %C, however %c is supported %n %S, however %s is supported %<num>$ specifier for picking which argument to use Basic examples This example creates a new field called new_field and creates string values based on the values in field_one and field_two. The values are formatted with 4 digits before the decimal and 4 digits after the decimal. The - specifies to left justify the string values. The 30 specifies the width of the field.", "code_examples": [{"language": "spl", "code": "...|evalnew_field=printf(\"%04.4fÂ %-30s\",field_one,field_two)"}], "tables": [{"headers": ["Conversion specifier", "Alias", "Description", "Examples"], "rows": [["%a orÂ %A", "", "Floating point number in hexadecimal format", "This example returns the value ofpito 3 decimal points, in hexadecimal format.printf(\"%.3A\",pi())which returns0X1.922P+1"], ["%c", "", "Single Unicode code point", "This example returns the unicode code point for 65 and the first letter  of the string \"Foo\".printf(\"%c,%c\",65,\"Foo\")which returnsA,F"], ["%d", "%i", "Signed decimal integer", "This example returns the positive or negative integer values, including any signs specified with those values.printf(\"%d,%i,%d\",-2,+4,30)which returns-2,4,30"], ["%e orÂ %E", "", "Floating point number, exponential format", "This example returns the number 5139 in exponential format with 2 decimal points.printf(\"%.2e\",5139)which returns5.14e+03"], ["%f orÂ %F", "", "Floating point number", "This example returns the value ofpito 2 decimal points.printf(\"%.2f\",pi())which returns3.14"], ["%g orÂ %G", "", "Floating point number. This specifier uses eitherÂ %e orÂ %f depending on the range of the numbers being formatted.", "This example returns the value ofpito 2 decimal points (using theÂ %f specifier) and the number 123 in exponential format with 2 decimal points (usingÂ %e specifier).printf(\"%.2g,%.2g\",pi(),123)which returns3.1,1.2e+02"], ["%o", "", "Unsigned octal number", "This example returns the base-8 number for 255.printf(\"%o\",255)which returns377"], ["%s", "%z", "String", "This example returns the concatenated string values of \"foo\" and \"bar\".printf(\"%s%z\", \"foo\", \"bar\")which returnsfoobar"], ["%u", "", "Unsigned, or non-negative, decimal integer", "This example returns the integer value of the number in the argument.printf(\"%u\",99)which returns99"], ["%x orÂ %X", "%p", "Unsigned hexadecimal number (lowercase or uppercase)", "This example returns the hexadecimal values that are equivalent to the numbers in the arguments. This example shows both upper and lowercase results when using this specifier.printf(\"%x,%X,%p\",10,10,10)which returnsa,A,A"], ["%%", "", "Percent sign", "This example returns the string value with a percent sign.printf(\"100%%\")which returns100%"]]}, {"headers": ["Flag characters", "Description", "Examples"], "rows": [["single quote or apostrophe ( ' )", "Adds commas as the thousands separator.", "printf(\"%'d\",12345), which returns12,345"], ["dash or minus ( - )", "Left justify. If this flag is not specified, the result keeps its default justification.Theprintffunction supports right justification of results only when it formats that way by default.", "printf(\"%-4d\",1)which returns1, which is left justified in the output."], ["zero ( 0\t)", "Zero pad", "This example returns the value in the argument with leading zeros such that the number has 4 digits.printf(\"%04d\",1), which returns0001"], ["plus ( + )", "Always include the sign ( + or - ).  If this flag is not specified, the conversion displays a sign only for negative values.", "printf(\"%+4d\",1), which returns+1"], ["<space>", "Reserve space for the sign. If the first character of a signed conversion is not a sign or if a signed conversion results in no characters, a <space> is added as a prefixed to the result. If both the <space> and + flags are specified,  the <space> flag is ignored.", "printf(\"% -4d\",1), which returns1"], ["hash, number, or pound ( # )", "Use an alternate form. For theÂ %o conversion specifier, the # flag increases the precision to force the first digit of the result to be zero. ForÂ %x orÂ %X conversion specifiers, a non-zero result has 0x (or 0X) prefixed to it. ForÂ %a,Â %A,Â %e,Â %E,Â %f,Â %F,Â %%g , and G conversion specifiers, the result always contains a radix character, even if no digits follow the radix character. Without this flag, a radix character appears in the result of these conversions only if a digit follows it. ForÂ %g andÂ %G conversion specifiers, trailing zeros are not removed from the result as they normally are. For other conversion specifiers, the behavior is undefined.", "printf(\"%#x\", 1), which returns0x1"]]}, {"headers": ["Field width specifier", "Description", "Examples"], "rows": [["number", "The minimum number of characters to print. If the value to print is shorter than this number, the result is padded with blank spaces. The value is not truncated even if the result is larger.", ""], ["* (asterisk)", "The width is not specified in the format string, but as an additional integer value argument preceding the argument that has to be formatted.", ""]]}, {"headers": ["Precision", "Description"], "rows": [["%d,Â %i,Â %o,Â %u,Â %x orÂ %X", "Precision specifies the minimum number of digits to be return. If the value to be return is shorter than this number, the result is padded with leading zeros. The value is not truncated even if the result is longer. A precision of 0 means that no character is returned for the value 0."], ["%a orÂ %A,Â %e orÂ %E,Â %f orÂ %F", "This is the number of digits to be returned after the decimal point. The default is 6 ."], ["%g orÂ %G", "This is the maximum number of significant digits to be returned."], ["%s", "This is the maximum number of characters to be returned. By default all characters are printed until the ending null character is encountered."], ["Specifying the period without a precision value", "If the period is specified without an explicit value for precision, 0 is assumed."], ["Specifying an asterisk for the precision value, for example.*", "The precision is not specified in the format string, but as an additional integer value argument preceding the argument that has to be formatted."]]}], "chunk_index": 1, "total_chunks": 11, "metadata": {"title": "Conversion functions", "section_heading": "printf(<format>,<arguments>)", "section_id": "id_96def039_e268_4989_bb78_35a70c7e1291--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/conversion-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:51.207516+00:00", "version": "10.2"}}
{"id": "3e1ab82f1b0bb231", "content": "Description This function takes one argument and returns the equivalent array value of the field, if any. You can use this function to convert a string or multivalue to an array. The toarray function infers the data type of each element as it converts a string or multivalue into an array. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The value argument can be a string or multivalue, or the name of a field that contains a string or multivalue. If the value is a string, it must be a list of comma-separated values enclosed in square brackets ( [ ] ), or else the function returns null. For example, this string value is a valid JSON array: [\"buttercup\", \"fluttershy\", \"rarity\"]. The following table describes how the toarray function converts specific types of values in search results. The function returns null for all other values. Basic examples The following search returns an array called my_array with the value [1,2,3]. The following search returns an array called grocery_array with the value [\"carrots\",1,\"potatoes\",1.75]. The following search returns True , indicating that \"somefield\" isn't an array. The toarray function returns null because \"somefield\" isn't an array, which in turn, causes the isnull function to return True because the result of the toarray function is null. Extended example The test_data field in these events contains multivalues. The following eval command converts the multivalues in the test_data field into arrays and stores them in a field named test_array : The results look like this:", "code_examples": [{"language": "spl", "code": "| makeresults\n|evalmy_array=toarray(split(\"1, 2, 3\",\",\"))"}, {"language": "spl", "code": "| makeresults\n|evalgrocery_array=toarray(\"[\\\"carrots\\\", 1.00, \\\"potatoes\\\", 1.75]\")"}, {"language": "spl", "code": "| makeresults\n|evalresult =if(isnull(toarray(\"somefield\")),\"True\",\"False\")"}, {"language": "spl", "code": "... |evaltest_array = toarray(test_data)"}], "tables": [{"headers": ["Value", "Result"], "rows": [["array", "The same value."], ["multivalue", "mv_to_json_array(<value>, true)"], ["string", "The string parsed as a JSON array."]]}, {"headers": ["_time", "test_data"], "rows": [["2024-12-10 00:46:39", "100200300"], ["2024-12-10 00:46:45", "456"]]}, {"headers": ["_time", "test_data", "test_array"], "rows": [["2024-12-10 00:46:39", "100200300", "[100,200,300]"], ["2024-12-10 00:46:45", "456", "[4,5,6]"]]}], "chunk_index": 2, "total_chunks": 11, "metadata": {"title": "Conversion functions", "section_heading": "toarray(<value>)", "section_id": "faff332a_9719_40f6_bd3f_ddc5af7cb21a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/conversion-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:51.207531+00:00", "version": "10.2"}}
{"id": "7ba8f23de24b3d21", "content": "Description This function takes one argument and returns the equivalent Boolean value of the field, if any. You can use this function to convert a string or a number to a Boolean value. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The value argument can be a string or Boolean, or the name of a field that contains a string or Boolean. You can use this function directly with the where command in searches, but the eval command can't directly accept a Boolean value. You must specify the function inside another function, such as the if function, which can accept a Boolean value as an input. The following table describes how the tobool function converts specific types of values in search results. If the value is Boolean, then the result is the same value. The function returns null for all other values. Basic examples Suppose you have data that looks like this: You need to run a search to determine which items are in stock. Because the eval command can't directly accept a Boolean value, your search uses the tobool function as the first argument in the if function, like this: Your search results look like this:", "code_examples": [{"language": "spl", "code": "â€¦ |evalin_stock=if(tobool(units),\"In Stock\",\"Not in Stock\")"}], "tables": [{"headers": ["Data type", "Value", "Returned Boolean value"], "rows": [["string", "\"true\" or \"True\"", "true"], ["", "\"false\" or \"False\"", "false"], ["number", "0", "false"], ["", "Any non-zero number", "true"]]}, {"headers": ["_time", "categoryId", "units"], "rows": [["2024-11-07 21:25:09", "Dream Crusher", "12"], ["2024-11-07 21:25:09", "Final Sequel", "0"], ["2024-11-07 21:25:09", "Grand Theft Scooter", "15"], ["2024-11-07 21:25:09", "Mediocre Kingdom", "35"], ["2024-11-07 21:25:09", "Orvil the Wolverine", "1"]]}, {"headers": ["_time", "categoryId", "units", "in_stock"], "rows": [["2024-11-07 21:25:09", "Dream Crusher", "12", "In Stock"], ["2024-11-07 21:25:09", "Final Sequel", "0", "Not in Stock"], ["2024-11-07 21:25:09", "Grand Theft Scooter", "15", "In Stock"], ["2024-11-07 21:25:09", "Mediocre Kingdom", "35", "In Stock"], ["2024-11-07 21:25:09", "Orvil the Wolverine", "1", "In Stock"]]}], "chunk_index": 3, "total_chunks": 11, "metadata": {"title": "Conversion functions", "section_heading": "tobool(<value>)", "section_id": "ec6841c8_fa5d_49ab_9b79_b025a79bfde9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/conversion-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:51.207548+00:00", "version": "10.2"}}
{"id": "4ffaa83135a015fd", "content": "Description This function takes one or two arguments and returns the equivalent double value of the field, if any. The second argument specifies the numeric base used to convert a string to a number using the tonumber(<value>, <base>) function. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The value argument can be a string or number, or the name of a field that contains a string or number. The base argument is optional and is used only when the value argument is a string and the function is converted to tonumber(<value>, <base>). The default base is 10. You can set the base argument to a number between 2 and 36, inclusive. If the todouble function can't parse a field value to a number, such as if the value contains a leading and trailing space, the function returns null. You can use the trim function with todouble to remove leading or trailing spaces. If the todouble function can't parse a string to a number, the function returns null. For example, the following search doesn't return any results: The following table describes how the todouble function converts specific types of values in search results. The function returns null for all other values. Basic examples The following example converts the value 16.00 from a string to a double, and stores the converted value in a field named numbers_double. The following example converts the value 5 from a number to a double so that it becomes 5.0 , and then stores the converted value in a field named numbers_double .", "code_examples": [{"language": "spl", "code": "| makeresults\n|evalresult = todouble(\"number\")"}, {"language": "spl", "code": "... |evalnumbers_double=todouble(\"16.00\")"}, {"language": "spl", "code": "... |evalnumbers_double=todouble(5)"}], "tables": [{"headers": ["Value", "Result"], "rows": [["number", "The same value."], ["string", "tonumber(<value>, <base>)"]]}], "chunk_index": 4, "total_chunks": 11, "metadata": {"title": "Conversion functions", "section_heading": "todouble(<value>, <base>)", "section_id": "id_476ef469_8e1c_458e_94e3_cca54332dc64--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/conversion-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:51.207556+00:00", "version": "10.2"}}
{"id": "03b3ccd0fc83a44e", "content": "Description This function takes one or two arguments and returns the equivalent integer value of the field, if any. The second argument specifies the numeric base used to convert strings. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The value argument can be a string or number, or the name of a field that contains a string or number. If the value includes decimal places, the toint function rounds the value down to the nearest whole number. The base argument is optional and is used only when the value argument is a number. The default base is 10. You can set the base argument to a number between 2 and 36, inclusive. If the toint function can't parse a field value to a number, such as if the value contains a leading and trailing space, the function returns null. You can use the trim function with toint to remove leading or trailing spaces. The following table describes how the toint function converts specific types of values in search results. The function returns null for all other values. Note: Splunk platform supports 53-bit integers with 8 bits of precision. Integers larger than 53 bits are truncated. Basic examples The following example converts the value 24 from a string to an integer, and stores the converted value in a field named numbers_int. The following example converts the value 3.14 from a double to an integer. The toint function rounds the value down to 3 and stores it in a field named numbers_int .", "code_examples": [{"language": "spl", "code": "... |evalnumbers_int=toint(\"24\")"}, {"language": "spl", "code": "...|evalnumbers_int=toint(3.14)"}], "tables": [{"headers": ["Value", "Result"], "rows": [["number", "The same value."], ["double", "floor(<value>)"], ["string", "floor(tonumber(<value>, <base>))"]]}], "chunk_index": 5, "total_chunks": 11, "metadata": {"title": "Conversion functions", "section_heading": "toint(<value>, <base>)", "section_id": "id_9d23025e_26ff_41f2_8388_3aaf68925ed5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/conversion-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:51.207566+00:00", "version": "10.2"}}
{"id": "63dc55f4cef0652e", "content": "Description This function takes one argument and returns the equivalent multivalue of the field, if any. You can use this function to convert a JSON array to a multivalue field. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The <value> argument can be a valid JSON array or the name of a field that contains a valid JSON array. If the JSON array contains any string values, the tomv function omits the double quotation marks ( \" ) that enclose those values. The following table describes how the tomv function converts specific types of values in search results. The function returns null for all other values. Basic examples The following example creates an array in the ponies field, converts that array to a multivalue, and then stores the result in a field named mv_ponies : The results look like this: Extended examples Say you have a field in an event that is a JSON object that you need to convert to a multivalue because you want to call other multivalue functions. You could extract a JSON array from that object and then convert it to a multivalue field using the tomv function. The following example uses a string dubois and the json_object function for the array values. The search results in a multivalue called mv_surname. Your search results look like this: Here is another example that uses this JSON object, which is in a field called cities in an event: The following search extracts the entire JSON object from the cities field. The cities field contains only one object. The key is the entire object. This extraction can return any type of value. Here are the results of the search: The following search converts the extracted JSON object to a multivalue using the tomv function: The search results look like this:", "code_examples": [{"language": "spl", "code": "... |evalponies = json_array(\"Buttercup\",\"Fluttershy\",\"Rarity\"), mv_ponies = tomv(ponies)"}, {"language": "spl", "code": "| makeresults\n|evalsurname = json_array(\"dubois\", json_object(\"name\",\"patel\")), surname_mv=tomv(surname)"}, {"language": "spl", "code": "{\"cities\": [\n    {\"name\":\"London\",\"Bridges\": [\n        {\"name\":\"Tower Bridge\",\"length\": 801 },\n        {\"name\":\"Millennium Bridge\",\"length\": 1066 }\n      ]\n    },\n    {\"name\":\"Venice\",\"Bridges\": [\n        {\"name\":\"Rialto Bridge\",\"length\": 157 },\n        {\"name\":\"Bridge of Sighs\",\"length\": 36 },\n        {\"name\":\"Ponte della Paglia\"}\n      ]\n    },\n    {\"name\":\"San Francisco\",\"Bridges\": [\n        {\"name\":\"Golden Gate Bridge\",\"length\": 8981 },\n        {\"name\":\"Bay Bridge\",\"length\": 23556 }\n      ]\n    }\n  ]\n}"}, {"language": "spl", "code": "... |evalextracted_cities = json_extract(cities,\"{}\")"}, {"language": "spl", "code": "... |evalextracted_cities = json_extract(cities,\"{}\"), mv_cities = tomv(extracted_cities)"}], "tables": [{"headers": ["Value", "Result"], "rows": [["multivalue", "The same value."], ["array", "json_array_to_mv(<value>)"], ["string", "There is no conversion from string."]]}, {"headers": ["_time", "mv_ponies", "ponies"], "rows": [["2024-12-10 21:20:18", "\"Buttercup\"\"Fluttershy\"\"Rarity\"", "[\"Buttercup\",\"Fluttershy\",\"Rarity\"]"]]}, {"headers": ["_time", "mv_surname", "surname"], "rows": [["2024-12-10 21:46:54", "\"dubois\"", "[\"dubois\",{\"name\":\"patel\"}]"], ["", "{\"name\":\"patel\"}", ""]]}, {"headers": ["Field", "Results"], "rows": [["extract_cities", "{\"cities\":[{\"name\":\"London\",\"Bridges\":[{\"name\":\"Tower Bridge\",\"length\":801},{\"name\":\"Millennium Bridge\",\"length\":1066}]},{\"name\":\"Venice\",\"Bridges\":[{\"name\":\"Rialto Bridge\",\"length\":157},{\"name\":\"Bridge of Sighs\",\"length\":36},{\"name\":\"Ponte della Paglia\"}]},{\"name\":\"San Francisco\",\"Bridges\":[{\"name\":\"Golden Gate Bridge\",\"length\":8981},{\"name\":\"Bay Bridge\",\"length\":23556}]}]}"]]}, {"headers": ["Field", "Results"], "rows": [["extract_cities", "{\"cities\":[{\"name\":\"London\",\"Bridges\":[{\"name\":\"Tower Bridge\",\"length\":801},{\"name\":\"Millennium Bridge\",\"length\":1066}]},{\"name\":\"Venice\",\"Bridges\":[{\"name\":\"Rialto Bridge\",\"length\":157},{\"name\":\"Bridge of Sighs\",\"length\":36},{\"name\":\"Ponte della Paglia\"}]},{\"name\":\"San Francisco\",\"Bridges\":[{\"name\":\"Golden Gate Bridge\",\"length\":8981},{\"name\":\"Bay Bridge\",\"length\":23556}]}]}"], ["mv_cities", "\"name\": \"London\",\"Bridges\": \n{ \"name\": \"Tower Bridge\", \"length\": 801 },\n{ \"name\": \"Millennium Bridge\", \"length\": 1066 }"], ["", "\"name\": \"Venice\",\"Bridges\": \n{ \"name\": \"Rialto Bridge\", \"length\": 157 },\n{ \"name\": \"Bridge of Sighs\", \"length\": 36 },\n{ \"name\": \"Ponte della Paglia\" }"], ["", "\"name\": \"San Francisco\",\"Bridges\": \n{ \"name\": \"Golden Gate Bridge\", \"length\": 8981 },\n{ \"name\": \"Bay Bridge\", \"length\": 23556 }"]]}], "chunk_index": 6, "total_chunks": 11, "metadata": {"title": "Conversion functions", "section_heading": "tomv(<value>)", "section_id": "id_7596ccc4_62e6_4311_8941_9275af605a18--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/conversion-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:51.207585+00:00", "version": "10.2"}}
{"id": "0d09834846bc4516", "content": "Description This function converts the input string to a number. The string can be a field name or a value. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The base argument is optional and is used only when the value argument is a string. The default base is 10. You can set the base argument to a number between 2 and 36, inclusive. If the string contains a decimal point (. ), then the tonumber function converts the string to a double. Otherwise, the function converts the string to an integer. If the tonumber function can't parse a field value to a number, for example if the value contains a leading and trailing space, the function returns NULL. Use the trim function to remove leading or trailing spaces. If the tonumber function can't parse a string to a number, the function returns an error. For example, the following search fails: Note: Splunk platform supports 53 bit integers with 8 bits of precision. Integers larger than 53 bits are truncated. Binary conversion You can use this function to convert a string representation of a binary number to return the corresponding number in base 10. For example, the result of the following function is 5 : eval result = tonumber(\"0101\", 2) This is because the decimal representation of 0101 is 5. For information about bitwise functions that you can use with the tonumber function, see Bitwise functions. Basic examples The following example converts the string values for the store_sales field to numbers. The following example takes the hexadecimal number and uses a base of 16 to return the number \"164\". The following example trims any leading or trailing spaces from the values in the celsius field before converting it to a number.", "code_examples": [{"language": "spl", "code": "| makeresults\n|evalresult=tonumber(\"seven\")"}, {"language": "spl", "code": "... |evaln=tonumber(store_sales)"}, {"language": "spl", "code": "... |evaln=tonumber(\"0A4\",16)"}, {"language": "spl", "code": "... |evaltemperature=tonumber(trim(celsius))"}], "tables": [], "chunk_index": 7, "total_chunks": 11, "metadata": {"title": "Conversion functions", "section_heading": "tonumber(<str>,<base>)", "section_id": "id_774e5c6c_2d77_4eec_b017_a5c315fdbe2b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/conversion-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:51.207591+00:00", "version": "10.2"}}
{"id": "c0636c9c5085bf8d", "content": "Description This function takes one argument and returns the equivalent object value of the field, if any. You can use this function to convert a string to a valid JSON object. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The <value> argument can be a string value or the name of a field that contains a string. The string must be formatted as a valid JSON object. The following table describes how the toobject function converts specific types of values in search results. The function returns null for all other values. Basic examples The following example converts the string {name: \"maria\", age:25, status: \"full-time\"} to a JSON object named employee_record. The results look like this:", "code_examples": [{"language": "spl", "code": "... |evalemployee_record = toobject(\"{\\\"name\\\": \\\"maria\\\", \\\"age\\\": 25, \\\"status\\\": \\\"full-time\\\"}\")"}], "tables": [{"headers": ["Value", "Result"], "rows": [["JSON object", "The same value."], ["string", "The string parsed as a JSON object."]]}, {"headers": ["_time", "employee_record"], "rows": [["2024-12-17 19:28:07", "{\"name\":\"maria\",\"age\":25,\"status\":\"full-time\"}"]]}], "chunk_index": 8, "total_chunks": 11, "metadata": {"title": "Conversion functions", "section_heading": "toobject(<value>)", "section_id": "id_50f8410d_9700_4d37_8c7e_1b7d99052459--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/conversion-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:51.207600+00:00", "version": "10.2"}}
{"id": "2172b0709bec9f25", "content": "Description This function converts a value to a string. If the value is a number, this function reformats it as a string. If the value is a Boolean value, it returns the corresponding string value, \"True\" or \"False\". Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The value argument can be a field name or a value. Only integers in the range of 0 to 2 53 -1 are accepted as input to the function. For example, tostring(\"5\", \"binary\") is not supported. When you use the tostring function with the eval command, the returned values might not sort as expected because they are converted to ASCII. Use the fieldformat command with the tostring function to format the displayed values. The underlying values are not changed by the fieldformat command. The format argument is optional and is only used when the value argument is a number. The tostring function supports the following formats. Binary conversion You can use this function to convert a number to a string of its binary representation. For example, the result of the following function is 1001 , because the binary representation of 9 is 1001 .: eval result = tostring(9, \"binary\") For information about bitwise functions that you can use with the tostring function, see Bitwise functions. Basic examples The following example returns \"True 0xF 12,345.68\". The following example returns foo=615 and foo2=00:10:15. The 615 seconds is converted into minutes and seconds. The following example formats the column totalSales to display values with a currency symbol and commas. You must use a period between the currency value and the tostring function.", "code_examples": [{"language": "spl", "code": "... |evaln=tostring(1==1) +\" \"+ tostring(15,\"hex\") +\" \"+ tostring(12345.6789,\"commas\")"}, {"language": "spl", "code": "... |evalfoo=615 |evalfoo2 = tostring(foo,\"duration\")"}, {"language": "spl", "code": "... | fieldformat totalSales=\"$\".tostring(totalSales,\"commas\")"}], "tables": [{"headers": ["Format", "Description"], "rows": [["\"binary\"", "Converts a number to a binary value."], ["\"hex\"", "Converts the number to a hexadecimal value."], ["\"commas\"", "Formats the number with commas.  If the number includes a decimal, the function rounds the number to nearest two decimal places."], ["\"duration\"", "Converts the value in seconds to the readable time format HH:MM:SS."]]}], "chunk_index": 9, "total_chunks": 11, "metadata": {"title": "Conversion functions", "section_heading": "tostring(<value>,<format>)", "section_id": "id_821f8876_7ce3_4038_b079_189e07a7a4ab--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/conversion-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:51.207608+00:00", "version": "10.2"}}
{"id": "fcad3f1075ef199e", "content": "Commands convert Functions strptime", "code_examples": [], "tables": [], "chunk_index": 10, "total_chunks": 11, "metadata": {"title": "Conversion functions", "section_heading": "See also", "section_id": "id_93717fa6_1495_439f_bd15_6c5ba03a09b6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/conversion-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:27:51.207612+00:00", "version": "10.2"}}
{"id": "760bae9440d45b69", "content": "Description This function takes no arguments and returns the time that the search was started when run as an ad-hoc search. If used with a scheduled search, returns the time that the search was scheduled to run, which might not be the time that the scheduled search actual runs. Usage The now() function is often used with other data and time functions. The time returned by the now() function is represented in UNIX time, or in seconds since Epoch time. When used in a search, this function returns the UNIX time when the search is run. If you want to return the UNIX time when each result is returned, use the time() function instead. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example The following example determines the UNIX time value of the start of yesterday, based on the value of now(). This example uses a \"snap-to\" time modifier to snap to the start of the day. See How to specify relative time modifiers. Extended example If you are looking for events that occurred within the last 30 minutes you need to calculate the event hour, event minute, the current hour, and the current minute. You use the now() function to calculate the current hour (curHour) and current minute (curMin). The event timestamp, in the _time field, is used to calculate the event hour (eventHour) and event minute (eventMin). For example:", "code_examples": [{"language": "spl", "code": "... |evaln=relative_time(now(),\"-1d@d\")"}, {"language": "spl", "code": "... earliest=-30d \n |evaleventHour=strftime(_time,\"%H\") \n |evaleventMin=strftime(_time,\"%M\")\n |evalcurHour=strftime(now(),\"%H\") \n |evalcurMin=strftime(now(),\"%M\")\n |where(eventHour=curHour and eventMin > curMin - 30) or \n   (curMin < 30 and eventHour=curHour-1 and eventMin>curMin+30)\n | bucket _time span=1d\n | chart count by _time"}], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "Date and Time functions", "section_heading": "now()", "section_id": "id_8dc4247b_a930_406e_825f_69f794269489--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/date-and-time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:07.480057+00:00", "version": "10.2"}}
{"id": "04094ff75503b239", "content": "Description This function takes a UNIX time as the first argument and a relative time specifier as the second argument and returns the UNIX time value of <specifier> applied to <time>. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example determines the UNIX time value of the start of yesterday, based on the value of now(). This example uses a \"snap-to\" time modifier to snap to the the start of the day. See How to specify relative time modifiers. The following example specifies an earliest time of 2 hours ago snapped to the hour and a latest time of 1 hour ago snapped to the hour. The offset -2h is processed first, followed by the snap-to time @h .", "code_examples": [{"language": "spl", "code": "... |evaln=relative_time(now(),\"-1d@d\")"}, {"language": "spl", "code": "... |where_time>relative_time(now(),\"-2h@h\") AND _time<relative_time(now(),\"-1h@h\")"}], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "Date and Time functions", "section_heading": "relative_time(<time>,<specifier>)", "section_id": "id_0131f275_1aff_4106_9b13_5c6657368fd2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/date-and-time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:07.480068+00:00", "version": "10.2"}}
{"id": "bd87a81862a9e9a7", "content": "Description This function takes a UNIX time value as the first argument and renders the time as a string using the format specified. The UNIX time must be in seconds. Use the first 10 digits of a UNIX time to use the time in seconds. You can use time format variables with the strftime function. For a complete list and descriptions of the format options, see Date and time format variables. Usage If the time is in milliseconds, microseconds, or nanoseconds you must convert the time into seconds. You can use the pow function to convert the number. To convert from milliseconds to seconds, divide the number by 1000 or 10^3. To convert from microseconds to seconds, divide the number by 10^6. To convert from nanoseconds to seconds, divide the number by 10^9. The following search uses the pow function to convert from nanoseconds to seconds: The results appear on the Statistics tab and look like this: In these results, the _time value is the date and time when the search was run. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example returns the hour and minute from the _time field. If the _time field value is 2022-08-10 11:48:23 , the value returned in the hour_min field is 11:48. The following example creates a new field called starttime in your search results. For the strftime values, the now() function is used to generate the current UNIX time and date and time variables are used to specify the ISO 8601 timestamp format; The results look something like this: For more information about date and time variables, see Date and time format variables. Extended example The following example creates a single result using the makeresults command. For example: The _time field is stored in UNIX time, even though it displays in a human readable format. To convert the UNIX time to some other format, you use the strftime function with the date and time format variables. The variables must be in quotations marks. For example, to return the week of the year that an event occurred in, use the %V variable. The results show that August 14th occurred in week 33. To return the date and time with subseconds and the time designator (the letter T) that precedes the time components of the format, use the %Y-%m-%dT%H:%M:%S.%Q variables. For example: The results are:", "code_examples": [{"language": "spl", "code": "| makeresults |evalStartTimestamp=\"1521467703049000000\"|evalstarttime=strftime(StartTimestamp/pow(10,9),\"%Y-%m-%dT%H:%M:%S.%Q\")"}, {"language": "spl", "code": "...|evalhour_min=strftime(_time,\"%H:%M\")"}, {"language": "spl", "code": "...|evalstarttime=strftime(now(),\"%Y-%m-%dT%H:%M:%S.%Q\")"}, {"language": "spl", "code": "| makeresults"}, {"language": "spl", "code": "| makeresults |evalweek=strftime(_time,\"%V\")"}, {"language": "spl", "code": "| makeresults |evalmytime=strftime(_time,\"%Y-%m-%dT%H:%M:%S.%Q\")"}], "tables": [{"headers": ["StartTimeStamp", "_time", "starttime"], "rows": [["1521467703049000000", "2018-08-10 09:04:00", "2018-03-19T06:55:03.049"]]}, {"headers": ["_starttime"], "rows": [["2022-02-11T01:55:00.000"]]}, {"headers": ["_time"], "rows": [["2022-08-14 14:00:15"]]}, {"headers": ["_time", "week"], "rows": [["2022-08-14 14:00:15", "33"]]}, {"headers": ["_time", "mytime"], "rows": [["2022-08-14 14:00:15", "2022-08-14T14:00:15.000"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "Date and Time functions", "section_heading": "strftime(<time>,<format>)", "section_id": "d2865f5b_df68_48c8_80a2_665cf3bfcfb9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/date-and-time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:07.480083+00:00", "version": "10.2"}}
{"id": "04e4a866b7ee9471", "content": "Description This function takes a time represented by a string and parses the time into a UNIX timestamp format. You use date and time variables to specify the format that matches string. For a complete list and descriptions of the variables, see Date and time format variables. The strptime function doesn't work with timestamps that consist of only a month and year. The timestamps must include a day. For example, if string X is 2022-08-13 11:22:33 , the format Y must be %Y-%m-%d %H:%M:%S. The string X date must be January 1, 1971 or later. The strptime function takes any date from January 1, 1971 or later, and calculates the UNIX time, in seconds, from January 1, 1970 to the date you provide. Note: The _time field is in UNIX time. In Splunk Web, the _time field appears in a human readable format in the UI but is stored in UNIX time. If you attempt to use the strptime function on the _time field, no action is performed on the values in the field. Usage With the strptime function, you must specify the time format of the string so that the function can convert the string time into the correct UNIX time. The following table shows some examples: You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example If the values in the timeStr field are hours and minutes, such as 11:59 , the following example returns the time as a timestamp: Extended example This example shows the results of using the strptime function. The following search does several things: The gentimes command generates a set of times with 6 hour intervals. This command returns four fields: startime , starthuman , endtime , and endhuman. The fields command returns only the starthuman and endhuman fields. The eval command takes the string time values in the starthuman field and returns the UNIX time that corresponds to the string time values. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "... |evaln=strptime(timeStr,\"%H:%M\")"}, {"language": "spl", "code": "| gentimes start=8/13/18 increment=6h \n| fields starthuman endhuman\n|evalstartunix=strptime(starthuman,\"%aÂ %BÂ %dÂ %H:%M:%S.%NÂ %Y\")"}], "tables": [{"headers": ["String time", "Matching time format variables"], "rows": [["Mon July 23 2022 17:19:01.89", "%aÂ %BÂ %dÂ %YÂ %H:%M:%S.%N"], ["Mon 7/23/2022 17:19:01.89", "%aÂ %m/%d/%YÂ %H:%M:%S.%N"], ["2022/07/23 17:19:01.89", "%Y/%m/%dÂ %H:%M:%S.%N"], ["2022-07-23T17:19:01.89", "%Y-%m-%dT%H:%M:%S.%N"]]}, {"headers": ["starthuman", "endhuman", "startunix"], "rows": [["Mon Aug 13 00:00:00 2018", "Mon Aug 13 05:59:59 2018", "1534143600.000000"], ["Mon Aug 13 06:00:00 2018", "Mon Aug 13 11:59:59 2018", "1534165200.000000"], ["Mon Aug 13 12:00:00 2018", "Mon Aug 13 17:59:59 2018", "1534186800.000000"], ["Mon Aug 13 18:00:00 2018", "Mon Aug 13 23:59:59 2018", "1534208400.000000"], ["Tue Aug 14 00:00:00 2018", "Tue Aug 14 05:59:59 2018", "1534230000.000000"], ["Tue Aug 14 06:00:00 2018", "Tue Aug 14 11:59:59 2018", "1534251600.000000"], ["Tue Aug 14 12:00:00 2018", "Tue Aug 14 17:59:59 2018", "1534273200.000000"], ["Tue Aug 14 18:00:00 2018", "Tue Aug 14 23:59:59 2018", "1534294800.000000"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "Date and Time functions", "section_heading": "strptime(<str>,<format>)", "section_id": "id_9e798c44_facd_47c0_b0be_2cc0fa30318f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/date-and-time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:07.480100+00:00", "version": "10.2"}}
{"id": "fbeccc5e46661503", "content": "Description This function returns the wall-clock time, in the UNIX time format, with microsecond resolution. Usage The value of the time() function will be different for each event, based on when that event was processed by the eval command. You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example This example shows the results of using the time() function. The following search does several things: The gentimes command generates a set of times with 6 hour intervals. This command returns four fields: startime , starthuman , endtime , and endhuman. The fields command returns only the startime and starthuman fields. The first eval command takes the numbers in the starttime field and returns them with microseconds included. The second eval command creates the testtime field and returns the UNIX time at the instant the result was processed by the eval command. The results appear on the Statistics tab and look something like this: Notice the difference in the microseconds between the values in the epoch_time and test_time fields. You can see that the test_time values increase with each result.", "code_examples": [{"language": "spl", "code": "| gentimes start=8/13/18 increment=6h \n| fields starttime starthuman\n|evalepoch_time=strptime(starttime,\"%s\") \n|evaltesttime=time()"}], "tables": [{"headers": ["starttime", "starthuman", "epoch_time", "testtime"], "rows": [["1534143600", "Mon Aug 13 00:00:00 2018", "1534143600.000000", "1534376565.299298"], ["1534165200", "Mon Aug 13 06:00:00 2018", "1534165200.000000", "1534376565.299300"], ["1534186800", "Mon Aug 13 12:00:00 2018", "1534186800.000000", "1534376565.299302"], ["1534208400", "Mon Aug 13 18:00:00 2018", "1534208400.000000", "1534376565.299304"], ["1534230000", "Tue Aug 14 00:00:00 2018", "1534230000.000000", "1534376565.299305"], ["1534251600", "Tue Aug 14 06:00:00 2018", "1534251600.000000", "1534376565.299306"], ["1534273200", "Tue Aug 14 12:00:00 2018", "1534273200.000000", "1534376565.299308"], ["1534294800", "Tue Aug 14 18:00:00 2018", "1534294800.000000", "1534376565.299309"]]}], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "Date and Time functions", "section_heading": "time()", "section_id": "e5b4434d_246c_4079_aed3_f6b51f7a516c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/date-and-time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:07.480111+00:00", "version": "10.2"}}
{"id": "00867f23cc2e60e8", "content": "Description This function takes one or more values and returns the average of numerical values as an integer. Each argument must be either a field (single or multivalue) or an expression that evaluates to a number. At least one numeric argument is required. When the function is applied to a multivalue field, each numeric value of the field is included in the total. The eval command ignores arguments that don't exist in an event or can't be converted to a number. To get the numerical average or mean of the values of two fields, x and y, note that avg(x,y) is equivalent to sum(x,y)/(mvcount(x) + mvcount(y)). Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example Example 1 : The following example creates a field called a with value 5.0, a field called b with value 9, and a field called x with value 7 that is the average of a and b. A field is not created for c and it is not included in the total because a value was not declared for that argument. Example 2 : The following example calculates the average of three numbers and returns c=2. However, the following example returns an error because one of the arguments in the function is a string. To use a quoted string as a number within the function, you must convert the number to an integer, as shown in the following example where c=2: Example 3 : In this example, a field with a value that is a string results in a field called a with value 1, and a field called c with value 2, Example 4 : When an argument is a field, the eval command retrieves the value and attempts to treat it as a number, even if it is a string. The following example creates a field called a with value somedata, and a field called c with value 2.5. However, the following example returns an error because the string argument is specified directly within the function.", "code_examples": [{"language": "spl", "code": "... |evala = 5.0, b =\"9\", x = avg(a, b, c)"}, {"language": "spl", "code": "... |evalc=avg(1, 2, 3)"}, {"language": "spl", "code": "... |evalc=avg(1, 2,\"3\")"}, {"language": "spl", "code": "... |evalc=avg(1, 2, tonumber(\"3\")"}, {"language": "spl", "code": "... |evala=\"1\", c=avg(a, 2, 3)"}, {"language": "spl", "code": "... |evala=\"somedata\", c=avg(a, 2, 3)"}, {"language": "spl", "code": "... |evalc=avg(\"somedata\", 2, 3)"}], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Statistical eval functions", "section_heading": "avg(<values>)", "section_id": "id_7de0d511_9caf_4588_9394_8a3a9cfad4e0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/statistical-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:24.863394+00:00", "version": "10.2"}}
{"id": "d53b73dd240978e9", "content": "Description This function takes one or more numeric or string values, and returns the maximum. Strings are greater than numbers. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example returns either \"foo\" or the value in the name field. Splunk searches use lexicographical order, where numbers are sorted before letters. If the value in the name field is \"baz\" , then \"foo\" is returned. If the value in the name field is \"zaz\" , then \"zaz\" is returned. The following example returns the maximum value in a multivalue field. This search creates a field called n with a single value, which is a series of numbers. The makemv command is used to make the single value into multiple values, each of which appears on it's own row in the results. Another new field called maxn is created which takes the values in n and returns the maximum value, 6. The results look like this:", "code_examples": [{"language": "spl", "code": "... |evaln=max(1, 3, 6, 7,\"foo\", name)"}, {"language": "spl", "code": "| makeresults |evaln =\"1 3 5 6 4 2\"| makemv n\n|evalmaxn = max(n)"}], "tables": [{"headers": ["_time", "maxn", "n"], "rows": [["2021-01-29 10:42:37", "6", "135642"]]}], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Statistical eval functions", "section_heading": "max(<values>)", "section_id": "b65cc483_8bf7_44eb_b30a_f8379391d42e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/statistical-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:24.863403+00:00", "version": "10.2"}}
{"id": "762e5d594f22fd88", "content": "Description This function takes one or more numeric or string values, and returns the minimum. Strings are greater than numbers. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example returns either 3 or the value in the size field. Splunk searches use lexicographical order, where numbers are sorted before letters. If the value in the size field is 9 , then 3 is returned. If the value in the size field is 1 , then 1 is returned. The following example returns the minimum value in a multivalue field. This search creates a field called n with a single value, which is a series of numbers. The makemv command is used to make the single value into multiple values, each of which appears on it's own row in the results. Another new field called minn is created which takes the values in n and returns the minimum value, 2. The results look like this:", "code_examples": [{"language": "spl", "code": "... |evaln=min(3, 6, 7,\"maria\", size)"}, {"language": "spl", "code": "| makeresults |evaln =\"3 5 6 4 7 2\"| makemv n\n|evalminn = min(n)"}], "tables": [{"headers": ["_time", "minn", "n"], "rows": [["2021-01-29 10:42:37", "2", "356472"]]}], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Statistical eval functions", "section_heading": "min(<values)", "section_id": "id_89e6b1f4_aef9_4a78_9b8f_e655f417cd71--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/statistical-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:24.863408+00:00", "version": "10.2"}}
{"id": "91763046f2b4b2a2", "content": "Description This function takes no arguments and returns a pseudo-random integer ranging from zero to 2 31 -1. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples The following example returns a random integer, such as 0...2147483647. The following example returns a random number within a specified range. In this example, the random number is between 1 and 100,000. This example takes a random number and uses the modulo mathematical operator ( % ) to divide the random number by 100000. This ensures that the random number returned is not greater than 100000. The number remaining after the division is increased by 1 to ensure that the number is at least greater than or equal to 1.", "code_examples": [{"language": "spl", "code": "... |evaln=random()"}, {"language": "spl", "code": "... |evaln=(random()Â % 100000) + 1"}], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Statistical eval functions", "section_heading": "random()", "section_id": "aa94cfeb_e273_42e2_99e4_e40f78932af4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/statistical-eval-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:24.863412+00:00", "version": "10.2"}}
{"id": "ec802148a044f68f", "content": "With the fieldformat command you can use an <eval-expression> to change the format of a field value when the results render. This command changes the appearance of the results without changing the underlying value of the field. Because commands that come later in the search pipeline cannot modify the formatted results, use the fieldformat command as late in the search pipeline as possible. The fieldformat command does not apply to commands that export data, such as the outputcsv and outputlookup commands. The export retains the original data format and not the rendered format. If you want the format to apply to exported data, use the eval command instead of the fieldformat command.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "fieldformat", "section_heading": "Description", "section_id": "id_248d9041_66cd_423c_83b6_9f18958c112b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fieldformat", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:39.937858+00:00", "version": "10.2"}}
{"id": "1f4eb34cc8d1c086", "content": "fieldformat <field>=<eval-expression> Required arguments <field> Description: The name of a new or existing field, non-wildcarded, for the output of the eval expression. <eval-expression> Syntax: <string> Description: A combination of values, variables, operators, and functions that represent the value of your destination field. You can specify only one <eval-expression> with the fieldformat command. To specify multiple formats you must use multiple fieldformat commands. See Examples. For more information, see the eval command. For information about supported functions, see Usage .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "fieldformat", "section_heading": "Syntax", "section_id": "id_606754ab_ac07_4498_b3e4_3aede1056dd7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fieldformat", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:39.937867+00:00", "version": "10.2"}}
{"id": "1d921f7f409cb96e", "content": "The fieldformat command is a distributable streaming command. See Command types. Time format variables are frequently used with the fieldformat command. See Date and time format variables. Functions You can use a wide range of functions with the fieldformat command. For general information about using functions, see Evaluation functions. The following table lists the supported functions by type of function. Use the links in the table to learn more about each function, and to see examples.", "code_examples": [], "tables": [{"headers": ["Type of function", "Supported functions and syntax", "", ""], "rows": [["Comparison and Conditional functions", "case(X,\"Y\",...)cidrmatch(\"X\",Y)coalesce(X,...)false()if(X,Y,Z)", "in(VALUE-LIST)like(TEXT, PATTERN)match(SUBJECT, \"REGEX\")null()", "nullif(X,Y)searchmatch(X)true()validate(X,Y,...)"], ["Conversion functions", "printf(\"format\",arguments)", "tonumber(NUMSTR,BASE)", "tostring(X,Y)"], ["Cryptographic functions", "md5(X)sha1(X)", "sha256(X)", "sha512(X)"], ["Date and Time functions", "now()relative_time(X,Y)", "strftime(X,Y)strptime(X,Y)", "time()"], ["Informational functions", "isbool(X)isint(X)isnotnull(X)", "isnull(X)isnum(X)", "isstr(X)typeof(X)"], ["Mathematical functions", "abs(X)ceiling(X)exact(X)exp(X)", "floor(X)ln(X)log(X,Y)pi()", "pow(X,Y)round(X,Y)sigfig(X)sqrt(X)"], ["Multivalue eval functions", "commands(X)mvappend(X,...)mvcount(MVFIELD)mvdedup(X)", "mvfilter(X)mvfind(MVFIELD,\"REGEX\")mvindex(MVFIELD,STARTINDEX,ENDINDEX)mvjoin(MVFIELD,STR)", "mvrange(X,Y,Z)mvsort(X)mvzip(X,Y,\"Z\")"], ["Statistical eval functions", "max(X,...)", "min(X,...)", "random()"], ["Text functions", "len(X)lower(X)ltrim(X,Y)replace(X,Y,Z)", "rtrim(X,Y)spath(X,Y)split(X,\"Y\")substr(X,Y,Z)", "trim(X,Y)upper(X)urldecode(X)"], ["Trigonometry and Hyperbolic functions", "acos(X)acosh(X)asin(X)asinh(X)atan(X)", "atan2(X,Y)atanh(X)cos(X)cosh(X)hypot(X,Y)", "sin(X)sinh(X)tan(X)tanh(X)"]]}], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "fieldformat", "section_heading": "Usage", "section_id": "bd1ca90d_3bcc_4389_b0bb_ba2f167bd6ef--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fieldformat", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:39.937879+00:00", "version": "10.2"}}
{"id": "f7f9c2588c6ce71e", "content": "1. Format numeric values to display commas This example uses the metadata command to return results for the sourcetypes in the main index. The metadata command returns many fields. The table command is used to return only the sourcetype and totalCount fields. The results appear on the Statistics tab and look like this: Use the fieldformat command to reformat the appearance of the field values. The values in the totalCount field are formatted to display the values with commas. The results appear on the Statistics tab and look something like this: 2. Display UNIX time in a readable format Assume that the start_time field contains UNIX time. Format the start_time field to display only the hours, minutes, and seconds that correspond to the UNIX time. 3. Add currency symbols to numerical values To format numerical values in a field with a currency symbol, you must specify the symbol as a literal and enclose it in quotation marks. Use a period character as a binary concatenation operator, followed by the tostring function, which enables you to display commas in the currency values.", "code_examples": [{"language": "spl", "code": "| metadatatype=sourcetypes \n| table sourcetype totalCount"}, {"language": "spl", "code": "| metadatatype=sourcetypes \n| table sourcetype totalCount\n| fieldformat totalCount=tostring(totalCount,\"commas\")"}, {"language": "spl", "code": "... | fieldformat start_time = strftime(start_time,\"%H:%M:%S\")"}, {"language": "spl", "code": "...| fieldformat totalSales=\"$\".tostring(totalSales,\"commas\")"}], "tables": [{"headers": ["sourcetype", "totalCount"], "rows": [["access_combined_wcookie", "39532"], ["cisco:esa", "112421"], ["csv", "9510"], ["secure", "40088"], ["vendor_sales", "30244"]]}, {"headers": ["sourcetype", "totalCount"], "rows": [["access_combined _wcookie", "39,532"], ["cisco:esa", "112,421"], ["csv", "9,510"], ["secure", "40,088"], ["vendor_sales", "30,244"]]}], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "fieldformat", "section_heading": "Basic examples", "section_id": "id_63de1381_01a7_48ca_8128_a015414a1da1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fieldformat", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:39.937891+00:00", "version": "10.2"}}
{"id": "62c1aca502cdd53d", "content": "1. Formatting multiple fields This example shows how to change the appearance of search results to display commas in numerical values and dates into readable formats. First, use the metadata command to return results for the sourcetypes in the main index. |metadata type=sourcetypes | table sourcetype totalCount |fieldformat totalCount=tostring(totalCount, \"commas\") The metadata command returns the fields firstTime , lastTime , recentTime , totalCount , and type. In addition, because the search specifies types=sourcetypes , a field called sourcetype is also returned. The totalCount , firstTime , lastTime , and recentTime fields are renamed to Count , First Event , Last Event , and Last Update. The First Event , Last Event , and Last Update fields display the values in UNIX time. The results appear on the Statistics tab and look something like this: Use the fieldformat command to reformat the appearance of the output of these fields. The Count field is formatted to display the values with commas. The First Event , Last Event , and Last Update fields are formatted to display the values in readable timestamps. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "| metadatatype=sourcetypes \n| rename totalCount as Count firstTime as\"First Event\"lastTime as\"Last Event\"recentTime as\"Last Update\"| table sourcetype Count\"First Event\"\"Last Event\"\"Last Update\""}, {"language": "spl", "code": "| metadatatype=sourcetypes \n| rename totalCount as Count firstTime as\"First Event\"lastTime as\"Last Event\"recentTime as\"Last Update\"| table sourcetype Count\"First Event\"\"Last Event\"\"Last Update\"| fieldformat Count=tostring(Count,\"commas\") \n| fieldformat\"First Event\"=strftime('First Event',\"%c\") \n| fieldformat\"Last Event\"=strftime('Last Event',\"%c\") \n| fieldformat\"Last Update\"=strftime('Last Update',\"%c\")"}], "tables": [{"headers": ["sourcetype", "Count", "First Event", "Last Event", "Last Update"], "rows": [["access_combined_wcookie", "39532", "1520904136", "1524014536", "1524067875"], ["cisco:esa", "112421", "1521501480", "1521515900", "1523471156"], ["csv", "9510", "1520307602", "1523296313", "1523392090"], ["secure", "40088", "1520838901", "1523949306", "1524067876"], ["vendor_sales", "30244", "1520904187", "1524014642", "1524067875"]]}, {"headers": ["sourcetype", "Count", "First Event", "Last Event", "Last Update"], "rows": [["access_combined _wcookie", "39,532", "Mon Mar 12 18:22:16 2018", "Tue Apr 17 18:22:16 2018", "Wed Apr 18 09:11:15 2018"], ["cisco:esa", "112,421", "Mon Mar 19 16:18:00 2018", "Mon Mar 19 20:18:20 2018", "Wed Apr 11 11:25:56 2018"], ["csv", "9,510", "Mon Mar 5 19:40:02 2018", "Mon Apr 9 10:51:53 2018", "Tue Apr 10 13:28:10 2018"], ["secure", "40,088", "Mon Mar 12 00:15:01 2018", "Tue Apr 17 00:15:06 2018", "Wed Apr 18 09:11:16 2018"], ["vendor_sales", "30,244", "Mon Mar 12 18:23:07 2018", "Tue Apr 17 18:24:02 2018", "Wed Apr 18 09:11:15 2018"]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "fieldformat", "section_heading": "Extended example", "section_id": "id_0e989b58_f762_4ccb_8ced_c4fd477d2b95--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fieldformat", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:39.937902+00:00", "version": "10.2"}}
{"id": "a3f45c70ec1f2c8d", "content": "eval , where Date and time format variables", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "fieldformat", "section_heading": "See also", "section_id": "c7c12ff2_44b1_425a_be22_02373f278fc6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fieldformat", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:39.937906+00:00", "version": "10.2"}}
{"id": "f18ff9e2de0064b3", "content": "Description This function takes a number and returns its absolute value. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example The following example creates a field called absnum , whose values are the absolute values of the numeric field number .", "code_examples": [{"language": "spl", "code": "... |evalabsnum=abs(number)"}], "tables": [], "chunk_index": 0, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "abs(<num>)", "section_id": "a6334ced_867b_4b47_b03b_78e57001330c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212158+00:00", "version": "10.2"}}
{"id": "bc8a2d234d443a42", "content": "Description This function rounds a number up to the next highest integer. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You can use the abbreviation ceil instead of the full name of the function. Basic example The following example returns n=2.", "code_examples": [{"language": "spl", "code": "... |evaln=ceil(1.9)"}], "tables": [], "chunk_index": 1, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "ceiling(<num>) or ceil(<num>)", "section_id": "c6b0bab9_01be_4a77_b9e1_3c5e9478cc29--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212166+00:00", "version": "10.2"}}
{"id": "e571a1d40afef328", "content": "Description This function renders the result of a numeric expression with a larger amount of precision in the formatted output. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example", "code_examples": [{"language": "spl", "code": "... |evaln=exact(3.14 * num)"}], "tables": [], "chunk_index": 2, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "exact(<expression>)", "section_id": "id_16c1bfab_662e_4ef2_a4c9_756b86156508--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212172+00:00", "version": "10.2"}}
{"id": "4db5cd5d5ddcba1b", "content": "Description This function takes a number and returns the exponential function e N. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example The following example returns y=e 3 .", "code_examples": [{"language": "spl", "code": "... |evaly=exp(3)"}], "tables": [], "chunk_index": 3, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "exp(<num>)", "section_id": "e2c4c3e9_fce3_4839_956a_cc81076f4f17--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212175+00:00", "version": "10.2"}}
{"id": "7c372712ef2e2e93", "content": "Description This function rounds a number down to the nearest whole integer. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example The following example returns 1.", "code_examples": [{"language": "spl", "code": "... |evaln=floor(1.9)"}], "tables": [], "chunk_index": 4, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "floor(<num>)", "section_id": "id_650aeef1_73c4_4f6f_a7c7_a0a3168d58e1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212179+00:00", "version": "10.2"}}
{"id": "2ab8a2c7982346da", "content": "Description This function takes a number and returns the natural logarithm. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example The following example returns the natural logarithm of the values of bytes.", "code_examples": [{"language": "spl", "code": "... |evallnBytes=ln(bytes)"}], "tables": [], "chunk_index": 5, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "ln(<num>)", "section_id": "ce55d08a_bfa6_4170_93eb_999b17907428--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212184+00:00", "version": "10.2"}}
{"id": "e3d6d0febd8f45e3", "content": "Description This function takes either one or two numeric arguments and returns the logarithm of the first argument <num> using the second argument <base>. If the second argument <base> is omitted, this function evaluates the logarithm of number with base 10. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example", "code_examples": [{"language": "spl", "code": "... |evalnum=log(number,2)"}], "tables": [], "chunk_index": 6, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "log(<num>,<base>)", "section_id": "d76ef79b_56e6_45ea_bc59_93bfd05a9844--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212188+00:00", "version": "10.2"}}
{"id": "ae6b1dd9f61fd6c7", "content": "Description This function takes no arguments and returns the constant pi to 11 digits of precision. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example The following example calculates the area of a circle, which is pi() multiplied by the radius to the power of 2.", "code_examples": [{"language": "spl", "code": "... |evalarea_circle=pi()*pow(radius,2)"}], "tables": [], "chunk_index": 7, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "pi()", "section_id": "bcd684f9_8f32_4a5c_88ed_332ffdde706d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212193+00:00", "version": "10.2"}}
{"id": "e77c6d51b59f7f8e", "content": "Description This function takes two numeric arguments <num> and <exp> and returns <num> <base> , <num> to the power of <base>. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example The following example calculates the area of a circle, which is pi() multiplied by the radius to the power of 2.", "code_examples": [{"language": "spl", "code": "... |evalarea_circle=pi()*pow(radius,2)"}], "tables": [], "chunk_index": 8, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "pow(<num>,<exp>)", "section_id": "id_00f5e6a1_5298_44ca_8d15_a381ff159c4d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212196+00:00", "version": "10.2"}}
{"id": "f65c7a9a56c47f4b", "content": "Description This function takes one or two numeric arguments <num> and <precision>, returning <num> rounded up to the amount of decimal places specified by <precision>. The default is to round up to an integer. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. You cannot specify a negative number for the decimal places. Basic examples The following example returns n=4. The following example returns n=2.56.", "code_examples": [{"language": "spl", "code": "... |evaln=round(3.5)"}, {"language": "spl", "code": "... |evaln=round(2.555, 2)"}], "tables": [], "chunk_index": 9, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "round(<num>,<precision>)", "section_id": "id_7593e609_d078_48c6_bc37_124ba4b313a2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212201+00:00", "version": "10.2"}}
{"id": "21835fc51bb88662", "content": "Description This function takes one argument, a number, and rounds that number to the appropriate number of significant figures. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. The computation for sigfig is based on the type of calculation that generates the number. For multiplication and division, the result should have the minimum number of significant figures of all of the operands. For addition and subtraction, the result should have the same number of decimal places as the least precise number of all of the operands. For example, the numbers 123.0 and 4.567 contain different precision with the decimal places. The first number is less precise because it has 1 decimal place. The second number is more precise because it has 3 decimal places. If the calculation is 123.0 + 4.567 = 127.567, then the sigfig function returns the fewest number of decimal places. In this example only one decimal place is returned. Because the numbers to the right of the last significant figure are greater than 5, the result returned is 127.6 Basic examples Example 1 : The following example shows how the sigfig function works. The calculation 1.00*1111 returns the value n=1111 , but the following search using the sigfig function returns n=1110. In this example, 1.00 has 3 significant figures and 1111 has 4 significant figures. In this example, the minimum number of significant figures for all operands is 3. Using the sigfig function, the final result is rounded to 3 digits, returning n=1110 and not 1111. Example 2 : There are situations where the results of a calculation can return a different accuracy to the very far right of the decimal point. For example, the following search calculates the average of 100 values: The result of this calculation is: When the count is changed to 10000, the results are different: The result of this calculation is: This occurs because numbers are treated as double-precision floating-point numbers. To mitigate this issue, you can use the sigfig function to specify the number of significant figures you want returned. However, first you need to make a change to the stats command portion of the search. You need to change the name of the field avg(test) to remove the parenthesis. For example stats avg(test) AS test. The sigfig function expects either a number or a field name for X. The sigfig function cannot accept a field name that looks like another function, in this case avg. To specify the number of decimal places you want returned, you multiply the field name by 1 and use zeros to specify the number of decimal places. If you want 4 decimal places returned, you would multiply the field name by 1.0000. To return 2 decimal places, multiply by 1.00, as shown in the following example: The result of this calculation is:", "code_examples": [{"language": "spl", "code": "... |evaln=sigfig(1.00*1111)"}, {"language": "spl", "code": "| makeresults count=100 |evaltest=3.99 | stats avg(test)"}, {"language": "spl", "code": "| makeresults count=10000 |evaltest=3.99 | stats avg(test)"}, {"language": "spl", "code": "| makeresults count=10000 |evaltest=3.99 | stats avg(test) AStest|evalnew_test=sigfig(test*1.00)"}], "tables": [{"headers": ["avg(test)"], "rows": [["3.9900000000000055"]]}, {"headers": ["avg(test)"], "rows": [["3.990000000000215"]]}, {"headers": ["test"], "rows": [["3.99"]]}], "chunk_index": 10, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "sigfig(<num>)", "section_id": "id_1ccffbba_b59f_442a_9517_1dd92136457b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212208+00:00", "version": "10.2"}}
{"id": "4c546f87e267703f", "content": "Description This function takes one numeric argument <num> and returns its square root. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic example The following example returns 3:", "code_examples": [{"language": "spl", "code": "... |evaln=sqrt(9)"}], "tables": [], "chunk_index": 11, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "sqrt(<num>)", "section_id": "id_78548409_490a_4b5a_9852_6a6af35c249d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212212+00:00", "version": "10.2"}}
{"id": "27cb5021f53af795", "content": "Description This function takes an arbitrary number of arguments and returns the sum of numerical values as an integer. Each argument must be either a field (single or multi value) or an expression that evaluates to a number. At least one numeric argument is required. When the function is applied to a multivalue field, each numeric value of the field is included in the total. The eval command ignores arguments that don't exist in an event or can't be converted to a number. Usage You can use this function with the eval , fieldformat , and where commands, and as part of eval expressions. Basic examples Example 1 : The following example creates a field called a with value 5.0, a field called b with value 9, and a field called x with value 14 that is the sum of a and b. A field is not created for c and it is not included in the sum because a value was not declared for that argument. Example 2 : The following example calculates the sum of three numbers and returns c=6. However, the following example returns an error because one of the arguments in the function is a string. To use a quoted string as a number within the function, you must convert the number to an integer, as shown in the following example that returns c=6. Example 3 : In this example, a field with a value that is a string results in a field called a with value 1, and a field called c with value 6, Example 4 : When an argument is a field, the eval command retrieves the value and attempts to treat it as a number, even if it is a string. The following example creates a field called a with value somedata, and a field called c with value 5. However, the following example returns an error because the string argument is specified directly within the function.", "code_examples": [{"language": "spl", "code": "... |evala = 5.0, b =\"9\", x = sum(a, b, c)"}, {"language": "spl", "code": "... |evalc=sum(1, 2, 3)"}, {"language": "spl", "code": "... |evalc=sum(1, 2,\"3\")"}, {"language": "spl", "code": "... |evalc=sum(1, 2, tonumber(\"3\"))"}, {"language": "spl", "code": "... |evala=\"1\", c=sum(a, 2, 3)"}, {"language": "spl", "code": "... |evala=\"somedata\", c=sum(a, 2, 3)"}, {"language": "spl", "code": "... |evalc=sum(\"somedata\", 2, 3)"}], "tables": [], "chunk_index": 12, "total_chunks": 13, "metadata": {"title": "Mathematical functions", "section_heading": "sum(<num>,...)", "section_id": "id_682a9724_dc9b_4033_95a5_3cdc09d7a9b8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/evaluation-functions/mathematical-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Evaluation Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:28:57.212216+00:00", "version": "10.2"}}
{"id": "fc73d22c5f4bdf8b", "content": "Calculates aggregate statistics, such as average, count, and sum, over the results set. This is similar to SQL aggregation. If the stats command is used without a BY clause, only one row is returned, which is the aggregation over the entire incoming result set. If a BY clause is used, one row is returned for each distinct value specified in the BY clause. The stats command can be used for several SQL-like operations. If you are familiar with SQL but new to SPL, see Splunk SPL for SQL users. Difference between stats and eval commands The stats command calculates statistics based on fields in your events. The eval command creates new fields in your events by using existing fields and an arbitrary expression.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "stats", "section_heading": "Description", "section_id": "id_6ecb717c_fbc0_4ef5_b47d_7a41c9446cd3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/stats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:13.083535+00:00", "version": "10.2"}}
{"id": "11fdb4ee66a4b793", "content": "Simple: stats (stats-function( field ) [AS field ])... [BY field-list ] Complete: Required syntax is in bold. | stats [partitions=<num>] [allnum=<bool>] [delim=<string>] ( <stats-agg-term> ... | <sparkline-agg-term> ... ) [<by-clause>] [<dedup_splitvals>] Required arguments stats-agg-term Syntax: <stats-func>(<evaled-field> | <wc-field>) [AS <wc-field>] Description: A statistical aggregation function. See Stats function options. The function can be applied to an eval expression, or to a field or set of fields. Use the AS clause to place the result into a new field with a name that you specify. You can use wild card characters in field names. For more information on eval expressions, see Types of eval expressions in the Search Manual. sparkline-agg-term Syntax: <sparkline-agg> [AS <wc-field>] Description: A sparkline aggregation function. Use the AS clause to place the result into a new field with a name that you specify. You can use wild card characters in the field name. Optional arguments allnum Syntax: allnum=<bool> Description: If true, computes numerical statistics on each field if and only if all of the values of that field are numerical. Default: false by-clause Syntax: BY <field-list> Description: The name of one or more fields to group by. You cannot use a wildcard character to specify multiple fields with similar names. You must specify each field separately. The BY clause returns one row for each distinct value in the BY clause fields. If no BY clause is specified, the stats command returns only one row, which is the aggregation over the entire incoming result set. dedup_splitvals Syntax: dedup_splitvals=<boolean> Description: Specifies whether to remove duplicate values in multivalued BY clause fields. Default: false delim Syntax: delim=<string> Description: Specifies how the values in the list() or values() aggregation are delimited. Default: a single space partitions Syntax: partitions=<num> Description: Partitions the input data based on the split-by fields for multithreaded reduce. The partitions argument runs the reduce step (in parallel reduce processing) with multiple threads in the same search process on the same machine. Compare that with parallel reduce, using the redistribute command, that runs the reduce step in parallel on multiple machines. When partitions=0 , the value of the partitions argument is the same as the value of the default_partitions setting in the limits.conf file. Default: 0. Set to the same value as the default_partitions setting in the limits.conf file, which is 1 by default. Stats function options stats-func Syntax: The syntax depends on the function that you use. Refer to the table below. Description: Statistical and charting functions that you can use with the stats command. Each time you invoke the stats command, you can use one or more functions. However, you can only use one BY clause. See Usage. The following table lists the supported functions by type of function. Use the links in the table to see descriptions and examples for each function. For an overview about using functions with commands, see Statistical and charting functions. Sparkline function options Sparklines are inline charts that appear within table cells in search results to display time-based trends associated with the primary key of each row. Read more about how to \" Add sparklines to your search results \" in the Search Manual. sparkline-agg Syntax: sparkline (count(<wc-field>), <span-length>) | sparkline (<sparkline-func>(<wc-field>), <span-length>) Description: A sparkline specifier, which takes the first argument of a aggregation function on a field and an optional timespan specifier. If no timespan specifier is used, an appropriate timespan is chosen based on the time range of the search. If the sparkline is not scoped to a field, only the count aggregator is permitted. You can use wildcard characters in the field name. See the Usage section. sparkline-func Syntax: c() | count() | dc() | mean() | avg() | stdev() | stdevp() | var() | varp() | sum() | sumsq() | min() | max() | range() Description: Aggregation function to use to generate sparkline values. Each sparkline value is produced by applying this aggregation to the events that fall into each particular time bin.", "code_examples": [], "tables": [{"headers": ["Type of function", "Supported functions and syntax", "", "", ""], "rows": [["Aggregate functions", "avg()count()distinct_count()estdc()estdc_error()", "exactperc<num>()max()median()min()mode()", "perc<num>()range()stdev()stdevp()", "sum()sumsq()upperperc<num>()var()varp()"], ["Event order functions", "first()", "last()", "", ""], ["Multivalue stats and chart functions", "list()", "values()", "", ""], ["Time functions", "earliest()earliest_time()", "latest()latest_time()", "rate()", ""]]}], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "stats", "section_heading": "Syntax", "section_id": "id_174c719b_e1d9_4e32_855a_4197a5c12471--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/stats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:13.083550+00:00", "version": "10.2"}}
{"id": "59eded361c21edf9", "content": "The stats command is a transforming command. See Command types. Eval expressions with statistical functions When you use the stats command, you must specify either a statistical function or a sparkline function. When you use a statistical function, you can use an eval expression as part of the statistical function. For example: Statistical functions that are not applied to specific fields With the exception of the count function, when you pair the stats command with functions that are not applied to specific fields or eval expressions that resolve into fields, the search head processes it as if it were applied to a wildcard for all fields. In other words, when you have | stats avg in a search, it returns results for | stats avg(*). This \"implicit wildcard\" syntax is officially deprecated, however. Make the wildcard explicit. Write | stats <function>(*) when you want a function to apply to all possible fields. Numeric calculations During calculations, numbers are treated as double-precision floating-point numbers, subject to all the usual behaviors of floating point numbers. If the calculation results in the floating-point special value NaN, it is represented as \"nan\" in your results. The special values for positive and negative infinity are represented in your results as \"inf\" and \"-inf\" respectively. Division by zero results in a null field. There are situations where the results of a calculation contain more digits than can be represented by a floating- point number. In those situations precision might be lost on the least significant digits. For an example of how to correct this, see Example 2 of the basic examples for the sigfig(X) function. Ensure correct search behavior when time fields are missing from input data Ideally, when you run a stats search that aggregates results on a time function such as latest() , latest_time() , or rate() , the search should not return results when _time or _origtime fields are missing from the input data. However, searches that fit this description return results by default, which means that those results might be incorrect or random. Correct this behavior by changing the check_for_invalid_time setting in limits.conf file. Splunk Cloud Platform To change the check_for_invalid_time setting, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Enterprise To change the check_for_invalid_time setting, follow these steps. Prerequisites Only users with file system access, such as system administrators, can change the check_for_invalid_time setting in the limits.conf configuration file. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. You can have configuration files with the same name in your default, local, and app directories. Read Where you can place (or find) your modified configuration files in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps Open or create a local limits.conf file at $SPLUNK_HOME/etc/system/local. Under the [stats] stanza, set check_for_invalid_time to true. When you set check_for_invalid_time=true , the stats search processor does not return results for searches on time functions when the input data does not include the _time or _origtime fields. Functions and memory usage Some functions are inherently more expensive, from a memory standpoint, than other functions. For example, the distinct_count function requires far more memory than the count function. The values and list functions also can consume a lot of memory. If you are using the distinct_count function without a split-by field or with a low-cardinality split-by by field, consider replacing the distinct_count function with the the estdc function (estimated distinct count). The estdc function might result in significantly lower memory usage and run times. Memory and stats search performance A pair of limits.conf settings strike a balance between the performance of stats searches and the amount of memory they use during the search process, in RAM and on disk. If your stats searches are consistently slow to complete you can adjust these settings to improve their performance, but at the cost of increased search-time memory usage, which can lead to search failures. If you use Splunk Cloud Platform, you need to file a Support ticket to change these settings. For more information, see Memory and stats search performance in the Search Manual. Event order functions Using the first and last functions when searching based on time does not produce accurate results. To locate the first value based on time order, use the earliest function, instead of the first function. To locate the last value based on time order, use the latest function, instead of the last function. For example, consider the following search. Replace the first and last functions when you use the stats and eventstats commands for ordering events based on time. The following search shows the function changes. Wildcards in BY clauses The stats command does not support wildcard characters in field values in BY clauses. For example, you cannot specify | stats count BY source*. Renaming fields You cannot rename one field with multiple names. For example if you have field A, you cannot rename A as B, A as C. The following example is not valid.", "code_examples": [{"language": "spl", "code": "index=* | stats count(eval(status=\"404\")) AS count_status BY sourcetype"}, {"language": "spl", "code": "index=testsourcetype=testDb\n| eventstats first(LastPass) as LastPass, last(_time) as mostRecentTestTime \nBY testCaseId \n|wherestartTime==LastPass OR _time==mostRecentTestTime \n| stats first(startTime) AS startTime, first(status) AS status, \nfirst(histID) AS currentHistId, last(histID) AS lastPassHistId BY testCaseId"}, {"language": "spl", "code": "index=testsourcetype=testDb \n| eventstats latest(LastPass) AS LastPass, earliest(_time) AS mostRecentTestTime \nBY testCaseId \n|wherestartTime==LastPass OR _time==mostRecentTestTime \n| stats latest(startTime) AS startTime, latest(status) AS status, \nlatest(histID) AS currentHistId, earliest(histID) AS lastPassHistId BY testCaseId"}, {"language": "spl", "code": "... | stats first(host) AS site, first(host) AS report"}], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "stats", "section_heading": "Usage", "section_id": "id_75acace1_90e1_4391_b384_403892163e8a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/stats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:13.083558+00:00", "version": "10.2"}}
{"id": "b0bd3f300b0f20c0", "content": "1. Return the average transfer rate for each host 2. Search the access logs, and return the total number of hits from the top 100 values of \"referer_domain\" Search the access logs, and return the total number of hits from the top 100 values of \"referer_domain\". The \"top\" command returns a count and percent value for each \"referer_domain\". 3. Calculate the average time for each hour for similar fields using wildcard characters Return the average, for each hour, of any unique field that ends with the string \"lay\". For example, delay, xdelay, relay, etc. 4. Remove duplicates in the result set and return the total count for the unique results Remove duplicates of results with the same \"host\" value and return the total count of the remaining results. 5. In a multivalue BY field, remove duplicate values For each unique value of mvfield , return the average value of field. Deduplicates the values in the mvfield .", "code_examples": [{"language": "spl", "code": "sourcetype=access* | stats avg(kbps) BY host"}, {"language": "spl", "code": "sourcetype=access_combined | toplimit=100 referer_domain | stats sum(count) AS total"}, {"language": "spl", "code": "... | stats avg(*lay) BY date_hour"}, {"language": "spl", "code": "... | stats dc(host)"}, {"language": "spl", "code": "...| stats avg(field) BY mvfield dedup_splitvals=true"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "stats", "section_heading": "Basic examples", "section_id": "id_8b8cc68c_3362_4c17_bc49_b11a31b8cfdf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/stats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:13.083565+00:00", "version": "10.2"}}
{"id": "19576d88c0d9d43e", "content": "1. Compare the difference between using the stats and chart commands This search uses the stats command to count the number of events for a combination of HTTP status code values and host: The BY clause returns one row for each distinct value in the BY clause fields. In this search, because two fields are specified in the BY clause, every unique combination of status and host is listed on separate row. The results appear on the Statistics tab and look something like this: If you click the Visualization tab, the status field forms the X-axis and the host and count fields form the data series. The problem with this chart is that the host values (www1, www2, www3) are strings and cannot be measured in a chart. Substitute the chart command for the stats command in the search. With the chart command, the two fields specified after the BY clause change the appearance of the results on the Statistics tab. The BY clause also makes the results suitable for displaying the results in a chart visualization. The first field you specify is referred to as the <row-split> field. In the table, the values in this field become the labels for each row. In the chart, this field forms the X-axis. The second field you specify is referred to as the <column-split> field. In the table, the values in this field are used as headings for each column. In the chart, this field forms the data series. The results appear on the Statistics tab and look something like this: If you click the Visualization tab, the status field forms the X-axis, the values in the host field form the data series, and the Y-axis shows the count. 2. Use eval expressions to count the different types of requests against each Web server Run the following search to use the stats command to determine the number of different page requests, GET and POST, that occurred for each Web server. This example uses eval expressions to specify the different field values for the stats command to count. The first clause uses the count() function to count the Web access events that contain the method field value GET. Then, using the AS keyword, the field that represents these results is renamed GET. The second clause does the same for POST events. The counts of both types of events are then separated by the web server, using the BY clause with the host field. The results appear on the Statistics tab and look something like this: Note: You can substitute the chart command for the stats command in this search. You can then click the Visualization tab to see a chart of the results. 3. Calculate a wide range of statistics by a specific field Count the number of earthquakes that occurred for each magnitude range Run the following search to calculate the number of earthquakes that occurred in each magnitude range. This data set is comprised of events over a 30-day period. This search uses span=1 to define each of the ranges for the magnitude field, mag. The rename command is then used to rename the field to \"Magnitude Range\". The results appear on the Statistics tab and look something like this: Click the Visualization tab to see the result in a chart. Calculate aggregate statistics for the magnitudes of earthquakes in an area Search for earthquakes in and around California. Calculate the number of earthquakes that were recorded. Use statistical functions to calculate the minimum, maximum, range (the difference between the min and max), and average magnitudes of the recent earthquakes. List the values by magnitude type. The results appear on the Statistics tab and look something like this: Find the mean, standard deviation, and variance of the magnitudes of the recent quakes Search for earthquakes in and around California. Calculate the number of earthquakes that were recorded. Use statistical functions to calculate the mean, standard deviation, and variance of the magnitudes for recent earthquakes. List the values by magnitude type. The results appear on the Statistics tab and look something like this: The mean values should be exactly the same as the values calculated using avg(). 4. In a table display items sold by ID, type, and name and calculate the revenue for each product Create a table that displays the items sold at the Buttercup Games online store by their ID, type, and name. Also, calculate the revenue for each product. This example uses the values() function to display the corresponding categoryId and productName values for each productId. Then, it uses the sum() function to calculate a running total of the values of the price field. Also, this example renames the various fields, for better display. For the stats functions, the renames are done inline with an \"AS\" clause. The rename command is used to change the name of the product_id field, since the syntax does not let you rename a split-by field. Finally, the results are piped into an eval expression to reformat the Revenue field values so that they read as currency, with a dollar sign and commas. This returns the following table of results: 5. Determine how much email comes from each domain Find out how much of the email in your organization comes from .com, .net, .org or other top level domains. The eval command in this search contains two expressions, separated by a comma. The first part of this search uses the eval command to break up the email address in the mailfrom field. The from_domain is defined as the portion of the mailfrom field after the @ symbol. The split() function is used to break the mailfrom field into a multivalue field called accountname. The first value of accountname is everything before the \"@\" symbol, and the second value is everything after. The mvindex() function is used to set from_domain to the second value in the multivalue field accountname. The results are then piped into the stats command. The count() function is used to count the results of the eval expression. The eval uses the match() function to compare the from_domain to a regular expression that looks for the different suffixes in the domain. If the value of from_domain matches the regular expression, the count is updated for each suffix, .com , .net , and .org. Other domain suffixes are counted as other. The results appear on the Statistics tab and look something like this: 6. Search Web access logs for the total number of hits from the top 10 referring domains This example searches the web access logs and return the total number of hits from the top 10 referring domains. This search uses the top command to find the ten most common referer domains, which are values of the referer field. Some events might use referer_domain instead of referer. The top command returns a count and percent value for each referer. You can then use the stats command to calculate a total for the top 10 referrer accesses. The sum() function adds the values in the count to produce the total number of times the top 10 referrers accessed the web site.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | stats count BY status, host"}, {"language": "spl", "code": "sourcetype=access_* | chart count BY status, host"}, {"language": "spl", "code": "sourcetype=access_* | stats count(eval(method=\"GET\")) AS GET, count(eval(method=\"POST\")) AS POST BY host"}, {"language": "spl", "code": "source=all_month.csv | chart count AS\"Number of Earthquakes\"BY mag span=1 | rename mag AS\"Magnitude Range\""}, {"language": "spl", "code": "source=all_month.csv place=*California* | stats count, max(mag), min(mag), range(mag), avg(mag) BY magType"}, {"language": "spl", "code": "source=usgs place=*California* | stats count mean(mag), stdev(mag), var(mag) BY magType"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase \n| stats values(categoryId) AS Type, values(productName) AS\"Product Name\", sum(price) \n  AS\"Revenue\"by productId \n| rename productId AS\"Product ID\"|evalRevenue=\"$ \".tostring(Revenue,\"commas\")"}, {"language": "spl", "code": "sourcetype=\"cisco:esa\"mailfrom=* \n|evalaccountname=split(mailfrom,\"@\"), from_domain=mvindex(accountname,-1) \n| stats count(eval(match(from_domain,\"[^\\n\\r\\s]+\\.com\"))) AS\".com\",\n  count(eval(match(from_domain,\"[^\\n\\r\\s]+\\.net\"))) AS\".net\", \n  count(eval(match(from_domain,\"[^\\n\\r\\s]+\\.org\"))) AS\".org\", \n  count(eval(NOT match(from_domain,\"[^\\n\\r\\s]+\\.(com|net|org)\"))) AS\"other\""}, {"language": "spl", "code": "sourcetype=access_* | toplimit=10 referer"}, {"language": "spl", "code": "sourcetype=access_* | toplimit=10 referer | stats sum(count) AS total"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["status", "host", "count"], "rows": [["200", "www1", "11835"], ["200", "www2", "11186"], ["200", "www3", "11261"], ["400", "www1", "233"], ["400", "www2", "257"], ["400", "www3", "211"], ["403", "www2", "228"], ["404", "www1", "244"], ["404", "www2", "209"]]}, {"headers": ["status", "www1", "www2", "www3"], "rows": [["200", "11835", "11186", "11261"], ["400", "233", "257", "211"], ["403", "0", "288", "0"], ["404", "244", "209", "237"], ["406", "258", "228", "224"], ["408", "267", "243", "246"], ["500", "225", "262", "246"], ["503", "324", "299", "329"], ["505", "242", "0", "238"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["host", "GET", "POST"], "rows": [["www1", "8431", "5197"], ["www2", "8097", "4815"], ["www3", "8338", "4654"]]}, {"headers": [], "rows": [["This search uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance.  This example uses theAll Earthquakesdata from  the past 30 days."]]}, {"headers": ["Magnitude Range", "Number of Earthquakes"], "rows": [["-1-0", "18"], ["0-1", "2088"], ["1-2", "3005"], ["2-3", "1026"], ["3-4", "194"], ["4-5", "452"], ["5-4", "109"], ["6-7", "11"], ["7-8", "3"]]}, {"headers": ["magType", "count", "max(mag)", "min(mag)", "range(mag)", "avg(mag)"], "rows": [["H", "123", "2.8", "0.0", "2.8", "0.549593"], ["MbLg", "1", "0", "0", "0", "0.0000000"], ["Md", "1565", "3.2", "0.1", "3.1", "1.056486"], ["Me", "2", "2.0", "1.6", ".04", "1.800000"], ["Ml", "1202", "4.3", "-0.4", "4.7", "1.226622"], ["Mw", "6", "4.9", "3.0", "1.9", "3.650000"], ["ml", "10", "1.56", "0.19", "1.37", "0.934000"]]}, {"headers": ["magType", "count", "mean(mag)", "std(mag)", "var(mag)"], "rows": [["H", "123", "0.549593", "0.356985", "0.127438"], ["MbLg", "1", "0.000000", "0.000000", "0.000000"], ["Md", "1565", "1.056486", "0.580042", "0.336449"], ["Me", "2", "1.800000", "0.346410", "0.120000"], ["Ml", "1202", "1.226622", "0.629664", "0.396476"], ["Mw", "6", "3.650000", "0.716240", "0.513000"], ["ml", "10", "0.934000", "0.560401", "0.314049"]]}, {"headers": [], "rows": [["This example uses the sample dataset fromthe Search Tutorialand a field lookup to add more information to the event data.Download the data set fromAdd data tutorialand follow the instructions to load the tutorial data.Download the CSV file fromUse field lookups tutorialand follow the instructions to set up the lookup definition to add price and productName to the events.After you configure the field lookup, you can run this search using the time range,All time."]]}, {"headers": [], "rows": [["This example uses sample email data. You should be able to run this search on any email data by replacing thesourcetype=cisco:esawith thesourcetypevalue and themailfromfield with email address field name in your data. For example, the email might beTo,From, orCc)."]]}, {"headers": [".com", ".net", ".org", "other"], "rows": [["4246", "9890", "0", "3543"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "stats", "section_heading": "Extended examples", "section_id": "id_877d44fe_b3b9_4500_8e38_6ab8ccdf7bf2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/stats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:13.083633+00:00", "version": "10.2"}}
{"id": "2c4cd476f4d0c394", "content": "Functions Statistical and charting functions Commands eventstats rare sistats streamstats top Blogs Getting started with stats, eventstats and streamstats Search commands > stats, chart, and timechart Smooth operator | Searching for multiple field values", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "stats", "section_heading": "See also", "section_id": "id_6f465d46_3946_4276_9c32_81e365fc4ee7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/stats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:13.083640+00:00", "version": "10.2"}}
{"id": "09312bced0bbb431", "content": "Generates summary statistics from fields in your events and saves those statistics in a new field. Only those events that have fields pertinent to the aggregation are used in generating the summary statistics. The generated summary statistics can be used for calculations in subsequent commands in your search. See Usage .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "eventstats", "section_heading": "Description", "section_id": "e5fb13c9_a75b_4b2e_b23f_ee62ad569638--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eventstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:29.835368+00:00", "version": "10.2"}}
{"id": "30fc84dfd69e0fa6", "content": "The required syntax is in bold. eventstats [allnum=<bool>] <stats-agg-term> ... [<by-clause>] Required arguments <stats-agg-term> Syntax: <stats-func>( <evaled-field> | <wc-field> ) [AS <wc-field>] Description: A statistical aggregation function. See Stats function options. The function can be applied to an eval expression, or to a field or set of fields. Use the AS clause to place the result into a new field with a name that you specify. You can use wild card characters in field names. Optional arguments allnum Syntax: allnum=<bool> Description: If set to true , computes numerical statistics on each field, if and only if ,all of the values of that field are numerical. If you have a BY clause, the allnum argument applies to each group independently. Default: false <by-clause> Syntax: BY <field-list> Description: The name of one or more fields to group by. Stats function options stats-func Syntax: The syntax depends on the function that you use. Refer to the table below. Description: Statistical and charting functions that you can use with the eventstats command. Each time you invoke the eventstats command, you can use one or more functions. However, you can only use one BY clause. See Usage. The following table lists the supported functions by type of function. Use the links in the table to see descriptions and examples for each function. For an overview about using functions with commands, see Statistical and charting functions .", "code_examples": [], "tables": [{"headers": ["Type of function", "Supported functions and syntax", "", "", ""], "rows": [["Aggregate functions", "avg()count()distinct_count()estdc()estdc_error()", "exactperc<int>()max()median()min()mode()", "perc<int>()range()stdev()stdevp()", "sum()sumsq()upperperc<int>()var()varp()"], ["Event order functions", "earliest()", "first()", "last()", "latest()"], ["Multivalue stats and chart functions", "list(X)", "values(X)"]]}], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "eventstats", "section_heading": "Syntax", "section_id": "fceca838_aaa5_4286_bdf1_7c1e2cc69e1c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eventstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:29.835379+00:00", "version": "10.2"}}
{"id": "e45c9eaef01d76f6", "content": "The eventstats command is a dataset processing command. See Command types. The eventstats search processor uses a limits.conf file setting named max_mem_usage_mb to limit how much memory the eventstats command can use to keep track of information. When the limit is reached, the eventstats command processor stops adding the requested fields to the search results. Do not set max_mem_usage_mb=0 as this removes the bounds to the amount of memory the eventstats command processor can use. This can lead to search failures. Splunk Cloud Platform To change the max_mem_usage_mb setting, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Enterprise To change the max_mem_usage_mb setting, follow these steps. Prerequisites Have the permissions to change the max_mem_usage_mb setting. Only users with file system access, such as system administrators, can increase the max_mem_usage_mb setting using configuration files. Know how to edit configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. Decide which directory to store configuration file changes in. There can be configuration files with the same name in your default, local, and app directories. See Where you can place (or find) your modified configuration files in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps Open or create a local limits.conf file at $SPLUNK_HOME/etc/system/local. Under the [default] stanza, look for the max_mem_usage_mb setting. Under Note , read the information about the eventstats command and how the max_mem_usage_mb and the maxresultrows settings are used to determine the maximum number of results to return. Change the value for the max_mem_usage_mb setting and if necessary the maxresultrows setting. Differences between eventstats and stats The eventstats command is similar to the stats command. You can use both commands to generate aggregations like average, sum, and maximum. The differences between these commands are described in the following table: How eventstats generates aggregations The eventstats command looks for events that contain the field that you want to use to generate the aggregation. The command creates a new field in every event and places the aggregation in that field. The aggregation is added to every event, even events that were not used to generate the aggregation. For example, you have 5 events and 3 of the events have the field you want to aggregate on. the eventstats command generates the aggregation based on the data in the 3 events. A new field is added to every event and the aggregation is added to that field in every event. Statistical functions that are not applied to specific fields With the exception of the count function, when you pair the eventstats command with functions that are not applied to specific fields or eval expressions that resolve into fields, the search head processes it as if it were applied to a wildcard for all fields. In other words, when you have | eventstats avg in a search, it returns results for | eventstats avg(*). This \"implicit wildcard\" syntax is officially deprecated, however. Make the wildcard explicit. Write | eventstats <function>(*) when you want a function to apply to all possible fields. Functions and memory usage Some functions are inherently more expensive, from a memory standpoint, than other functions. For example, the distinct_count function requires far more memory than the count function. The values and list functions also can consume a lot of memory. If you are using the distinct_count function without a split-by field or with a low-cardinality split-by by field, consider replacing the distinct_count function with the the estdc function (estimated distinct count). The estdc function might result in significantly lower memory usage and run times. Event order functions Using the first and last functions when searching based on time does not produce accurate results. To locate the first value based on time order, use the earliest function, instead of the first function. To locate the last value based on time order, use the latest function, instead of the last function. For example, consider the following search. When you use the stats and eventstats commands for ordering events based on time, use the earliest and latest functions. The following search is the same as the previous search except the first and last functions are replaced with the earliest and latest functions.", "code_examples": [{"language": "spl", "code": "index=testsourcetype=testDb\n| eventstats first(LastPass) as LastPass, last(_time) as mostRecentTestTime \nBY testCaseId \n|wherestartTime==LastPass OR _time==mostRecentTestTime \n| stats first(startTime) AS startTime, first(status) AS status, \nfirst(histID) AS currentHistId, last(histID) AS lastPassHistId BY testCaseId"}, {"language": "spl", "code": "index=testsourcetype=testDb \n| eventstats latest(LastPass) AS LastPass, earliest(_time) AS mostRecentTestTime \nBY testCaseId \n|wherestartTime==LastPass OR _time==mostRecentTestTime \n| stats latest(startTime) AS startTime, latest(status) AS status, \nlatest(histID) AS currentHistId, earliest(histID) AS lastPassHistId BY testCaseId"}], "tables": [{"headers": ["stats command", "eventstats command"], "rows": [["Events are transformed into a table of aggregated search results", "Aggregations are placed into a new field that is added to each of the events in your output"], ["You can only use the fields in your aggregated results in subsequent commands in the search", "You can use the fields in your events in subsequent commands in your search, because the events have not been transformed"]]}], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "eventstats", "section_heading": "Usage", "section_id": "id_37ad4f39_7bb2_447d_b910_c2eec51cc830--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eventstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:29.835387+00:00", "version": "10.2"}}
{"id": "91c26c25d653e216", "content": "1. Calculate the overall average duration Calculate the overall average duration of a set of transactions, and place the calculation in a new field called avgdur. Because no BY clause is specified, a single aggregation is generated and added to every event in a new field called avgdur. When you look at the list of Interesting Fields, you will see that avgdur has only one value. 2. Calculate the average duration grouped by a specific field This example is the same as the previous example except that an average is calculated for each distinct value of the date_minute field. The new field avgdur is added to each event with the average value based on its particular value of date_minute. When you look at the list of Interesting Fields, you will see that avgdur has 79 values, based on the timestamp, duration, and date_minute values. 3. Search for spikes in the volume of errors This searches for spikes in error volume. You can use this search to trigger an alert if the count of errors is higher than average, for example.", "code_examples": [{"language": "spl", "code": "host=www1 \n| transaction clientip host maxspan=30s maxpause=5s \n| eventstats avg(duration) AS avgdur"}, {"language": "spl", "code": "host=www1 \n| transaction clientip host maxspan=30s maxpause=5s \n| eventstats avg(duration) As avgdur  BY date_minute"}, {"language": "spl", "code": "eventtype=\"error\"| eventstats avg(bytes) AS avg |wherebytes>avg"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "eventstats", "section_heading": "Basic examples", "section_id": "b239064d_a2fb_4594_88c2_2da3f3a3f6c0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eventstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:29.835394+00:00", "version": "10.2"}}
{"id": "4e910ad8a1f9b62f", "content": "The following example provides you with a better understanding of how the eventstats command works. This example is actually a progressive set of small examples, where one example builds on or extends the previous example. It's much easier to see what the eventstats command does by showing you examples, using a set of simple events. These examples use the makeresults command to create a set of events. The streamstats and eval commands are used to create additional fields in the events. Creating a set of events Let's start by creating a set of four events. One of the events contains a null value in the age field. The streamstats command is used to create the count field. The streamstats command calculates a cumulative count for each event, at the time the event is processed. The eval command is used to create two new fields, age and city. The eval command uses the value in the count field. The case function takes pairs of arguments, such as count=1, 25. The first argument is a Boolean expression. When that expression is TRUE, the corresponding second argument is returned. The results of the search look like this: Using eventstats with a BY clause The BY clause in the eventstats command is optional, but is used frequently with this command. The BY clause groups the generated statistics by the values in a field. You can use any of the statistical functions with the eventstats command to generate the statistics. See the Statistical and charting functions. In this example, the eventstats command generates the average age for each city. The generated averages are placed into a new field called avg(age). The following search is the same as the previous search, with the eventstats command added at the end: For San Francisco , the average age is 28 = (25 + 31) / 2. For Seattle , there is only one event with a value. The average is 39 = 39 / 1. The eventstats command places that average in every event for Seattle, including events that did not contain a value for age. The results of the search look like this: Renaming the new field By default, the name of the new field that is generated is the name of the statistical calculation. In these examples, that name is avg(age). You can rename the new field using the AS keyword. In the following search, the eventstats command has been adjusted to rename the new field to average age by city. The results of the search look like this: Events with text values The previous examples show how an event is processed that does not contain a value in the age field. Let's see how events are processed that contain an alphabetic character value in the field that you want to use to generate statistics. The following search includes the word test as a value in the age field. The results of the search look like this: Let's add the eventstats command to the search. The alphabetic values are treated like null values. The results of the search look like this: Using the allnum argument But suppose you don't want statistics generated when there are alphabetic characters in the field or the field is empty? The allnum argument controls how the eventstats command processes field values. The default setting for the allnum argument is FALSE. Which means that the field used to generate the statistics does not need to contain all numeric values. Fields with empty values or alphabetic character values are ignored. You've seen this in the earlier examples. You can force the eventstats command to generate statistics only when the fields contain all numeric values. To accomplish this, you can set the allnum argument to TRUE. The results of the search look like this: Because the age field contains values for Seattle that are not all numbers, the entire set of values for Seattle are ignored. No average is calculated. The allnum=true argument applies to empty values as well as alphabetic character values.", "code_examples": [{"language": "spl", "code": "| makeresults count=4 \n| streamstats count \n|evalage =case(count=1, 25, count=2, 39, count=3, 31, count=4, null())\n|evalcity =case(count=1 OR count=3,\"San Francisco\", count=2 OR count=4,\"Seattle\")"}, {"language": "spl", "code": "| makeresults count=4 \n| streamstats count \n|evalage =case(count=1, 25, count=2, 39, count=3, 31, count=4, null())\n|evalcity =case(count=1 OR count=3,\"San Francisco\", count=2 OR count=4,\"Seattle\")\n| eventstats avg(age) BY city"}, {"language": "spl", "code": "| makeresults count=4 \n| streamstats count \n|evalage =case(count=1, 25, count=2, 39, count=3, 31, count=4, null())\n|evalcity =case(count=1 OR count=3,\"San Francisco\", count=2 OR count=4,\"Seattle\")\n| eventstats avg(age) AS\"average age by city\"BY city"}, {"language": "spl", "code": "| makeresults count=4 \n| streamstats count \n|evalage =case(count=1, 25, count=2, 39, count=3, 31, count=4,\"test\")\n|evalcity =case(count=1 OR count=3,\"San Francisco\", count=2 OR count=4,\"Seattle\")"}, {"language": "spl", "code": "| makeresults count=4 \n| streamstats count \n|evalage =case(count=1, 25, count=2, 39, count=3, 31, count=4,\"test\")\n|evalcity =case(count=1 OR count=3,\"San Francisco\", count=2 OR count=4,\"Seattle\")\n| eventstats avg(age) BY city"}, {"language": "spl", "code": "| makeresults count=4 \n| streamstats count \n|evalage =case(count=1, 25, count=2, 39, count=3, 31, count=4,\"test\")\n|evalcity =case(count=1 OR count=3,\"San Francisco\", count=2 OR count=4,\"Seattle\")\n| eventstats allnum=trueavg(age) BY city"}], "tables": [{"headers": ["_time", "age", "city", "count"], "rows": [["2020-02-05 18:32:07", "25", "San Francisco", "1"], ["2020-02-05 18:32:07", "39", "Seattle", "2"], ["2020-02-05 18:32:07", "31", "San Francisco", "3"], ["2020-02-05 18:32:07", "", "Seattle", "4"]]}, {"headers": ["_time", "age", "avg(age)", "city", "count"], "rows": [["2020-02-05 18:32:07", "25", "28", "San Francisco", "1"], ["2020-02-05 18:32:07", "39", "39", "Seattle", "2"], ["2020-02-05 18:32:07", "31", "28", "San Francisco", "3"], ["2020-02-05 18:32:07", "", "39", "Seattle", "4"]]}, {"headers": ["_time", "age", "average age by city", "city", "count"], "rows": [["2020-02-05 18:32:07", "25", "28", "San Francisco", "1"], ["2020-02-05 18:32:07", "39", "39", "Seattle", "2"], ["2020-02-05 18:32:07", "31", "28", "San Francisco", "3"], ["2020-02-05 18:32:07", "", "39", "Seattle", "4"]]}, {"headers": ["_time", "age", "city", "count"], "rows": [["2020-02-05 18:32:07", "25", "San Francisco", "1"], ["2020-02-05 18:32:07", "39", "Seattle", "2"], ["2020-02-05 18:32:07", "31", "San Francisco", "3"], ["2020-02-05 18:32:07", "test", "Seattle", "4"]]}, {"headers": ["_time", "age", "avg(age)", "city", "count"], "rows": [["2020-02-05 18:32:07", "25", "28", "San Francisco", "1"], ["2020-02-05 18:32:07", "39", "39", "Seattle", "2"], ["2020-02-05 18:32:07", "31", "28", "San Francisco", "3"], ["2020-02-05 18:32:07", "test", "39", "Seattle", "4"]]}, {"headers": ["_time", "age", "avg(age)", "city", "count"], "rows": [["2020-02-05 18:32:07", "25", "28", "San Francisco", "1"], ["2020-02-05 18:32:07", "39", "", "Seattle", "2"], ["2020-02-05 18:32:07", "31", "28", "San Francisco", "3"], ["2020-02-05 18:32:07", "test", "", "Seattle", "4"]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "eventstats", "section_heading": "Extended example", "section_id": "id_523b1747_d0a9_41c5_ae94_4cd1c82cc939--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eventstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:29.835412+00:00", "version": "10.2"}}
{"id": "f4cb377ec89953f9", "content": "Commands stats streamstats Blogs Search commands > stats, eventstats and streamstats", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "eventstats", "section_heading": "See also", "section_id": "a25871e4_c441_4efa_9335_cebe53d6b22c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eventstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:29.835417+00:00", "version": "10.2"}}
{"id": "e6026b07754e68c0", "content": "Description Returns the average of the values of the field specified. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands, and also with sparkline() charts. For a list of the related statistical and charting commands that you can use with this function, see Statistical and charting functions. Basic examples Example 1 The following example returns the average (mean) \"size\" for each distinct \"host\". Example 2 The following example returns the average \"thruput\" of each \"host\" for each 5 minute time span. Example 3 The following example charts the ratio of the average (mean) \"size\" to the maximum \"delay\" for each distinct \"host\" and \"user\" pair. Example 4 The following example displays a timechart of the average of cpu_seconds by processor, rounded to 2 decimal points. Extended examples Example 1 There are situations where the results of a calculation can return a different accuracy to the very far right of the decimal point. For example, the following search calculates the average of 100 values: The result of this calculation is: When the count is changed to 10000, the results are different: The result of this calculation is: This occurs because numbers are treated as double-precision floating-point numbers. To mitigate this issue, you can use the sigfig function to specify the number of significant figures you want returned. However, first you need to make a change to the stats command portion of the search. You need to change the name of the field avg(test) to remove the parenthesis. For example stats avg(test) AS test. The sigfig function expects either a number or a field name. The sigfig function cannot accept a field name that looks like another function, in this case avg. To specify the number of decimal places you want returned, you multiply the field name by 1 and use zeros to specify the number of decimal places. If you want 4 decimal places returned, you would multiply the field name by 1.0000. To return 2 decimal places, multiply by 1.00, as shown in the following example: The result of this calculation is: Example 2 Chart the average number of events in a transaction, based on transaction duration. Run the following search to create a chart to show the average number of events in a transaction based on the duration of the transaction. The transaction command adds two fields to the results duration and eventcount. The eventcount field tracks the number of events in a single transaction. In this search, the transactions are piped into the chart command. The avg() function is used to calculate the average number of events for each duration. Because the duration is in seconds and you expect there to be many values, the search uses the span argument to bucket the duration into bins using logarithm with a base of 2. Use the field format option to enable number formatting. Click the Visualization tab and change the display to a pie chart. Each wedge of the pie chart represents a duration for the event transactions. You can hover over a wedge to see the average values.", "code_examples": [{"language": "spl", "code": "... | stats avg(size) BY host"}, {"language": "spl", "code": "... | bin _time span=5m | stats avg(thruput) BY _time host"}, {"language": "spl", "code": "... | charteval(avg(size)/max(delay)) AS ratio BY host user"}, {"language": "spl", "code": "... | timecharteval(round(avg(cpu_seconds),2)) BY processor"}, {"language": "spl", "code": "| makeresults count=100 |evaltest=3.99 | stats avg(test)"}, {"language": "spl", "code": "| makeresults count=10000 |evaltest=3.99 | stats avg(test)"}, {"language": "spl", "code": "| makeresults count=10000 |evaltest=3.99 | stats avg(test) AStest|evalnew_test=sigfig(test*1.00)"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | transaction clientip maxspan=30m  | chart avg(eventcount) by duration span=log2"}], "tables": [{"headers": ["avg(test)"], "rows": [["3.9900000000000055"]]}, {"headers": ["avg(test)"], "rows": [["3.990000000000215"]]}, {"headers": ["test"], "rows": [["3.99"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}], "chunk_index": 0, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "avg(<value>)", "section_id": "id_9871df7d_0656_4b4f_9e36_c393d80efdff--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.055903+00:00", "version": "10.2"}}
{"id": "c4ba7824f593d648", "content": "Description Returns the number of occurrences of the field specified. To indicate a specific field value to match, format <value> as eval(field=\"value\"). Processes field values as strings. To use this function, you can specify count(<value>) , or the abbreviation c(<value>). Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands, and also with sparkline() charts. Basic examples The following example returns the count of events where the status field has the value \"404\". This example uses an eval expression with the count function. See Using eval expressions in stats functions. The following example separates search results into 10 bins and returns the count of raw events for each bin. The following example generates a sparkline chart to count the events that use the _raw field. The following example generates a sparkline chart to count the events that have the user field. The following example uses the timechart command to count the events where the action field contains the value purchase. Extended examples Count the number of earthquakes that occurred for each magnitude range Run the following search to calculate the number of earthquakes that occurred in each magnitude range. This data set is comprised of events over a 30-day period. This search uses span=1 to define each of the ranges for the magnitude field, mag. The rename command is then used to rename the field to \"Magnitude Range\". The results look something like this: Count the number of different page requests for each Web server Run the following search to use the chart command to determine the number of different page requests, GET and POST, that occurred for each Web server. This example uses eval expressions to specify the different field values for the stats command to count. The first clause uses the count() function to count the Web access events that contain the method field value GET. Then, using the AS keyword, the field that represents these results is renamed GET. The second clause does the same for POST events. The counts of both types of events are then separated by the web server, using the BY clause with the host field. The results appear on the Statistics tab and look something like this: Click the Visualization tab. If necessary, format the results as a column chart. This chart displays the total count of events for each event type, GET or POST, based on the host value.", "code_examples": [{"language": "spl", "code": "...| stats count(eval(status=\"404\")) AS count_status BY sourcetype"}, {"language": "spl", "code": "... | bin size bins=10 | stats count(_raw) BY size"}, {"language": "spl", "code": "... sparkline(count)"}, {"language": "spl", "code": "... sparkline(count(user))"}, {"language": "spl", "code": "sourcetype=access_* | timechart count(eval(action=\"purchase\")) BY productName usenull=f useother=f"}, {"language": "spl", "code": "source=all_month.csv | chart count AS\"Number of Earthquakes\"BY mag span=1 | rename mag AS\"Magnitude Range\""}, {"language": "spl", "code": "sourcetype=access_* | chart count(eval(method=\"GET\")) AS GET, count(eval(method=\"POST\")) AS POST BY host"}], "tables": [{"headers": [], "rows": [["This search uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance.  This example uses theAll Earthquakesdata from  the past 30 days."]]}, {"headers": ["Magnitude Range", "Number of Earthquakes"], "rows": [["-1-0", "18"], ["0-1", "2088"], ["1-2", "3005"], ["2-3", "1026"], ["3-4", "194"], ["4-5", "452"], ["5-4", "109"], ["6-7", "11"], ["7-8", "3"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["host", "GET", "POST"], "rows": [["www1", "8431", "5197"], ["www2", "8097", "4815"], ["www3", "8338", "465"]]}], "chunk_index": 1, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "count(<value>) or c(<value>)", "section_id": "d07e79b5_259a_40eb_8e30_d025198ee7cd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.055928+00:00", "version": "10.2"}}
{"id": "4657ce3598185da4", "content": "Description Returns the count of distinct values of the field specified. This function processes field values as strings. To use this function, you can specify distinct_count(<value>) , or the abbreviation dc(<value>). Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands, and also with sparkline() charts. Basic examples The following example removes duplicate results with the same \"host\" value and returns the total count of the remaining results. The following example generates sparklines for the distinct count of devices and renames the field, \"numdevices\". The following example counts the distinct sources for each sourcetype, and buckets the count for each five minute spans. Extended example Run the following search to count the number of different customers who purchased something from the Buttercup Games online store yesterday. The search organizes the count by the type of product (accessories, t-shirts, and type of games) that customers purchased. This example first searches for purchase events, action=purchase. These results are piped into the stats command and the dc() function counts the number of different users who make purchases. The BY clause is used to break up this number based on the different category of products, the categoryId. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "... | stats dc(host)"}, {"language": "spl", "code": "...sparkline(dc(device)) AS numdevices"}, {"language": "spl", "code": "...sparkline(dc(source),5m) BY sourcetype"}, {"language": "spl", "code": "sourcetype=access_* action=purchase | stats dc(clientip) BY categoryId"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}, {"headers": ["categoryId", "dc(clientip)"], "rows": [["ACCESSORIES", "37"], ["ARCADE", "58"], ["NULL", "8"], ["SHOOTER", "31"], ["SIMULATION", "34"], ["SPORTS", "13"], ["STRATEGY", "74"], ["TEE", "38"]]}], "chunk_index": 2, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "distinct_count(<value>) or dc(<value>)", "section_id": "id_029a5fa5_c91c_4c29_8774_521580916010--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.055944+00:00", "version": "10.2"}}
{"id": "bff50ab5a52bbc4f", "content": "Description Returns the estimated count of the distinct values of the field specified. This function processes field values as strings. The string values 1.0 and 1 are considered distinct values and counted separately. Usage You can use this function with the chart , stats , timechart , and tstats commands. Note: By default, if the actual number of distinct values returned by a search is below 1000, the Splunk software does not estimate the distinct value count for the search. It uses the actual distinct value count instead. This threshold is set by the approx_dc_threshold setting in limits.conf. Basic examples The following example removes duplicate results with the same \"host\" value and returns the estimated total count of the remaining results. The results look something like this: The following example generates sparklines for the estimated distinct count of the devices field and renames the results field, \"numdevices\". The following example estimates the distinct count for the sources for each sourcetype. The results are displayed for each five minute span in sparkline charts.", "code_examples": [{"language": "spl", "code": "... | stats estdc(host)"}, {"language": "spl", "code": "...sparkline(estdc(device)) AS numdevices"}, {"language": "spl", "code": "...sparkline(estdc(source),5m) BY sourcetype"}], "tables": [{"headers": ["estdc(host)"], "rows": [["6"]]}], "chunk_index": 3, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "estdc(<value>)", "section_id": "c83208db_4443_4da0_b917_1d07c8ea9bd1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.055952+00:00", "version": "10.2"}}
{"id": "573dec09f8f8bdaa", "content": "Description Returns the theoretical error of the estimated count of the distinct values of the field specified. The error represents a ratio of the absolute_value(estimate_distinct_count - real_distinct_count)/real_distinct_count. This function processes field values as strings. Usage You can use this function with the chart , stats , and timechart commands. Basic examples The following example determines the error ratio for the estimated distinct count of the \"host\" values.", "code_examples": [{"language": "spl", "code": "... | stats estdc_error(host)"}], "tables": [], "chunk_index": 4, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "estdc_error(<value>)", "section_id": "fceff11a_6477_4045_8720_e9e7ec6cdfba--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.055957+00:00", "version": "10.2"}}
{"id": "0a1d2fd32c1875a6", "content": "Description Returns a percentile value of the numeric field specified. Usage You can use this function with the chart , stats , timechart , and tstats commands, and also with sparkline() charts. The exactperc function provides the exact value, but is very resource expensive for high cardinality fields. The exactperc function can consume a large amount of memory in the search head, which might impact how long it takes for a search to complete. Examples See the perc<percentile>(<value>) function.", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "exactperc<percentile>(<value>)", "section_id": "id_9d66e405_e0d6_4be4_8611_fb8824d71a8e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.055963+00:00", "version": "10.2"}}
{"id": "e48e9797cbb5ec6b", "content": "Description Returns the maximum value of the field specified. If the values in the field are non-numeric, the maximum value is found using lexicographical ordering. Processes field values as numbers if possible, otherwise processes field values as strings. Usage You can use this function with the chart , mstats , stats , and timechart commands, and also with sparkline() charts. Lexicographical order sorts items based on the values used to encode the items in computer memory. In Splunk software, this is almost always UTF-8 encoding, which is a superset of ASCII. If the items are all numeric, they're sorted in numerical order based on the first digit. For example, the numbers 10, 9, 70, 100 are sorted as 10, 100, 70, 9. if the items are mixed, they're sorted in numeric and then lexicographical order with all numbers sorted before non-numeric items. For example, the items 1, c, a, 2, 100, b, 4, 9 are sorted as 1, 2, 4, 9, 100, a, b, c. if all items are non-numeric, they're sorted in lexicographical order. Uppercase letters are sorted before lowercase letters. Symbols are not standard. Some symbols are sorted before numeric values. Other symbols are sorted before or after letters. Basic examples This example returns the maximum value of the size field. Extended example Calculate aggregate statistics for the magnitudes of earthquakes in an area Search for earthquakes in and around California. Calculate the number of earthquakes that were recorded. Use statistical functions to calculate the minimum, maximum, range (the difference between the min and max), and average magnitudes of the recent earthquakes. List the values by magnitude type. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "... | stats max(size)"}, {"language": "spl", "code": "source=all_month.csv place=*California* | stats count, max(mag), min(mag), range(mag), avg(mag) BY magType"}], "tables": [{"headers": [], "rows": [["This search uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance.  This example uses theAll Earthquakesdata from  the past 30 days."]]}, {"headers": ["magType", "count", "max(mag)", "min(mag)", "range(mag)", "avg(mag)"], "rows": [["H", "123", "2.8", "0.0", "2.8", "0.549593"], ["MbLg", "1", "0", "0", "0", "0.0000000"], ["Md", "1565", "3.2", "0.1", "3.1", "1.056486"], ["Me", "2", "2.0", "1.6", ".04", "1.800000"], ["Ml", "1202", "4.3", "-0.4", "4.7", "1.226622"], ["Mw", "6", "4.9", "3.0", "1.9", "3.650000"], ["ml", "10", "1.56", "0.19", "1.37", "0.934000"]]}], "chunk_index": 6, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "max(<value>)", "section_id": "ede6b1ab_d6f7_49d9_bd23_9fe9b433193e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.055978+00:00", "version": "10.2"}}
{"id": "641dcfabb9ff108e", "content": "Description Returns the arithmetic mean of the field specified. The mean values should be exactly the same as the values calculated using the avg() function. Usage You can use this function with the chart , mstats , stats , and timechart commands, and also with sparkline() charts. Basic examples The following example returns the mean of \"kbps\" values: Extended example Run the following search to find the mean, standard deviation, and variance of the magnitudes of recent quakes by magnitude type. The results look something like this:", "code_examples": [{"language": "spl", "code": "... | stats mean(kbps)"}, {"language": "spl", "code": "source=usgs place=*California* | stats count mean(mag), stdev(mag), var(mag) BY magType"}], "tables": [{"headers": [], "rows": [["This search uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance.  This example uses theAll Earthquakesdata from the past 30 days."]]}, {"headers": ["magType", "count", "mean(mag)", "std(mag)", "var(mag)"], "rows": [["H", "123", "0.549593", "0.356985", "0.127438"], ["MbLg", "1", "0.000000", "0.000000", "0.000000"], ["Md", "1565", "1.056486", "0.580042", "0.336449"], ["Me", "2", "1.800000", "0.346410", "0.120000"], ["Ml", "1202", "1.226622", "0.629664", "0.396476"], ["Mw", "6", "3.650000", "0.716240", "0.513000"], ["ml", "10", "0.934000", "0.560401", "0.314049"]]}], "chunk_index": 7, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "mean(<value>)", "section_id": "f8d8860b_352f_4002_983d_f94b37247548--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.055993+00:00", "version": "10.2"}}
{"id": "aab04ef5929e7c6c", "content": "Description Returns the middle-most value of the field specified. Usage You can use this function with the chart , mstats , stats , and timechart commands. If you have an even number of events, by default the median calculation is approximated to the higher of the two values. Note: This function is, by its nature, nondeterministic. This means that subsequent runs of a search using this function over identical data can contain slight variances in their results. If you require results that are more exact and consistent you can use exactperc50() instead. However, the exactperc<percentile>(<value>) function is very resource expensive for high cardinality fields. See perc<percentile>(<value>). Basic examples Consider the following list of values, which counts the number of different customers who purchased something from the Buttercup Games online store yesterday. The values are organized by the type of product (accessories, t-shirts, and type of games) that customers purchased. When the list is sorted the median, or middle-most value, is 37.", "code_examples": [], "tables": [{"headers": ["categoryId", "count"], "rows": [["ACCESSORIES", "37"], ["ARCADE", "58"], ["NULL", "8"], ["SIMULATION", "34"], ["SPORTS", "13"], ["STRATEGY", "74"], ["TEE", "38"]]}, {"headers": ["categoryId", "count"], "rows": [["NULL", "8"], ["SPORTS", "13"], ["SIMULATION", "34"], ["ACCESSORIES", "37"], ["TEE", "38"], ["ARCADE", "58"], ["STRATEGY", "74"]]}], "chunk_index": 8, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "median(<value>)", "section_id": "ee14a1e7_a1cf_4d94_9fbc_1801f71f5cb8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056009+00:00", "version": "10.2"}}
{"id": "8cffb48ea2553b1c", "content": "Description Returns the minimum value of the field specified. If the values of X are non-numeric, the minimum value is found using lexicographical ordering. This function processes field values as numbers if possible, otherwise processes field values as strings. Usage You can use this function with the chart , mstats , stats , and timechart commands. Lexicographical order sorts items based on the values used to encode the items in computer memory. In Splunk software, this is almost always UTF-8 encoding, which is a superset of ASCII. If the items are all numeric, they're sorted in numerical order based on the first digit. For example, the numbers 10, 9, 70, 100 are sorted as 10, 100, 70, 9. If the items are mixed, they're sorted in numeric and then lexicographical order with all numbers sorted before non-numeric items. For example, the items 1, c, a, 2, 100, b, 4, 9 are sorted as 1, 2, 4, 9, 100, a, b, c. If all items are non-numeric, they're sorted in lexicographical order. Uppercase letters are sorted before lowercase letters. Symbols are not standard. Some symbols are sorted before numeric values. Other symbols are sorted before or after letters. Basic examples The following example returns the minimum size and maximum size of the HotBucketRoller component in the _internal index. The following example returns a list of processors and calculates the minimum cpu_seconds and the maximum cpu_seconds. Extended example See the Extended example for the max() function. That example includes the min() function.", "code_examples": [{"language": "spl", "code": "index=_internal component=HotBucketRoller | stats min(size), max(size)"}, {"language": "spl", "code": "index=_internal | chart min(cpu_seconds), max(cpu_seconds) BY processor"}], "tables": [], "chunk_index": 9, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "min(<value>)", "section_id": "ce3bb5a6_d9fa_4ae1_b247_d3d2d55c42bb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056015+00:00", "version": "10.2"}}
{"id": "e140072eb225244c", "content": "Description Returns the most frequent value of the field specified. Processes field values as strings. Usage You can use this function with the chart , stats , and timechart commands. Basic examples The mode returns the most frequent value. Consider the following data: When you search for the mode in the age field, the value 45 is returned. You can also use mode with fields that contain string values. When you search for the mode in the surname field, the value Garcia is returned. Here's another set of sample data: If you run a search that looks for the mode in the host field, the value www1 is returned because it is the most common value in the host field. For example: The results look something like this:", "code_examples": [{"language": "spl", "code": "...| stats mode(age)"}, {"language": "spl", "code": "...| stats mode(surname)"}, {"language": "spl", "code": "... |stats mode(host)"}], "tables": [{"headers": ["firstname", "surname", "age"], "rows": [["Claudia", "Garcia", "32"], ["David", "Mayer", "45"], ["Alex", "Garcia", "29"], ["Wei", "Zhang", "45"], ["Javier", "Garcia", "37"]]}, {"headers": ["_time", "host", "sourcetype"], "rows": [["04-06-2020 17:06:23.000 PM", "www1", "access_combined"], ["04-06-2020 10:34:19.000 AM", "www1", "access_combined"], ["04-03-2020 13:52:18.000 PM", "www2", "access_combined"], ["04-02-2020 07:39:59.000 AM", "www3", "access_combined"], ["04-01-2020 19:35:58.000 PM", "www1", "access_combined"]]}, {"headers": ["mode(host)"], "rows": [["www1"]]}], "chunk_index": 10, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "mode(<value>)", "section_id": "beb34bf7_3cd2_4c4a_8bfa_1e2a065ee7df--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056031+00:00", "version": "10.2"}}
{"id": "cc6039e9b2fe5d24", "content": "Description The percentile functions return the Nth percentile value of the numeric field <value>. You can think of this as an estimate of where the top percentile starts. For example, a 95th percentile says that 95% of the values in field Y are below the estimate and 5% of the values in field <value> are above the estimate. Valid percentile values are floating point numbers between 0 and 100, such as 99.95. There are three different percentile functions that you can use: The percentile functions process field values as strings. Note: The perc and upperperc functions are, by their nature, nondeterministic, which means that that subsequent runs of searches using these functions over identical data can contain slight variances in their results. If you require exact and consistent results, you can use exactperc<X>(Y) instead. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands. Differences between Splunk and Excel percentile algorithms If there are less than 1000 distinct values, the Splunk percentile functions use the nearest rank algorithm. See http://en.wikipedia.org/wiki/Percentile#Nearest_rank. Excel uses the NIST interpolated algorithm, which basically means you can get a value for a percentile that does not exist in the actual data, which is not possible for the nearest rank approach. Splunk algorithm with more than 1000 distinct values If there are more than 1000 distinct values for the field, the percentiles are approximated using a custom radix-tree digest-based algorithm. This algorithm is much faster and uses much less memory, a constant amount, than an exact computation, which uses memory in linear relation to the number of distinct values. By default this approach limits the approximation error to < 1% of rank error. That means if you ask for 95th percentile, the number you get back is between the 94th and 96th percentile. You always get the exact percentiles even for more than 1000 distinct values by using the exactperc function compared to the perc. Basic examples Consider this list of values Y = {10,9,8,7,6,5,4,3,2,1}. The following example returns 5.5. The following example returns 9.55. Extended example Consider the following set of data, which shows the number of visitors for each hour a store is open: This data resides in the visitor_count index. You can use the streamstats command to create a cumulative total for the visitors. The results from this search look like this: Let's add the stats command with the perc function to determine the 50th and 95th percentiles. The results from this search look like this: The perc50 estimates the 50th percentile, when 50% of the visitors had arrived. You can see from the data that the 50th percentile was reached between visitor number 1996 and 1997, which was sometime between 1200 and 1300 hours. The perc95 estimates the 95th percentile, when 95% of the visitors had arrived. The 95th percentile was reached with visitor 3858, which occurred between 1600 and 1700 hours.", "code_examples": [{"language": "spl", "code": "...| stats perc50(Y)"}, {"language": "spl", "code": "...| stats perc95(Y)"}, {"language": "spl", "code": "index=visitor_count | streamstats sum(visitors) as'visitors total'"}, {"language": "spl", "code": "index=visitor_count | streamstats sum(visitors) as'visitors total'| stats perc50('visitors total') perc95('visitors total')"}], "tables": [{"headers": ["Function", "Description"], "rows": [["perc<percentile>(<value>) or the abbreviation p<percentile>(<value>)", "Use thepercfunction to calculate an approximate threshold, such that of the values in field Y, X percent fall below the threshold. Thepercfunction returns a single number that represents the lower end of the approximate values for the percentile requested."], ["upperperc<percentile>(<value>)", "When there are more than 1000 values, theupperpercfunction gives the approximate upper bound for the percentile requested. Otherwise theupperpercfunction returns the same percentile as thepercfunction."], ["exactperc<percentile>(<value>)", "Theexactpercfunction provides the exact value, but is very resource expensive for high cardinality fields. Theexactpercfunction can consume a large amount of memory, which might impact how long it takes for a search to complete."]]}, {"headers": ["hour", "visitors"], "rows": [["0800", "0"], ["0900", "212"], ["1000", "367"], ["1100", "489"], ["1200", "624"], ["1300", "609"], ["1400", "492"], ["1500", "513"], ["1600", "376"], ["1700", "337"]]}, {"headers": ["hour", "visitors", "visitors total"], "rows": [["0800", "0", "0"], ["0900", "212", "212"], ["1000", "367", "579"], ["1100", "489", "1068"], ["1200", "624", "1692"], ["1300", "609", "2301"], ["1400", "492", "2793"], ["1500", "513", "3306"], ["1600", "376", "3673"], ["1700", "337", "4010"]]}, {"headers": ["perc50(visitors total)", "perc95(visitors total)"], "rows": [["1996.5", "3858.35"]]}], "chunk_index": 11, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "perc<percentile>(<value>)", "section_id": "b18dc3da_8880_4003_bea0_c8dbf1387444--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056055+00:00", "version": "10.2"}}
{"id": "129d9b900d7f4499", "content": "Description Returns the difference between the max and min values of the field specified. The values in the field must be numeric. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands, and also with sparkline() charts. Basic example This example uses events that list the numeric sales for each product and quarter, for example: It is easiest to understand the range if you also determine the min and max values. To determine the range of sales by product, run this search: The results look like this: The range(sales) is the max(sales) minus the min(sales). Extended example See the Extended example for the max() function. That example includes the range() function.", "code_examples": [{"language": "spl", "code": "source=\"addtotalsData.csv\"| chart sum(sales) min(sales) max(sales) range(sales) BY products"}], "tables": [{"headers": ["products", "quarter", "sales", "quota"], "rows": [["ProductA", "QTR1", "1200", "1000"], ["ProductB", "QTR1", "1400", "1550"], ["ProductC", "QTR1", "1650", "1275"], ["ProductA", "QTR2", "1425", "1300"], ["ProductB", "QTR2", "1175", "1425"], ["ProductC", "QTR2", "1550", "1450"], ["ProductA", "QTR3", "1300", "1400"], ["ProductB", "QTR3", "1250", "1125"], ["ProductC", "QTR3", "1375", "1475"], ["ProductA", "QTR4", "1550", "1300"], ["ProductB", "QTR4", "1700", "1225"], ["ProductC", "QTR4", "1625", "1350"]]}, {"headers": ["quarter", "sum(sales)", "min(sales)", "max(sales)", "range(sales)"], "rows": [["QTR1", "4250", "1200", "1650", "450"], ["QTR2", "4150", "1175", "1550", "375"], ["QTR3", "3925", "1250", "1375", "125"], ["QTR4", "4875", "1550", "1700", "150"]]}], "chunk_index": 12, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "range(<value>)", "section_id": "b9edc62b_98e6_46e0_a415_8923b3beb207--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056076+00:00", "version": "10.2"}}
{"id": "ed30246ef773de04", "content": "Description Returns the sample standard deviation of the field specified. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands, and also with sparkline() charts. Basic examples This example returns the standard deviation of wildcarded fields \"*delay\" which can apply to both, \"delay\" and \"xdelay\". Extended example Run the following search to find the mean, standard deviation, and variance of the magnitudes of recent quakes by magnitude type. The results look something like this:", "code_examples": [{"language": "spl", "code": "... | stats stdev(*delay)"}, {"language": "spl", "code": "source=usgs place=*California* | stats count mean(mag), stdev(mag), var(mag) BY magType"}], "tables": [{"headers": [], "rows": [["This search uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance.  This example uses theAll Earthquakesdata from the past 30 days."]]}, {"headers": ["magType", "count", "mean(mag)", "std(mag)", "var(mag)"], "rows": [["H", "123", "0.549593", "0.356985", "0.127438"], ["MbLg", "1", "0.000000", "0.000000", "0.000000"], ["Md", "1565", "1.056486", "0.580042", "0.336449"], ["Me", "2", "1.800000", "0.346410", "0.120000"], ["Ml", "1202", "1.226622", "0.629664", "0.396476"], ["Mw", "6", "3.650000", "0.716240", "0.513000"], ["ml", "10", "0.934000", "0.560401", "0.314049"]]}], "chunk_index": 13, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "stdev(<value>)", "section_id": "c8827295_da68_4277_afba_2819385d6fd9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056090+00:00", "version": "10.2"}}
{"id": "0a21006411d44ccd", "content": "Description Returns the population standard deviation of the field specified. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands, and also with sparkline() charts. Basic examples Extended example", "code_examples": [], "tables": [], "chunk_index": 14, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "stdevp(<value>)", "section_id": "edffce38_879e_47c6_b8d7_524d7d3ee4ce--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056095+00:00", "version": "10.2"}}
{"id": "7045a360b44fd269", "content": "Description Returns the sum of the values of the field specified. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands, and also with sparkline() charts. Basic examples You can create totals for any numeric field. For example: The results look like this: You can rename the column using the AS keyword: The results look something like this: You can organize the results using a BY clause: The results look something like this:", "code_examples": [{"language": "spl", "code": "...| stats sum(bytes)"}, {"language": "spl", "code": "...| stats sum(bytes) AS\"total bytes\""}, {"language": "spl", "code": "...| stats sum(bytes) AS\"total bytes\"by date_hour"}], "tables": [{"headers": ["sum(bytes)"], "rows": [["21502"]]}, {"headers": ["total bytes"], "rows": [["21502"]]}, {"headers": ["date_hour", "total bytes"], "rows": [["07", "6509"], ["11", "3726"], ["15", "6569"], ["23", "4698"]]}], "chunk_index": 15, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "sum(<value>)", "section_id": "e187070e_4762_4c01_8ef0_1ab3095aa055--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056105+00:00", "version": "10.2"}}
{"id": "41f83a27ac0141df", "content": "Description Returns the sum of the squares of the values of the field specified. The sum of the squares is used to evaluate the variance of a dataset from the dataset mean. A large sum of the squares indicates a large variance, which tells you that individual values fluctuate widely from the mean. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands, and also with sparkline() charts. Basic examples The following table contains the temperatures taken every day at 8 AM for a week. You calculate the mean of the these temperatures and get 48.9 degrees. To calculate the deviation from the mean for each day, take the temperature and subtract the mean. If you square each number, you get results like this: Take the total of the squares, 954.9, and divide by 6 which is the number of days minus 1. This gets you the sum of squares for this series of temperatures. The standard deviation is the square root of the sum of the squares. The larger the standard deviation the larger the fluctuation in temperatures during the week. You can calculate the mean, sum of the squares, and standard deviation with a few statistical functions: This search returns these results:", "code_examples": [{"language": "spl", "code": "...|stats mean(temp), sumsq(temp), stdev(temp)"}], "tables": [{"headers": ["day", "temp", "mean", "deviation", "square of temperatures"], "rows": [["sunday", "65", "48.9", "16.1", "260.6"], ["monday", "42", "48.9", "-6.9", "47.0"], ["tuesday", "40", "48.9", "-8.9", "78.4"], ["wednesday", "31", "48.9", "-17.9", "318.9"], ["thursday", "47", "48.9", "-1.9", "3.4"], ["friday", "53", "48.9", "4.1", "17.2"], ["saturday", "64", "48.9", "15.1", "229.3"]]}, {"headers": ["mean(temp)", "sumsq(temp)", "stdev(temp)"], "rows": [["48.857142857142854", "17664", "12.615183595289349"]]}], "chunk_index": 16, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "sumsq(<value>)", "section_id": "id_34f1fd79_28d6_48a9_8bb4_c58e6dba9e64--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056118+00:00", "version": "10.2"}}
{"id": "48932d858893d50b", "content": "Description Returns an approximate percentile value, based on the requested percentile of the numeric field. When there are more than 1000 values, the upperperc function gives the approximate upper bound for the percentile requested. Otherwise the upperperc function returns the same percentile as the perc function. See the percentile<percentile>(<value>) function. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands, and also with sparkline() charts. Examples See the perc function.", "code_examples": [], "tables": [], "chunk_index": 17, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "upperperc<percentile>(<value>)", "section_id": "id_06c02da6_94c6_4f6e_93a2_0ae0e940b720--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056123+00:00", "version": "10.2"}}
{"id": "400b540019338646", "content": "Description Returns the sample variance of the field specified. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands, and also with sparkline() charts. Example See the Extended example for the mean() function. That example includes the var() function.", "code_examples": [], "tables": [], "chunk_index": 18, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "var(<value>)", "section_id": "id_79a0abd1_d079_4e3e_8e30_21ffce6a272f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056127+00:00", "version": "10.2"}}
{"id": "fd725ac2b2915c18", "content": "Description Returns the population variance of the field specified. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands, and also with sparkline() charts. Basic examples", "code_examples": [], "tables": [], "chunk_index": 19, "total_chunks": 20, "metadata": {"title": "Aggregate functions", "section_heading": "varp(<value>)", "section_id": "id_18535c33_5e31_42db_ab9b_0940680bc4d3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/aggregate-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:29:45.056131+00:00", "version": "10.2"}}
{"id": "093f37086c59b0cf", "content": "The chart command is a transforming command that returns your results in a table format. The results can then be used to display the data as a chart, such as a column, line, area, or pie chart. See the Visualization Reference in the Dashboards and Visualizations manual. You must specify a statistical function when you use the chart command. See Statistical and charting functions .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "chart", "section_heading": "Description", "section_id": "id_03ec316a_c120_4987_ba7a_e707fc920280--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/chart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:04.542853+00:00", "version": "10.2"}}
{"id": "28f2c172afdbcde2", "content": "The required syntax is in bold. chart [<chart-options>] [agg=<stats-agg-term>] ( <stats-agg-term> | <sparkline-agg-term> | \"(\"<eval-expression>\")\" )... [ BY <row-split> <column-split> ] | [ OVER <row-split> ] [BY <column-split>] [<dedup_splitvals>] Required arguments You must include one of the following arguments when you use the chart command. stats-agg-term Syntax: <stats-func> ( <evaled-field> | <wc-field> ) [AS <wc-field>] Description: A statistical aggregation function. See Stats function options. The function can be applied to an eval expression, or to a field or set of fields. Use the AS clause to place the result into a new field with a name that you specify. You can use wildcard characters in field names. sparkline-agg-term Syntax: <sparkline-agg> [AS <wc-field>] Description: A sparkline aggregation function. Use the AS clause to place the result into a new field with a name that you specify. You can use wild card characters in field names. See Sparkline options. eval-expression Syntax: <eval-math-exp> | <eval-concat-exp> | <eval-compare-exp> | <eval-bool-exp> | <eval-function-call> Description: A combination of literals, fields, operators, and functions that represent the value of your destination field. For more information, see the Evaluation functions. See Usage. Note: For these evaluations to work, your values need to be valid for the type of operation. For example, with the exception of addition, arithmetic operations might not produce valid results if the values are not numerical. If both operands are strings, they can be concatenated. When concatenating values with a period, the search treats both values as strings regardless of their actual type. Optional arguments agg Syntax: agg=<stats-agg-term> Description: Specify an aggregator or function. For a list of stats functions with descriptions and examples, see Statistical and charting functions. chart-options Syntax: cont | format | limit | sep Description: Options that you can specify to refine the result. See the Chart options section in this topic. Default: column-split Syntax: <field> [<tc-options>]... [<where-clause>] Description: Specifies a field to use as the columns in the result table. By default, when the result are visualized, the columns become the data series in the chart. If the field is numerical, discretization is applied using the tc-options argument. See the tc options and the where clause sections in this topic. Default: The number of columns included is limited to 10 by default. You can change the number of columns by including a <where-clause>. Note: When a column-split field is included, the output is a table where each column represents a distinct value of the split-by field. This is in contrast with the by-clause , where each row represents a single unique combination of values of the group-by fields. For additional information, see the Usage section in this topic. dedup_splitvals Syntax: dedup_splitvals=<boolean> Description: Specifies whether to remove duplicate values in multivalued BY clause fields. Default: false row-split Syntax: <field> [<bin-options>]... Description: The field that you specify becomes the first column in the results table. The field values become the row labels in the results table. In a chart, the field name is used to label the X-axis. The field values become the X-axis values. See the Bin options section in this topic. Default: None. Chart options cont Syntax: cont=<bool> Description: Specifies if the bins are continuous. If cont=false, replots the x-axis so that a noncontinuous sequence of x-value bins show up adjacently in the output. If cont=true, bins that have no values will display with a count of 0 or null values. Default: true format Syntax: format=<string> Description: Used to construct output field names when multiple data series are used in conjunction with a split-by-field. format takes precedence over sep and allows you to specify a parameterized expression with the stats aggregator and function ($AGG$) and the value of the split-by-field ($VAL$). limit Syntax: limit=(top | bottom) <int> Description: Only valid when a column-split is specified. Use the limit option to specify the number of results that should appear in the output. When you set limit=N the top or bottom N values are retained, based on the sum of each series and the prefix you have selected. If limit=0 , all results are returned. If you opt not to provide a prefix, the Splunk software provides the top results. Default: top 10 sep Syntax: sep=<string> Description: Used to construct output field names when multiple data series are used in conjunctions with a split-by field. This is equivalent to setting format to $AGG$<sep>$VAL$. Stats function options stats-func Syntax: The syntax depends on the function you use. See Usage. Description: Statistical and charting functions that you can use with the chart command. Each time you invoke the chart command, you can use one or more functions. However, you can only use one BY clause. Sparkline options Sparklines are inline charts that appear within table cells in search results and display time-based trends associated with the primary key of each row. sparkline-agg Syntax: sparkline (count(<wc-field>), <span-length>) | sparkline (<sparkline-func>(<wc-field>), <span-length>) Description: A sparkline specifier, which takes the first argument of an aggregation function on a field and an optional timespan specifier. If no timespan specifier is used, an appropriate timespan is chosen based on the time range of the search. If the sparkline is not scoped to a field, only the count aggregate function is permitted. You can use wild card characters in field names. span-length See the Span options section in this topic. sparkline-func Syntax: c() | count() | dc() | mean() | avg() | stdev() | stdevp() | var() | varp() | sum() | sumsq() | min() | max() | range() Description: Aggregation function to use to generate sparkline values. Each sparkline value is produced by applying this aggregation to the events that fall into each particular time bin. Note: The size of the sparkline is defined by settings in the limits.conf file. The sparkline_maxsize setting defines the maximum number of elements to emit for a sparkline. For more information see Add sparklines to your search results in the Search Manual. Bin options The bin options control the number and size of the bins that the search results are separated, or discretized, into. Syntax: bins | span | <start-end> | aligntime Description: Discretization options. Default: bins=300 bins Syntax: bins=<int> Description: Sets the maximum number of bins to discretize into. For example, if bin=300, the search finds the smallest bin size that results in no more than 300 distinct bins. Default: 300 span Syntax: span=<log-span> | span=<span-length> Description: Sets the size of each bin, using a span length based on time or log-based span. See the Span options section in this topic. <start-end> Syntax: end=<num> | start=<num> Description: Sets the minimum and maximum extents for numerical bins. Data outside of the [start, end] range is discarded. aligntime Syntax: aligntime=(earliest | latest | <time-specifier>) Description: Align the bin times to something other than base UNIX time (epoch 0). The aligntime option is valid only when doing a time-based discretization. Ignored if span is in days, months, or years. Span options <log-span> Syntax: [<num>]log[<num>] Description: Sets to a logarithm-based span. The first number is a coefficient. The second number is the base. If the first number is supplied, it must be a real number >= 1.0 and < base. Base, if supplied, must be real number > 1.0 (strictly greater than 1). span-length Syntax: <span>[<timescale>] Description: A span length based on time. <span> Syntax: <int> Description: The span of each bin. If using a timescale, this is used as a time range. If not, this is an absolute bucket \"length.\" <timescale> Syntax: <sec> | <min> | <hr> | <day> | <month> | <subseconds> Description: Time scale units. tc options The timechart options are part of the <column-split> argument and control the behavior of splitting search results by a field. There are options that control the number and size of the bins that the search results are separated into. There are options that control what happens when events do not contain the split-by field, and for events that do not meet the criteria of the <where-clause>. tc-options Syntax: <bin-options> | usenull=<bool> | useother=<bool> | nullstr=<string> | otherstr=<string> Description: Options for controlling the behavior of splitting by a field. bin-options See the Bin options section in this topic. nullstr Syntax: nullstr=<string> Description: Specifies the name of the field for data series for events that do not contain the split-by field. The nullstri option is only applicable when the usenull option is set to true. Default: NULL otherstr String: otherstr=<string> Description: Specifies the name of the field for data series that do not meet the criteria of the <where-clause>. The otherstr option is only applicable when the useother option is set to true. Default: OTHER usenull Syntax: usenull=<bool> Description: Controls whether or not a series is created for events that do not contain the split-by field. Default: true useother Syntax: useother=<bool> Description: Specifies if a series should be added for data series not included in the graph because the series did not meet the criteria of the <where-clause>. Default: true where clause The <where-clause> is part of the <column-split> argument. where clause Syntax: <single-agg> <where-comp> Description: Specifies the criteria for including particular data series when a field is given in the tc-by-clause. The most common use of this option is to select for spikes rather than overall mass of distribution in series selection. The default value finds the top ten series by area under the curve. Alternately one could replace sum with max to find the series with the ten highest spikes. This has no relation to the where command. single-agg Syntax: count | <stats-func>(<field>) Description: A single aggregation applied to a single field, including an evaluated field. No wildcards are allowed. The field must be specified, except when using the count aggregate function, which applies to events as a whole. <stats-func> See the Statistical functions section in this topic. <where-comp> Syntax: <wherein-comp> | <wherethresh-comp> Description: The criteria for the <where-clause>. <wherein-comp> Syntax: (in | notin) (top | bottom)<int> Description: A grouping criteria for the <where-clause>. The aggregated series value be in or not in some top or bottom grouping. <wherethresh-comp> Syntax: ( < | > ) <num> Description: A threshold for the <where-clause>. The aggregated series value must be greater than or less than the specified numeric threshold.", "code_examples": [], "tables": [{"headers": ["Time scale", "Syntax", "Description"], "rows": [["<sec>", "s | sec | secs | second | seconds", "Time scale in seconds."], ["<min>", "m | min |  mins |  minute |  minutes", "Time scale in minutes."], ["<hr>", "h | hr |  hrs |  hour | hours", "Time scale in hours."], ["<day>", "d |  day | days", "Time scale in days."], ["<month>", "mon | month |  months", "Time scale in months."], ["<subseconds>", "us | ms |  cs |  ds", "Time scale in microseconds (us), milliseconds (ms), centiseconds (cs), or deciseconds (ds)"]]}], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "chart", "section_heading": "Syntax", "section_id": "id_81ec59ca_0536_40e8_aff5_9a63c2212cd0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/chart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:04.542867+00:00", "version": "10.2"}}
{"id": "e340c3e52c6cbd4f", "content": "The chart command is a transforming command. See Command types. Evaluation expressions You can use the chart command with an eval expression. Unless you specify a split-by clause, the eval expression must be renamed. Supported functions You can use a wide range of functions with the stats command. For general information about using functions, see Statistical and charting functions. For a list of statistical functions by category, see Function list by category For an alphabetical list of statistical functions, see Alphabetical list of functions Functions and memory usage Some functions are inherently more expensive, from a memory standpoint, than other functions. For example, the distinct_count function requires far more memory than the count function. The values and list functions also can consume a lot of memory. If you are using the distinct_count function without a split-by field or with a low-cardinality split-by by field, consider replacing the distinct_count function with the the estdc function (estimated distinct count). The estdc function might result in significantly lower memory usage and run times. Apply a statistical function to all available fields Some statistical commands, such as stats , process functions that are not paired with one or more fields as if they are implicitly paired with a wildcard, so the command applies the function all available fields. For example, | stats sum is treated as if it is | stats sum(*). The chart command allows this behavior only with the count function. If you do not specify a field for count , chart applies it to all events returned by the search. If you want to apply other functions to all fields, you must make the wildcard explicit: | chart sum(*). X-axis You can specify which field is tracked on the x-axis of the chart. The x-axis variable is specified with a by field and is discretized if necessary. Charted fields are converted to numerical quantities if necessary. Unlike the timechart command which generates a chart with the _time field as the x-axis, the chart command produces a table with an arbitrary field as the x-axis. You can also specify the x-axis field after the over keyword, before any by and subsequent split-by clause. The limit and agg options allow easier specification of series filtering. The limit and agg options are ignored if an explicit where-clause is provided. Using row-split and column-split fields When a column-split field is included, the output is a table where each column represents a distinct value of the column-split field. This is in contrast with the stats command, where each row represents a single unique combination of values of the group-by fields. The number of columns included is limited to 10 by default. You can change the number of columns by including a where-clause. With the chart and timechart commands, you cannot specify the same field in a function and as the row-split field. For example, you cannot run this search. The field A is specified in the sum function and the row-split argument. You must specify a different field as in the row-split argument. Alternatively, you can work around this problem by using an eval expression. For example: Subsecond bin time spans Subsecond span timescales, which are time spans that are made up of deciseconds (ds), centiseconds (cs), milliseconds (ms), or microseconds (us), should be numbers that divide evenly into a second. For example, 1s = 1000ms. This means that valid millisecond span values are 1, 2, 4, 5, 8, 10, 20, 25, 40, 50, 100, 125, 200, 250, or 500ms. In addition, span = 1000ms is not allowed. Use span = 1s instead.", "code_examples": [{"language": "spl", "code": "... | chart sum(A) by A span=log2"}, {"language": "spl", "code": "... |evalA1=A | chart sum(A) by A1 span=log2"}], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "chart", "section_heading": "Usage", "section_id": "f2af5a1c_1311_4740_a2c5_f4a6bab63174--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/chart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:04.542875+00:00", "version": "10.2"}}
{"id": "f8607cc09cae9152", "content": "1. Chart the max(delay) for each value in a field Return the maximum delay for each value in the site field. 2. Chart the max(delay) for each value in a field, split by the value of another field Return the maximum delay for each value in the site field split by the value in the org field. 3. Chart the ratio of the average to the maximum \"delay\" for each distinct \"host\" and \"user\" pair Return the ratio of the average (mean) of the size field to the maximum \"delay\" for each distinct host and user pair. 4. Chart the maximum \"delay\" by \"size\" and separate \"size\" into bins Return the maximum value in the delay field by the size field, where size is broken down into a maximum of 10 equal sized bins. 5. Chart the average size for each distinct value in a filed Return the average (mean) value in the size field for each distinct value in the host field. 6. Chart the number of events, grouped by date and hour Return the number of events, grouped by date and hour of the day, using span to group per 7 days and 24 hours per half days. The span applies to the field immediately prior to the command. 7. Align the chart time bins to local time Align the time bins to 5am (local time). Set the span to 12h. The bins will represent 5am - 5pm, then 5pm - 5am (the next day), and so on. 8. In a multivalue BY field, remove duplicate values For each unique value of mvfield , chart the average value of field. Deduplicates the values in the mvfield .", "code_examples": [{"language": "spl", "code": "... | chart max(delay) OVER site"}, {"language": "spl", "code": "... | chart max(delay) OVER site BY org"}, {"language": "spl", "code": "... | charteval(avg(size)/max(delay)) AS ratio BY host user"}, {"language": "spl", "code": "... | chart max(delay) BY size bins=10"}, {"language": "spl", "code": "... | chart avg(size) BY host"}, {"language": "spl", "code": "... | chart count BY date_mday span=3 date_hour span=12"}, {"language": "spl", "code": "...| chart count BY _time span=12h aligntime=@d+5h"}, {"language": "spl", "code": "...| chart avg(field) BY mvfield dedup_splitval=true"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "chart", "section_heading": "Basic examples", "section_id": "id_0228cad2_443d_410b_b9f5_38bcdae5b6a0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/chart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:04.542883+00:00", "version": "10.2"}}
{"id": "52d46d1fe6de7efa", "content": "1. Specify <row-split> and <column-split> values with the chart command This example uses events that list the numeric sales for each product and quarter, for example: To summarize the data by product for each quarter, run this search: In this example, there are two fields specified in the BY clause with the chart command. The products field is referred to as the <row-split> field. In the chart, this field forms the X-axis. The quarter field is referred to as the <column-split> field. In the chart, this field forms the data series. The results appear on the Statistics tab and look something like this: Click on the Visualization tab to see the results as a chart. See the addtotals command for an example that adds a total column for each product. 2. Chart the number of different page requests for each Web server Chart the number of different page requests, GET and POST, that occurred for each Web server. This example uses eval expressions to specify the different field values for the stats command to count. The first clause uses the count() function to count the Web access events that contain the method field value GET. Then, using the AS keyword, the field that represents these results is renamed GET. The second clause does the same for POST events. The counts of both types of events are then separated by the web server, using the BY clause with the host field. The results appear on the Statistics tab and look like this: Click the Visualization tab. If necessary, format the results as a column chart. This chart displays the total count of events for each event type, GET or POST, based on the host value. 3. Chart the number of transactions by duration Create a chart to show the number of transactions based on their duration (in seconds). This search uses the transaction command to define a transaction as events that share the clientip field and fit within a ten minute time span. The transaction command creates a new field called duration , which is the difference between the timestamps for the first and last events in the transaction. (Because maxspan=10s , the duration value should not be greater than this.) The transactions are then piped into the chart command. The count() function is used to count the number of transactions and separate the count by the duration of each transaction. Because the duration is in seconds and you expect there to be many values, the search uses the span argument to bucket the duration into bins of log2 ( span=log2 ). The results appear on the Statistics tab and look something like this: Click the Visualization tab. If necessary, format the results as a column chart. In this data set, most transactions take between 0 and 2 seconds to complete. 4. Chart the average number of events in a transaction, based on transaction duration Create a chart to show the average number of events in a transaction based on the duration of the transaction. The transaction command adds two fields to the results duration and eventcount. The eventcount field tracks the number of events in a single transaction. In this search, the transactions are piped into the chart command. The avg() function is used to calculate the average number of events for each duration. Because the duration is in seconds and you expect there to be many values, the search uses the span argument to bucket the duration into bins using logarithm with a base of 2. Use the field format option to enable number formatting. Click the Visualization tab and change the display to a pie chart. Each wedge of the pie chart represents a duration for the event transactions. You can hover over a wedge to see the average values. 5. Chart customer purchases Chart how many different people bought something and what they bought at the Buttercup Games online store Yesterday. This search takes the purchase events and pipes it into the chart command. The dc() or distinct_count() function is used to count the number of unique visitors (characterized by the clientip field). This number is then charted over each hour of the day and broken out based on the category_id of the purchase. Also, because these are numeric values, the search uses the usenull=f argument to exclude fields that don't have a value. The results appear on the Statistics tab and look something like this: Click the Visualization tab. If necessary, format the results as a line chart: Each line represents a different type of product that is sold at the Buttercup Games online store. The height of each line shows the number of different people who bought the product during that hour. In general, it looks like the most popular items at the online shop were Arcade games. You can format the report as a stacked column chart, which will show you the total purchases at each hour of day. Change the chart type to a Column Chart. Use the Format menu, and on the General tab select stacked. 6. Chart the number of earthquakes and the magnitude of each earthquake Create a chart that list the number of earthquakes, and the magnitude of each earthquake that occurred in and around Alaska. Run the search using the time range All time. This search counts the number of earthquakes that occurred in the Alaska regions. The count is then broken down for each place based on the magnitude of the quake. Because the place value is non-numeric, the search uses the useother=f argument to exclude events that don't match. The results appear on the Statistics tab and look something like this: Click on the Visualization tab to view the results on a chart. This chart shows the number of earthquakes by magnitude.", "code_examples": [{"language": "spl", "code": "source=\"addtotalsData.csv\"| chart sum(sales) BY products quarter"}, {"language": "spl", "code": "sourcetype=access_* | chart count(eval(method=\"GET\")) AS GET, count(eval(method=\"POST\")) AS POST by host"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | transaction clientip maxspan=10m | chart count BY duration span=log2"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | transaction clientip maxspan=30m  | chart avg(eventcount) by duration span=log2"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | chart dc(clientip) OVER date_hour BY categoryId usenull=f"}, {"language": "spl", "code": "source=all_month.csv place=*alaska* mag>=3.5 | chart count BY mag place useother=f | rename mag AS Magnitude"}], "tables": [{"headers": ["products", "quarter", "sales"], "rows": [["ProductA", "QTR1", "1200"], ["ProductB", "QTR1", "1400"], ["ProductC", "QTR1", "1650"], ["ProductA", "QTR2", "1425"], ["ProductB", "QTR2", "1175"], ["ProductC", "QTR2", "1550"], ["ProductA", "QTR3", "1300"], ["ProductB", "QTR3", "1250"], ["ProductC", "QTR3", "1375"], ["ProductA", "QTR4", "1550"], ["ProductB", "QTR4", "1700"], ["ProductC", "QTR4", "1625"]]}, {"headers": ["products", "QTR1", "QTR2", "QTR3", "QTR4"], "rows": [["ProductA", "1200", "1425", "1300", "1550"], ["ProductB", "1400", "1175", "1250", "1700"], ["ProductC", "1650", "1550", "1375", "1625"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["host", "GET", "POST"], "rows": [["www1", "8431", "5197"], ["www2", "8097", "4815"], ["www3", "8338", "4654"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["duration", "count"], "rows": [["0", "970"], ["1-2", "593"], ["2-4", "208"], ["4-8", "173"], ["8-16", "26"], ["64-128", "3"], ["128-256", "3"], ["256-512", "12"], ["512-1024", "2"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}, {"headers": ["date_hour", "ACCESSORIES", "ARCADE", "SHOOTER", "SIMULATION", "SPORTS", "STRATEGY", "TEE"], "rows": [["0", "2", "6", "0", "4", "0", "4", "4"], ["1", "4", "7", "2", "3", "0", "10", "5"], ["2", "2", "2", "2", "1", "1", "2", "0"], ["3", "3", "5", "3", "5", "0", "7", "1"], ["4", "3", "4", "0", "0", "1", "4", "0"], ["5", "3", "0", "3", "0", "1", "6", "1"]]}, {"headers": [], "rows": [["This example uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand add it as an input."]]}, {"headers": ["Magnitude", "145km ENE of Chirikof Island, Alaska", "225km SE of Kodiak, Alaska", "250km SE of Kodiak, Alaska", "252km SE of Kodiak, Alaska", "254km SE of Kodiak, Alaska", "255km SE of Kodiak, Alaska", "259km SE of Kodiak, Alaska", "264km SE of Kodiak, Alaska", "265km SE of Kodiak, Alaska", "Gulf of Alaska"], "rows": [["3.5", "1", "1", "0", "1", "0", "1", "0", "0", "2", "2"], ["3.6", "0", "0", "1", "0", "0", "0", "0", "1", "0", "1"], ["3.7", "0", "0", "0", "0", "1", "0", "0", "0", "0", "2"], ["3.8", "0", "1", "0", "0", "0", "0", "1", "1", "0", "3"], ["3.9", "0", "0", "1", "0", "1", "0", "0", "0", "0", "0"], ["4", "0", "0", "0", "0", "1", "1", "0", "0", "0", "1"], ["4.1", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1"], ["4.2", "0", "0", "0", "1", "0", "0", "0", "0", "0", "1"], ["4.3", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1"], ["4.4", "0", "0", "0", "0", "0", "0", "1", "0", "0", "1"], ["4.6", "1", "0", "0", "0", "0", "0", "0", "0", "0", "0"], ["5", "0", "0", "0", "0", "0", "0", "0", "0", "0", "1"]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "chart", "section_heading": "Extended examples", "section_id": "id_2d2cb6e3_cb5f_458d_b724_7001c4baa77e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/chart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:04.542925+00:00", "version": "10.2"}}
{"id": "ef654880a2934bd2", "content": "Commands timechart bin sichart Blogs Search commands > stats, chart, and timechart", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "chart", "section_heading": "See also", "section_id": "id_16655b93_2cf4_42fe_9c0d_0a35c839b738--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/chart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:04.542930+00:00", "version": "10.2"}}
{"id": "b7bebda8084ee29e", "content": "Description Returns the chronologically earliest seen occurrence of a value in a field. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands. This function processes field values as strings. Basic example You run the following search to locate invalid user login attempts against a sshd (Secure Shell Daemon). You use the table command to see the values in the _time , source , and _raw fields. The results appear on the Statistics tab and look something like this: You extend the search using the earliest function. The search returns the event with the _time value 2023-04-28 00:23:28 , which is the event with the oldest timestamp.", "code_examples": [{"language": "spl", "code": "sourcetype=secure invalid user\"sshd[5258]\"| table _timesource_raw"}, {"language": "spl", "code": "sourcetype=secure invalid user\"sshd[5258]\"| table _timesource_raw | stats earliest(_raw)"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["_time", "source", "_raw"], "rows": [["2023-05-01 00:15:05", "tutorialdata.zip:./mailsv/secure.log", "Mon May 01 2023 00:15:05 mailsv1 sshd[5258]: Failed password for invalid user tomcat from 67.170.226.218 port 1490 ssh2"], ["2023-04-30 00:16:17", "tutorialdata.zip:./www2/secure.log", "Sun Apr 30 2023 00:16:17 www2 sshd[5258]: Failed password for invalid user brian from 130.253.37.97 port 4284 ssh2"], ["2023-04-30 00:11:25", "tutorialdata.zip:./www3/secure.log", "Sun Apr 30 2023 00:11:25 www3 sshd[5258]: Failed password for invalid user operator from 222.169.224.226 port 1711 ssh2"], ["2023-04-29 00:19:01", "tutorialdata.zip:./www1/secure.log", "Sat Apr 29 2023 00:19:01 www1 sshd[5258]: Failed password for invalid user rightscale from 87.194.216.51 port 3361 ssh2"], ["2023-04-29 00:13:45", "tutorialdata.zip:./mailsv/secure.log", "Sat Apr 29 2023 00:13:45 mailsv1 sshd[5258]: Failed password for invalid user testuser from 194.8.74.23 port 3626 ssh2"], ["2023-04-28 00:23:28", "tutorialdata.zip:./www1/secure.log", "Fri Apr 28 2023 00:23:28 www1 sshd[5258]: Failed password for invalid user redmine from 91.208.184.24 port 3587 ssh2"]]}, {"headers": ["_time", "source", "_raw"], "rows": [["2022-04-28 00:23:28", "tutorialdata.zip:./www1/secure.log", "Fri Apr 28 2023 00:23:28 www1 sshd[5258]: Failed password for invalid user redmine from 91.208.184.24 port 3587 ssh2"]]}], "chunk_index": 0, "total_chunks": 11, "metadata": {"title": "Time functions", "section_heading": "earliest(<value>)", "section_id": "id_95f026db_da98_4506_8aee_b21e30e4b9db--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:24.012639+00:00", "version": "10.2"}}
{"id": "81a335826317e359", "content": "Description Returns the UNIX time of the chronologically earliest-seen occurrence of a given field value. Usage You can use this function with the mstats , stats , and tstats commands. This function processes field values as strings. If you have metrics data, you can use earliest_time function in conjunction with the earliest , latest , and latest_time functions to calculate the rate of increase for a counter. Alternatively you can use the rate counter to do the same thing. Basic example The following search runs against metric data. It is designed to return the earliest UNIX time values on every minute for each metric_name that begins with deploy. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "| mstats earliest_time(_value)whereindex=_metrics metric_name=deploy* BY metric_name span=1m"}], "tables": [{"headers": ["_time", "metric_name", "earliest_time(_value)"], "rows": [["2023-12-18 09:30:00", "deploy-connections.nCurrent", "1702920600.000000"], ["2023-12-18 09:31:00", "deploy-connections.nStarted", "1702920660.000000"], ["2023-12-18 09:32:00", "deploy-server.volumeCompletedKB", "1702920720.000000"], ["2023-12-18 09:33:00", "deploy-connections.nCurrent", "1702920780.000000"], ["2023-12-18 09:34:00", "deploy-connections.nStarted", "1702920840.000000"], ["2023-12-18 09:35:00", "deploy-server.volumeCompletedKB", "1702920900.000000"]]}], "chunk_index": 1, "total_chunks": 11, "metadata": {"title": "Time functions", "section_heading": "earliest_time(<value>)", "section_id": "id_89420c39_1a93_46c9_8de4_ee63c3ec8456--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:24.012670+00:00", "version": "10.2"}}
{"id": "793d7eac89e61b63", "content": "Description Returns the chronologically latest seen occurrence of a value in a field. Usage You can use this function with the chart , mstats , stats , timechart , and tstats commands. This function processes field values as strings. Basic example You run the following search to locate invalid user login attempts against a specific sshd (Secure Shell Daemon). You use the table command to see the values in the _time , source , and _raw fields. The results appear on the Statistics tab and look something like this: You extend the search using the latest function. The search returns the event with the _time value 2023-05-01 00:15:05 , which is the event with the most recent timestamp.", "code_examples": [{"language": "spl", "code": "sourcetype=secure invalid user\"sshd[5258]\"| table _timesource_raw"}, {"language": "spl", "code": "sourcetype=secure invalid user\"sshd[5258]\"| table _timesource_raw | stats latest(_raw)"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["_time", "source", "_raw"], "rows": [["2023-05-01 00:15:05", "tutorialdata.zip:./mailsv/secure.log", "Mon May 01 2023 00:15:05 mailsv1 sshd[5258]: Failed password for invalid user tomcat from 67.170.226.218 port 1490 ssh2"], ["2023-04-30 00:16:17", "tutorialdata.zip:./www2/secure.log", "Sun Apr 30 2023 00:16:17 www2 sshd[5258]: Failed password for invalid user brian from 130.253.37.97 port 4284 ssh2"], ["2023-04-30 00:11:25", "tutorialdata.zip:./www3/secure.log", "Sun Apr 30 2023 00:11:25 www3 sshd[5258]: Failed password for invalid user operator from 222.169.224.226 port 1711 ssh2"], ["2023-04-29 00:19:01", "tutorialdata.zip:./www1/secure.log", "Sat Apr 29 2023 00:19:01 www1 sshd[5258]: Failed password for invalid user rightscale from 87.194.216.51 port 3361 ssh2"], ["2023-04-29 00:13:45", "tutorialdata.zip:./mailsv/secure.log", "Sat Apr 29 2023 00:13:45 mailsv1 sshd[5258]: Failed password for invalid user testuser from 194.8.74.23 port 3626 ssh2"], ["2023-04-28 00:23:28", "tutorialdata.zip:./www1/secure.log", "Fri Apr 28 2023 00:23:28 www1 sshd[5258]: Failed password for invalid user redmine from 91.208.184.24 port 3587 ssh2"]]}, {"headers": ["_time", "source", "_raw"], "rows": [["2023-05-01 00:15:05", "tutorialdata.zip:./mailsv/secure.log", "Mon May 01 2023 00:15:05 mailsv1 sshd[5258]: Failed password for invalid user tomcat from 67.170.226.218 port 1490 ssh2"]]}], "chunk_index": 2, "total_chunks": 11, "metadata": {"title": "Time functions", "section_heading": "latest(<value>)", "section_id": "id_39123a89_8cc7_43f2_bd34_f923a9df61e9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:24.012700+00:00", "version": "10.2"}}
{"id": "042391174cb500ec", "content": "Description Returns the UNIX time of the chronologically latest-seen occurrence of a given field value. Usage You can use this function with the mstats , stats , and tstats commands. This function processes field values as strings. If you have metrics data, you can use latest_time function in conjunction with earliest , latest , and earliest_time functions to calculate the rate of increase for a counter. Alternatively, you can use the rate function counter to do the same thing. Basic example The following search runs against metric data. It is designed to return the latest UNIX time values in the past 60 minutes for metrics with names that begin with queue. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "| mstats latest_time(_value)whereindex=_metrics metric_name=queue.* BY metric_name span=1m"}], "tables": [{"headers": ["_time", "metric_name", "latest_time(_value)"], "rows": [["2023-12-18 09:39:00", "queue.current_size", "1702921140.000000"], ["2023-12-18 09:38:00", "queue.current_size_kb", "1702921080.000000"], ["2023-12-18 09:37:00", "queue.largest_size", "1702921020.000000"], ["2023-12-18 09:36:00", "queue.max_size_kb", "1702921020.000000"], ["2023-12-18 09:35:00", "queue.smallest_size", "1702920900.000000"], ["2023-12-18 09:34:00", "queue.current_size", "1702920840.000000"], ["2023-12-18 09:33:00", "queue.current_size_kb", "1702920780.000000"], ["2023-12-18 09:32:00", "queue.largest_size", "1702920720.000000"], ["2023-12-18 09:31:00", "queue.max_size_kb", "1702920660.000000"], ["2023-12-18 09:30:00", "queue.smallest_size", "1702920600.000000"]]}], "chunk_index": 3, "total_chunks": 11, "metadata": {"title": "Time functions", "section_heading": "latest_time(<value>)", "section_id": "id_684cc64b_f98d_4ad4_811b_f4bade3db4a5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:24.012720+00:00", "version": "10.2"}}
{"id": "8340ff5a3c4a6e40", "content": "Description Returns the values in a field or eval expression for each day. Usage You can use this function with the timechart command. Basic examples The following example returns the values for the field total for each day. The following example returns the results of the eval expression eval(method=\"GET\")) AS Views. Extended example This search uses the per_day() function and eval expressions to determine how many times the web pages were viewed and how many times items were purchased. The results appear on the Statistics tab. To determine the number of Views and Purchases for each hour, minute, or second you can add the other time functions to the search. For example: Use the field format option to change the number formatting for the field values.", "code_examples": [{"language": "spl", "code": "... | timechart per_day(total)"}, {"language": "spl", "code": "... | timechart per_day(eval(method=\"GET\")) AS Views"}, {"language": "spl", "code": "sourcetype=access_* | timechart per_day(eval(method=\"GET\")) AS Views_day, per_day(eval(action=\"purchase\")) AS Purchases"}, {"language": "spl", "code": "sourcetype=access_* | timechart per_day(eval(method=\"GET\")) AS Views_day, per_hour(eval(method=\"GET\")) AS Views_hour, per_minute(eval(method=\"GET\")) AS Views_minute, per_day(eval(action=\"purchase\")) AS Purchases"}], "tables": [{"headers": [], "rows": [["This example uses the sample dataset fromthe Search Tutorialbut should work with any format of Apache Web access log. Download the data set fromthis topic in the Search Tutorialand follow the instructions to upload it to your Splunk deployment."]]}], "chunk_index": 4, "total_chunks": 11, "metadata": {"title": "Time functions", "section_heading": "per_day(<value>)", "section_id": "c339f248_f8ce_4fa4_96f2_00c25ea7d48e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:24.012730+00:00", "version": "10.2"}}
{"id": "979ef33f91ab2dbe", "content": "Description Returns the values in a field or eval expression for each hour. Usage You can use this function with the timechart command. Basic examples The following example returns the values for the field total for each hour. The following example returns the the results of the eval expression eval(method=\"POST\")) AS Views .", "code_examples": [{"language": "spl", "code": "... | timechart per_hour(total)"}, {"language": "spl", "code": "... | timechart per_hour(eval(method=\"POST\")) AS Views"}], "tables": [], "chunk_index": 5, "total_chunks": 11, "metadata": {"title": "Time functions", "section_heading": "per_hour(<value>)", "section_id": "cbeb890f_4379_4cd9_a915_0a51ff2a0308--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:24.012743+00:00", "version": "10.2"}}
{"id": "45454d8165e472df", "content": "Description Returns the values in a field or eval expression for each minute. Usage You can use this function with the timechart command. Basic examples The following example returns the values for the field total for each minute. The following example returns the the results of the eval expression eval(method=\"GET\")) AS Views .", "code_examples": [{"language": "spl", "code": "... | timechart per_minute(total)"}, {"language": "spl", "code": "... | timechart per_minute(eval(method=\"GET\")) AS Views"}], "tables": [], "chunk_index": 6, "total_chunks": 11, "metadata": {"title": "Time functions", "section_heading": "per_minute(<value>)", "section_id": "id_99aba311_0781_4070_ba68_5f94c9d5c168--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:24.012762+00:00", "version": "10.2"}}
{"id": "609459a65e617bac", "content": "Description Returns the values in a field or eval expression for each second. Usage You can use this function with the timechart command. Basic examples The following example returns the values for the field kb for each second.", "code_examples": [{"language": "spl", "code": "... | timechart per_second(kb)"}], "tables": [], "chunk_index": 7, "total_chunks": 11, "metadata": {"title": "Time functions", "section_heading": "per_second(<value>)", "section_id": "id_79a36f90_a139_4880_bf95_17d289b42818--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:24.012779+00:00", "version": "10.2"}}
{"id": "4f928f4128201108", "content": "Description Returns the per-second rate change of the value in a field. The rate function represents the following formula: The rate function also handles the largest value reset if there is at least one reset. Usage You can use this function with the mstats , stats , and tstats commands. Provides the per-second rate change for an accumulating counter metric. Accumulating counter metrics report the total counter value since the last counter reset. See Investigate counter metrics in Metrics Requires the earliest and latest values of the field to be numerical, and the earliest_time and latest_time values to be different. Requires at least two metric data points in the search time range. Should be used to provide rate information about single, rather than multiple, counters. Basic example The following search runs against metric data. It provides the hourly hit rate for a metric that provides measurements of incoming web traffic. It uses the processor filter to ensure that it is not reporting on multiple metric series ( name and processor combinations). The resulting chart shows you that the counter hit rate for the traffic.incoming metric spiked at 1 pm, 4 pm, and 11 am, but otherwise remained stable.", "code_examples": [{"language": "spl", "code": "(latest(<value>) - earliest(<value>)) / (latest_time(<value>) - earliest_time(<value>))"}, {"language": "spl", "code": "| mstats rate(traffic.incoming) as rate_hitswhereindex=_metrics name=indexerpipe processor=index_thruput span=1h"}], "tables": [], "chunk_index": 8, "total_chunks": 11, "metadata": {"title": "Time functions", "section_heading": "rate(<value>)", "section_id": "efd46c12_5d9d_4daa_83b6_c0f8efd7407d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:24.012788+00:00", "version": "10.2"}}
{"id": "f5840827c2274d60", "content": "Description Computes the per metric time series rates for an accumulating counter metric. Returns the averages of those rates. For a detailed explanation of metric time series, see Perform statistical calculations on metric time series in Metrics. Usage You can use this function with the mstats command. To ensure accurate results, Splunk software uses the latest value of a metric measurement from the previous timespan as the starting basis for a rate computation. When you calculate the average rates for accumulating counter metrics, the cleanest way to do it is to split the counter metric rate calculations out by metric time series and then compute the average rate across all of the metric time series. Unlike rate , the rate_avg function can calculate rates even when there is only a single metric data point per time series per timespan. It can pull in data across timespans to calculate rates when necessary. The rate_avg function does not support prestats=true. It needs the final list of dimensions to split by. Basic example In your _metrics index, you have data for the metric spl.intr.resource_usage.PerProcess.data.elapsed. This is an accumulating counter metric. It contains a number of metric time series. The following example search uses the rate_avg function to calculate the rate(X) for each spl.mlog.thruput.thruput.total_k_processed time series in the time range. Then it gets the average rate across all of the time series. Lastly, it splits the results by time, so they can be plotted on a chart.", "code_examples": [{"language": "spl", "code": "| mstats rate_avg(spl.mlog.thruput.thruput.total_k_processed)whereindex=_metrics span=1h"}], "tables": [], "chunk_index": 9, "total_chunks": 11, "metadata": {"title": "Time functions", "section_heading": "rate_avg(<value>)", "section_id": "a7af9300_749b_4f31_854c_d799818522a4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:24.012795+00:00", "version": "10.2"}}
{"id": "d2d0fb59ccd1c2f8", "content": "Description Computes the per metric time series rates for an accumulating counter metric. Returns the aggregate of those rates. For a detailed explanation of metric time series, see Perform statistical calculations on metric time series in Metrics. Usage You can use this function with the mstats command. To ensure accurate results, Splunk software uses the latest value of a metric measurement from the previous timespan as the starting basis for a rate computation. When you calculate the aggregated rates for accumulating counter metrics, the cleanest way to do it is to split the counter metric rate calculations out by metric time series and then compute the aggregate rate across all of the metric time series. Unlike rate , the rate_sum function can calculate rates even when there is only a single metric data point per time series per timespan. It can pull in data across timespans to calculate rates when necessary. The rate_sum function does not support prestats=true. It needs the final list of dimensions to split by. Basic example In your _metrics index, you have data for the metric spl.intr.resource_usage.PerProcess.data.elapsed. This is an accumulating counter metric. It contains a number of metric time series. The following example search uses the rate_sum function to calculate the rate(X) for each spl.mlog.thruput.thruput.total_k_processed time series in the time range. Then it gets the aggregate rate across all of the time series. Lastly, it splits the results by time, so they can be plotted on a chart.", "code_examples": [{"language": "spl", "code": "| mstats rate_sum(spl.mlog.thruput.thruput.total_k_processed)whereindex=_metrics span=1h"}], "tables": [], "chunk_index": 10, "total_chunks": 11, "metadata": {"title": "Time functions", "section_heading": "rate_sum(<value>)", "section_id": "f3540daf_2e96_49fd_ae73_a5eef8866005--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/time-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:24.012807+00:00", "version": "10.2"}}
{"id": "542cca426a55851d", "content": "Use the mstats command to analyze metrics. This command performs statistics on the measurement , metric_name , and dimension fields in metric indexes. You can use mstats in historical searches and real-time searches. When you use mstats in a real-time search with a time window, a historical search runs first to backfill the data. Note: The mstats command provides the best search performance when you use it to search a single metric_name value or a small number of metric_name values. Note: Certain restricted search commands, including mpreview and mstats might stop working if your organization uses field filters to protect sensitive data. See Plan for field filters in your organization in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "mstats", "section_heading": "Description", "section_id": "aad3a5bf_276f_47e8_9dfb_2eed4a40d16e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:42.277734+00:00", "version": "10.2"}}
{"id": "94263747b6671b85", "content": "The required syntax is in bold. | mstats [chart=<bool>] [<chart-options>] [prestats=<bool>] [append=<bool>] [backfill=<bool>] [update_period=<integer>] [fillnull_value=<string>] [chunk_size=<unsigned int>] <stats-metric-term>... WHERE [<logical-expression>]... [ (BY|GROUPBY) <field-list> ] [<span-length>] Required arguments <stats-metric-term> Syntax: <stats-func> | <stats-func-value> Description: Provides two options for performing statistical calculations on metrics. Use <stats-func> to perform statistical calculations on one or more metrics that you name in the argument. Use <stats-func-value> for cases where a wildcard can be used to represent several metrics. You cannot blend the <stats-func> syntax and the <stats-func-value syntax in a single mstats search. Use the <stats-func> syntax for most cases. You only need to use the <stats-func-value> syntax in cases where a single metric may be represented by several different metric names, such as cpu.util and cpu.utilization. In these cases you can apply a wildcard to catch all of the permutations of the metric_name. See Stats metric term options for details on the <stats-func> and <stats-func-value> syntax options. Optional arguments append Syntax: append=<bool> Description: Valid only when prestats=true. This argument runs the mstats command and adds the results to an existing set of results instead of generating new results. Default: false backfill Syntax: backfill=<bool> Description: Valid only with real-time searches that have a time window. When backfill=true , the mstats command runs a search on historical data to backfill events before searching the in-memory real-time data. Default: true chart Syntax: chart=<bool> Description: When set to chart=t , the mstats data output has a format suitable for charting. The mstats charting mode is valid only when prestats=f. When a span is provided, the mstats chart mode format resembles that of the timechart command, and can support at most one group-by field, which is used as the series splitting field. When no span is provided, the chart mode follows a format similar to that of the chart or timechart commands. Without a span , the mstats chart mode requires one or two grouping fields. The first grouping field represents the chart x-axis. The second grouping field represents the y-axis and is a series split field. Default: chart=f <chart-options> Syntax: chart.limit | chart.agg | chart.usenull | chart.useother | chart.nullstr | chart.otherstr Description: Options that you can specify to refine the result. See the Chart options section in this topic. chunk_size Syntax: chunk_size=<unsigned_int> Description: Advanced option. This argument controls how many metric time series are retrieved at a time from a single time-series index file ( .tsidx file) when the Splunk software processes searches. Lower this setting from its default only when you find a particular mstats search is using too much memory, or when it infrequently returns events. This can happen when a search groups by excessively high-cardinality dimensions (dimensions with very large amounts of distinct values). In such situations, a lower chunk_size value can make mstats searches more responsive, but potentially slower to complete. A higher chunk_size , on the other hand, can help long-running searches to complete faster, with the potential tradeoff of causing the search to be less responsive. For mstats , chunk_size cannot be set lower than 10000. Default: 10000000 (10 million) fillnull_value Description: This argument sets a user-specified value that the mstats command substitutes for null values for any field within its group-by field list. Null values include field values that are missing from a subset of the returned events as well as field values that are missing from all of the returned events. If you do not provide a fillnull_value argument, mstats omits rows for events with one or more null field values from its results. Default: empty string <field-list> Syntax: <field>, ... Description: Specifies one or more fields to group the results by. Required when using the BY clause. <logical-expression> Syntax: <time-opts>|<search-modifier>|((NOT)? <logical-expression>)|<search-modifier>|<comparison-expression>|(<logical-expression> (OR)? <logical-expression>) Description: An expression describing the filters that are applied to your search. Includes time and search modifiers, and comparison expressions. See the following sections for descriptions of each of these logical expression components. Cannot filter on metric_name. Does not support CASE or TERM directives. You also cannot use the WHERE clause to search for terms or phrases. prestats Syntax: prestats=true | false Description: Specifies whether to use the prestats format. The prestats format is a Splunk internal format that is designed to be consumed by commands that generate aggregate calculations. When you use the prestats format, you can pipe the data into the chart , stats , or timechart commands, which are designed to accept the prestats format. When prestats is set to true , instructions with the AS clause are not relevant. The field names for the aggregates are determined by the command that consumes the prestats format and produces the aggregate output. Default: false <span-length> Syntax: span=<int><timescale> [every=<int><timescale>] Description: The span of each time bin. If used with a <timescale> , the <span-length> is treated as a time range. If not, this is an absolute bucket length. If you do not specify a <span-length> , the default is auto , which means that the number of time buckets adjusts to produce a reasonable number of results. For example, if seconds are used initially for the <timescale> and too many results are returned, the <timescale> is changed to a longer value, such as minutes, to return fewer time buckets. To improve the performance of mstats searches you can optionally use the every argument in conjunction with span to cause the search to reduce the amount of data it samples per span. In other words you could design a search where the search head samples a span of only ten minutes of data for every hour covered by the search. See Span length options. update_period Syntax: update_period=<integer> Description: Valid only with real-time searches. Specifies how frequently, in milliseconds, the real-time summary for the mstats command is updated. A larger number means less frequent updates to the summary and less impact on index processing. Default: 1000 (1 second) Stats metric term options <stats-func> Syntax: <stats-func> | <mstats-specific-func> \"(\"<metric_name>\")\" [AS <string>]... Description: Perform statistical calculations on one or more metric_name fields. You can rename the result of each function using the AS clause, unless prestats is set to true. The metric_name must be enclosed in parenthesis. When you use the <stats-func> syntax, the WHERE clause cannot filter on metric_name. <mstats-specific-func> Syntax: rate_avg | rate_sum Description: Two functions that are specific to mstats. rate_avg computes the per metric time series rates for an accumulating counter metric and then returns the average of those rates. rate_sum does the same thing as rate_avg except that it returns the sum of the rates. For more about counter metrics and these functions see Investigate counter metrics in Metrics. <stats-func-value> Syntax: count(_value) | <function>(_value) [AS <string>] WHERE metric_name=<metric_name> Description: Specify a basic count of the _value field or a function on the _value field. The _value field uses a specific format to store the numeric value of the metric. You can specify one or more functions. You can rename the result of the function using AS unless prestats=true. When you use the <stats-func-value> syntax, the WHERE clause must filter on the metric_name. Wildcards are okay. Note: The stats-func-value syntax does not support real-time searches. If you must run a real-time search, use the stats-func syntax instead. The following table lists the supported functions for the mstats command by type of function. Use the links in the table to see descriptions and examples for each function. For an overview of using functions with commands, see Statistical and charting functions. Chart options chart.limit Syntax: chart.limit=(top | bottom)<int> Description: Only valid when a column-split is specified. Use the chart.limit option to specify the number of results that should appear in the output. When you set chart.limit=N the top or bottom N values are retained, based on the sum of each series and the prefix you have selected. If chart.limit=0 , all results are returned. If you opt not to provide a top or bottom prefix before the chart.limit value, the Splunk software provides the top N results. For example, if you set chart.limit=10 the Splunk software defaults to providing the top 10 results. This argument is identical to the limit argument of the chart and timechart commands. Default: top10 chart.agg Syntax: chart.agg=( <stats-func> ( <evaled-field> | <wc-field> ) [AS <wc-field>] ) Description: A statistical aggregation function. See the table of supported functions in Stats metric term options. The function can be applied to an eval expression, or to a field or set of fields. Use the AS clause to place the result into a new field with a name that you specify. You can use wild card characters in field names. This argument is identical to the agg argument of the chart and timechart commands. Default: sum chart.nullstr Syntax: chart.nullstr=<string> Description: If chart.usenull is true, this series is labeled by the value of the chart.nullstr option, and defaults to NULL. This argument is identical to the nullstr argument of the chart and timechart commands. chart.otherstr Syntax: chart.otherstr=<string> Description: If chart.useother is true, this series is labeled by the value of the code.otherstr option, and defaults to OTHER. This argument is identical to the otherstr argument of the chart and timechart commands. chart.usenull Syntax: chart.usenull=<bool> Description: Determines whether a series is created for events that do not contain the split-by field. This argument is identical to the usenull argument of the chart and timechart commands. chart.useother Syntax: chart.useother=<bool> Description: Specifies whether a series should be added for data series not included in the graph because they did not meet the criteria of the WHERE clause. This argument is identical to the useother argument of the chart and timechart commands. Logical expression options <comparison-expression> Syntax: <field><comparison-operator><value> | <field> IN (<value-list>) Description: Compares a field to a literal value or provides a list of values that can appear in the field. <search-modifier> Syntax: <sourcetype-specifier> | <host-specifier> | <source-specifier> | <splunk_server-specifier> Description: Search for events from specified fields. For example, search for one or a combination of hosts, sources, and source types. See searching with default fields in the Knowledge Manager manual. <time-opts> Syntax: [<timeformat>] (<time-modifier>)* Description: Describes the format of the <starttime> and <endtime> terms of the search. Comparison expression options <comparison-operator> Syntax: = | != | < | <= | > | >= Description: Use comparison expressions when searching field-value pairs. Comparison expressions with the equal ( = ) or not equal ( != ) operator compare string values. For example, \"1\" does not match \"1.0\". Comparison expressions with greater than or less than operators < > <= >= numerically compare two numbers and lexicographically compare other values. See Usage. <field> Syntax: <string> Description: The name of a field. <value> Syntax: <literal-value> Description: In comparison expressions, this is the literal number or string value of a field. <value-list> Syntax: (<literal-value>, <literal-value>, ...) Description: Used with the IN operator to specify two or more values. For example use error IN (400, 402, 404, 406) instead of error=400 OR error=402 OR error=404 OR error=406. Search modifier options <sourcetype-specifier> Syntax: sourcetype=<string> Description: Search for events from the specified sourcetype field. <host-specifier> Syntax: host=<string> Description: Search for events from the specified host field. <source-specifier> Syntax: source=<string> Description: Search for events from the specified source field. <splunk_server-specifier> Syntax: splunk_server=<string> Description: Search for events from a specific server. Use \"local\" to refer to the search head. Span length options every Syntax : every=<int><timescale> Description : Use in conjunction with span to search data in discrete time intervals over the full timespan of a search. The every argument is valid only when span is set to a valid value other than auto. Set the every timespan to a value that is greater than the span timespan. This method of \"downsampling\" the search data improves search performance at the expense of data granularity. For example, this search returns an average of the active_logins measurement for the first ten seconds of every twenty seconds covered by the time range of the search: | mstats avg(active_logins) span=10s every=20s Note: Month intervals for every are exactly 30 days long. Year intervals for every are exactly 365 days long. <timescale> Syntax: <sec> | <min> | <hr> | <day> | <month> | <subseconds> Description: Time scale units. Default: sec Note: mstats only supports subsecond timescales such as ms when it is searching metric indexes that are configured for millisecond timestamp resolution. For more information about enabling metrics indexes to index metric data points with millisecond timestamp precision, see: Manage Splunk Cloud Platform indexes in the Splunk Cloud Platform Admin Manual if you use Splunk Cloud Platform. Create custom indexes in Managing indexers and clusters of indexers if you use Splunk Enterprise. Time options <timeformat> Syntax: timeformat=<string> Description: Set the time format for starttime and endtime terms. Default: timeformat=%m/%d/%Y:%H:%M:%S. For more about setting exact times with the available timeformat options, see Date and time format variables. Subsecond options are only available if you are searching over a metrics index with millisecond timestamp resolution. <time-modifier> Syntax: starttime=<string> | endtime=<string> | earliest=<time_modifier> | latest=<time_modifier> Description: Specify start and end times using relative or absolute time. Note: You can also use the earliest and latest arguments to specify absolute and relative time ranges for your search. For more about the relative <time_modifier> syntax, see Time modifiers. For more information about setting absolute time ranges see Date and time format variables. Subsecond options are only available if you are searching over a metrics index with millisecond timestamp resolution. starttime Syntax: starttime=<string> Description: Events must be later or equal to this time. The starttime must match the timeformat. endtime Syntax: endtime=<string> Description: All events must be earlier or equal to this time.", "code_examples": [], "tables": [{"headers": ["Type of function", "Supported functions and syntax", "", ""], "rows": [["Aggregate functions", "avg()count()max()median()min()", "perc<num>range()stdev()stdevp()", "sum()sumsq()upperperc<num>var()varp()"], ["Time functions", "earliest()earliest_time()latest()", "latest_time()rate()", "rate_avg()rate_sum()"]]}, {"headers": ["Time scale", "Syntax", "Description"], "rows": [["<sec>", "s | sec | secs | second | seconds", "Time scale in seconds."], ["<min>", "m | min |  mins |  minute |  minutes", "Time scale in minutes."], ["<hr>", "h | hr |  hrs |  hour | hours", "Time scale in hours."], ["<day>", "d |  day | days", "Time scale in days."], ["<month>", "mon | month |  months", "Time scale in months."], ["<subseconds>", "us | ms |  cs |  ds", "Time scale in microseconds (us), milliseconds (ms), centiseconds (cs), or deciseconds (ds)"]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "mstats", "section_heading": "Syntax", "section_id": "id_9c7b826e_8ef9_4c54_b5c2_ed807a11991c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:42.277753+00:00", "version": "10.2"}}
{"id": "dd052bc53457bcdc", "content": "The mstats command is a report-generating command , except when append=true. See Command types. Generating commands use a leading pipe character and should be the first command in a search, except when append=true is specified with the command. Use the mstats command to search metrics data. The metrics data uses a specific format for the metrics fields. See Metrics data format in Metrics. Note: All metrics search commands are case sensitive. This means, for example, that mstats treats as the following as three distinct values of metric_name : cap.gear , CAP.GEAR , and Cap.Gear. mstats searches cannot return results for metric data points with metric_name fields that are empty or which contain blank spaces. Append mstats searches together The mstats command does not support subsearches. You can use the append argument to add the results of an mstats search to the results of a preceding mstats search. See the topic on the tstats command for an append usage example. Aggregations If you are using the <stats-func> syntax, numeric aggregations are only allowed on specific values of the metric_name field. The metric name must be enclosed in parenthesis. If there is no data for the specified metric_name in parenthesis, the search is still valid. If you are using the <stats-func-value> syntax, numeric aggregations are only allowed on the _value field. Aggregations are not allowed for values of any other field, including the _time field. Note: When prestats = true and you run an mstats search that uses the c and count aggregation functions without an aggregation field, the Splunk software processes them as if they are actually count(_value). In addition, any statistical functions that follow in the search string must reference the _value field. For example: | mstats count | timechart count(_value) Wildcard characters The mstats command supports wildcard characters in any search filter, with the following exceptions: You cannot use wildcard characters in the GROUP BY clause. If you are using the <stats_func_value> syntax, you cannot use wildcard characters in the _value field. If you are using wildcard characters in your aggregations and you are renaming them, your rename must have matching wildcards. For example, this search is invalid: This search is valid: Real-time mstats searches cannot utilize wildcarded metric aggregations when you use the <stats-func> syntax. For example, this search is invalid, when you set it up as a real-time search: This real-time search is valid: WHERE clause Use the WHERE clause to filter by any of the supported dimensions. If you are using the <stats-func> syntax, the WHERE clause cannot filter by metric_name. Filtering by metric_name is performed based on the metric_name fields specified with the <stats-func> argument. If you are using the <stats-func-value> syntax, the WHERE clause must filter by metric_name. The WHERE clause is case-sensitive when it filters mstats results by field values. For example, these two searches return different result sets: If you do not specify an index name in the WHERE clause, the mstats command returns results from the default metrics indexes associated with your role. If you do not specify an index name and you have no default metrics indexes associated with your role, mstats returns no results. To search against all metrics indexes use WHERE index=*. The WHERE clause must come before the BY or GROUPBY clause, if they are both used in conjunction with mstats. For more information about defining default metrics indexes for a role, see Add and edit roles with Splunk Web in Securing Splunk Enterprise. Group results by metric name and dimension You can group results by the metric_name and dimension fields. You can also group by time. You must specify a timespan using the <span-length> argument to group by time buckets. For example, span=1hr or span=auto. The <span-length> argument is separate from the BY clause and can be placed at any point in the search between clauses. Grouping by the _value or _time fields is not allowed. Group by metric time series You can group results by metric time series. A metric time series is a set of metric data points that share the same metrics and the same dimension field-value pairs. Grouping by metric time series ensures that you are not mixing up data points from different metric data sources when you perform statistical calculations on them. Use BY _timeseries to group by metric time series. The _timeseries field is internal and won't display in your results. If you want to display the _timeseries values in your search, add | rename _timeseries AS timeseries to the search. For a detailed overview of the _timeseries field with examples, see Perform statistical calculations on metric time series in Metrics. Time dimensions The mstats command does not recognize the following time-related dimensions. Subsecond bin time spans You can only use subsecond span timescales, which are time spans that are made up of deciseconds (ds), centiseconds (cs), milliseconds (ms), or microseconds (us), for mstats searches over metrics indexes that have been configured to have millisecond timestamp resolution. Subsecond span timescales should be numbers that divide evenly into a second. For example, 1s = 1000ms. This means that valid millisecond span values are 1, 2, 4, 5, 8, 10, 20, 25, 40, 50, 100, 125, 200, 250, or 500ms. In addition, span = 1000ms is not allowed. Use span = 1s instead. For more information about giving indexes millisecond timestamp resolution: For Splunk Cloud Platform: See Manage Splunk Cloud Platform indexes in the Splunk Cloud Platform Admin Manual. For Splunk Enterprise: See Create custom indexes in Managing indexes and clusters of indexes. Search over a set of indexes with varying levels of timestamp resolution If you run an mstats search over multiple metrics indexes with varying levels of timestamp resolution, the results of the search may contain results with timestamps of different resolutions. For example, say you have two metrics indexes. Your \"metrics-second\" metrics index has a second timestamp resolution. Your \"metrics-ms\" metrics index has a millisecond timestamp resolution. You run the following search over both indexes: | mstats count(*) WHERE index=metric* span=100ms. The search produces the following results: The 11549496110 row counts results from both indexes. The count from \"metric-ms\" includes only metric data points with timestamps from 1549496110.000 to 1549496110.099. The \"metric-ms\" metric data points with timestamps from 1549496110.100 to 1549496110.199 appear in the 1549496110.100 row. Meanwhile, the metric data points in the \"metric-second\" index do not have millisecond timestamp precision. The 1549496110 row only counts those \"metric-second\" metric data points with the 11549496110 timestamp, and no metric data points from \"metric-second\" are counted in the 1549496110.100 row. Time bin limits for mstats search jobs Splunk software regulates mstats search jobs that use span or a similar method to group results by time. When Splunk software processes these jobs, it limits the number of \"time bins\" that can be allocated within a single .tsidx file. For metrics indexes with second timestamp resolution, this only affects searches with large time ranges and very small time spans, such as a search over a year with span = 1s. If you are searching on a metrics index with millisecond timestamp resolution, you might encounter this limit over shorter ranges, such as a search over an hour with span = 1ms. This limit is set by time_bin_limit in limits.conf , which is set to 1 million bins by default. If you need to run these kinds of mstats search jobs, lower this value if they are using too much memory per search. Raise this value if these kinds of search jobs are returning errors. The Splunk platform estimates the number of time bins that a search requires by dividing the search time range by its group-by span. If this produces a number that is larger than the time_bin_limit , the Splunk platform returns an error. The search time range is determined by the earliest and latest values of the search. Some kinds of searches, such as all-time searches, do not have earliest and latest. In such cases the Splunk platform checks within each single TSIDX file to derive a time range for the search. Note: Metrics indexes have second timestamp resolution by default. You can give a metrics index a millisecond timestamp resolution when you create it, or you can edit an existing metrics index to switch it to millisecond timestamp resolution. If you use Splunk Cloud, see Manage Splunk Cloud Platform indexes in the Splunk Cloud Platform Admin Manual. If you use Splunk Enterprise, see Create custom indexes in Managing indexes and clusters of indexes. Memory and mstats search performance A pair of limits.conf settings strike a balance between the performance of mstats searches and the amount of memory they use during the search process, in RAM and on disk. If your mstats searches are consistently slow to complete you can adjust these settings to improve their performance, but at the cost of increased search-time memory usage, which can lead to search failures. If you use Splunk Cloud Platform, you will need to file a Support ticket to change these settings. For more information, see Memory and stats search performance in the Search Manual. Lexicographical order Lexicographical order sorts items based on the values used to encode the items in computer memory. In Splunk software, this is almost always UTF-8 encoding, which is a superset of ASCII. Numbers are sorted before letters. Numbers are sorted based on the first digit. For example, the numbers 10, 9, 70, 100 are sorted lexicographically as 10, 100, 70, 9. Uppercase letters are sorted before lowercase letters. Symbols are not standard. Some symbols are sorted before numeric values. Other symbols are sorted before or after letters. You can specify a custom sort order that overrides the lexicographical order. See the blog Order Up! Custom Sort Orders .", "code_examples": [{"language": "spl", "code": "| mstats sum(*.free) as FreeSum"}, {"language": "spl", "code": "| mstats sum(*.free) as *FreeSum"}, {"language": "spl", "code": "| mstats avg(cpu.*) max(cpu.*)whereindex=sysmetrics"}, {"language": "spl", "code": "| mstats avg(cpu.sys) max(cpu.usr)whereindex=sysmetrics"}, {"language": "spl", "code": "| mstats max(df.used) as\"Disk Utilization\"WHERE (itsi_entity_type_nix_metrics_indexes) AND host=test"}, {"language": "spl", "code": "| mstats max(df.used) as\"Disk Utilization\"WHERE (itsi_entity_type_nix_metrics_indexes) AND host=Test"}], "tables": [{"headers": ["Unsupported dimensions", "", ""], "rows": [["date_hourdate_mdaydate_minutedate_monthdate_second", "date_wdaydate_yeardate_zonemetric_timestamptime", "timeendpostimestamptimestartpos"]]}, {"headers": ["_time", "count(cpu.nice)"], "rows": [["1549496110", "48"], ["1549496110.100", "2"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "mstats", "section_heading": "Usage", "section_id": "id_6f2851d3_9869_4939_9bd1_78e0690b7eb9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:42.277766+00:00", "version": "10.2"}}
{"id": "e2c5a3f08b402a6a", "content": "1. Calculate a single metric grouped by time Return the average value of the aws.ec2.CPUUtilization metric in the mymetricdata metric index. Bucket the results into 30 second time spans. 2. Combine metrics with different metric names Return the average value of both the aws.ec2.CPUUtilization metric and the os.cpu.utilization metric. Group the results by host and bucket the results into 1 minute time spans. Both metrics are combined and considered a single metric series. 3. Use chart=t mode to chart metric event counts by the top ten hosts Return a chart of the number of aws.ec2.CPUUtilization metric data points for each day, split by the top ten hosts. 4. Filter the results on a dimension value and split by the values of another dimension Return the average value of the aws.ec2.CPUUtilization metric for all measurements with host=www2 and split the results by the values of the app dimension. 5. Specify multiple aggregations of multiple metrics Return the average and maximum of the resident set size and virtual memory size. Group the results by metric_name and bucket them into 1 minute spans 6. Aggregate a metric across all of your default metrics indexes, using downsampling to speed up the search Find the median of the aws.ec2.CPUUtilization metric. Do not include an index filter to search for measurements in all of the default metrics indexes associated with your role. Speed up the search by using every to compute the median for one minute of every five minutes covered by the search. 7. Get the rate of an accumulating counter metric and group the results by time series See Perform statistical calculations on metric time series in Metrics for more information. 8. Stats-func-value example Use the <stats-func-value> syntax to get a count of all of the measurements for the aws.ec2.CPUUtilization metric in the mymetricdata index.", "code_examples": [{"language": "spl", "code": "| mstats avg(aws.ec2.CPUUtilization) WHERE index=mymetricdata span=30s"}, {"language": "spl", "code": "| mstats avg(aws.ec2.CPUUtilization) avg(os.cpu.utilization) WHERE index=mymetricdata BY host span=1m"}, {"language": "spl", "code": "| mstats chart=t count(aws.ec2.CPUUtilization) WHERE index=mymetricdata by host span=1d chart.limit=top10"}, {"language": "spl", "code": "| mstats avg(aws.ec2.CPUUtilization) WHERE host=www2 BY app"}, {"language": "spl", "code": "| mstats avg(os.mem.rss) AS\"AverageRSS\"max(os.mem.rss) AS\"MaxRSS\"avg(os.mem.vsz) AS\"AverageVMS\"max(os.mem.vsz) AS\"MaxVMS\"WHERE index=mymetricdata BY metric_name span=1m"}, {"language": "spl", "code": "| mstats median(aws.ec2.CPUUtilization) span=1m every=5m"}, {"language": "spl", "code": "| mstats rate(spl.intr.resource_usage.PerProcess.data.elapsed) as data.elapsedwhereindex=_metrics BY _timeseries | rename _timeseries AS timeseries"}, {"language": "spl", "code": "| mstats count(_value) WHERE metric_name=aws.ec2.CPUUtilization AND index=mymetricdata"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "mstats", "section_heading": "Examples", "section_id": "id_486dc98c_8525_40d5_bf85_ec647bf2cb03--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:42.277775+00:00", "version": "10.2"}}
{"id": "424dde7d6dc3ad07", "content": "Related information Overview of metrics in Metrics", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "mstats", "section_heading": "See also", "section_id": "a96d9dde_ed25_4162_a65e_d332c370916c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:42.277779+00:00", "version": "10.2"}}
{"id": "5f05c7c05b8db019", "content": "Description The list function returns a multivalue entry from the values in a field. The order of the values reflects the order of the events. Usage You can use this function with the chart , stats , and timechart commands. If more than 100 values are in a field, only the first 100 are returned. This function processes field values as strings. Basic example To illustrate what the list function does, let's start by generating a few simple results. Use the makeresults and streamstats commands to generate a set of results that are simply timestamps and a count of the results which are used as row numbers. The results appear on the Statistics tab and look something like this: Notice that each result appears on a separate row. Add the stats command with the list function to the search. The numbers are returned in ascending order in a single, multivalue result. The results appear on the Statistics tab and look something like this: Notice that it is a single result. There are no alternating row background colors. Compare this result with the results returned by the values function.", "code_examples": [{"language": "spl", "code": "| makeresults count=1000 | streamstats count AS rowNumber"}, {"language": "spl", "code": "| makeresults count=1000 | streamstats count AS rowNumber | stats list(rowNumber) AS numbers"}], "tables": [{"headers": ["_time", "rowNumber"], "rows": [["2018-04-02 20:27:11", "1"], ["2018-04-02 20:27:11", "2"], ["2018-04-02 20:27:11", "3"], ["2018-04-02 20:27:11", "4"], ["2018-04-02 20:27:11", "5"]]}, {"headers": ["numbers"], "rows": [["12345"]]}], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Multivalue stats and chart functions", "section_heading": "list(<value>)", "section_id": "id_51b0d21a_a314_46a0_827c_1e13d57f2256--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/multivalue-stats-and-chart-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:58.716650+00:00", "version": "10.2"}}
{"id": "d758f7c46ad7a9cc", "content": "Description The values function returns a list of the distinct values in a field as a multivalue entry. The order of the values is lexicographical. Usage You can use the values(X) function with the chart , stats , timechart , and tstats commands. By default there is no limit to the number of values returned. Users with the appropriate permissions can specify a limit in the limits.conf file. You specify the limit in the [stats | sistats] stanza using the maxvalues setting. This function processes field values as strings. Lexicographical order Lexicographical order sorts items based on the values used to encode the items in computer memory. In Splunk software, this is almost always UTF-8 encoding, which is a superset of ASCII. Numbers are sorted before letters. Numbers are sorted based on the first digit. For example, the numbers 10, 9, 70, 100 are sorted lexicographically as 10, 100, 70, 9. Uppercase letters are sorted before lowercase letters. Symbols are not standard. Some symbols are sorted before numeric values. Other symbols are sorted before or after letters. Basic example To illustrate what the values function does, let's start by generating a few simple results. Use the makeresults and streamstats commands to generate a set of results that are simply timestamps and a count of the results, which are used as row numbers. The results appear on the Statistics tab and look something like this: Notice that each result appears on a separate row. Add the stats command with the values function to the search. The results are returned in lexicographical order. The results appear on the Statistics tab and look something like this: Notice that it is a single result. There are no alternating row background colors. Compare these results with the results returned by the list function.", "code_examples": [{"language": "spl", "code": "| makeresults count=1000 | streamstats count AS rowNumber"}, {"language": "spl", "code": "| makeresults count=1000 | streamstats count AS rowNumber | stats values(rowNumber) AS numbers"}], "tables": [{"headers": ["_time", "rowNumber"], "rows": [["2018-04-02 20:27:11", "1"], ["2018-04-02 20:27:11", "2"], ["2018-04-02 20:27:11", "3"], ["2018-04-02 20:27:11", "4"], ["2018-04-02 20:27:11", "5"]]}, {"headers": ["numbers"], "rows": [["110100100010110210310410510610710810911110"]]}], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Multivalue stats and chart functions", "section_heading": "values(<values>)", "section_id": "id_214f2cf6_9560_45b0_b477_3c97c0ac9a7b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/multivalue-stats-and-chart-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:30:58.716661+00:00", "version": "10.2"}}
{"id": "d8c435a651dfb890", "content": "Description Returns the first seen value in a field. The first seen value of the field is the most recent instance of this field, based on the order in which the events are seen by the stats command. The order in which the events are seen is not necessarily chronological order. Usage You can use this function with the chart , stats , and timechart commands. To locate the first value based on time order, use the earliest function instead. This function works best when the search includes the sort command immediately before the statistics or charting command. This function processes field values as strings. Basic example You run the following search to locate invalid user login attempts against a specific sshd (Secure Shell Daemon). You use the table command to see the values in the _time , source , and _raw fields. The results look something like this: You extend the search using the first function. The search returns the value for _raw field with the timestamp 2020-04-28 00:15:05 , which is the first event in the original list of values returned. Extended example The Basic example uses the _raw field to show how the first function works. That's useful because the _raw field contains a timestamp. However, you can use the first function on any field. Let's start by creating some results. You can use the makeresults command to create a series of results to test your search syntax. Include the streamstats command to count your results: The results look like this: With the count field, you can create different dates in the _time field, using the eval command. Use 3600, the number of seconds in an hour, to create a series of hours. The calculation multiplies the value in the count field by the number of seconds in an hour. The result is subtracted from the original _time field to get new dates equivalent to 1 hours ago, 2 hours ago, and so forth. The results look like this: The hours in the results begin with the 1 hour earlier than the original date, 2020-05-09 at 14:24. The minutes and seconds are slightly different because the date is refreshed each time you run the search. Use the eval command to add a field to your search with values in descending order: The results look like this: As you can see from the results, the first result contains the highest number in field1. This shows the order in which the results were processed. The first result was processed first (20-1=19) followed by the remaining results in order. When you add the first function to the search, the only value returned is the value in the field you specify: The results look like this:", "code_examples": [{"language": "spl", "code": "sourcetype=secure invalid user\"sshd[5258]\"| table _timesource_raw"}, {"language": "spl", "code": "sourcetype=secure invalid user\"sshd[5258]\"| table _timesource_raw | stats first(_raw)"}, {"language": "spl", "code": "| makeresults count=5 \n| streamstats count"}, {"language": "spl", "code": "| makeresults count=5 \n| streamstats count\n|eval_time=_time-(count*3600)"}, {"language": "spl", "code": "| makeresults count=5 \n| streamstats count\n|eval_time=_time-(count*3600)\n|evalfield1=20-count"}, {"language": "spl", "code": "| makeresults count=5 \n| streamstats count\n|eval_time=_time-(count*3600)\n|evalfield1=20-count\n| stats first(field1)"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["_time", "source", "_raw"], "rows": [["2020-04-28 00:15:05", "tutorialdata.zip:./mailsv/secure.log", "Mon Apr 28 2020 00:15:05 mailsv1 sshd[5258]: Failed password for invalid user tomcat from 67.170.226.218 port 1490 ssh2"], ["2020-05-01 00:15:04", "tutorialdata.zip:./www2/secure.log", "Thu May 01 2020 00:15:04 www2 sshd[5258]: Failed password for invalid user brian from 130.253.37.97 port 4284 ssh2"], ["2020-04-30 00:15:02", "tutorialdata.zip:./www3/secure.log", "Wed Apr 30 2020 00:15:02 www3 sshd[5258]: Failed password for invalid user operator from 222.169.224.226 port 1711 ssh2"], ["2020-04-28 00:15:01", "tutorialdata.zip:./www1/secure.log", "Mon Apr 28 2020 00:15:01 www1 sshd[5258]: Failed password for invalid user rightscale from 87.194.216.51 port 3361 ssh2"], ["2020-05-01 00:15:05", "tutorialdata.zip:./mailsv/secure.log", "Thu May 01 2020 00:15:05 mailsv1 sshd[5258]: Failed password for invalid user testuser from 194.8.74.23 port 3626 ssh2"], ["2020-04-27 00:15:01", "tutorialdata.zip:./www1/secure.log", "Sun Apr 27 2020 00:15:01 www1 sshd[5258]: Failed password for invalid user redmine from 91.208.184.24 port 3587 ssh2"]]}, {"headers": ["first(_raw)"], "rows": [["Mon Apr 28 2020 00:15:05 mailsv1 sshd[5258]: Failed password for invalid user tomcat from 67.170.226.218 port 1490 ssh2"]]}, {"headers": ["_time", "count"], "rows": [["2020-05-09 14:35:58", "1"], ["2020-05-09 14:35:58", "2"], ["2020-05-09 14:35:58", "3"], ["2020-05-09 14:35:58", "4"], ["2020-05-09 14:35:58", "5"]]}, {"headers": ["_time", "count"], "rows": [["2020-05-09 13:45:24", "1"], ["2020-05-09 12:45:24", "2"], ["2020-05-09 11:45:24", "3"], ["2020-05-09 10:45:24", "4"], ["2020-05-09 09:45:24", "5"]]}, {"headers": ["_time", "count", "field1"], "rows": [["2020-05-09 14:45:24", "1", "19"], ["2020-05-09 13:45:24", "2", "18"], ["2020-05-09 12:45:24", "3", "17"], ["2020-05-09 11:45:24", "4", "16"], ["2020-05-09 10:45:24", "5", "15"]]}, {"headers": ["first(field1)"], "rows": [["19"]]}], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Event order functions", "section_heading": "first(<value>)", "section_id": "id_64bee018_d8b7_45ce_a21e_2a6b443f1264--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/event-order-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:16.249219+00:00", "version": "10.2"}}
{"id": "7e396931c485ae6b", "content": "Description Returns the last seen value in a field. The last seen value of the field is the oldest instance of this field, based on the order in which the events are seen by the stats command. The order in which the events are seen is not necessarily chronological order. Usage You can use this function with the chart , stats , and timechart commands. To locate the last value based on time order, use the latest function instead. This function works best when the search includes the sort command immediately before the statistics or charting command. This function processes field values as strings. Basic example The following example returns the first \"log_level\" value for each distinct \"sourcetype\". You run the following search to locate invalid user login attempts against a specific sshd (Secure Shell Daemon). You use the table command to see the values in the _time , source , and _raw fields. The results appear on the Statistics tab and look something like this: You extend the search using the last function. The search returns the event with the _time value 2020-04-27 00:15:01 , which is the last event in the list of events. However it is not the last chronological event. Extended example The Basic example uses the _raw field to show how the last function works. That's useful because the _raw field contains a timestamp. However, you can use the last function on any field. Let's start by creating some results. You can use the makeresults command to create a series of results to test your search syntax. Include the streamstats command to count your results: The results look like this: With the count field, you can create different dates in the _time field, using the eval command. Use 86400, the number of seconds in a day, to create a series of days. The calculation multiplies the value in the count field by the number of seconds in an day. The result is subtracted from the original _time field to get new dates equivalent to 1 day ago, 2 days ago, and so forth. The results look like this: The dates in the results begin with the 1 day earlier than the original date, 2020-05-09 at 14:45:24. The minutes and seconds are slightly different because the date is refreshed each time you run the search. Use the eval command to add a field to your search with values in descending order: The results look like this: As you can see from the results, the last result contains the lowest number in field1. This shows the order in which the results were processed. The fifth result was processed last (20-5=15) after all of the other results. When you add the last function to the search, the only value returned is the value in the field you specify: The results look like this:", "code_examples": [{"language": "spl", "code": "sourcetype=secure invalid user\"sshd[5258]\"| table _timesource_raw"}, {"language": "spl", "code": "sourcetype=secure invalid user\"sshd[5258]\"| table _timesource_raw | stats last(_raw)"}, {"language": "spl", "code": "| makeresults count=5 \n| streamstats count"}, {"language": "spl", "code": "| makeresults count=5 \n| streamstats count\n|eval_time=_time-(count*86400)"}, {"language": "spl", "code": "| makeresults count=5 \n| streamstats count\n|eval_time=_time-(count*86400)\n|evalfield1=20-count"}, {"language": "spl", "code": "| makeresults count=5 \n| streamstats count\n|eval_time=_time-(count*86400)\n|evalfield1=20-count\n| stats last(field1)"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["_time", "source", "_raw"], "rows": [["2020-04-28 00:15:05", "tutorialdata.zip:./mailsv/secure.log", "Mon Apr 28 2020 00:15:05 mailsv1 sshd[5258]: Failed password for invalid user tomcat from 67.170.226.218 port 1490 ssh2"], ["2020-05-01 00:15:04", "tutorialdata.zip:./www2/secure.log", "Thu May 01 2020 00:15:04 www2 sshd[5258]: Failed password for invalid user brian from 130.253.37.97 port 4284 ssh2"], ["2020-04-30 00:15:02", "tutorialdata.zip:./www3/secure.log", "Wed Apr 30 2020 00:15:02 www3 sshd[5258]: Failed password for invalid user operator from 222.169.224.226 port 1711 ssh2"], ["2020-04-28 00:15:01", "tutorialdata.zip:./www1/secure.log", "Mon Apr 28 2020 00:15:01 www1 sshd[5258]: Failed password for invalid user rightscale from 87.194.216.51 port 3361 ssh2"], ["2020-05-01 00:15:05", "tutorialdata.zip:./mailsv/secure.log", "Thu May 01 2020 00:15:05 mailsv1 sshd[5258]: Failed password for invalid user testuser from 194.8.74.23 port 3626 ssh2"], ["2020-04-27 00:15:01", "tutorialdata.zip:./www1/secure.log", "Sun Apr 27 2020 00:15:01 www1 sshd[5258]: Failed password for invalid user redmine from 91.208.184.24 port 3587 ssh2"]]}, {"headers": ["_time", "source", "_raw"], "rows": [["2020-04-27 00:15:01", "tutorialdata.zip:./www1/secure.log", "Sun Apr 27 2020 00:15:01 www1 sshd[5258]: Failed password for invalid user redmine from 91.208.184.24 port 3587 ssh2"]]}, {"headers": ["_time", "count"], "rows": [["2020-05-09 14:35:58", "1"], ["2020-05-09 14:35:58", "2"], ["2020-05-09 14:35:58", "3"], ["2020-05-09 14:35:58", "4"], ["2020-05-09 14:35:58", "5"]]}, {"headers": ["_time", "count"], "rows": [["2020-05-08 14:45:24", "1"], ["2020-05-07 14:45:24", "2"], ["2020-05-06 14:45:24", "3"], ["2020-05-05 14:45:24", "4"], ["2020-05-04 14:45:24", "5"]]}, {"headers": ["_time", "count", "field1"], "rows": [["2020-05-08 14:45:24", "1", "19"], ["2020-05-07 14:45:24", "2", "18"], ["2020-05-06 14:45:24", "3", "17"], ["2020-05-05 14:45:24", "4", "16"], ["2020-05-04 14:45:24", "5", "15"]]}, {"headers": ["lastfield1)"], "rows": [["15"]]}], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Event order functions", "section_heading": "last(<value>)", "section_id": "id_11b43faa_861f_4b38_b401_728b2d699d48--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/event-order-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:16.249236+00:00", "version": "10.2"}}
{"id": "1f4b652e61c932f2", "content": "Commands eval makeresults", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Event order functions", "section_heading": "See also", "section_id": "id_75457955_0886_4aea_b0e7_565e9ca883a0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/statistical-and-charting-functions/event-order-functions", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Statistical and Charting Functions", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:16.249241+00:00", "version": "10.2"}}
{"id": "c1a712911fe90c8c", "content": "The summary indexing version of the chart command. The sichart command populates a summary index with the statistics necessary to generate a chart visualization. For example, it can create a column, line, area, or pie chart. After you populate the summary index, you can use the chart command with the exact same search that you used with the sichart command to search against the summary index.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "sichart", "section_heading": "Description", "section_id": "feaae33d_dd95_466e_8ae1_ca114abe1b2c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sichart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:33.319950+00:00", "version": "10.2"}}
{"id": "9fbd38f748a7eadc", "content": "Required syntax is in bold. sichart [sep=<string>] [format=<string>] [cont=<bool>] [limit=<int>] [agg=<stats-agg-term>] ( <stats-agg-term> | <sparkline-agg-term> | \"(\"<eval-expression>\")\" )... [ BY <field> [<bins-options>... ] [<split-by-clause>] ] | [ OVER <field> [<bins-options>...] [BY <split-by-clause>] ] For syntax descriptions, refer to the chart command.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "sichart", "section_heading": "Syntax", "section_id": "id_8c692709_938c_42f0_8a2e_da9f0daeaca4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sichart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:33.319957+00:00", "version": "10.2"}}
{"id": "216054655aa5c305", "content": "Supported functions You can use a wide range of functions with the sichart command. For general information about using functions, see Statistical and charting functions. For a list of functions by category, see Function list by category For an alphabetical list of functions, see Alphabetical list of functions", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "sichart", "section_heading": "Usage", "section_id": "ff8ca9a5_d5a1_4b4a_9097_dd90a8c6ea5d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sichart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:33.319963+00:00", "version": "10.2"}}
{"id": "e13d32766318411d", "content": "Example 1: Compute the necessary information to later do 'chart avg(foo) by bar' on summary indexed results.", "code_examples": [{"language": "spl", "code": "... | sichart avg(foo) by bar"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "sichart", "section_heading": "Examples", "section_id": "id_3771ad85_7431_4933_bf19_bcda4be91ada--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sichart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:33.319968+00:00", "version": "10.2"}}
{"id": "dd6a4dccbcef29f1", "content": "chart , collect , overlap , sirare , sistats , sitimechart , sitop", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "sichart", "section_heading": "See also", "section_id": "id_98339c57_98c5_44e0_9aeb_94187bc185ad--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sichart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:33.319971+00:00", "version": "10.2"}}
{"id": "92922401ea41212b", "content": "Use the geostats command to generate statistics to display geographic data and summarize the data on maps. The command generates statistics which are clustered into geographical bins to be rendered on a world map. The events are clustered based on latitude and longitude fields in the events. Statistics are then evaluated on the generated clusters. The statistics can be grouped or split by fields using a BY clause. For map rendering and zooming efficiency, the geostats command generates clustered statistics at a variety of zoom levels in one search, the visualization selecting among them. The quantity of zoom levels is controlled by the binspanlat , binspanlong , and maxzoomlevel options. The initial granularity is selected by the binspanlat and the binspanlong. At each level of zoom, the number of bins is doubled in both dimensions for a total of 4 times as many bins for each zoom in.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "geostats", "section_heading": "Description", "section_id": "id_5e460108_ddb2_4e61_afb4_fb525f4cf1a5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geostats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:51.187793+00:00", "version": "10.2"}}
{"id": "9d167ed438b7d7d4", "content": "The required syntax is in bold. geostats [ translatetoxy=<bool> ] [ latfield=<string> ] [ longfield=<string> ] [ globallimit=<int> ] [ locallimit=<int> ] [ outputlatfield=<string> ] [ outputlongfield=<string> ] [ binspanlat=<float> binspanlong=<float> ] [ maxzoomlevel=<int> ] <stats-agg-term>... [ <by-clause> ] Required arguments stats-agg-term Syntax: <stats-func> ( <evaled-field> | <wc-field> ) [AS <wc-field>] Description: A statistical aggregation function. See Stats function options. The function can be applied to an eval expression, or to a field or set of fields. Use the AS clause to place the result into a new field with a name that you specify. You can use wild card characters in field names. For more information on eval expressions, see Types of eval expressions in the Search Manual. Optional arguments binspanlat Syntax: binspanlat=<float> Description: The size of the bins in latitude degrees at the lowest zoom level. If you set binspanlat lower than the default value, the visualizations on the map might not render. Default: 22.5. If the default values for binspanlat and binspanlong are used, a grid size of 8x8 is generated. binspanlong Syntax: binspanlong=<float> Description: The size of the bins in longitude degrees at the lowest zoom level. If you set binspanlong lower than 33, the visualizations on the map might not render. Default: 45.0. If the default values for binspanlat and binspanlong are used, a grid size of 8x8 is generated. by-clause Syntax: BY <field> Description: The name of the field to group by. globallimit Syntax: globallimit=<int> Description: Controls the number of named categories to add to each pie chart. There is one additional category called \"OTHER\" under which all other split-by values are grouped. Setting globallimit=0 removes all limits and all categories are rendered. Currently the grouping into \"OTHER\" only works intuitively for count and additive statistics. Default: 10 locallimit Syntax : locallimit=<int> Description: Specifies the limit for series filtering. When you set locallimit= N , the top N values are filtered based on the sum of each series. If locallimit=0 , no filtering occurs. Default: 10 latfield Syntax: latfield=<field> Description: Specify a field from the pre-search that represents the latitude coordinates to use in your analysis. Defaults: lat longfield Syntax: longfield=<field> Description: Specify a field from the pre-search that represents the longitude coordinates to use in your analysis. Default: lon maxzoomlevel Syntax: maxzoomlevel=<int> Description: The maximum number of levels to create in the quadtree. Default: 9. Specifies that 10 zoom levels are created, 0-9. outputlatfield Syntax: outputlatfield=<string> Description: Specify a name for the latitude field in your geostats output data. Default: latitude outputlongfield Syntax: outputlongfield=<string> Description: Specify a name for the longitude field in your geostats output data. Default: longitude translatetoxy Syntax: translatetoxy=<bool> Description: If true, geostats produces one result per each locationally binned location. This mode is appropriate for rendering on a map. If false, geostats produces one result per category (or tuple of a multiply split dataset) per locationally binned location. Essentially this causes the data to be broken down by category. This mode cannot be rendered on a map. Default: true Stats function options stats-func Syntax: The syntax depends on the function that you use. See Usage. Description: Statistical and charting functions that you can use with the geostats command. Each time you invoke the geostats command, you can use one or more functions.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "geostats", "section_heading": "Syntax", "section_id": "id_591c707d_4fc0_4c07_8aa9_c2a176604dd5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geostats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:51.187803+00:00", "version": "10.2"}}
{"id": "51ff26f0553dd918", "content": "To display the information on a map, you must run a reporting search with the geostats command. If you are using a lookup command before the geostats command, see Optimizing your lookup search. Supported functions You can use a wide range of functions with the geostats command. For general information about using functions, see Statistical and charting functions. For a list of statistical functions by category, see Function list by category For an alphabetical list of statistical functions, see Alphabetical list of functions Memory and geostats search performance A pair of limits.conf settings strike a balance between the performance of geostats searches and the amount of memory they use during the search process, in RAM and on disk. If your geostats searches are consistently slow to complete you can adjust these settings to improve their performance, but at the cost of increased search-time memory usage, which can lead to search failures. For more information, see Memory and stats search performance in the Search Manual .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "geostats", "section_heading": "Usage", "section_id": "id_5ba68078_ccf6_4bc3_bf87_be08e675dbf7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geostats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:51.187808+00:00", "version": "10.2"}}
{"id": "ae2921de7ef9a21b", "content": "1. Use the default settings and calculate the count Cluster events by default latitude and longitude fields \"lat\" and \"lon\" respectively. Calculate the count of the events. 2. Specify the latfield and longfield and calculate the average of a field Compute the average rating for each gender after clustering/grouping the events by \"eventlat\" and \"eventlong\" values.", "code_examples": [{"language": "spl", "code": "... | geostats count"}, {"language": "spl", "code": "... | geostats latfield=eventlat longfield=eventlong avg(rating) by gender"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "geostats", "section_heading": "Basic examples", "section_id": "id_2d750293_aa2f_4aea_bbf3_30e8355a3e77--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geostats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:51.187813+00:00", "version": "10.2"}}
{"id": "d2b391e361759dcd", "content": "3. Count each product sold by a vendor and display the information on a map This search uses the stats command to narrow down the number of events that the lookup and geostats commands need to process. Use the following search to count each product sold by a vendor and display the information on a map. In this example, sourcetype=vendor_sales is associated with a log file that is included in the Search Tutorial sample data. This log file contains vendor information that looks like this: The vendors_lookup is used to output all the fields in vendors.csv file that match to the VentorID in the vendor_sales.log file. The fields in the vendors.csv file are : Vendor, VendorCity, VendorID, VendorLatitude, VendorLongitude, VendorStateProvince, and VendorCountry. The prices_lookup is used to match the Code field in each event to a product_name in the table. This search produces a table displayed on the Statistics tab: Click the Visualization tab. The results are plotted on a world map. There is a pie chart for each vendor in the results. The larger the pie chart, the larger the count value. In this screen shot, the mouse pointer is over the pie chart for a region in the northeastern part of the United States. An popup information box displays the latitude and longitude for the vendor, as well as a count of each product that the vendor sold. You can zoom in to see more details on the map.", "code_examples": [{"language": "spl", "code": "sourcetype=vendor_sales | stats  count by Code VendorID | lookup prices_lookup Code OUTPUTNEW product_name | table product_name VendorID | lookup vendors_lookup VendorID | geostats latfield=VendorLatitude longfield=VendorLongitude count by product_name"}, {"language": "spl", "code": "[10/Apr/2018:18:24:02]  VendorID=5036  Code=B  AcctID=6024298300471575"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search.In addition, this example uses several lookup files that you must download (prices.csv.zipandvendors.csv.zip) and unzip the files. You must complete the steps in theEnabling field lookupssection of the tutorial for both theprices.csvand thevendors.csvfiles. The steps in the tutorial are specific to theprices.csvfile. For thevendors.csvfile, use the namevendors_lookupfor the lookup definition. Skip the step in the tutorial that makes the lookups automatic."]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "geostats", "section_heading": "Extended examples", "section_id": "d453dec3_b889_4c5c_982c_1d94f0015463--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geostats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:51.187820+00:00", "version": "10.2"}}
{"id": "007e4dbdc8127b77", "content": "Commands iplocation stats xyseries Reference information Mapping data in Dashboards and Visualizations", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "geostats", "section_heading": "See also", "section_id": "id_44a7d1ec_9e7f_4b9a_a256_979cda2d696f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geostats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:31:51.187824+00:00", "version": "10.2"}}
{"id": "40da0ee17c7ac4ff", "content": "Adds cumulative summary statistics to all search results in a streaming manner. The streamstats command calculates statistics for each event at the time the event is seen. For example, you can calculate the running total for a particular field. The total is calculated by using the values in the specified field for every event that has been processed, up to the current event. As the streamstats command processes each event, it adds a field to that event or result that represents the accumulation of all of the events before it in the time window. The value of that field changes per event or result as the composition of events in the window or result set changes. In other words, the streamstats command produces a running total that is applied to each event, or the result of another transforming search, as they stream in. The streamstats command operates on whatever search output it receives and is the accumulation of the average, sum, count or so on, of one the following two elements: All of the events in the search window that have been collected up to that point, if a window is applied. If you don't set the time_window or the window arguments, the default of 10,000 events set by the max_stream_window in the limits.conf file applies to your streamstats searches. See Optional arguments. The results returned by a transforming command, such as the stats , chart , or timechart command, as each result is received and the running total of the events preceding that result is applied to it.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "streamstats", "section_heading": "Description", "section_id": "id_4ce58d3d_51f3_4a7f_84af_b4a19839cb57--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/streamstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:08.767643+00:00", "version": "10.2"}}
{"id": "432b9d5b76866833", "content": "The required syntax is in bold. streamstats [reset_on_change=<bool>] [reset_before=\"(\"<eval-expression>\")\"] [reset_after=\"(\"<eval-expression>\")\"] [current=<bool>] [window=<int>] [time_window=<span-length>] [global=<bool>] [allnum=<bool>] <stats-agg-term> ... [<by-clause>] Required arguments stats-agg-term Syntax: <stats-func>( <evaled-field> | <wc-field> ) [AS <wc-field>] Description: A statistical aggregation function. See Stats function options. The function can be applied to an eval expression, or to a field or set of fields. Use the AS clause to place the result into a new field with a name that you specify. You can use wild card characters in field names. For more information on eval expressions, see Types of eval expressions in the Search Manual. Optional arguments allnum Syntax: allnum=<boolean> Description: If true, computes numerical statistics on each field only if all of the values in that field are numerical. Default : false by-clause Syntax: BY <field-list> Description: The name of one or more fields to group by. current Syntax: current=<boolean> Description: If true, the search includes the given, or current, event in the summary calculations. If false, the search uses the field value from the previous event. Default: true global Syntax: global=<boolean> Description: Used only when the window argument is set. Defines whether to use a single window, global=true , or to use separate windows based on the by clause. If global=false and window is set to a non-zero value, a separate window is used for each group of values of the field specified in the by clause. Default: true reset_after Syntax: reset_after=\"(\"<eval-expression>\")\" Description: After the streamstats calculations are produced for an event, reset_after specifies that all of the accumulated statistics are reset if the eval-expression returns true. The eval-expression must evaluate to true or false. The eval-expression can reference fields that are returned by the streamstats command. When the reset_after argument is combined with the window argument, the window is also reset when the accumulated statistics are reset. Default : false reset_before Syntax: reset_before=\"(\"<eval-expression>\")\" Description: Before the streamstats calculations are produced for an event, reset_before specifies that all of the accumulated statistics are reset when the eval-expression returns true. The eval-expression must evaluate to true or false. When the reset_before argument is combined with the window argument, the window is also reset when the accumulated statistics are reset. Default : false reset_on_change Syntax: reset_on_change=<bool> Description: Specifies that all of the accumulated statistics are reset when the group by fields change. The reset is as if no previous events have been seen. Only events that have all of the group by fields can trigger a reset. Events that have only some of the group by fields are ignored. When the reset_on_change argument is combined with the window argument, the window is also reset when the accumulated statistics are reset. See the Usage section. Default : false time_window Syntax: time_window=<span-length> Description: Specifies the window size for the streamstats calculations, based on time. After each time window passes, the streamstats calculations are reset. The time_window argument is limited by range of values in the _time field in the events. To use the time_window argument, the events must be sorted in either ascending or descending time order. You can use the window argument with the time_window argument to specify the maximum number of events in a window. To specify five minutes for the span length, use time_window=5m. To specify 2 days, use time_window=2d. The following table shows additional time ranges and valid values that you can set for the span length. Default: None. However, the value of the max_stream_window attribute in the limits.conf file applies. The default value is 10000 events. window Syntax: window=<integer> Description: Specifies the number of events to use when computing the statistics. Default: 0, which means that all previous and current events are used. Stats function options stats-func Syntax: The syntax depends on the function that you use. Refer to the table below. Description: Statistical and charting functions that you can use with the streamstats command. Each time you invoke the streamstats command, you can use one or more functions. However, you can only use one BY clause. See Usage. The following table lists the supported functions by type of function. Use the links in the table to see descriptions and examples for each function. For an overview about using functions with commands, see Statistical and charting functions .", "code_examples": [], "tables": [{"headers": ["Time range", "Valid values"], "rows": [["seconds", "1s, 2s, ..."], ["minutes", "1m, 2m, â€¦"], ["hours", "1h,2h, â€¦"], ["days", "1d, 2d, ..."], ["weeks", "1w, 2w, ..."], ["months", "1mon, 2mon, 3mon, 4mon, 6mon, 12mon"], ["quarters", "1q, 2q, 4q"], ["years", "1y, 2y, ..."]]}, {"headers": ["Type of function", "Supported functions and syntax", "", "", ""], "rows": [["Aggregate functions", "avg()count()distinct_count()estdc()estdc_error()", "exactperc<int>()max()median()min()mode()", "perc<int>()range()stdev()stdevp()", "sum()sumsq()upperperc<int>()var()varp()"], ["Event order functions", "earliest()", "first()", "last()", "latest()"], ["Multivalue stats and chart functions", "list(X)", "values(X)", "", ""]]}], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "streamstats", "section_heading": "Syntax", "section_id": "cfb366d5_7fbf_46ca_b614_5b32ffe85660--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/streamstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:08.767662+00:00", "version": "10.2"}}
{"id": "2533a6794b609a05", "content": "The streamstats command is a centralized streaming command. See Command types. The streamstats command is similar to the eventstats command except that it uses events before the current event to compute the aggregate statistics that are applied to each event. If you want to include the current event in the statistical calculations, use current=true , which is the default. The streamstats command is also similar to the stats command in that streamstats calculates summary statistics on search results. Unlike stats , which works on the group of results as a whole, streamstats calculates statistics for each event at the time the event is seen. Statistical functions that are not applied to specific fields With the exception of the count function, when you pair the streamstats command with functions that are not applied to specific fields or eval expressions that resolve into fields, the search head processes it as if it were applied to a wildcard for all fields. In other words, when you have | streamstats avg in a search, it returns results for | stats avg(*). This \"implicit wildcard\" syntax is officially deprecated, however. Make the wildcard explicit. Write | streamstats <function>(*) when you want a function to apply to all possible fields. Escaping string values If your <eval-expression> contains a value instead of a field name, you must escape the quotation marks around the value. The following example is a simple way to see this. Start by using the makeresults command to create 3 events. Use the streamstats command to produce a cumulative count of the events. Then use the eval command to create a simple test. If the value of the count field is equal to 2, display yes in the test field. Otherwise display no in the test field. The results appear something like this: Use the streamstats command to reset the count when the match is true. You must escape the quotation marks around the word yes. The following example shows the complete search. Here is another example. You want to look for the value session is closed in the description field. Because the value is a string, you must enclose it in quotation marks. You then need to escape those quotation marks. The reset_on_change argument You have a dataset with the field \"shift\" that contains either the value DAY or the value NIGHT. You run this search: If the dataset is: shift DAY DAY NIGHT NIGHT NIGHT NIGHT DAY NIGHT Running the command with reset_on_change=true produces the following streamstats results: shift , count DAY, 1 DAY, 2 NIGHT, 1 NIGHT, 2 NIGHT, 3 NIGHT, 4 DAY, 1 NIGHT, 1 Memory and maximum results The streamstats search processor uses two limits.conf settings to determine the maximum number of results that it can store in memory for the purpose of computing statistics. The maxresultrows setting specifies a top limit for the window argument. This sets the number of result rows that the streamstats command processor can store in memory. The max_mem_usage_mb setting limits how much memory the streamstats command uses to keep track of information. When the max_mem_usage_mb limit is reached, the streamstats command processor stops adding the requested fields to the search results. Do not set max_mem_usage_mb=0 as this removes the bounds to the amount of memory the streamstats command processor can use. This can lead to search failures. Prerequisites Only users with file system access, such as system administrators, can increase the maxresultrows and max_mem_usage_mb settings using configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. You can have configuration files with the same name in your default, local, and app directories. Read Where you can place (or find) your modified configuration files in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. If you have Splunk Cloud Platform and want to change these limits, file a Support ticket.", "code_examples": [{"language": "spl", "code": "| makeresults count=3 | streamstats count |evaltest=if(count==2,\"yes\",\"no\")"}, {"language": "spl", "code": "| makeresults count=3 | streamstats count |evaltest=if(count==2,\"yes\",\"no\") | streamstats count as testCount reset_after=\"(\"match(test,\\\"yes\\\")\")\""}, {"language": "spl", "code": "... | streamstats reset_after=\"(\"description==\\\"session is closed\\\"\")\""}, {"language": "spl", "code": "...| streamstats count BYshiftreset_on_change=true"}], "tables": [{"headers": ["_time", "count", "test"], "rows": [["2017-01-11 11:32:43", "1", "no"], ["2017-01-11 11:32:43", "2", "yes"], ["2017-01-11 11:32:43", "3", "no"]]}], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "streamstats", "section_heading": "Usage", "section_id": "id_940594cd_2a36_4139_a23d_b2bdadf4bc43--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/streamstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:08.767672+00:00", "version": "10.2"}}
{"id": "15977c19f3534c27", "content": "1. Compute the average of a field over the last 5 events For each event, compute the average of the foo field over the last 5 events, including the current event. This is similar to using the trendline command to compute a simple moving average (SMA), such as trendline sma5(foo). 2. Compute the average of a field, with a by clause, over the last 5 events For each event, compute the average value of foo for each value of bar including only 5 events, specified by the window size, with that value of bar. 3. For each event, add a count of the number of events processed This example adds to each event a count field that represents the number of events seen so far, including that event. For example, it adds 1 for the first event, 2 for the second event, and so on. If you did not want to include the current event, you would specify: 4. Apply a time-based window to streamstats Assume that the max_stream_window argument in the limits.conf file is the default value of 10000 events. The following search counts the events, using a time window of five minutes. This search adds a count field to each event. If the events are in descending time order (most recent to oldest), the value in the count field represents the number of events in the next 5 minutes. If the events are in ascending time order (oldest to most recent), the count field represents the number of events in the previous 5 minutes. If there are more events in the time-based window than the value for the max_stream_window argument, the max_stream_window argument takes precedence. The count will never be greater than 10000, even if there are actually more than 10,000 events in any 5 minute period.", "code_examples": [{"language": "spl", "code": "... | streamstats avg(foo) window=5"}, {"language": "spl", "code": "... | streamstats avg(foo) by bar window=5 global=f"}, {"language": "spl", "code": "... | streamstats count"}, {"language": "spl", "code": "... | streamstats count current=f"}, {"language": "spl", "code": "... | streamstats count time_window=5m"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "streamstats", "section_heading": "Basic examples", "section_id": "id_1f6aafe6_e368_4461_8604_b2ee8fd8feeb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/streamstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:08.767679+00:00", "version": "10.2"}}
{"id": "7c99f074a28c38ef", "content": "1. Create events for testing You can use the streamstats command with the makeresults command to create a series events. This technique is often used for testing search syntax. The eval command is used to create events with different hours. You use 3600, the number of seconds in an hour, in the eval command. The makeresults command is used to create the count field. The streamstats command calculates a cumulative count for each event, at the time the event is processed. The results look something like this: Notice that the hours in the timestamp are 1 hour apart. You can create additional fields by using the eval command. The eval command is used to create two new fields, age and city. The eval command uses the value in the count field. The case function takes pairs of arguments, such as count=1, 25. The first argument is a Boolean expression. When that expression is TRUE, the corresponding second argument is returned. The results of the search look like this: 2. Calculate a snapshot of summary statistics You want to determine the number of the bytes used over a set period of time. The following search uses the first 5 events. Because search results typically display the most recent event first, the sort command is used to sort the 5 events in ascending order to see the oldest event first and the most recent event last. Ascending order enables the streamstats command to calculate statistics over time. Add the streamstats command to the search to generate a running total of the bytes over the 5 events and organize the results by clientip. When you click on the ASimpleSumOfBytes field in the list of Interesting fields , an information window shows the cumulative sum of the bytes, as shown in this image: The streamstats command aggregates the statistics to the original data, which means that all of the original data is accessible for further calculations. Add the table command to the search to display the only the values in the _time , clientip , bytes , and ASimpleSumOfBytes fields. Each event shows the timestamp for the event, the clientip, and the number of bytes used. The ASimpleSumOfBytes field shows a cumulative summary of the bytes for each clientip. 3. Calculate the running total of distinct users over time Each day you track unique users, and you would like to track the cumulative count of distinct users. This example calculates the running total of distinct users over time. The bin command breaks the time into days. The stats command calculates the distinct users (clientip) and user count per day. The streamstats command finds the running distinct count of users. This search returns a table that includes: day , ips , dc(clientip) , and Cumulative total. 4. Calculate hourly cumulative totals This example uses streamstats to produce hourly cumulative totals. This search returns 3 columns: _time, SumOfBytes, and accu_total_SumOfBytes. The timechart command buckets the events into spans of 1 hour and counts the total values for each category. The timechart command also fills NULL values, so that there are no missing values. Then, the streamstats command is used to calculate the accumulated total. This example uses streamstats to produce hourly cumulative totals for category values. 5. Calculate when a DHCP IP lease address changed for a specific MAC address This example uses streamstats to figure out when a DHCP IP lease address changed for a MAC address, 54:00:00:00:00:00. You can also clean up the presentation to display a table of the DHCP IP address changes and the times the occurred. 6. Compare a number with itself over a few months Say a community needs to find out how their vaccination booster campaign is going. They want to check the gap after their citizens get the second booster shot, which should keep up with their target of about 6 months between the second and third doses. The following search is across the daily number of people by number of doses. The search results return a graph that looks something like this, with 3 lines for the second booster showing the gap at 5 months, 6 months, and 7 months, which shows that the community is almost keeping up with a 6-month booster gap:", "code_examples": [{"language": "spl", "code": "| makeresults count=5 \n | streamstats count\n |eval_time=_time-(count*3600)"}, {"language": "spl", "code": "| makeresults count=5\n| streamstats count \n|eval_time=_time-(count*3600)\n|evalage =case(count=1, 25, count=2, 39, count=3, 31, count=4, 27, count=5, null())\n|evalcity =case(count=1 OR count=3,\"San Francisco\", count=2 OR count=4,\"Seattle\",count=5,\"Los Angeles\")"}, {"language": "spl", "code": "sourcetype=access_combined* | head 5 | sort _time"}, {"language": "spl", "code": "sourcetype=access_combined* | head 5 |sort _time | streamstats sum(bytes) AS ASimpleSumOfBytes BY clientip"}, {"language": "spl", "code": "sourcetype=access_combined* | head 5 |sort _time | streamstats sum(bytes) as ASimpleSumOfBytes by clientip | table _time, clientip, bytes, ASimpleSumOfBytes"}, {"language": "spl", "code": "eventtype=\"download\"| bin _time span=1d as day | stats values(clientip) as ips dc(clientip) by day | streamstats dc(ips) as\"Cumulative total\""}, {"language": "spl", "code": "... | timechart span=1h sum(bytes) as SumOfBytes | streamstats global=f sum(*) as accu_total_*"}, {"language": "spl", "code": "... | timechart span=1h sum(value) as total by category | streamstats global=f | addtotals | accum Total | rename Total as accu_total"}, {"language": "spl", "code": "source=dhcp MAC=54:00:00:00:00:00 | head 10 | streamstats current=f last(DHCP_IP) as new_dhcp_ip last(_time) as time_of_change by MAC"}, {"language": "spl", "code": "source=dhcp MAC=54:00:00:00:00:00 | head 10 | streamstats current=f last(DHCP_IP) as new_dhcp_ip last(_time) as time_of_change by MAC |whereDHCP_IP!=new_dhcp_ip | convert ctime(time_of_change) as time_of_change | rename DHCP_IP as old_dhcp_ip | table time_of_change, MAC, old_dhcp_ip, new_dhcp_ip"}, {"language": "spl", "code": "... | sort _time\n| streamstats time_window=153d earliest(two_doses) as two_doses_5mon\n| streamstats time_window=183d earliest(two_doses) as two_doses_6mon\n| streamstats time_window=214d earliest(two_doses) as two_doses_7mon\n| foreach two_doses_* [evalbooster_gap_<<MATCHSTR>> = <<FIELD>> - three_doses ]\n| timechart span=1d max(booster_gap_*) as booster_gap_*"}], "tables": [{"headers": ["_time", "count"], "rows": [["2020-01-09 15:35:14", "1"], ["2020-01-09 14:35:14", "2"], ["2020-01-09 13:35:14", "3"], ["2020-01-09 12:35:14", "4"], ["2020-01-09 11:35:14", "5"]]}, {"headers": ["_time", "age", "city", "count"], "rows": [["2020-01-09 15:35:14", "25", "San Francisco", "1"], ["2020-01-09 14:35:14", "39", "Seattle", "2"], ["2020-01-09 13:35:14", "31", "San Francisco", "3"], ["2020-01-09 12:35:14", "27", "Seattle", "4"], ["2020-01-09 11:35:14", "", "Los Angeles", "5"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "streamstats", "section_heading": "Extended examples", "section_id": "eb84fc7e_017c_4a9e_83db_24673fe183e8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/streamstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:08.767694+00:00", "version": "10.2"}}
{"id": "d4fcd09b8072fee6", "content": "Commands accum autoregress delta fillnull eventstats makeresults trendline Blogs Getting started with stats, eventstats and streamstats", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "streamstats", "section_heading": "See also", "section_id": "id_96219498_01f4_46a9_8dc8_040faa561ae8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/streamstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:08.767698+00:00", "version": "10.2"}}
{"id": "a077b3a964f93eea", "content": "The sistats command is one of several commands that you can use to create summary indexes. Summary indexing is one of the methods that you can use to speed up searches that take a long time to run. The sistats command is the summary indexing version of the stats command, which calculates aggregate statistics over the dataset. The sistats command populates a summary index. You must then create a report to generate the summary statistics. See the Usage section.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "sistats", "section_heading": "Description", "section_id": "id_3e24582f_1f80_4b1e_838c_8bdb94bf82a7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sistats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:23.831648+00:00", "version": "10.2"}}
{"id": "560eab63d3b5b745", "content": "sistats [allnum=<bool>] [delim=<string>] ( <stats-agg-term> | <sparkline-agg-term> ) [<by clause>] For descriptions of each of the arguments in this syntax, refer to the stats command. For information about functions that you can use with the sistats command, see Statistical and charting functions .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "sistats", "section_heading": "Syntax", "section_id": "id_55cff5bc_1704_409c_bf95_83babc760763--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sistats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:23.831655+00:00", "version": "10.2"}}
{"id": "fc53c2f22197ed14", "content": "The summary indexes exist separately from your main indexes. After you create the summary index, create a report by running a search against the summary index. You use the exact same search string that you used to populate the summary index, substituting the stats command for the sistats command, to create your reports. For more information, see About report acceleration and summary indexing and Use summary indexing for increased reporting efficiency in the Knowledge Manager Manual. Statistical functions that are not applied to specific fields With the exception of the count function, when you pair the sistats command with functions that are not applied to specific fields or eval expressions that resolve into fields, the search head processes it as if it were applied to a wildcard for all fields. In other words, when you have | sistats avg in a search, it returns results for | sistats avg(*). This \"implicit wildcard\" syntax is officially deprecated, however. Make the wildcard explicit. Write | sistats <function>(*) when you want a function to apply to all possible fields. Memory and sistats search performance A pair of limits.conf settings strike a balance between the performance of sistats searches and the amount of memory they use during the search process, in RAM and on disk. If your sistats searches are consistently slow to complete you can adjust these settings to improve their performance, but at the cost of increased search-time memory usage, which can lead to search failures. If you have Splunk Cloud Platform, you need to file a Support ticket to change these settings. For more information, see Memory and stats search performance in the Search Manual .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "sistats", "section_heading": "Usage", "section_id": "id_2f8ac7d3_11ab_454f_ba7d_2f8c5a29c927--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sistats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:23.831661+00:00", "version": "10.2"}}
{"id": "38be5dd9daf41ac8", "content": "Example 1: Create a summary index with the statistics about the average, for each hour, of any unique field that ends with the string \"lay\". For example, delay, xdelay, relay, etc. To create a report, run a search against the summary index using this search", "code_examples": [{"language": "spl", "code": "... | sistats avg(*lay) BY date_hour"}, {"language": "spl", "code": "index=summary | stats avg(*lay) BY date_hour"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "sistats", "section_heading": "Examples", "section_id": "bba90f0f_2354_4260_9cd5_c4afdff1482c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sistats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:23.831667+00:00", "version": "10.2"}}
{"id": "83439e40ff5c9f0d", "content": "collect , overlap , sichart , sirare , sitop , sitimechart For a detailed explanation and examples of summary indexing, see Use summary indexing for increased reporting efficiency in the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "sistats", "section_heading": "See also", "section_id": "id_289ab13e_a642_4839_95d1_84de99262f35--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sistats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:23.831671+00:00", "version": "10.2"}}
{"id": "7206ec7c93befc62", "content": "Creates a time series chart with corresponding table of statistics. A timechart is a statistical aggregation applied to a field to produce a chart, with time used as the X-axis. You can specify a split-by field, where each distinct value of the split-by field becomes a series in the chart. If you use an eval expression, the split-by clause is required. With the limit and agg options, you can specify series filtering. These options are ignored if you specify an explicit where-clause. If you set limit=0, no series filtering occurs.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 7, "metadata": {"title": "timechart", "section_heading": "Description", "section_id": "e88f8dc8_c45d_4c30_8cd5_6e626290f80f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:39.544192+00:00", "version": "10.2"}}
{"id": "19a2a2fd84236887", "content": "The required syntax is in bold. timechart [sep=<string>] [format=<string>] [partial=<bool>] [cont=<bool>] [limit=<chart-limit-opt>] [agg=<stats-agg-term>] [<bin-options>... ] ( ( <single-agg> [BY < split-by-clause >] ) | ( <eval-expression> ) BY < split-by-clause > ) [<dedup_splitvals>] Required arguments When specifying timechart command arguments, either <single-agg> or <eval-expression> BY <split-by-clause> is required. eval-expression Syntax: <math-exp> | <concat-exp> | <compare-exp> | <bool-exp> | <function-call> Description: A combination of literals, fields, operators, and functions that represent the value of your destination field. For these evaluations to work, your values need to be valid for the type of operation. For example, with the exception of addition, arithmetic operations might not produce valid results if the values are not numerical. Additionally, the search can concatenate the two operands if they are both strings. When concatenating values with a period '.' the search treats both values as strings, regardless of their actual data type. single-agg Syntax: count | <stats-func>(<field>) Description: A single aggregation applied to a single field, including an evaluated field. For <stats-func>, see Stats function options. No wildcards are allowed. The field must be specified, except when using the count function, which applies to events as a whole. split-by-clause Syntax: <field> (<tc-options>)... [<where-clause>] Description: Specifies a field to split the results by. If field is numerical, default discretization is applied. Discretization is defined with the tc-options. Use the <where-clause> to specify the number of columns to include. See the tc options and the where clause sections in this topic. Optional arguments agg=<stats-agg-term> Syntax: agg=( <stats-func> ( <evaled-field> | <wc-field> ) [AS <wc-field>] ) Description: A statistical aggregation function. See Stats function options. The function can be applied to an eval expression, or to a field or set of fields. Use the AS clause to place the result into a new field with a name that you specify. You can use wild card characters in field names. bin-options Syntax: bins | minspan | span | <start-end> | aligntime Description: Options that you can use to specify discrete bins, or groups, to organize the information. The bin-options set the maximum number of bins, not the target number of bins. See the Bin options section in this topic. Default: bins=100 cont Syntax: cont=<bool> Description: Specifies whether the chart is continuous or not. If set to true , the Search application fills in the time gaps. Default: true dedup_splitvals Syntax: dedup_splitvals=<boolean> Description: Specifies whether to remove duplicate values in multivalued <split-by-clause> fields. Default: false fixedrange Syntax: fixedrange=<bool> Description: Specifies whether or not to enforce the earliest and latest times of the search. Setting fixedrange=false allows the timechart command to constrict or expand to the time range covered by all events in the dataset. Default: true format Syntax: format=<string> Description: Used to construct output field names when multiple data series are used in conjunction with a split-by-field. format takes precedence over sep and allows you to specify a parameterized expression with the stats aggregator and function ($AGG$) and the value of the split-by-field ($VAL$). limit Syntax: limit=(top | bottom)<int> Description: Specifies a limit for the number of distinct values of the split-by field to return. If set to limit=0 , all distinct values are used. Setting limit=N or limit=topN keeps the N highest scoring distinct values of the split-by field. Setting limit=bottomN keeps the lowest scoring distinct values of the split-by field. All other values are grouped into 'OTHER', as long as useother is not set to false. The scoring is determined as follows: If a single aggregation is specified, the score is based on the sum of the values in the aggregation for that split-by value. For example, for timechart avg(host) BY <field> , the avg(host) values are added up for each value of <field> to determine the scores. If multiple aggregations are specified, the score is based on the frequency of each value of <field>. For example, for timechart avg(host) max(amount) BY <field> , the top scoring values for <field> are the most common values of <field>. Ties in scoring are broken lexicographically, based on the value of the split-by field. For example, 'AMOUNT' takes precedence over 'amount', which takes precedence over 'host'. See Usage. Default : top10 partial Syntax: partial=<bool> Description: Controls if partial time bins should be retained or not. Only the first and last bin can be partial. Default: True. Partial time bins are retained. sep Syntax: sep=<string> Description: Used to construct output field names when multiple data series are used in conjunctions with a split-by field. This is equivalent to setting format to $AGG$<sep>$VAL$. Stats function options stats-func Syntax: The syntax depends on the function that you use. See Usage. Description: Statistical functions that you can use with the timechart command. Each time you invoke the timechart command, you can use one or more functions. However, you can only use one BY clause. Bin options bins Syntax: bins=<int> Description: Sets the maximum number of bins to discretize into. This does not set the target number of bins. It finds the smallest bin size that results in no more than N distinct bins. Even though you specify a number such as 300, the resulting number of bins might be much lower. Default: 100 minspan Syntax: minspan=<span-length> Description: Specifies the smallest span granularity to use automatically inferring span from the data time range. See Usage. span Syntax: span=<log-span> | span=<span-length> | span=<snap-to-time> Description: Sets the size of each bin, using either a log-based span, a span length based on time, or a span that snaps to a specific time. For descriptions of each of these options, see Span options. The starting time of a bin might not match your local timezone. see Usage. <start-end> Syntax: end=<num> | start=<num> Description: Sets the minimum and maximum extents for numerical bins. Data outside of the [start, end] range is discarded. aligntime Syntax: aligntime=(earliest | latest | <time-specifier>) Description: Align the bin times to something other than base UNIX time (epoch 0). The aligntime option is valid only when doing a time-based discretization. Ignored if span is in days, months, or years. Span options <log-span> Syntax: [<num>]log[<num>] Description: Sets to log-based span. The first number is a coefficient. The second number is the base. If the first number is supplied, it must be a real number greater than or equal to 1.0 and less than the base. If supplied, the base must be real number greater than 1.0 and strictly greater than 1. The log-span option must come at the end of the timechart command in the search like this: <span-length> Syntax: <int>[<timescale>] Description: A span of each bin, based on time. If the timescale is provided, this is used as a time range. If not, this is an absolute bin length. <timescale> Syntax: <sec> | <min> | <hr> | <day> | <week> | <month> | <quarter> | <subseconds> Description: Timescale units. Default: <sec> <snap-to-time> Syntax: [+|-] [<time_integer>] <relative_time_unit>@<snap_to_time_unit> Description: A span of each bin, based on a relative time unit and a snap to time unit. The <snap-to-time> must include a relative_time_unit, the @ symbol, and a snap_to_time_unit. The offset, represented by the plus (+) or minus (-) is optional. If the <time_integer> is not specified, 1 is the default. For example, if you specify w as the relative_time_unit, 1 week is assumed. tc options The <tc-option> is part of the <split-by-clause>. tc-option Syntax: <bin-options> | usenull=<bool> | useother=<bool> | nullstr=<string> | otherstr=<string> Description: Timechart options for controlling the behavior of splitting by a field. bin-options See the Bin options section in this topic. nullstr Syntax: nullstr=<string> Description: If usenull=true , specifies the label for the series that is created for events that do not contain the split-by field. Default: NULL otherstr Syntax: otherstr=<string> Description: If useother=true , specifies the label for the series that is created in the table and the graph. Default: OTHER usenull Syntax: usenull=<bool> Description: Controls whether or not a series is created for events that do not contain the split-by field. The label for the series is controlled by the nullstr option. Default: true useother Syntax: useother=<bool> Description: You specify which series to include in the results table by using the <agg>, <limit>, and <where-clause> options. The useother option specifies whether to merge all of the series not included in the results table into a single new series. If useother=true , the label for the series is controlled by the otherstr option. Default: true where clause The <where-clause> is part of the <split-by-clause>. The <where-clause> is comprised of two parts, a single aggregation and some options. See Where clause examples. where clause Syntax: <single-agg> <where-comp> Description: Specifies the criteria for including particular data series when a field is given in the <tc-by-clause>. The most common use of this option is to look for spikes in your data rather than overall mass of distribution in series selection. The default value finds the top ten series by area under the curve. Alternately one could replace sum with max to find the series with the ten highest spikes. Essentially the default is the same as specifying where sum in top10. The <where-clause> has no relation to the where command. <where-comp> Syntax: <wherein-comp> | <wherethresh-comp> Description: Specify either a grouping for the series or the threshold for the series. <wherein-comp> Syntax: (in | notin) (top | bottom)<int> Description: A grouping criteria that requires the aggregated series value be in or not in some top or bottom group. <wherethresh-comp> Syntax: (< | >) [\" \"] <num> Description: A threshold criteria that requires the aggregated series value be greater than or less than some numeric threshold. You can specify the threshold with or without a space between the sign and the number.", "code_examples": [{"language": "spl", "code": "...| timechart dc(data.search_props.sid) by data.search_props.type span=log2"}], "tables": [{"headers": ["Timescale", "Valid syntax", "Description"], "rows": [["<sec>", "s | sec | secs | second | seconds", "Time scale in seconds."], ["<min>", "m | min | mins | minute | minutes", "Time scale in minutes."], ["<hr>", "h | hr | hrs | hour | hours", "Time scale in hours."], ["<day>", "d | day | days", "Time scale in days."], ["<week>", "w | week | weeks", "Time scale in weeks."], ["<month>", "mon | month | months", "Time scale in months."], ["<quarter>", "q | qtr | qtrs | quarter | quarters", "Time scale in quarters."], ["<subseconds>", "us | ms | cs | ds", "Time scale in microseconds (us), milliseconds (ms), centiseconds (cs), or deciseconds (ds)"]]}], "chunk_index": 1, "total_chunks": 7, "metadata": {"title": "timechart", "section_heading": "Syntax", "section_id": "id_0ace3a15_f19d_4eb6_8207_13b892b1e200--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:39.544208+00:00", "version": "10.2"}}
{"id": "c0fe9add5f8d079b", "content": "The timechart command is a transforming command. See Command types. Note: Do not run searches that modify the _time field using eval and timechart commands with the span argument. The _time field is an internal field that should not be overwritten. See Use default fields. bins and span arguments The timechart command accepts either the bins argument OR the span argument. If you specify both bins and span , span is used. The bins argument is ignored. If you do not specify either bins or span , the timechart command uses the default bins=100. Default time spans If you use the predefined time ranges in the time range picker, and do not specify the span argument, the following table shows the default span that is used. (Thanks to Splunk users MuS and Martin Mueller for their help in compiling this default time span information.) Spans used when minspan is specified When you specify a minspan value, the span that is used for the search must be equal to or greater than one of the span threshold values in the following table. For example, if you specify minspan=15m that is equivalent to 900 seconds. The minimum span that can be used is 1800 seconds, or 30 minutes. Bin time spans and local time The span argument always rounds down the starting date for the first bin. There is no guarantee that the bin start time used by the timechart command corresponds to your local timezone. In part this is due to differences in daylight savings time for different locales. To use day boundaries, use span=1d. Do not use not span=86400s, or span=1440m, or span=24h. Bin time spans versus per_* functions The functions, per_day() , per_hour() , per_minute() , and per_second() are aggregator functions and are not responsible for setting a time span for the resultant chart. These functions are used to get a consistent scale for the data when an explicit span is not provided. The resulting span can depend on the search time range. For example, per_hour() converts the field value so that it is a rate per hour, or sum()/<hours in the span>. If your chart span ends up being 30m, it is sum()*2. If you want the span to be 1h, you still have to specify the argument span=1h in your search. Note: You can do per_hour() on one field and per_minute() (or any combination of the functions) on a different field in the same search. Subsecond bin time spans Subsecond span timescales, which are time spans that are made up of deciseconds (ds), centiseconds (cs), milliseconds (ms), or microseconds (us), should be numbers that divide evenly into a second. For example, 1s = 1000ms. This means that valid millisecond span values are 1, 2, 4, 5, 8, 10, 20, 25, 40, 50, 100, 125, 200, 250, or 500ms. In addition, span = 1000ms is not allowed. Use span = 1s instead. Split-by fields If you specify a split-by field, ensure that you specify the bins and span arguments before the split-by field. If you specify these arguments after the split-by field, Splunk software assumes that you want to control the bins on the split-by field, not on the time axis. If you use chart or timechart , you cannot use a field that you specify in a function as your split-by field as well. For example, you will not be able to run: However, you can work around this with an eval expression, for example: Prepending VALUE to the names of some fields that begin with underscore ( _ ) In timechart searches that include a split-by-clause, when search results include a field name that begins with a leading underscore ( _ ), Splunk software prepends the field name with VALUE and creates as many columns as there are unique entries in the argument of the BY clause. Prepending the string with VALUE distinguishes the field from internal fields and avoids naming a column with a leading underscore, which ensures that the field is not hidden in the output schema like most internal fields. For example, consider the following search: The results look something like this: The columns are displayed in the search results as VALUE_audit and VALUE_internal. Supported functions You can use a wide range of functions with the timechart command. For general information about using functions, see Statistical and charting functions. For a list of functions by category, see Function list by category For an alphabetical list of functions, see Alphabetical list of functions Functions and memory usage Some functions are inherently more expensive, from a memory standpoint, than other functions. For example, the distinct_count function requires far more memory than the count function. The values and list functions also can consume a lot of memory. If you are using the distinct_count function without a split-by field or with a low-cardinality split-by by field, consider replacing the distinct_count function with the the estdc function (estimated distinct count). The estdc function might result in significantly lower memory usage and run times. Lexicographical order Lexicographical order sorts items based on the values used to encode the items in computer memory. In Splunk software, this is almost always UTF-8 encoding, which is a superset of ASCII. Numbers are sorted before letters. Numbers are sorted based on the first digit. For example, the numbers 10, 9, 70, 100 are sorted lexicographically as 10, 100, 70, 9. Uppercase letters are sorted before lowercase letters. Symbols are not standard. Some symbols are sorted before numeric values. Other symbols are sorted before or after letters. You can specify a custom sort order that overrides the lexicographical order. See the blog Order Up! Custom Sort Orders. Run the timechart command in non-streaming mode when overwriting the _time field You might encounter unexpected results when you run a search that modifies the _time field with an eval command and then uses timechart with a span. To ensure that the timechart command correctly interprets the _time field after it has been overwritten by an eval command, you must force the search to run in a non-streaming mode by including the sort command before the timechart command. The sort command forces all preceding commands, including the eval that modifies _time , to complete their processing before timechart begins, which ensures that timechart operates on the final, modified _time values. For example, run the following search in Splunk Web to see this solution in action:", "code_examples": [{"language": "spl", "code": "... | chart sum(A) by A span=log2"}, {"language": "spl", "code": "... |evalA1=A | chart sum(A) by A1 span=log2"}, {"language": "spl", "code": "index=\"_internal\"OR index=\"_audit\"| timechart span=1m sum(linecount) by index"}, {"language": "spl", "code": "index=_internal earliest=-1m \n| head 1000 \n|eval_time = now() - (random() % (86400 * 90) ) \n| sort _time \n| timechart span=1w count"}], "tables": [{"headers": ["Time range", "Default span"], "rows": [["Last 15 minutes", "10 seconds"], ["Last 60 minutes", "1 minute"], ["Last 4 hours", "5 minutes"], ["Last 24 hours", "30 minutes"], ["Last 7 days", "1 day"], ["Last 30 days", "1 day"], ["Previous year", "1 month"]]}, {"headers": ["Span threshold", "Time equivalents"], "rows": [["1 second", ""], ["5 seconds", ""], ["10 seconds", ""], ["30 seconds", ""], ["60 seconds", "1 minute"], ["300 seconds", "5 minutes"], ["600 seconds", "10 minutes"], ["1800 seconds", "30 minutes"], ["3600 seconds", "1 hour"], ["86400 seconds", "1 day"], ["2592000 seconds", "30 days"]]}, {"headers": ["_time", "VALUE_audit", "VALUE_internal"], "rows": [["2023-06-26 21:00:00", "1", "586"], ["2023-06-26 21:01:00", "1", "295"], ["2023-06-26 21:02:00", "1", "555"]]}], "chunk_index": 2, "total_chunks": 7, "metadata": {"title": "timechart", "section_heading": "Usage", "section_id": "dab97bc4_850b_476b_b4f0_dd6f613d3d09--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:39.544227+00:00", "version": "10.2"}}
{"id": "3b4b19b4dbd93e96", "content": "1. Chart the product of the average \"CPU\" and average \"MEM\" for each \"host\" For each minute, compute the product of the average \"CPU\" and average \"MEM\" for each \"host\". 2. Chart the average of cpu_seconds by processor This example uses an eval expression that includes a statistical function, avg to calculate the average of cpu_seconds field, rounded to 2 decimal places. The results are organized by the values in the processor field. When you use a eval expression with the timechart command, you must also use BY clause. 3. Chart the average of \"CPU\" for each \"host\" For each minute, calculate the average value of \"CPU\" for each \"host\". 4. Chart the average \"cpu_seconds\" by \"host\" and remove outlier values Calculate the average \"cpu_seconds\" by \"host\". Remove outlying values that might distort the timechart axis. 5. Chart the average \"thruput\" of hosts over time 6. Chart the eventypes by source_ip For each minute, count the eventypes by source_ip , where the count is greater than 10. 7. Align the chart time bins to local time Align the time bins to 5am (local time). Set the span to 12h. The bins will represent 5am - 5pm, then 5pm - 5am (the next day), and so on. 8. In a multivalue BY field, remove duplicate values For each unique value of mvfield , return the average value of field. Deduplicates the values in the mvfield. 9. Rename fields prepended with VALUE To rename fields with leading underscores that are prepended with VALUE , add the following command to your search: The columns in your search results now display without the leading VALUE_ in the field name.", "code_examples": [{"language": "spl", "code": "... | timechart span=1meval(avg(CPU) * avg(MEM)) BY host"}, {"language": "spl", "code": "... | timecharteval(round(avg(cpu_seconds),2)) BY processor"}, {"language": "spl", "code": "... | timechart span=1m avg(CPU) BY host"}, {"language": "spl", "code": "... | timechart avg(cpu_seconds) BY host | outlier action=tf"}, {"language": "spl", "code": "... | timechart span=5m avg(thruput) BY host"}, {"language": "spl", "code": "sshd failed OR failure | timechart span=1m count(eventtype) BY source_ip usenull=f WHERE count>10"}, {"language": "spl", "code": "...| timechart _time span=12h aligntime=@d+5h"}, {"language": "spl", "code": "...| timechart avg(field) BY mvfield dedup_splitval=true"}, {"language": "spl", "code": "... | rename VALUE_* as *"}], "tables": [], "chunk_index": 3, "total_chunks": 7, "metadata": {"title": "timechart", "section_heading": "Basic Examples", "section_id": "id_16feed5c_2d7c_4af9_8bcc_a61b7025de27--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:39.544235+00:00", "version": "10.2"}}
{"id": "5eb39a218909b83e", "content": "1. Chart revenue for the different products Chart the revenue for the different products that were purchased yesterday. This example searches for all purchase events (defined by the action=purchase ). The results are piped into timechart command. The per_hour() function sums up the values of the price field for each productName and organizes the total by time. This search produces the following table of results in the Statistics tab. To format the numbers to the proper digits for currency, click the format icon in the column heading. On the Number Formatting tab, select the Precision. Click the Visualization tab. If necessary, change the chart to a column chart. On the Format menu, the General tab contains the Stack Mode option where you can change the chart to a stacked chart. After you create this chart, you can position your mouse pointer over each section to view more metrics for the product purchased at that hour of the day. Notice that the chart does not display the data in hourly spans. Because a span is not provided (such as span=1hr), the per_hour() function converts the value so that it is a sum per hours in the time range (which in this example is 24 hours). 2. Chart daily purchases by product type Chart the number of purchases made daily for each type of product. This example searches for all purchases events, defined by the action=purchase , and pipes those results into the timechart command. The span=1day argument buckets the count of purchases over the week into daily chunks. The usenull=f argument ignore any events that contain a NULL value for categoryId. The results appear on the Statistics tab and look something like this: Click the Visualization tab. If necessary, change the chart to a column chart. Compare the number of different items purchased each day and over the course of the week. 3. Display results in 1 week intervals This search counts the number of earthquakes in Alaska where the magnitude is greater than or equal to 3.5. The results are organized in spans of 1 week, where the week begins on Monday. The <by-clause> is used to group the earthquakes by magnitude. You can only use week spans with the snap-to span argument in the timechart command. For more information, see Specify a snap to time unit. The results appear on the Statistics tab and look something like this: 4. Count the revenue for each item over time Count the total revenue made for each item sold at the shop over the last 7 days. This example shows two different searches to generate the calculations. Search 1 The first search uses the span argument to bucket the times of the search results into 1 day increments. The search then uses the sum() function to add the price for each product_name. Search 2 This second search uses the per_day() function to calculate the total of the price values for each day. Both searches produce similar results. Search 1 produces values with two decimal places. Search 2 produces values with six decimal places. The following image shows the results from Search 1. Click the Visualization tab. If necessary, change the chart to a column chart. Now you can compare the total revenue made for items purchased each day and over the course of the week. 5. Chart product views and purchases for a single day Chart a single day's views and purchases at the Buttercup Games online store. This search uses the per_hour() function and eval expressions to search for page views ( method=GET ) and purchases ( action=purchase ). The results of the eval expressions are renamed as Views and Purchases , respectively. The results appear on the Statistics tab and look something like this: Click the Visualization tab. Format the results as an area chart. The difference between the two areas indicates that many of the views did not become to purchases. If all of the views became purchases, you would expect the areas to overlay on top each other completely. There would be no difference between the two areas.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* action=purchase | timechart per_hour(price) by productName usenull=f useother=f"}, {"language": "spl", "code": "sourcetype=access_* action=purchase | timechart span=1d count by categoryId usenull=f"}, {"language": "spl", "code": "source=all_month.csv place=*alaska* mag>=3.5 | timechart span=w@w1 count BY mag"}, {"language": "spl", "code": "sourcetype=access_* action=purchase | timechart span=1d sum(price) by productName usenull=f"}, {"language": "spl", "code": "sourcetype=access_* action=purchase | timechart per_day(price) by productName usenull=f"}, {"language": "spl", "code": "sourcetype=access_* | timechart per_hour(eval(method=\"GET\")) AS Views, per_hour(eval(action=\"purchase\")) AS Purchases"}], "tables": [{"headers": [], "rows": [["This example uses the sample dataset from the Search Tutorial and a field lookup to add more information to the event data. To try this example for yourself:Download thetutorialdata.zipfile fromthis topic in the Search Tutorialand follow the instructions to upload the file to your Splunk deployment.Download thePrices.csv.zipfile fromthis topic in the Search Tutorialand follow the instructions to set up your field lookup.Use the time rangeYesterdaywhen you run the search.The tutorialdata.zip file includes aproductIdfield that is the catalog number for the items sold at the Buttercup Games online store. The field lookup uses theprices.csvfile to add two new fields to your events:productName, which is a descriptive name for the item, andprice, which is the cost of the item."]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["_time", "ACCESSORIES", "ARCADE", "SHOOTER", "SIMULATION", "SPORTS", "STRATEGY", "TEE"], "rows": [["2018-03-29", "5", "17", "6", "3", "5", "32", "9"], ["2018-03-30", "62", "63", "39", "30", "22", "127", "56"], ["2018-03-31", "65", "94", "38", "42", "34", "128", "60"], ["2018-04-01", "54", "82", "42", "39", "13", "115", "66"], ["2018-04-02", "52", "63", "45", "42", "22", "124", "52"], ["2018-04-03", "46", "76", "34", "42", "19", "123", "59"], ["2018-04-04", "57", "70", "36", "38", "20", "130", "56"], ["2018-04-05", "46", "72", "35", "37", "13", "106", "46"]]}, {"headers": [], "rows": [["This search uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance.  This example uses theAll Earthquakesdata from  the past 30 days."]]}, {"headers": ["_time", "3.5", "3.6", "3.7", "3.8", "4", "4.1", "4.1", "4.3", "4.4", "4.5", "OTHER"], "rows": [["2018-03-26", "3", "3", "2", "2", "3", "1", "0", "2", "1", "1", "1"], ["2018-04-02", "5", "7", "2", "0", "3", "2", "1", "0", "0", "1", "1"], ["2018-04-09", "2", "3", "1", "2", "0", "2", "1", "1", "0", "1", "2"], ["2018-04-16", "6", "5", "0", "1", "2", "2", "2", "0", "0", "2", "1"], ["2018-04-23", "2", "0", "0", "0", "0", "2", "1", "2", "2", "0", "1"]]}, {"headers": [], "rows": [["This example uses the sample dataset from the Search Tutorial and a field lookup to add more information to the event data. Before you run this example:Download the data set fromthis topic in the Search Tutorialand follow the instructions to upload it to your Splunk deployment.Download thePrices.csv.zipfile fromthis topic in the Search Tutorialand follow the instructions to set up your field lookup.The original data set includes aproductIdfield that is the catalog number for the items sold at the Buttercup Games online store. The field lookup adds two new fields to your events:productName, which is a descriptive name for the item, andprice, which is the cost of the item."]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}, {"headers": ["_time", "Views", "Purchases"], "rows": [["2018-04-05 00:00:00", "150.000000", "44.000000"], ["2018-04-05 00:30:00", "166.000000", "54.000000"], ["2018-04-05 01:00:00", "214.000000", "72.000000"], ["2018-04-05 01:30:00", "242.000000", "80.000000"], ["2018-04-05 02:00:00", "158.000000", "26.000000"], ["2018-04-05 02:30:00", "166.000000", "20.000000"], ["2018-04-05 03:00:00", "220.000000", "56.000000"]]}], "chunk_index": 4, "total_chunks": 7, "metadata": {"title": "timechart", "section_heading": "Extended Examples", "section_id": "id_853d4af9_8e01_4ce6_b03b_e4539861490b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:39.544263+00:00", "version": "10.2"}}
{"id": "7bca5fb3402238ed", "content": "These examples use the where clause to control the number of series values returned in the time-series chart. Example 1: Show the 5 most rare series based on the minimum count values. All other series values will be labeled as \"other\". Example 2: Show the 5 most frequent series based on the maximum values. All other series values will be labeled as \"other\". These two searches return six data series: the five top or bottom series specified and the series labeled other. To hide the \"other\" series, specify the argument useother=f. Example 3: Show the source series count of INFO events, but only where the total number of events is larger than 100. All other series values will be labeled as \"other\". Example 4: Using the where clause with the count function measures the total number of events over the period. This yields results similar to using the sum function. The following two searches returns the sources series with a total count of events greater than 100. All other series values will be labeled as \"other\".", "code_examples": [{"language": "spl", "code": "index=_internal | timechart span=1h count bysourceWHERE mininbottom5"}, {"language": "spl", "code": "index=_internal | timechart span=1h count bysourceWHERE maxintop5"}, {"language": "spl", "code": "index=_internal | timechart span=1h sum(eval(if(log_level==\"INFO\",1,0))) bysourceWHERE sum > 100"}, {"language": "spl", "code": "index=_internal | timechart span=1h count bysourceWHERE count > 100"}, {"language": "spl", "code": "index=_internal | timechart span=1h count bysourceWHERE sum > 100"}], "tables": [], "chunk_index": 5, "total_chunks": 7, "metadata": {"title": "timechart", "section_heading": "Where clause examples", "section_id": "id_775d8258_efc9_441f_83fd_a4f343cbcea4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:39.544270+00:00", "version": "10.2"}}
{"id": "3c766036d260078f", "content": "Commands bin chart sitimechart timewrap Blogs Search commands > stats, chart, and timechart", "code_examples": [], "tables": [], "chunk_index": 6, "total_chunks": 7, "metadata": {"title": "timechart", "section_heading": "See also", "section_id": "id_6f34827b_1422_4064_bacc_ccc4d7536031--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:39.544274+00:00", "version": "10.2"}}
{"id": "ae882de59f464aec", "content": "Use the tstats command to perform statistical queries on indexed fields in tsidx files. The indexed fields can be from indexed data or accelerated data models. Because it searches on index-time fields instead of raw events, the tstats command is faster than the stats command. By default, the tstats command runs over accelerated and unaccelerated data models.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 7, "metadata": {"title": "tstats", "section_heading": "Description", "section_id": "fa0aff2d_5b55_4648_943e_fd54b889c93d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:56.935188+00:00", "version": "10.2"}}
{"id": "e1cd1a0210182f70", "content": "The required syntax is in bold. | tstats [prestats=<bool>] [local=<bool>] [append=<bool>] [summariesonly=<bool>] [include_reduced_buckets=<bool>] [allow_old_summaries=<bool>] [chunk_size=<unsigned int>] [fillnull_value=<string>] <stats-func>... [ FROM datamodel=<data_model_name>.<root_dataset_name> [where nodename = <root_dataset_name>.<...>.<target_dataset_name>]] [ WHERE <search-query> | <field> IN (<value-list>)] [ BY (<field-list> | (PREFIX(<field>))) [span=<timespan>]] Required arguments <stats-func> Syntax: (count [<field>] | <function>(PREFIX(<string>) | <field>))... [AS <string>] Description: Either perform a basic count of a field or perform a function on a field. For a list of the supported functions for the tstats command, refer to the table below. You must specify one or more functions. You can apply the function to a field, or to a PREFIX() directive if you want to aggregate a raw segment in your indexed events as if it were an extracted field-value pair. You can also rename the result using the AS keyword, unless you are in prestats mode ( prestats=true ). You cannot specify functions without applying them to fields or eval expressions that resolve into fields. You cannot use wildcards to specify field names. See Usage to learn more about using PREFIX() , and about searches you can run to find raw segments in your data. The following table lists the supported functions by type of function. Use the links in the table to see descriptions and examples for each function. For an overview about using functions with commands, see Statistical and charting functions. Optional arguments append Syntax: append=<bool> Description: When in prestats mode ( prestats=true ), enables append=true where the prestats results append to existing results, instead of generating them. Default: false allow_old_summaries Syntax: allow_old_summaries=true | false Description: Only applies when selecting from an accelerated data model. When you change the constraints that define a data model but the Splunk software has not fully updated the summaries to reflect that change, the summaries may have some data that matches the old definition and some data that matches the new definition. To return results from summary directories only when those directories are up-to-date, set this parameter to false. If the data model definition has changed, summary directories that are older than the new definition are not used when producing output from the tstats command. This default ensures that the output from tstats always reflects your current configuration. When set to true , the tstats command uses both current summary data and summary data that was generated prior to the definition change. This is an advanced performance feature for cases where you know that the old summaries are \"good enough,\" meaning the old summary data is close enough to the new summary data that its results are reliable. See When the data model definition changes and your summaries have not been updated to match it in the Splunk Cloud Platform Knowledge Manager Manual. Default: false chunk_size Syntax: chunk_size=<unsigned_int> Description: Advanced option. This argument controls how many events are retrieved at a time from a single tsidx file when the Splunk software processes searches. Lower this setting from its default only when you find a particular tstats search is using too much memory, or when it infrequently returns events. This can happen when a search groups by excessively high-cardinality fields (fields with very large amounts of distinct values). In such situations, a lower chunk_size value can make tstats searches more responsive, but potentially slower to complete. However, a higher chunk_size can help long-running searches to complete faster, with the potential tradeoff of causing the search to be less responsive. For tstats , chunk_size cannot be set lower than 10000. Default: 10000000 (10 million) Note: The default value for the chunk_size argument is set by the chunk_size setting for the [tstats] stanza in limits.conf. If you have Splunk Cloud Platform, file a Support ticket to change this setting. fillnull_value Description: This argument sets a user-specified value that the tstats command substitutes for null values for any field within its group-by field list. Null values include field values that are missing from a subset of the returned events as well as field values that are missing from all of the returned events. If you do not provide a fillnull_value argument, tstats omits rows for events with one or more null field values from its results. Default: no default value include_reduced_buckets Syntax: include_reduced_buckets=true | false Description: This setting only applies when enableTSIDXReduction=true in indexes.conf. When set to false, the tstats command generates results only from index buckets that are not reduced. Set to true if you want tstats to use results from reduced buckets. Default: false local Syntax: local=true | false Description: If true , forces the tstats search processor to run only on the search head. This setting is useful for troubleshooting. For example, you can use it to determine whether data on a search head is or has been improperly accelerated. In systems that forward search head data to indexers, this setting may cause the search to produce few or no results. See Best practice: Forward search head data to the indexer layer in Splunk Enterprise Distributed Search. Default: false prestats Syntax: prestats=true | false Description: Specifies whether to use the prestats format. The prestats format is a Splunk internal format that is designed to be consumed by commands that generate aggregate calculations. When using the prestats format you can pipe the data into the chart , stats , or timechart commands, which are designed to accept the prestats format. When prestats=true , AS instructions are not relevant. The field names for the aggregates are determined by the command that consumes the prestats format and produces the aggregate output. prestats=true is an advanced setting. Use it only in special circumstances when you need to pass tstats-generated data directly to the chart , stats , or timechart command. Default: false summariesonly Syntax: summariesonly=<bool> Description: When summariesonly is set to false , if the time range of the tstats search exceeds the summarization range for the selected data model, the tstats command returns results for the entire time range of the search. It quickly returns results from the summarized data, and returns results more slowly from the raw, unsummarized data that exists outside of the data model summary range. If an accelerated data model is running behind in its summarization, or if its summarization searches are scheduled infrequently, setting summariesonly = false might result in a slower tstats search. This is because the data model has more unsummarized data to search through than usual. When summariesonly is set to true , the tstats search returns results only from summarized data, even when the time range of the search exceeds the summarization range of the data model. This means the search runs fast, but no unsummarized data is included in the search results. If you set summariesonly to true , the tstats won't run over unaccelerated data models. Also, when the tstats runs over accelerated data models, it returns events only from the data model's acceleration summary. You might set summariesonly = true if you need to identify the data that is currently summarized in a given data model, or if you value search efficiency over completeness of results. See Using the summariesonly argument in the Splunk Cloud Platform Knowledge Manager Manual. Default: false FROM clause arguments The FROM clause is optional. See Selecting data for more information about this clause. datamodel Syntax: datamodel=<data_model_name>.<root_dataset_name> [where nodename = <root_dataset_name>.<...>.<target_dataset_name>] Description: The name of a data model, concatenated with the name of the root dataset that you are searching. If you wish to filter on a child dataset, you need to use a where clause that uses nodename to reference a specific child dataset in a dataset hierarchy in the data model. See Selecting data for more information. WHERE clause arguments The optional WHERE clause is used as a filter. You can specify either a search or a field and a set of values with the IN operator. <search-query> Specify the search criteria to filter on. <field> IN (<value-list>) For the field , specify a list of values to include in the search results. WHERE clauses in tstat searches must contain field-value pairs that are indexed, as well as characters that are not major breakers or minor breakers. For example, consider the following search: The results look something like this: This search returns valid results because sourcetype=splunkd* is an indexed field-value pair and wildcard characters are accepted in the search criteria. The asterisk at the end of the sourcetype=splunkd* clause is treated as a wildcard, and is not regarded as either a major or minor breaker. BY clause arguments The BY clause is optional. You cannot use wildcards in the BY clause with the tstats command. See Usage. If you use the BY clause, you must specify a field-list. You can also specify a span. <field-list> Syntax: <field>, ... Description: Specify one or more fields to group results. PREFIX() Syntax: PREFIX(<string>) Description: Specify a raw segment in your indexed events that you want to split by as if it were an extracted field-value pair. See Usage for more information about the PREFIX() directive, and for a search you can run to find raw segments in your indexed data. span Syntax: span=<timespan> Description: The span of each time bin. If you use the BY clause to group by _time , use the span argument to group the time buckets. You can specify timespans such as BY _time span=1h or BY _time span=5d. If you do not specify a <timespan> , the default is auto , which means that the number of time buckets adjusts to produce a reasonable number of results. For example if initially seconds are used for the <timespan> and too many results are being returned, the <timespan> is changed to a longer value, such as minutes, to return fewer time buckets. Default: auto <timespan> Syntax: auto | <int><timescale> <timescale> Syntax: <sec> | <min> | <hr> | <day> | <month> Description: Time scale units. For the tstats command, <timescale> does not support subseconds. Default: sec", "code_examples": [{"language": "spl", "code": "| tstats count WHERE index=_internal sourcetype=splunkd* by sourcetype"}], "tables": [{"headers": ["Type of function", "Supported functions and syntax", "", "", ""], "rows": [["Aggregate functions", "avg()count()distinct_count()estdc()", "exactperc<int>()max()median()min()mode()", "perc<int>()range()stdev()stdevp()", "sum()sumsq()upperperc<int>()var()varp()"], ["Event order functions", "first()", "last()", "", ""], ["Multivalue stats and chart functions", "values()", "", "", ""], ["Time functions", "earliest()earliest_time()", "latest()latest_time()", "rate()", ""]]}, {"headers": ["sourcetype", "count"], "rows": [["splunkd", "2602154"], ["splunkd_access", "319019"], ["splunkd_conf", "19"]]}, {"headers": ["Time scale", "Syntax", "Description"], "rows": [["<sec>", "s | sec | secs | second | seconds", "Time scale in seconds."], ["<min>", "m | min | mins | minute | minutes", "Time scale in minutes."], ["<hr>", "h | hr | hrs | hour | hours", "Time scale in hours."], ["<day>", "d | day | days", "Time scale in days."], ["<month>", "mon | month | months", "Time scale in months."]]}], "chunk_index": 1, "total_chunks": 7, "metadata": {"title": "tstats", "section_heading": "Syntax", "section_id": "be76743a_19cc_4f95_81db_fd1c11aba063--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:56.935211+00:00", "version": "10.2"}}
{"id": "a3cd35ab6317e949", "content": "The tstats command is a report-generating command , except when prestats=true. When prestats=true , the tstats command is an event-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search, except when prestats=true. By default, the tstats command runs over accelerated and unaccelerated data models. Properly indexed fields should appear in the fields.conf file. See Create custom fields at index time in Getting Data In. When you use a statistical function with the tstats command, you can't use an eval expression as part of the statistical function. See Complex aggregate functions. Selecting data Use the tstats command to perform statistical queries on indexed fields in tsidx files. You can select the data for the indexed fields in several ways. Indexed data Use a FROM clause to specify a data model. If you do not specify a FROM clause, the Splunk software selects from index data in the same way as the search command. You are restricted to selecting data from your allowed indexes by user role. You control exactly which indexes you select data from by using the WHERE clause. If no indexes are mentioned in the WHERE clause, the Splunk software uses the default indexes. By default, role-based search filters are applied, but can be turned off in the limits.conf file. An accelerated data model You can select data from a high-performance analytics store, which is a collection of .tsidx data summaries, for an accelerated data model. You can select data from this accelerated data model by using FROM datamodel=<data_model_name>.<root_dataset_name>. When you select a data model for a tstats search, you also have to select the root dataset within that data model that you intend to search. You cannot select all of the root datasets within a data model at once. Note: Search filters cannot be applied to accelerated data models. This includes both role-based and user-based search filters. A child dataset in an accelerated data model You can select data from a child dataset within an accelerated data model. Use a WHERE clause to specify the nodename of the child dataset. The nodename argument indicates where the target dataset is in the data model hierarchy. The syntax looks like this: ...| tstats <stats-func> FROM datamodel=<data_model_name>.<root_dataset_name> where nodename=<root_dataset_name>.<...>.<target_dataset_name> For example, say you have a data model with three root datasets, each with their own dataset hierarchies. With this hierarchy, if you wanted to run a tstats search that selects from the dataset containing records of the MechaGodzilla giant robot battles staged by the Tokyo office, you would use the following search: Note: Search filters cannot be applied to accelerated data model datasets. This includes both role-based and user-based search filters. Filtering data using the WHERE clause You can use the optional WHERE clause to filter queries with the tstats command in much the same ways as you use it with the search command. For example, WHERE supports the same time arguments, such as earliest=-1y , with the tstats command and the search command. WHERE clauses used in tstats searches can contain only indexed fields. Fields that are extracted at search time are not supported. If you don't know which of your fields are indexed, run a search on a specific index using the walklex command. Grouping data by _time You can provide any number of BY fields. If you are grouping by _time , supply a timespan with span for grouping the time buckets, for example ...BY _time span=1h or ...BY _time span=3d .", "code_examples": [{"language": "spl", "code": "ButtercupGamesPromos\n     - NYC (BaseEvent)\n          - TShirtStore (NYC)\n               - FashionShows (TShirtStore)\n               - Giveaways (TShirtStore)\n     - Chicago (BaseEvent)\n          - BeerAndBrautsPopup (Chicago)\n               - BeerSales (BeerAndBrautsPopup)\n               - BrautSales (BeerAndBrautsPopup)\n     - Tokyo (BaseSearch)\n          - GiantRobotBattles (Tokyo)\n               - UFORobotGrendizer (GiantRobotBattles)\n               - MechaGodzilla (GiantRobotBattles)"}, {"language": "spl", "code": "... | tstats count FROM datamodel=ButtercupGamesPromos.Tokyowherenodename=Tokyo.GiantRobotBattles.MechaGodzilla"}], "tables": [], "chunk_index": 2, "total_chunks": 7, "metadata": {"title": "tstats", "section_heading": "Usage", "section_id": "id_52cb454f_4cd6_47ef_9844_bd9dfc2860e5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:56.935219+00:00", "version": "10.2"}}
{"id": "600663c1a0359913", "content": "Tstats and Federated Search for Splunk tstats searches that include a FROM clause are blocked for transparent mode federated searches over federated providers with Splunk Cloud Platform versions lower than 9.0.2303 or Splunk Enterprise versions lower than 9.1.0. If you use multiple transparent mode federated providers, the tstats search is processed only on federated providers with qualifying versions. For more information see About Federated Search for Splunk in Federated Search. Tstats and tsidx bucket reduction tstats searches over indexes that have undergone tsidx bucket reduction will return incorrect results. For more information see Reduce tsidx disk usage in Managing indexers and clusters of indexers. Sparkline charts You can generate sparkline charts with the tstats command only if you specify the _time field in the BY clause and use the stats command to generate the actual sparkline. For example: Multiple time ranges The tstats command is unable to handle multiple time ranges. This is because the tstats command is a generating command and doesn't perform post-search filtering, which is required to return results for multiple time ranges. The following example of a search using the tstats command on events with relative times of 5 seconds to 1 second in the past displays a warning that the results may be incorrect because the tstats command doesn't support multiple time ranges. If you want to search events in multiple time ranges, use another command such as stats , or use multiple tstats commands with append as shown in the following example. The results in this example look something like this. Wildcard characters The tstats command does not support wildcard characters in field values in aggregate functions or BY clauses. For example, you cannot specify | tstats avg(foo*) or | tstats count WHERE host=x BY source*. Aggregate functions include avg() , count() , max() , min() , and sum(). For more information, see Aggregate functions. Any results returned where the aggregate function or BY clause includes a wildcard character are only the most recent few minutes of data that has not been summarized. Include the summariesonly=t argument with your tstats command to return only summarized data. Statistical functions must have named fields With the exception of count , the tstats command supports only statistical functions that are applied to fields or eval expressions that resolve into fields. For example, you cannot specify | tstats sum or | tstats sum(). Instead the tstats syntax requires that at least one field argument be provided for the function: | tstats sum(<field>). Nested eval expressions not supported You cannot use eval expressions inside aggregate functions with the tstats command. For example, | tstats count(eval(...)) is not supported. While nested eval expressions are supported with the stats command, they are not supported with the tstats command. Complex aggregate functions The tstats command does not support complex aggregate functions such as ...count(eval('Authentication.action'==\"failure\")). Consider the following query. This query will not return accurate results because complex aggregate functions are not supported by the tstats command. Instead, separate out the aggregate functions from the eval functions, as shown in the following search. The results from this search look something like this: Limitations of CIDR matching with tstats As with the search command, you can use the tstats command to filter events with CIDR match on fields that contain IPv4 and IPv6 addresses. However, unlike the search command, the tstats command may not correctly filter strings containing non-numeric wildcard octets. As a result, your searches may return unpredictable results. If you are filtering fields with a CIDR match using the tstats command in a BY clause, you can work around this issue and correctly refilter your results by appending your search with a search command, regex command, or WHERE clause. Unfortunately, you can't use this workaround if the search doesn't include the filtered field in a BY clause. Example of using CIDR match with tstats in a BY clause Let's take a look at an example of how you could use CIDR match with the tstats command in a BY clause. Say you create a file called data.csv containing the following lines: Then follow these steps: Upload the file and set the sourcetype to csv , which ensures that all fields in the file are indexed as required by the tstats command. Run the following search against the index you specified when you uploaded the file. This example uses the main index. The results look like this: Even though only two addresses are legitimate IP addresses, all four rows of addresses are displayed in the results. Invalid IP addresses are displayed along with the valid IP addresses because the tstats command uses string matching to satisfy search requests and doesn't directly support IP address-based searches. The tstats command does its best to return the correct results for CIDR search clauses, but the tstats search may return more results than you want if the source data contains mixed IP and non-IP data such as host names. To make sure your searches only return the results you want, make sure that your data set is clean and only contains data in the correct format. If that is not possible, use the search command or WHERE clause to do post-filtering of the search results. For example, the following search using the search command displays correct results because the piped search command further filters the results from the tstats command. Alternatively, you can use the WHERE clause to filter your results, like this. Both of these searches using the search command and the WHERE clause return only the valid IP addresses in the results, which look like this: The tstats command doesn't respect the srchTimeWin parameter The tstats command doesn't respect the srchTimeWin parameter in the authorize.conf file and other role-based access controls that are intended to improve search performance. This is because the tstats command is already optimized for performance, which makes parameters like srchTimeWin irrelevant. For example, say you previously set the srchTimeWin parameter on a role for one of your users named Alex, so they are just allowed to run searches back over 1 day. You limited the search time range to prevent searches from running over longer periods of time, which could potentially impact overall system performance and slow down searches for other users. Alex has been running a stats search, but didn't notice that they were getting results for just 1 day, even though they specified 30 days. If Alex then changes their search to a tstats search, or changes their search in such a way that Splunk software automatically optimizes it to a tstats search, the 1 day setting for the srchTimeWin parameter no longer applies. As a result, Alex gets many times more results than before, since their search is returning all 30 days of events, not just 1 day of results. This is expected behavior.", "code_examples": [{"language": "spl", "code": "| tstats count from datamodel=Authentication.Authentication BY _time, Authentication.src span=1h  \n| stats sparkline(sum(count),1h) AS sparkline, sum(count) AS count BY Authentication.src"}, {"language": "spl", "code": "| tstats countwhereindex=\"_internal\"(earliest =-5s latest=-4s) OR (earliest=-3s latest=-1s)"}, {"language": "spl", "code": "| tstats prestats=t countwhereindex=_internal earliest=-5s latest=-4s \n| tstats prestats=t append=truecountwhereindex=_internal earliest=-3s latest=-2s \n| stats count"}, {"language": "spl", "code": "| tstats count(eval(server.status=200)) from datamodel=internal_server.serverwherenodename=server.splunkdaccess by server.status uri"}, {"language": "spl", "code": "| tstats count from datamodel=internal_server.serverwherenodename=server.splunkdaccess by server.status, uri\n|evalsuccess=if('server.status'=\"200\", count, 0)\n| stats sum(success) as success by uri"}, {"language": "spl", "code": "ip,description\n1.2.3.4,\"An IP address\"5.6.7.8,\"Another IP address\"this.is.a.hostname,\"A hostname\"this.is.another.hostname,\"Another hostname\""}, {"language": "spl", "code": "| tstats countwhereindex=mainsource=*data.csv ip=\"0.0.0.0/0\"by ip"}, {"language": "spl", "code": "| tstats countwhereindex=mainsource=*data.csv ip=\"0.0.0.0/0\"by ip  \n| search ip=\"0.0.0.0/0\""}, {"language": "spl", "code": "| tstats countwhereindex=mainsource=*data.csv ip=\"0.0.0.0/0\"by ip  \n| WHERE cidrmatch(\"0.0.0.0/0\", ip)"}], "tables": [{"headers": ["count"], "rows": [["264"]]}, {"headers": ["uri", "success"], "rows": [["//services/cluster/config?output_mode=json", "0"], ["//services/cluster/config?output_mode=json", "2862"], ["/services/admin/kvstore-collectionstats?count=0", "1"], ["/services/admin/transforms-lookup?count=0&getsize=true", "1"]]}, {"headers": ["ip", "count"], "rows": [["1.2.3.4", "1"], ["5.6.7.8", "1"], ["this.is.a.hostname", "1"], ["this.is.another.hostname", "1"]]}, {"headers": ["ip", "count"], "rows": [["1.2.3.4", "1"], ["5.6.7.8", "1"]]}], "chunk_index": 3, "total_chunks": 7, "metadata": {"title": "tstats", "section_heading": "Limitations", "section_id": "id_1e6e82ff_e14b_4081_a920_57f0d463d489--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:56.935236+00:00", "version": "10.2"}}
{"id": "653dcd46aab2b845", "content": "Use PREFIX() to aggregate or group by raw tokens in indexed data The PREFIX() directive allows you to search on a raw segment in your indexed data as if it were an extracted field. This causes the search to run over the tsidx file in your indexers rather than the log line. This is a practice that can significantly reduce the CPU load on your indexers. The PREFIX() directive is similar to the CASE() and TERM() directives in that it matches strings in your raw data. You can use PREFIX() to locate a recurring segment in your raw event data that is actually a key-value pair separated by a delimiter that is also a minor breaker, like = or :. You give PREFIX() the text that precedes the value, which is the \"prefix\", and then the search returns the values that follow the prefix. This enables you to group by those values and aggregate them with tstats functions. The values can be strings or purely numeric. For example, say you have indexed segments in your event data that look like kbps=10 or kbps=333. You can isolate the numerical values in these segments and perform aggregations or group-by operations on them by using the PREFIX() directive to identify kbps= as a common prefix string. Run a tstats search with PREFIX(kbps=) against your event data and it will return 10 and 333. These values are perfect for tstats aggregation functions that require purely numeric input. Notice that in this example you need to include the = delimiter. If you run PREFIX(kbps) , the search returns =10 and =333. Efforts to aggregate on such results may return unexpected results, especially if you are running them through aggregation functions that require purely numeric values. Note: The text you provide for the PREFIX() directive must be in lower case. For example, the tstats search processor will fail to process PREFIX(connectionType=). Use PREFIX(connectiontype=) instead. It will still match connectionType= strings in your events. The Splunk software separates events into raw segments when it indexes data, using rules specified in segmenters.conf. You can run the following search to identify raw segments in your indexed events: Note: You cannot apply the PREFIX() directive to segment prefixes and values that contain major breakers such as spaces, square or curly brackets, parentheses, semicolons, or exclamation points. For more information about the CASE() and TERM() directives, see Use CASE() and TERM() to match phrases in the Search Manual. For more information about the segmentation of indexed events, see About event segmentation in Getting Data In For more information about minor and major breakers in segments, see Event segmentation and searching in the Search Manual. Memory and tstats search performance A pair of limits.conf settings strike a balance between the performance of tstats searches and the amount of memory they use during the search process, in RAM and on disk. If your tstats searches are consistently slow to complete you can adjust these settings to improve their performance, but at the cost of increased search-time memory usage, which can lead to search failures. If you have Splunk Cloud Platform, you need to file a Support ticket to change these settings. For more information, see Memory and stats search performance in the Search Manual. Functions and memory usage Some functions are inherently more expensive, from a memory standpoint, than other functions. For example, the distinct_count function requires far more memory than the count function. The values and list functions also can consume a lot of memory. If you are using the distinct_count function without a split-by field or with a low-cardinality split-by by field, consider replacing the distinct_count function with the estdc function (estimated distinct count). The estdc function might result in significantly lower memory usage and run times.", "code_examples": [{"language": "spl", "code": "| walklex index=<target-index>type=term | stats sum(count) by term"}], "tables": [], "chunk_index": 4, "total_chunks": 7, "metadata": {"title": "tstats", "section_heading": "Performance", "section_id": "id_79890a08_37d0_45e3_b1d6_860dbf942904--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:56.935242+00:00", "version": "10.2"}}
{"id": "5cf53268952263f4", "content": "1. Get a count of all events in an index This search tells you how many events there are in the _internal index. 2. Use a filter to get the average This search returns the average of the field size in myindex , specifically where test is value2 and the value of result is greater than 5. Both test and result are indexed fields. 3. Return the count by splitting by source This search gives the count by source for events with host=x. 4. Produce a timechart This search produces a timechart of all the data in your default indexes with a day granularity. To avoid unpredictable results, the value of the tstats span argument should be smaller than or equal to the value of the timechart span argument. 5. Use summariesonly to get a time range of summarized data This search uses the summariesonly argument to get the time range of the summary for an accelerated data model named mydm. 6. Find out how much data has been summarized This search uses summariesonly in conjunction with the timechart command to reveal the data that has been summarized in 1 hour blocks of time for an accelerated data model called mydm. The span argument indicates how the events are grouped into buckets or blocks of time, but it doesn't indicate how long the search should run. To run your search over a specific length of time, use the time range picker in the Search app to set the time window for your search. Alternatively, you can include a WHERE clause in your search like this, which searches events in 1 hour blocks across a 3 hour time window: 7. Get a list of values for source returned by the internal log data model This search uses the values statistical function to provide a list of all distinct values for the source that is returned by the internal log data model. The list is returned as a multivalue entry. The results look something like this: Note: If you don't have the internal_server data model defined, check under Settings->Data models for a list of the data models you have access to. 8. Get a list of values for source returned by the Alerts dataset in the internal log data model This search uses the values statistical function to provide a list of all distinct values for source returned by the Alerts dataset within the internal log data model. 9. Get the count and average This search gets the count and average of a raw, unindexed term using the PREFIX kbps= , then splits this by an indexed source and another unindexed term using the PREFIX group= .", "code_examples": [{"language": "spl", "code": "| tstats count WHERE index=_internal"}, {"language": "spl", "code": "| tstats avg(size) WHERE index=myindextest=value2 result>5"}, {"language": "spl", "code": "| tstats count WHERE index=myindex host=x bysource"}, {"language": "spl", "code": "| tstats prestats=t count WHERE index=_internal BY _time span=1h\n| timechart span=1d count"}, {"language": "spl", "code": "| tstats summariesonly=t min(_time) AS min, max(_time) AS max FROM datamodel=mydm \n|evalprettymin=strftime(min,\"%c\") \n|evalprettymax=strftime(max,\"%c\")"}, {"language": "spl", "code": "| tstats summariesonly=t prestats=t count FROM datamodel=mydm BY _time span=1h \n| timechart span=1h count"}, {"language": "spl", "code": "| tstats summariesonly=f prestats=t count FROM datamodel=mydm WHERE earliest=-3h BY _time span=1h\n| timechart span=1h count"}, {"language": "spl", "code": "| tstats values(source) FROM datamodel=internal_server"}, {"language": "spl", "code": "| tstats values(source) FROM datamodel=internal_serverwherenodename=server.scheduler.alerts"}, {"language": "spl", "code": "| tstats count avg(PREFIX(exec_time=)) as avg_exec_timewhereindex=_audit by PREFIX(user=) PREFIX(action=)  fillnull_value=\"N/A\""}], "tables": [{"headers": ["values(source)"], "rows": [["/Applications/Splunk/var/log/splunk/license_usage.log/Applications/Splunk/var/log/splunk/metrics.log/Applications/Splunk/var/log/splunk/metrics.log.1/Applications/Splunk/var/log/splunk/scheduler.log/Applications/Splunk/var/log/splunk/splunkd.log/Applications/Splunk/var/log/splunk/splunkd_access.log"]]}], "chunk_index": 5, "total_chunks": 7, "metadata": {"title": "tstats", "section_heading": "Examples", "section_id": "id_0e44afe2_df81_4c18_9116_32cd5c1c3dcd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:56.935251+00:00", "version": "10.2"}}
{"id": "59b05d348b49918f", "content": "Commands datamodel stats walklex", "code_examples": [], "tables": [], "chunk_index": 6, "total_chunks": 7, "metadata": {"title": "tstats", "section_heading": "See also", "section_id": "adb332da_939b_4e22_b7fc_04d705440af4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tstats", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:32:56.935257+00:00", "version": "10.2"}}
{"id": "ab8ab6da16bb3ff0", "content": "The sitimechart command is the summary indexing version of the timechart command, which creates a time-series chart visualization with a corresponding table of statistics. The sitimechart command populates a summary index with the statistics necessary to generate a timechart report. After you use an sitimechart search to populate the summary index, use the regular timechart command with the exact same search string as the sitimechart search to report against the summary index.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "sitimechart", "section_heading": "Description", "section_id": "id_63b60fc7_2de1_4eeb_b2f3_79a72d01aab7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sitimechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:33:12.448817+00:00", "version": "10.2"}}
{"id": "5d28b07b414461f4", "content": "The required syntax is in bold. sitimechart [sep=<string>] [partial=<bool>] [cont=<bool>] [limit=<int>] [agg=<stats-agg-term>] [<bin-options>... ] <single-agg> [BY <split-by-clause>] | <eval-expression> BY <split-by-clause> When specifying sitimechart command arguments, either <single-agg> or <eval-expression> BY <split-by-clause> is required. For descriptions of each of these arguments, see the timechart command .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "sitimechart", "section_heading": "Syntax", "section_id": "a4075eef_c6c6_46ce_b641_32af70086f81--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sitimechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:33:12.448825+00:00", "version": "10.2"}}
{"id": "05a10969d50d4c85", "content": "Supported functions You can use a wide range of functions with the sitimechart command. For general information about using functions, see Statistical and charting functions. For a list of functions by category, see Function list by category For an alphabetical list of functions, see Alphabetical list of functions", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "sitimechart", "section_heading": "Usage", "section_id": "id_9513cfc4_209a_4f8a_a065_127b9159c16e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sitimechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:33:12.448830+00:00", "version": "10.2"}}
{"id": "c014050c11246f01", "content": "Example 1: Use the collect command to populate a summary index called mysummary with the statistics about CPU usage organized by host, Note: The collect command adds the results of a search to a summary index that you specify. You must create the summary index before you invoke the collect command. Then use the timechart command with the same search to generate a timechart report.", "code_examples": [{"language": "spl", "code": "... | sitimechart avg(cpu) BY host | collect index=mysummary"}, {"language": "spl", "code": "index=mysummary | timechart avg(cpu) BY host"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "sitimechart", "section_heading": "Examples", "section_id": "add0ecef_06e2_4a05_ada8_33391c794a95--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sitimechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:33:12.448834+00:00", "version": "10.2"}}
{"id": "bc7b4f7a439065c7", "content": "collect , overlap , sichart , sirare , sistats , sitop", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "sitimechart", "section_heading": "See also", "section_id": "id_4d2aca80_ce90_4f12_aed8_ae7cfec2b599--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sitimechart", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:33:12.448843+00:00", "version": "10.2"}}
{"id": "0da7ac4e6d3b5b05", "content": "Highlights specified terms in the events list. Matches a string or list of strings and highlights them in the display in Splunk Web. The matching is not case sensitive.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "highlight", "section_heading": "Description", "section_id": "f4022938_872f_46df_ba38_efcf3ef6f534--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/highlight", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:33:28.213321+00:00", "version": "10.2"}}
{"id": "91ac48cd9a6afc00", "content": "highlight <string>... Required arguments <string> Syntax: <string> ... Description: A space-separated list of strings to highlight in the results. The list you specify is not case-sensitive. Any combination of uppercase and lowercase letters that match the string are highlighted.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "highlight", "section_heading": "Syntax", "section_id": "a00bf404_b5ba_472f_abe8_9a40c39ae701--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/highlight", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:33:28.213329+00:00", "version": "10.2"}}
{"id": "f40e1ce9a18c26bb", "content": "The highlight command is a distributable streaming command. See Command types. The string that you specify must be a field value. The string cannot be a field name. You must use the highlight command in a search that keeps the raw events and displays output on the Events tab. You cannot use the highlight command with commands, such as stats which produce calculated or generated results.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "highlight", "section_heading": "Usage", "section_id": "id_6cd6d58c_8b7f_4559_94cf_6029b872dc9d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/highlight", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:33:28.213334+00:00", "version": "10.2"}}
{"id": "ea4d79d52024d452", "content": "Example 1: Highlight the terms \"login\" and \"logout\". Example 2: Highlight the phrase \"Access Denied\".", "code_examples": [{"language": "spl", "code": "... | highlight login,logout"}, {"language": "spl", "code": "... | highlight\"access denied\""}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "highlight", "section_heading": "Examples", "section_id": "d7417568_56b1_415a_8f63_b7c2f17a45d1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/highlight", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:33:28.213338+00:00", "version": "10.2"}}
{"id": "77b2db615e8843f7", "content": "rangemap", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "highlight", "section_heading": "See also", "section_id": "id_2bcbb084_8b1c_424c_b87a_23a414f340d9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/highlight", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:33:28.213343+00:00", "version": "10.2"}}
{"id": "a54da25a90b2559d", "content": "The eventstats command works in exactly the same manner as the stats command, except that the aggregation results of the command are added inline to each event, and only the aggregations that are pertinent to each event. Using split-by clauses To fully utilize the stats command, you need to include a \"split by\" clause. For example, the following report won't provide much information: It gives you the average of kbps for all events with a sourcetype of access_combined , which returns a single value. The resulting column chart contains only one column. But if you break out the report with a split by field, Splunk software generates a report that breaks down the statistics by that field. The following report generates a column chart that sorts through the access_combined logs to get the average thruput (kbps), broken out by host:", "code_examples": [{"language": "spl", "code": "sourcetype=access_combined | stats avg(kbps)"}, {"language": "spl", "code": "sourcetype=access_combined | stats avg(kbps) by host"}], "tables": [], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Create reports that display summary statistics", "section_heading": "The stats and eventstats commands", "section_id": "id_7a219e87_8a95_4c3e_9155_5915e29cbb15--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/create-reports-that-display-summary-statistics", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:33:45.199180+00:00", "version": "10.2"}}
{"id": "3623220f873c118a", "content": "Example 1: Create a report that shows you the CPU utilization of Splunk processes, sorted in descending order: Example 2: Create a report to display the average kbps for all events with a sourcetype of access_combined , broken out by host. You specify the field name for the eventstats results by adding the as argument. So the first example above could be restated with \"avgkbps\" being the name of the new field that contains the results of the eventstats avg(kbps) operation: When you run this set of commands, Splunk software adds a new avgkbps field to each sourcetype=access_combined event that includes the kbps field. The value of avgkbps is the average kbps for that event.", "code_examples": [{"language": "spl", "code": "index=_internal\"group=pipeline\"| stats sum(cpu_seconds) by processor | sort sum(cpu_seconds) desc"}, {"language": "spl", "code": "sourcetype=access_combined | eventstats avg(kbps) as avgkbps by host"}], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Create reports that display summary statistics", "section_heading": "Examples", "section_id": "id_2ab89147_740d_4895_a5fb_302886a57ddd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/create-reports-that-display-summary-statistics", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:33:45.199189+00:00", "version": "10.2"}}
{"id": "81ee7a46168a8fc9", "content": "Let's say you want to report on data from a cluster of application servers. The events gathered from each server contain information such as counts of active sessions, requests handled since last update, etc. and are placed in the applications_servers index. You want to display each server instance and the number of sessions per instance on the same timechart so that you can compare the distributions of sessions and load. Ideally, you want to be able to run a timechart report, such as: However, the timechart command does not support multiple data series. Instead, you need run a search similar to the following:", "code_examples": [{"language": "spl", "code": "index=application_servers \n| timechart sum(handledRequests) avg(sessions) bysource"}, {"language": "spl", "code": "index=application_servers \n| bin _time \n| stats sum(handledRequests) as hRs, avg(sessions) as ssns by _time,source|evals1=\"handledReqs sessions\"| makemv s1 \n| mvexpand s1 \n|evalyval=case(s1==\"handledReqs\",hRs,s1==\"sessions\",ssns) \n|evalseries=source+\":\"+s1 \n| xyseries _time,series,yval"}], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Build a chart of multiple data series", "section_heading": "Scenario", "section_id": "id_2714f23e_7072_4146_97bf_9da2491063c5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/build-a-chart-of-multiple-data-series", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:33:58.704805+00:00", "version": "10.2"}}
{"id": "926c15fb449eedfe", "content": "Let's break this search down so it's easier to understand what each part of the search is doing: The first thing that you need to do, before the stats command, is to separate the events by time. The stats command is used to calculate statistics for each source value: The sum of handledRequests values are renamed as hRs , and the average number of sessions are renamed as ssns. The following portion of the search uses the eval command to add a single-valued field called \"s1\" to each result from the stats command. Then, the makemv command converts the values in the s1 field into a multivalued field, where the first value is \"handledRequests\" and the second value is \"sessions\". The mvexpand command then creates separate series for each value of s1. Then the search uses the eval command to define a new field called \"yval\", and assign values to the field based on the case that it matches. So, if the value of s1 is \"handledRequests\", the yval field is assigned the \"hRs\" value. And, if the value of the s1 field is \"sessions\", the yval field is assigned the \"ssns\" value. Then the search uses the eval command to define a new field called \"series\", which concatenates the value of the source and s1 fields. Finally, the xyseries command is used to define a chart with _time on the x-axis, yval on the y-axis, and data defined by the series field.", "code_examples": [{"language": "spl", "code": "... | bin _time"}, {"language": "spl", "code": "... | stats sum(handledRequests) as hRs, avg(sessions) as ssns by _time,source"}, {"language": "spl", "code": "... |evals1=\"handledRequests sessions\"| makemv s1 | mvexpand s1"}, {"language": "spl", "code": "... |evalyval=case(s1==\"handledRequests\",hRs,s1==\"sessions\",ssns)"}, {"language": "spl", "code": "... |evalseries=source+\":\"+s1"}, {"language": "spl", "code": "... | xyseries _time, yval, series"}], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Build a chart of multiple data series", "section_heading": "Walkthrough", "section_id": "id_258c8fc5_6242_4d68_9118_ff3eeaa7b2ea--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/build-a-chart-of-multiple-data-series", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:33:58.704815+00:00", "version": "10.2"}}
{"id": "ca82b1a37bc338fe", "content": "In the Search Reference , see the following commands: bin eval makemv mvexpand stats xyseries", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Build a chart of multiple data series", "section_heading": "See also", "section_id": "id_4102827d_1860_4ff0_91c0_702b20c58b35--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/build-a-chart-of-multiple-data-series", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:33:58.704820+00:00", "version": "10.2"}}
{"id": "92b49c9e4cabc9b3", "content": "The timechart command generates a table of summary statistics. This table can then be formatted as a chart visualization, where your data is plotted against an x-axis that is always a time field. Use the timechart command to display statistical trends over time You can split the data with another field as a separate series in the chart. Timechart visualizations are usually line, area, or column charts. When you use the timechart command, the x-axis represents time. The y-axis can be any other field value, count of values, or statistical calculation of a field value. For more information, see the Data structure requirements for visualizations in the Dashboards and Visualizations manual.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Create time-based charts", "section_heading": "The timechart command", "section_id": "abc129df_7267_401c_b363_005ac8a44ce0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/create-time-based-charts", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:34:16.577140+00:00", "version": "10.2"}}
{"id": "4c4e8fbb8e3f4a6b", "content": "Example 1: This report uses internal Splunk log data to visualize the average indexing thruput (indexing kbps) of Splunk processes over time. The information is separated, or split, by processor:", "code_examples": [{"language": "spl", "code": "index=_internal\"group=thruput\"| timechart avg(instantaneous_eps) by processor"}], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Create time-based charts", "section_heading": "Examples", "section_id": "e930ad8e_4af5_4f21_9867_519e3d534088--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/create-time-based-charts", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:34:16.577148+00:00", "version": "10.2"}}
{"id": "04d134aa335fa642", "content": "Build a chart of multiple data series", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Create time-based charts", "section_heading": "See also", "section_id": "ee215f90_859d_42c8_b7b6_0bddce12137d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/create-time-based-charts", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:34:16.577153+00:00", "version": "10.2"}}
{"id": "f86a3d78bb605517", "content": "Real-time searches scan events as the events arrive for indexing. When you kick off a real-time search, Splunk software scans the incoming events. The scan looks for events that contain index-time fields that indicate the event could be a match for your search. As the real-time search runs, the software periodically evaluates the scanned events against your search criteria to find actual matches within the sliding time range window that you have defined for the search. The number of matching events can fluctuate up or down over time as the search discovers matching events at a faster or slower rate. If you are running the search in Splunk Web, the search timeline also displays the matching events that the search has returned within the chosen time range. Here is an example of a real-time search with a one minute time range window. At the point that the following screen capture was taken, the search had scanned a total of 436 events since it was launched. The matching event count of 333 represents the number of events matching the search criteria that were identified in the past minute. This number fluctuated between 312 and 357 for the following minute. If the number spiked or dropped dramatically, that could indicate that something interesting was happening that requires a closer look. As you can see, the newest events are on the right side of the timeline. As time passes, the events move left until the events move off the left side, disappearing from the time range window entirely. A real-time search should continue running until you or another user stops the search or deletes the search job. The real-time search should not \"time out\" for any other reason. If your events are stopping it could be a performance-related issue (see Expected performance and known limitations ). Real-time searches can take advantage of all search functionality, including advanced functionality like lookups, transactions, and so on. There are also search commands that are to be used specifically in conjunction with real-time searches, such as streamstats and rtorder. Note that real-time searches are resource intensive and can impact the overall health and performance of your searches.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "About real-time searches and reports", "section_heading": "Real-time search mechanics", "section_id": "id_7c6cf99f_ad91_4c10_af30_d4c22322bd53--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/about-real-time-searches-and-reports", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T13:34:33.459760+00:00", "version": "10.2"}}
{"id": "15665c73d8c6e8e2", "content": "By default, when you run a real-time search, the search runs before events are indexed. Alternatively, you can configure your real-time searches to run after the events are indexed, which can greatly improve indexing performance. This is especially true if you're running a lot of concurrent real-time searches because indexed real-time search decreases the impact on the indexer. Indexed real-time search runs searches like historical searches , but also continually updates the search with new events as the events appear on disk. Use indexed real-time search when up-to-the-second accuracy is not needed. Splunk Cloud Platform For Splunk Cloud Platform on Victoria Experience, indexed real-time search is turned on. Splunk Enterprise For Splunk Enterprise, indexed real-time search is turned off by default. To turn on indexed real-time search, follow these steps. Prerequisites Only users with file system access, such as system administrators, can turn on indexed real-time search. Review the steps in How to edit a configuration file in the Admin Manual. Note: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make the changes in the local directory. Steps Open the local limits.conf file for the Search app. For example, $SPLUNK_HOME/etc/apps/<app_name>/local. Under the [realtime] stanza, set indexed_realtime_use_by_default to true. Indexed real-time search and newly added search peers If your deployment includes newly added search peers and you are using indexed real-time search, all events from the new search peers show up by default in historical searches. However, existing indexed real-time searches will not pick up events from the search peers until you kick off another indexed real-time search. As a result, you should restart your indexed real-time searches every time you add new search peers. If you want to automatically pick up all events in your real-time searches after you've added new search peers to your deployment, use real-time search instead of indexed real-time search. You can do this by changing the indexed_realtime_use_by_default setting in the local limits.conf file from true back to the default, which is false. See How to edit a configuration file in the Splunk Enterprise Admin Manual. Note: Using real-time search by setting indexed_realtime_use_by_default to false makes events available to searches with lower latency, but reduces indexing throughput. About the sync delay lag time The results returned by an indexed real-time search will always lag behind a real-time search. Built into indexed real-time searches is a sync (synchronizing) delay. The sync delay is a precaution so that none of the data is missed. Indexed data does not necessarily appear on disk in the order that the data is indexed because: Multiple threads are used for indexing simultaneously The sync delay ordering that is on your operating system An indexed real-time must remember the latest indexed event that is returned for the current iteration of the time range window. That event is used as the start point for the next iteration of the time range window. If a sync delay is not imposed, some of the events before the latest event might not be searchable yet. These events are not returned during that iteration of the time range window and will never be returned. The likelihood of an unreturned event increases as the indexing and system load increases. You can control the number of seconds of sync delay lag time with the indexed_realtime_disk_sync_delay = <int> setting. By default, this delay is set to 60 seconds. The default of 60 seconds is fairly conservative. For most systems a 30 second delay will probably work successfully. If, for your system and usage, it is acceptable for indexed real-time searches to miss some events, you can set a very low or 0 sync delay. However, you will not be able to tell if you are missing events, except for searches that should match all events. Other indexed real time settings There are other settings that you can use to configure indexed real-time search behavior, including: indexed_realtime_default_span indexed_realtime_maximum_span indexed_realtime_cluster_update_interval These settings are described in the limits.conf.spec file.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "About real-time searches and reports", "section_heading": "Indexed real-time search", "section_id": "id_6a359d2b_f61f_4d6d_87cd_e7e8018d7d26--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/about-real-time-searches-and-reports", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T13:34:33.459768+00:00", "version": "10.2"}}
{"id": "9333d954c2aa693b", "content": "Real-time searches and reports in Splunk Web Real-time searches and reports in the CLI Expected performance and known limitations of real-time searches and reports How to restrict usage of real-time searches Blogs Splunk: Limiting Real-Time Searches and Maximizing Performance Gainz by Hurricane Labs", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "About real-time searches and reports", "section_heading": "See also", "section_id": "dd6dd84d_3c46_4ad9_b5fb_9c11f7826422--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/about-real-time-searches-and-reports", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T13:34:33.459772+00:00", "version": "10.2"}}
{"id": "270ec11a2e9bb519", "content": "The chart command returns your results in a data structure that supports visualization of your data series as a chart such as a column, line, area, and pie chart. Unlike the timechart command, which uses _time as the default field as the x-axis, charts created with the chart command use an arbitrary field as the x-axis. With the chart command, you use the over keyword to determine what field takes the x-axis.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Create charts that are not (necessarily) time-based", "section_heading": "The chart command", "section_id": "id_3de506de_83a0_4f82_bf36_cf3b96828f6a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/create-charts-that-are-not-necessarily-time-based", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:34:49.595415+00:00", "version": "10.2"}}
{"id": "da0bb80e74cd7e68", "content": "Example 1: Use web access data to show the average count of unique visitors over each weekday. One of the options you have is to split the data by another field, meaning that each distinct value of the \"split by\" field is a separate series in the chart. If your search includes a \"split by\" clause, place the over clause before the \"split by\" clause. The following report generates a chart showing the sum of kilobytes processed by each clientip within a given timeframe, split by host. The finished chart shows the bytes value taking the y-axis while clientip takes the x-axis. The delay value is broken out by host. After you run this search, format the report as a stacked bar chart. Example 2: Create a stacked bar chart that splits out the http and https requests hitting your servers. To do this, first create ssl_type , a search-time field extraction that contains the inbound port number or the incoming URL request, assuming that it is logged. The finished search would look like this: After you run the search, format the results as a stacked bar chart.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | chart avg(clientip) over date_wday"}, {"language": "spl", "code": "sourcetype=access_* | chart sum(bytes) over clientip by host"}, {"language": "spl", "code": "sourcetype=access_* | chart count over ssl_type"}], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Create charts that are not (necessarily) time-based", "section_heading": "Examples", "section_id": "id_4db9ff75_0de6_4ae0_849e_7af9d876e9a6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/create-statistical-tables-and-chart-visualizations/create-charts-that-are-not-necessarily-time-based", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Create Statistical Tables and Chart Visualizations", "manual": "search-manual", "scraped_at": "2026-01-23T13:34:49.595423+00:00", "version": "10.2"}}
{"id": "376448818943ba07", "content": "A multivalue field is a field that contains more than one value. For example, events such as email logs often have multivalue fields in the To: and Cc: information. Multivalue fields are parsed at search time , which enables you to process the values in the search pipeline. Search commands that work with multivalue fields include makemv , mvcombine , mvexpand , and nomv. The eval and where commands support functions, such as mvcount(), mvfilter(), mvindex(), and mvjoin() that you can use with multivalue fields. See Evaluation functions in the Search Reference and the examples in this topic. You can also use the statistical eval functions, max and min , on multivalue fields. See Statistical eval functions in the Search Reference. If you are using Splunk Enterprise, you can configure multivalue fields in the fields.conf file to specify how Splunk software detects more than one field value in a single extracted field value. Edit the fields.conf in $SPLUNK_HOME/etc/system/local/ , or your own custom application directory in $SPLUNK_HOME/etc/apps/. For more information on how to do this, see Configure extractions of multivalue fields with fields.conf in the Knowledge Manager Manual. If your search produces results, such as a table, the results get written to the results.csv.gz file. The contents of the results.csv.gz file include fields that begin with \"__mv_\". These fields are for internal use only and are used to encode multivalue fields. Note: For Splunk Cloud Platform, you must create a private app to configure multivalue fields. If you are a Splunk Cloud Platform administrator with experience creating private apps, see Manage private apps in your Splunk Cloud Platform deployment in the Splunk Cloud Platform Admin Manual. If you have not created private apps, contact your Splunk account representative for help with this customization.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Evaluate and manipulate fields with multiple values", "section_heading": "About multivalue fields", "section_id": "id_8937d53e_a483_4f11_be92_aa744ee084ff--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/evaluate-and-manipulate-fields-with-multiple-values", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:06.454492+00:00", "version": "10.2"}}
{"id": "9fb009bb5c915603", "content": "One of the more common examples of multivalue fields is email address fields, which typically appear two or three times in a single sendmail event--one time for the sender, another time for the list of recipients, and possibly a third time for the list of Cc addresses. Count the number of values in a field Use the mvcount() function to count the number of values in a single value or multivalue field. In this example, mvcount() returns the number of email addresses in the To, From, and Cc fields and saves the addresses in the specified \"_count\" fields. This search takes the values in the To field and uses the split function to separate the email address on the @ symbol. The split function is also used on the Cc field for the same purpose. If only a single email address exists in the From field, as you would expect, mvcount(From) returns 1. If there is no Cc address, the Cc field might not exist for the event. In that situation mvcount(cc) returns NULL. Filter values from a multivalue field Use the mvfilter() function to filter a multivalue field using an arbitrary Boolean expression. The mvfilter function works with only one field at a time. In this example, mvfilter() keeps all of the values for the field email that end in .net or .org. Note: This example also uses the match() function to compare the pattern defined in quotes to the value of email. See Evaluation functions in the Search Reference. Return a subset of values from a multivalue field Use the mvindex() function to reference a specific value or a subset of values in a multivalue field. Since the index numbering starts at 0, if you want to reference the 3rd value of a field, you would specify it as 2. In this example, mvindex() returns the first email address in the \"To\" field for every email sent by Sender: If you want to see the top 3 email addresses that Sender writes to, use the following search. In this example, top_three is, itself, a multivalue field.", "code_examples": [{"language": "spl", "code": "eventtype=\"sendmail\"|evalTo_count=mvcount(split(To,\"@\"))-1 |evalFrom_count=mvcount(From) |evalCc_count= mvcount(split(Cc,\"@\"))-1"}, {"language": "spl", "code": "eventtype=\"sendmail\"|evalemail=mvfilter(match(email,\"\\.net$\") OR match(email,\"\\.org$\"))"}, {"language": "spl", "code": "eventtype=\"sendmail\"from=Sender@* |evalto_first=mvindex(to,0)"}, {"language": "spl", "code": "eventtype=\"sendmail\"from=Sender@* |evaltop_three=mvindex(to,0,2)"}], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Evaluate and manipulate fields with multiple values", "section_heading": "Evaluate multivalue fields", "section_id": "id_83fbfc7b_0151_4afa_bc86_9452d7c20472--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/evaluate-and-manipulate-fields-with-multiple-values", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:06.454501+00:00", "version": "10.2"}}
{"id": "658b64ebfc6cacee", "content": "Use nomv to convert a multivalue field into a single value You can use the nomv command to convert values of the specified multivalue field into one single value. The nomv command overrides the multivalue field configurations that are set in fields.conf file. In this example for sendmail events, you want to combine the values of the senders field into a single value. Use makemv to separate a multivalue field You can use the makemv command to separate multivalue fields into multiple single value fields. In this example for sendmail search results, you want to separate the values of the senders field into multiple field values. After you separate the field values, you can pass the results to other commands. For example, you can display the top senders. Use mvexpand to create multiple events based on a multivalue field You can use the mvexpand command to expand the values of a multivalue field into separate events for each value of the multivalue field. In this example, new events are created for each value in the multivalue field, \"address\". Use mvcombine to create a multivalue field from similar events Combine the values in the field temperature , using the a colon ( : ) as a delimiter.", "code_examples": [{"language": "spl", "code": "eventtype=\"sendmail\"| nomv senders"}, {"language": "spl", "code": "eventtype=\"sendmail\"| makemv delim=\",\"senders"}, {"language": "spl", "code": "eventtype=\"sendmail\"| makemv delim=\",\"senders | top senders"}, {"language": "spl", "code": "... | mvexpand address"}, {"language": "spl", "code": "... | mvcombine delim=\":\"temperature"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Evaluate and manipulate fields with multiple values", "section_heading": "Manipulate multivalue fields", "section_id": "id_4881527b_bc9e_482a_a915_4bdae1767627--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/evaluate-and-manipulate-fields-with-multiple-values", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:06.454506+00:00", "version": "10.2"}}
{"id": "1f4cb9598475d7ca", "content": "Configure extractions of multivalue fields with fields.conf in the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Evaluate and manipulate fields with multiple values", "section_heading": "See also", "section_id": "f4b069d4_fb5a_465f_a36c_1e70952b96ba--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/evaluate-and-manipulate-fields-with-multiple-values", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:06.454511+00:00", "version": "10.2"}}
{"id": "88685447c1de9697", "content": "You have a field lookup named dnslookup which references a Python script that performs a DNS and reverse DNS lookup and accepts either a host name or IP address as arguments. You can use the lookup command to match the host name values in your events to the host name values in the lookup table, and add the corresponding IP address values to your events.", "code_examples": [{"language": "spl", "code": "... | lookup dnslookup clienthost AS host OUTPUT clientip"}], "tables": [], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Use lookup to add fields from lookup tables", "section_heading": "Example", "section_id": "id_7dd6e48e_95b7_4437_8482_b8add4a984dc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/use-lookup-to-add-fields-from-lookup-tables", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:23.128049+00:00", "version": "10.2"}}
{"id": "2a55aa2a888e4623", "content": "Configure external lookups in the Knowledge Manager Manual", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Use lookup to add fields from lookup tables", "section_heading": "See also", "section_id": "id_063309a8_de97_42fb_88f8_2504d02f511d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/use-lookup-to-add-fields-from-lookup-tables", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:23.128057+00:00", "version": "10.2"}}
{"id": "77351c305c660486", "content": "An eval expression is a combination of literals, fields, operators, and functions that represent the value of your destination field. The expression can involve a mathematical operation, a string concatenation, a comparison expression, a Boolean expression, or a call to one of the eval functions. eval expressions require that the field's values are valid for the type of operation. For example, with the exception of addition, arithmetic operations may not produce valid results if the values are not numerical. For addition, the eval command can concatenate the two operands if they are both strings. When concatenating values with a period (. ), the eval command treats both values as strings, regardless of their actual type. You can think of an eval expression as everything that follows the eval command, typically to the next pipe. For example, this is an eval expression: You can use eval expressions with the stats command. For example, the eval expression in this search is (eval(status=404)) , which is followed by an AS clause: Example 1: Use an eval expression with a stats function This example searches the status field which contains HTTP status codes like 200 an 404. If you run this search, it returns a total count of all events with a value in the status field. This results look something like this: You can add a BY clause to organize the count by HTTP code. This results look something like this: However, if you want the results for only one specific status, you can use an eval expression. Because the eval expression returns an evaluated field, you must use the AS keyword to specify a name for the evaluated field. This search retrieves all of the events where the sourcetype is any Apache web access log and counts the number of events where the status field value is 404. The results are placed into a field called status_count. The results look something like this: You can arrange the results based on another field using a BY clause. The results look something like this: Example 2: Define a field that is the sum of the areas of two circles Use the eval command to define a field that is the sum of the areas of two circles, A and B. The area of circle is Ï€r^2 , where r is the radius. For circles A and B, the radii are radius_a and radius_b, respectively. This eval expression uses the pi and pow functions to calculate the area of each circle, then adds them together and saves the result in a field named sum_of_areas. Example 3: Define a location field using the city and state fields Use the eval command to define a location field using the city and state fields. For example, if the city=Philadelphia and state=PA, you can define a new field called location where location=Philadelphia, PA. This eval expression is a simple string concatenation. Example 4: Use eval functions to classify where an email came from This example classifies where an email came from based on the email address domain. The .com, .net, and .org addresses are considered local , while anything else is considered abroad. There are many domain names. Of course, domains that are not .com, .net, or .org are not necessarily from abroad. This is just an example. The eval command in this search contains multiple expressions, separated by commas. The first half of this search is similar to previous example. The split() function is used to break up the email address in the mailfrom field. The mvindex function defines the from_domain as the portion of the mailfrom field after the @ symbol. Then, the if() and match() functions are used. If the from_domain value ends with a .com, .net., or .org , the location field is assigned the value local. If from_domain does not match .com, .net, or .org , the location field is assigned the value abroad. The results are then piped into the stats command to count the number of results for each location value. The results appear on the Statistics tab and look something like this: Note: This example merely illustrates using the match() function. If you want to classify your events and quickly search for those events, the better approach is to use event types. Read more about event types in the Knowledge Manager Manual .", "code_examples": [{"language": "spl", "code": "... |evallocation=city.\", \".state"}, {"language": "spl", "code": "sourcetype=access_* | stats count(eval(status=404)) AS status_count"}, {"language": "spl", "code": "sourcetype=access_* | stats count(status)"}, {"language": "spl", "code": "sourcetype=access_* | stats count(status) by status"}, {"language": "spl", "code": "sourcetype=access_* | stats count(eval(status=404)) AS status_count"}, {"language": "spl", "code": "sourcetype=access_* | stats count(eval(status=\"404\")) AS status_count BYsource"}, {"language": "spl", "code": "... |evalsum_of_areas = pi() * pow(radius_a, 2) + pi() * pow(radius_b, 2)"}, {"language": "spl", "code": "... |evallocation=city.\", \".state"}, {"language": "spl", "code": "sourcetype=\"cisco:esa\"mailfrom=*|evalaccountname=split(mailfrom,\"@\"),  from_domain=mvindex(accountname,-1), location=if(match(from_domain,\"[^\\n\\r\\s]+\\.(com|net|org)\"),\"local\",\"abroad\") | stats count BY location"}], "tables": [{"headers": ["count(status)"], "rows": [["39532"]]}, {"headers": ["status", "count(status)"], "rows": [["200", "34282"], ["400", "701"], ["403", "228"], ["404", "690"], ["406", "710"], ["408", "756"], ["500", "733"], ["503", "952"], ["505", "480"]]}, {"headers": ["status_count"], "rows": [["690"]]}, {"headers": ["source", "status_count"], "rows": [["tutorialdata.zip:./www1/access.log", "244"], ["tutorialdata.zip:./www2/access.log", "209"], ["tutorialdata.zip:./www3/access.log", "237"]]}, {"headers": [], "rows": [["This example uses sample email data. You should be able to run this search on any email data by replacing thesourcetype=cisco:esawith thesourcetypevalue and themailfromfield with the name of the email address field  in your data. For example, the email might beTo,From, orCc)."]]}, {"headers": ["location", "count"], "rows": [["abroad", "3543"], ["local", "14136"]]}], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Use the eval command and functions", "section_heading": "Types of eval expressions", "section_id": "id_214fd3f9_0f74_4c02_9ef5_036ef7e347da--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/use-the-eval-command-and-functions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:39.807865+00:00", "version": "10.2"}}
{"id": "1a1610be9ec3e095", "content": "If you find that you use a particular eval expression on a regular basis, consider defining the field as a calculated field. Doing this means that when you're writing a search, you can omit the eval expression and refer to the field like you do any other extracted field. When you run the search, the fields will be extracted at search time and will be added to the events that include the fields in the eval expressions. Read more about how to configure this in Define calculated fields in the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Use the eval command and functions", "section_heading": "Defining calculated fields", "section_id": "b9de1c47_f53c_43a4_9276_cb7870aa2349--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/use-the-eval-command-and-functions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:39.807874+00:00", "version": "10.2"}}
{"id": "34cfe956d514aca3", "content": "The rex command performs field extractions using named groups in Perl regular expressions that you include in the search criteria. The rex command matches segments of your raw events with the regular expression and saves these matched values into a field. In this example, values that occur after the strings From: and To: are saved into the from and to fields. If a raw event contains From: Susan To: Bob , the search extracts the field name and value pairs: from= Susan and to= Bob. For a primer on regular expression syntax and usage, see www.regular-expressions.info. The following are useful third-party tools for writing and testing regular expressions: regex101 RegExr Debuggex", "code_examples": [{"language": "spl", "code": "... | rex field=_raw\"From: (?<from>.*) To: (?<to>.*)\""}], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "Extract fields with search commands", "section_heading": "Extract fields using regular expressions", "section_id": "id_6bdc3851_b528_4163_bde5_699b839dcd82--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/extract-fields-with-search-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:56.627999+00:00", "version": "10.2"}}
{"id": "38e0dafd3a6a744b", "content": "The extract command forces field/value extraction on the result set. If you use the extract command without specifying any arguments, fields are extracted using field extraction stanzas that have been added to the props.conf file. You can also use the extract command to test field extractions that you add to the conf files.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "Extract fields with search commands", "section_heading": "Extract fields from .conf files", "section_id": "id_0095fcca_393b_462c_9e4b_2454a96ea853--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/extract-fields-with-search-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:56.628007+00:00", "version": "10.2"}}
{"id": "465505e6f9312f68", "content": "Use the multikv command to force field and value extractions on multiline, tabular-formatted events. The multikv command creates a new event for each table row and derives field names from the table title.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "Extract fields with search commands", "section_heading": "Extract fields from events formatted as tables", "section_id": "a39f1a5b_d6ad_40dd_bd09_a4c818748864--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/extract-fields-with-search-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:56.628012+00:00", "version": "10.2"}}
{"id": "01bcc1bfebbcb6ef", "content": "The xmlkv command enables you to force field and value extractions on XML-formatted tags in event data, such as transactions from web pages.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "Extract fields with search commands", "section_heading": "Extract fields from events formatted in XML", "section_id": "id_1e1656bf_e07f_4f0b_b3cf_0b64c8df8b2b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/extract-fields-with-search-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:56.628016+00:00", "version": "10.2"}}
{"id": "39a7764c08d55f8b", "content": "The spath command extracts information from structured data formats, such as XML and JSON, and store the extracted values in fields.", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "Extract fields with search commands", "section_heading": "Extract fields from XML and JSON documents", "section_id": "ce57668c_a0c2_450a_8d19_9c11fe422c76--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/extract-fields-with-search-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:56.628020+00:00", "version": "10.2"}}
{"id": "8bf72a7c4f5dfacc", "content": "The kvform command extracts field and value pairs from events based on form templates that are predefined and stored in $SPLUNK_HOME/etc/system/local/ , or your own custom application directory in $SPLUNK_HOME/etc/apps/. For example, if form=sales_order , the search looks for a sales_order.form , and matches all processed events against that form to extract values. Note: For Splunk Cloud Platform, you must create a private app to extract fields using form templates. If you are a Splunk Cloud administrator with experience creating private apps, see Manage private apps in your Splunk Cloud deployment in the Splunk Cloud Admin Manual. If you have not created private apps, contact your Splunk account representative for help with this customization.", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "Extract fields with search commands", "section_heading": "Extract fields from events based on form templates", "section_id": "id_04ecf3fc_0dc6_43f2_a20e_095061f335fd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/evaluate-and-manipulate-fields/extract-fields-with-search-commands", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Evaluate and Manipulate Fields", "manual": "search-manual", "scraped_at": "2026-01-23T13:35:56.628024+00:00", "version": "10.2"}}
{"id": "8ed73ac29475493e", "content": "In Splunk Web, to view a list of your jobs select Activity > Jobs. This opens the Job Management page. The Job Management page displays a list of the following different types of search jobs. Jobs resulting from ad hoc searches or pivots that you have recently run manually. Jobs for searches that are run when dashboards are loaded or reports are opened. Jobs for scheduled searches.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 8, "metadata": {"title": "Manage search jobs", "section_heading": "Opening the Job Management page", "section_id": "eee653cf_5a5b_4a32_ba59_0622c06d4838--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/manage-search-jobs", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:12.996899+00:00", "version": "10.2"}}
{"id": "af1efee06a74583c", "content": "The list of jobs in the Job Management page does not automatically refresh. Jobs that are created after you display the Job Management page are not visible until you reload the Jobs page. If a job expires while you have the Job Management page open, the job appears in the Job Management page list, but you cannot view the job results. To refresh the Job Management page, reload the page. Job actions You can use the Actions column to perform actions on a job. Select the more icon ( ) in the Actions column to open a job the in Search & Reporting app, inspect the job, edit the job settings, extend the job's expiration time, export the job, share a link to the job, or delete the job. Use the action icons to pause or stop jobs. To perform a subset of these actions on multiple jobs, select the jobs and click Edit Selected. Then select the action that you want to perform. Note: When you pause a search job, it might take some time for indexers to stop running remote searches and clear out search results that are buffered in memory.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 8, "metadata": {"title": "Manage search jobs", "section_heading": "Refreshing the jobs list", "section_id": "ab43a8a8_7b73_4782_a543_14d299d0b72a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/manage-search-jobs", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:12.996907+00:00", "version": "10.2"}}
{"id": "a6490cd42567d0cf", "content": "You can see a list of the jobs you have recently dispatched or saved for later review. Use the list to compare job statistics such as run time, total count of events matched, size, and so on. Active job count In the upper right corner of the Job Management page there is a count of the total number of jobs in the list. The count reflects the number of jobs at the moment you opened the Job Management page. If a job expires while the Job Management page is open, the job count does not refresh. Sort the job list By default, the list of jobs is sorted by the Created at column. You can sort the list by any column that displays a sort button in the column heading. For example, you can sort the list by the job expiration or by the job owner. Click once on the column heading to sort the list in ascending order. Click again to sort the list in descending order. Filter the job list You can filter the list of jobs by application, by owner, and by status. In the Filter box, type a term or expression that appears in the search criteria to filter the list. For example, you can specify diskUsage , EMBED AND diskUsage=8* , or label=EMBED AND diskUsage=8* in the Filter box. View job search results You can view the results of a search that is listed on the Job Management page. In the Actions column for the specific job, select the more icon, , which is a set of 3 vertical dots. Select Open in search. The results open in the Search app view. Check the progress of ongoing jobs You can check on jobs that are dispatched by scheduled searches, real-time searches, and long-running historical searches. Use the Status column to check on the progress of ongoing jobs. The Status column shows the percent of the events that have been processed. Current jobs have a status of Running. Jobs that are running in the background have a status of Backgrounded. Change the per page job count You can change the number of jobs that appear on each page in the list. The default is to display 20 jobs on each page. On the right side of the window, you can select to display 20, 50 or 100 jobs per page.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 8, "metadata": {"title": "Manage search jobs", "section_heading": "View and compare jobs", "section_id": "id_7e232a41_d1bd_4719_b261_af3f64573ac9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/manage-search-jobs", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:12.996914+00:00", "version": "10.2"}}
{"id": "ae29ea51af6fa2b7", "content": "You can inspect a job to take a closer look at what a search is doing and see where the Splunk software is spending most of its processing time. Use the Search Job Inspector to view information about the current job, such as the search ID (sid), job execution costs, and search job properties. In the Actions column for the specific job, select the more icon , which is a set of 3 vertical dots. Select Inspect Job. For more information about the using the Search Job Inspector, see View search job properties .", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 8, "metadata": {"title": "Manage search jobs", "section_heading": "Inspect jobs", "section_id": "b4861c30_0331_4631_80f0_260566f54782--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/manage-search-jobs", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:12.996918+00:00", "version": "10.2"}}
{"id": "897abff3c54ed979", "content": "From the Job Management page, there are several ways to change the lifetime of a job. To learn more about lifetimes for specific types of jobs, see Extending job lifetimes. Quickly extend job lifetimes You can quickly extend the lifetime of a job. In the Actions column for the specific job, select the more icon , which is a set of 3 vertical dots.. Select Extend Job Expiration. Extend the lifetimes for multiple jobs You can extend the lifetimes of multiple jobs at the same time. Select the jobs whose lifetimes you want to extend. Above the job list, click Edit Selected. Select Extend Expiration. Choose the period of time for the lifetime extension Use this method to select the time period for the extension. In the Actions column for the specific job, select the more icon , which is a set of 3 vertical dots. Select Edit Job Settings. Select either 10 minutes or 7 days. Click Save .", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 8, "metadata": {"title": "Manage search jobs", "section_heading": "Extending search job lifetimes", "section_id": "dd465736_d317_4c40_8c4d_5d9b03366c07--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/manage-search-jobs", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:12.996927+00:00", "version": "10.2"}}
{"id": "50880604072e517c", "content": "You can share a job by changing the job permissions, or by sharing a URL to the job. Change job permissions You can share a job by changing the permissions on that job. By default, all jobs are set to Private. In the Actions column for the specific job, select the more icon , which is a set of 3 vertical dots. Select Edit Job Settings to display the Job Settings dialog box. Change Read Permissions to Everyone. Click Save. Share a job URL You can share a job with other Splunk users by sending them a link to the job. The users that you send the link to must also have permissions to use the app that the job belongs to. In the Actions column for the job you want to share, select the more icon , which is a set of 3 vertical dots. Select Share job link. The job lifetime is extended by 7 days , and read permissions are set to Everyone. In Share Job dialog box, select Copy job link. Send the link to the users that you want to share the job results with.", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 8, "metadata": {"title": "Manage search jobs", "section_heading": "Share jobs", "section_id": "id_7d974ec3_e1c1_4d78_bf1a_47229003908b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/manage-search-jobs", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:12.996931+00:00", "version": "10.2"}}
{"id": "40cc51a17817d8d8", "content": "You can export the event data from a job in a variety of formats such as CSV, JSON, PDF, Raw Events, and XML. You can then archive the file, or use the file with a third-party charting application. The format options depend on the type of job artifact that you are working with. In the Actions column for the job you want to export, select the more icon , which is a set of 3 vertical dots. Select Export job. In the Export job results dialog box, select the format that you want the search results to be exported in. In the File Name field, type a name for the export file where the event data will be stored. In the Number of Results field, specify the number of results that you want to export. Click Export to save the job events in the export file. The export file is saved in the default download directory for your browser or operating system. Note: If your search returns a large number of results, you can access all of the results in the Search app. However, the full set of results might not be stored with the search job artifact. When you export search results, the export process is based on the search job artifact, not the results in the Search app. If the artifact does not contain the full set of results, a message appears at the bottom of the Export Results dialog box to tell you that the search will be rerun by the Splunk software before the results are exported. For more information, see Export search results. If the Export job option is not visible, the icon has been hidden by your system administrator to prevent data export. Extend the session timeout when exporting large amounts of data When you try to export large amounts of data using the Export button, the session might timeout. Users with the Admin role, or a role with an equivalent set of capabilities, can use the following procedure to extend the session timeout limit. Click Settings > Server Settings > General Settings. In the Splunk Web section, increase the number in the Session timeout field. Click Save. Increasing the timeout settings allows Splunk Web more time for the connection between your browser and Splunk Web.", "code_examples": [], "tables": [], "chunk_index": 6, "total_chunks": 8, "metadata": {"title": "Manage search jobs", "section_heading": "Export jobs", "section_id": "id_331ddf5b_babb_4373_82d7_253249f349a1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/manage-search-jobs", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:12.996935+00:00", "version": "10.2"}}
{"id": "a15f77de6115853a", "content": "You can delete one or more jobs from the jobs list. Delete a single job You can delete a job from the job list. In the Actions column for the specific job, select the more icon , which is a set of 3 vertical dots. Select Delete job. Delete multiple jobs You can delete multiple jobs at one time. Select the jobs that you want to delete. Above the job list, select Edit Selected. Select Delete .", "code_examples": [], "tables": [], "chunk_index": 7, "total_chunks": 8, "metadata": {"title": "Manage search jobs", "section_heading": "Delete jobs", "section_id": "b70da19b_3860_4d9d_b373_1e82493e73bb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/manage-search-jobs", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:12.996938+00:00", "version": "10.2"}}
{"id": "ebd7caa1fc51be5f", "content": "You can access the Search Job Inspector for a search job, as long as the search job has not expired (which means that the search artifact still exists). The search does not need to be running to access the Search Job Inspector. 1. Run the search. 2. From the Job menu, select Inspect Job. This opens the Search Job Inspector in a separate window. Change the search job After the Search Job Inspector opens in a separate window, you can use the URL for that window to inspect any valid search job artifact if you have its search ID (SID). You can find the SID of a search on the Jobs page (select Activity > Jobs ) or listed in the dispatch directory, $SPLUNK_HOME/var/run/splunk/dispatch. For more information about the Jobs page, see Manage search jobs in this manual. If you look at the URI path for the Search Job Inspector window, you will see something like this at the end of the string: The sid is the SID number. The namespace is the name of the app that the SID is associated with. In this example, the SID is 1299600721.22. Type the search artifact SID in the URI path, after sid= and press Enter. As long as you have the necessary ownership permissions to view the search, you will be able to inspect it.", "code_examples": [{"language": "spl", "code": ".../manager/search/job_inspector?sid=1299600721.22"}], "tables": [], "chunk_index": 0, "total_chunks": 8, "metadata": {"title": "View search job properties", "section_heading": "Open the Search Job Inspector", "section_id": "id_4357ef1e_d4f0_4bee_b3f9_d8e22824ce5a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/view-search-job-properties", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:31.627090+00:00", "version": "10.2"}}
{"id": "e51477e7df577837", "content": "At the top of the Search Job Inspector window, an information message appears. The message depends on whether the job is paused, running, or finished. For example, if the job is finished the message tells you how many results it found and the time it took to complete the search. Any error messages are also displayed at the top of the window. Below these messages you should find the SID for the search job and two links: The search.log link takes pulls up a text file with the raw search.log data for this search job. The displays of the Search Job Inspector and Job Details dashboard are based on this search.log data. The Job Details Dashboard link goes to the Job Details dashboard, which provides a condensed summary of important metrics and facts about the search job you are investigating. You may want to look at this dashboard before you try to do a deep dive into the Search Job Inspector's comprehensive report.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 8, "metadata": {"title": "View search job properties", "section_heading": "What the Search Job Inspector shows you", "section_id": "ae1c759d_7b2c_4735_8a83_4940fbca0b56--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/view-search-job-properties", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:31.627112+00:00", "version": "10.2"}}
{"id": "336bd96eb4245222", "content": "After opening the Search Job Inspector for a search, click the Job Details Dashboard link to open the Job Details dashboard. Note: You can also reach the Job Details dashboard from the Dashboards page. You can easily reload the dashboard for any search job currently listed on the Jobs page by placing its SID in the provided field and pressing the Return key. The dashboard supplies the essential properties of the search job in a format that is designed for quick scanning. It breaks down into four sections: Summary The Summary presents basic metrics and facts about the search job, including the number of events scanned per second, the result count, and the search mode of the search. Search Strings The Search Strings section shows the search string that was provided to run the search alongside the string for the optimized version of the search that was actually run behind the scenes to improve search performance. The Search String section also includes the search strings for the map and reduce phases of the search, if applicable. Search Costs The Search Costs section provides simple breakdowns of search costs by command and phase, as well as the cumulative startup handoff time. Indexers The Indexers section lists the indexers that processed the search job. The shading of duration values on the Time Spent Running Search Per Indexer table helps you quickly identify indexers that did not process the search as efficiently as their peers. Click on an indexer name in that table to see the time the indexer spent on the search, broken out by search process. The following Job Details dashboard panels will not display results for users with roles that do not have access to the _introspection index: Search Costs by Command Approximate Time Spent in Reduce Phase Map Phase Search Costs by Command Reduce Phase Search Costs by Command Contact your administrator if you require access to this information. To return to the main Search Job Inspector, right-click on the dashboard and select Back .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 8, "metadata": {"title": "View search job properties", "section_heading": "Open the Job Details dashboard to get a concise overview of your search job", "section_id": "ab6a566b_85b5_42d2_808d_919f71b2c526--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/view-search-job-properties", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:31.627127+00:00", "version": "10.2"}}
{"id": "a17fd13542c60d92", "content": "The key information that the Search Job Inspector displays are the execution costs and the search job properties. Execution costs The Execution costs section lists information about the components of the search and how much impact each component has on the overall performance of the search. Search job properties The Search job properties section lists other characteristics of the job.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 8, "metadata": {"title": "View search job properties", "section_heading": "Review the specific execution costs and properties of your search job", "section_id": "id_414b3ea9_5a05_401e_bdd5_075bfcae57e7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/view-search-job-properties", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:31.627146+00:00", "version": "10.2"}}
{"id": "cbd047a40951bdeb", "content": "With the information in the Execution costs section, you can troubleshoot the efficiency of your search. You can narrow down which processing components are impacting the search performance. This section contains information about the search processing components that were used to process your search. The component duration in seconds. How many times each component was invoked while the search ran. The input and output event counts for each component. The Search Job Inspector lists the components alphabetically. The number of components that you see depend on the search that you run. The following tables describes the significance of each individual command and distributed component in a typical keyword search. Execution costs of search commands In general, for each command that is part of the search job, there is a parameter command.<command_name>. The values for these parameters represent the time spent in processing each <command_name>. For example, if the table command is used, you will see command.table. There is a relationship between the type of commands used and the numbers you can expect to see for Invocations, Input count, and Output count. For searches that generate events, you expect the input count to be 0 and the output count to be some number of events X. If the search is both a generating search and a filtering search, the filtering search would have an input (equal to the output of the generating search, X) and an output=X. The total counts would then be input=X, output=2*X, and the invocation count is doubled. Execution costs of dispatched searches", "code_examples": [], "tables": [{"headers": ["Search command component name", "Description"], "rows": [["command.search", "After the Splunk software identifies the events that contain the indexed fields matching your search, the events are analyzed to identify which events match the other search criteria. These are concurrent operations, not consecutive.command.search.index- tells how long it took to look into the TSIDX files for the location to read in the raw data.  This is the time spent identifying, from the tokens in the base search, what events to retrieve.command.search.rawdata- tells how long it took to read the actual events from the rawdata files.command.search.typer- tells how long it took to assign event types to events.command.search.kv- tells how long it took to apply field extractions to the events.command.search.fieldalias- tells how long it took to rename fields based according toprops.conf.command.search.lookups- tells how long it took to create new fields based on existing fields (perform field lookups).command.search.filter- tells how long it took to filter out events that do not match, for example  fields and phrases.command.search.tags- tells how long it took to assign tags to events."]]}, {"headers": ["Distributed search component name", "Description"], "rows": [["dispatch.check_disk_usage", "The time spent checking the disk usage of this job."], ["dispatch.createdSearchResultInfrastructure", "The time to create and set up the collectors for each peer and execute the HTTP post to each peer."], ["dispatch.earliest_time", "Specifies the earliest time for this search. Can be a relative or absolute time. The default is an empty string."], ["dispatch.emit_prereport_files", "When running atransforming search, Splunk Enterprise cannot compute the statistical results of the report until the search completes. After it fetches events from the search peers (dispatch.fetch), it, writes out the results to local files.dispatch.emit_prereport_filesprovides the time that it takes for Splunk Enterprise to write the transforming search results to those local files."], ["dispatch.evaluate", "The time spent parsing the search and setting up the data structures needed to run the search.  This component also includes the time it takes to evaluate and run subsearches. This is broken down further for each search command that is used. In general,dispatch.evaluate.<command_name>tells you the time spent parsing and evaluating the<command_name>argument. For example,dispatch.evaluate.searchindicates the time spent evaluating and parsing thesearchcommand argument."], ["dispatch.fetch", "The time spent by the search head waiting for or fetching events from search peers. Thedispatch.fetchvalue is different than thecommand.searchvalue.  Thecommand.searchvalue includes time spent by all indexers, which can be greater than the actual elapsed time of the search. If you have only a single node, then thedispatch.fetchand thecommand.searchvalues will be similar. In a distributed environment, depending on the search, these values can be very different."], ["dispatch.preview", "The time spent generating preview results."], ["dispatch.process_remote_timeline", "The time spent decoding timeline information generated by search peers."], ["dispatch.reduce", "The time spent reducing the intermediate report output."], ["dispatch.stream.local", "The time spent by search head on the streaming part of the search."], ["dispatch.stream.remote", "The time spent executing the remote search in a distributed search environment, aggregated across all peers. Additionally, the time spent executing the remote search on each remote search peer is indicated with:dispatch.stream.remote.<search_peer_name>.output_countrepresents bytes sent rather than events in this case."], ["dispatch.timeline", "The time spent generating the timeline and fields sidebar information."], ["dispatch.writeStatus", "The time spent periodically updatingstatus.csvandinfo.csvin the job's dispatch directory."], ["startup.configuration", "The time spent generating the startup configuration."], ["startup.handoff", "The time elapsed between the forking of a separate search process and the beginning of useful work of the forked search processes. In other words it is the approximate time it takes to build the search apparatus. This is cumulative across all involved peers. If this takes a long time, it could be indicative of I/O issues with .conf files or the dispatch directory."]]}], "chunk_index": 4, "total_chunks": 8, "metadata": {"title": "View search job properties", "section_heading": "Execution costs", "section_id": "id_1111af2f_09bd_4b20_9771_f20bdb75cd9a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/view-search-job-properties", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:31.627185+00:00", "version": "10.2"}}
{"id": "6d04952ebabf385a", "content": "The Search job properties fields provide information about the search job. The Search job properties fields are listed in alphabetical order. Note: When troubleshooting search performance, it's important to understand the difference between the scanCount and resultCount costs. For dense searches, the scanCount and resultCount are similar ( scanCount = resultCount ); and for sparse searches, the scanCount is much greater than the result count ( scanCount > resultCount ). Search performance should not so much be measured using the resultCount /time rate but scanCount /time instead. Typically, the scanCount /second event rate should hover between 10k and 20k events per second for performance to be deemed good. Debug messages Configure the Search Job Inspector to display DEBUG messages when there are errors in your search. For example, DEBUG messages can warn you when there are fields missing from your results. The Search Job Inspector displays DEBUG messages at the top of the Search Job Inspector window, after the search has completed. By default the Search Job Inspector hides DEBUG messages. Prerequisites Only users with file system access, such as system administrators, can configure the Search Job Inspector to display DEBUG messages. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. You can have configuration files with the same name in your default, local, and app directories. Read Where you can place (or find) your modified configuration files in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. To configure DEBUG messages to display, open limits.conf. Set the infocsv_log_level parameter in the [search_info] stanza to DEBUG .", "code_examples": [{"language": "spl", "code": "[search_info]\ninfocsv_log_level = DEBUG"}], "tables": [{"headers": ["Parameter name", "Description"], "rows": [["cursorTime", "The earliest time from which no events are later scanned. Can be used to indicate progress. See description fordoneProgress."], ["delegate", "For saved searches, specifies jobs that were started by the user. Defaults to scheduler."], ["diskUsage", "The total amount of disk space used, in bytes."], ["dispatchState", "The state of the search. Can be any of the following states:QUEUEDPARSINGRUNNINGPAUSEFINALIZINGINTERNAL_CANCELUSER_CANCELBAD_INPUT_CANCELQUITFAILEDDONE"], ["doneProgress", "A number between 0 and 1.0 that indicates the approximate progress of the search.doneProgress = (latestTime â€“ cursorTime) / (latestTime â€“ earliestTime)"], ["dropCount", "For real-time searches only, the number of possible events that were dropped due to thert_queue_size(defaults to 100000)."], ["earliestTime", "A time string that sets the earliest (inclusive), respectively, time bounds for the search. Can be used to indicate progress. See description fordoneProgress."], ["eai:acl", "Describes the app and user-level permissions. For example, is the app shared globally, and what users can run or view the search?"], ["eventAvailableCount", "The number of events that are available for export."], ["eventCount", "The number of events returned by the search. In other words, this is the subset of scanned events (represented by thescanCount) that actually matches the search terms."], ["eventFieldCount", "The number of fields found in the search results."], ["eventIsStreaming", "Indicates if the events of this search are being streamed."], ["eventIsTruncated", "Indicates if events of the search have not been stored, and thus not available from the events endpoint for the search."], ["eventSearch", "Subset of the entire search that is before any transforming commands. The timeline and events endpoint represents the result of this part of the search."], ["eventSorting", "Indicates if the events of this search are sorted, and in which order.asc= ascending;desc= descending;none= not sorted"], ["isBatchMode", "Indicates whether or not the search in running in batch mode. This applies only to searches that include transforming commands."], ["isDone", "Indicates if the search has completed."], ["isFailed", "Indicates if there was a fatal error executing the search. For example, if the search string had invalid syntax."], ["isFinalized", "Indicates if the search was finalized (stopped before completion)."], ["isPaused", "Indicates if the search has been paused."], ["isPreviewEnabled", "Indicates if previews are enabled."], ["isRealTimeSearch", "Indicates if the search is a real time search."], ["isRemoteTimeline", "Indicates if the remote timeline feature is enabled."], ["isSaved", "Indicates that the search job is saved, storing search artifacts on disk for 7 days from the last time that the job has been viewed or touched. Add or edit thedefault_save_ttlvalue inlimits.confto override the default value of 7 days."], ["isSavedSearch", "Indicates if this is a saved search run using the scheduler."], ["isTimeCursored", "Specifies if the cursorTime can be trusted or not. Typically this parameter it set to true if the first command is search."], ["isZombie", "Indicates if the process running the search is dead, but with the search not finished."], ["keywords", "All positive keywords used by this search. A positive keyword is a keyword that is not in a NOT clause."], ["label", "Custom name created for this search."], ["latestTime", "A time string that sets the latest (exclusive), respectively, time bounds for the search. Can be used to indicate progress. See description fordoneProgress."], ["numPreviews", "Number of previews that have been generated so far for this search job."], ["messages", "Errors and debug messages."], ["optimizedSearch", "The restructured syntax for the search that was run. The built-in optimizers analyze your search and restructure the search syntax, where possible, to improve search performance. The search that you ran is displayed under thesearchjob property."], ["performance", "This is another representation of theExecution costs."], ["phase0", "Lists the elements in a search that run in the first phase of search processing. In a typical distributed search process, this is the mapping phase, which takes place remotely across the indexers in a deployment."], ["phase1", "Lists the elements in a search that run in the second phase in search processing. In a typical distributed search process, this is the reduce phase, which takes place on the search head. In a three-phase parallel reduce search, this is the intermediate reduce phase of the search process."], ["phase2", "Lists the elements in a search that run in the third phase in search processing. In a three-phase parallel reduce search, this is the final intermediate reduce phase of the search process."], ["remoteSearch", "The search string that is sent to every search peer."], ["reportSearch", "If reporting commands are used, the reporting search."], ["request", "GET arguments that the search sends tosplunkd."], ["resultCount", "The total number of results returned by the search."], ["resultIsStreaming", "Indicates if the final results of the search are available using streaming (for example, no transforming operations)."], ["resultPreviewCount", "The number of result rows in the latest preview results."], ["runDuration", "Time in seconds that the search took to complete."], ["scanCount", "The number of events that are scanned or read off disk."], ["search", "The search string."], ["searchCanBeEventType", "If the search can be saved as an event type, this will be 1, otherwise, 0.Only base searches (no subsearches or pipes) can be saved as event types."], ["searchProviders", "A list of all the search peers that were contacted."], ["searchTelemetry", "A JSON object that contains search job metadata. The structure of this JSON object is likely to change from release to release."], ["sid", "The search ID number."], ["statusBuckets", "Maximum number of timeline buckets."], ["ttl", "The time to live, or time before the search job expires after it completes."], ["Additional info", "Links to further information about your search. These links may not always be available.timelinefield summarysearch.log"]]}], "chunk_index": 5, "total_chunks": 8, "metadata": {"title": "View search job properties", "section_heading": "Search job properties", "section_id": "id_0fc475ef_5f4d_4041_9dcd_b83f7d54c63d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/view-search-job-properties", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:31.627260+00:00", "version": "10.2"}}
{"id": "5c7b17ac1bfe34b8", "content": "Here's an example of the execution costs for a dedup search, run over All time: The search commands component of the Execution costs panel might look something like this: The command.search component, and everything under it, gives you the performance impact of the search command portion of your search, which is everything before the pipe character. The command.prededup gives you the performance impact of processing the results of the search command before passing it into the dedup command. The Input count of command.prededup matches the Output count of command.search. The Input count of command.dedup matches the Output count of command.prededup. In this case, the Output count of command.prededup should match the number of events returned at the completion of the search. This is the value of resultCount , under Search job properties .", "code_examples": [{"language": "spl", "code": "* | dedup punct"}], "tables": [], "chunk_index": 6, "total_chunks": 8, "metadata": {"title": "View search job properties", "section_heading": "Examples of Search Job Inspector output", "section_id": "id_3621d654_9f63_4ae8_b319_8534cbcd2d27--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/view-search-job-properties", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:31.627273+00:00", "version": "10.2"}}
{"id": "724dff973e55af30", "content": "Video Using the Splunk Search Job Inspector", "code_examples": [], "tables": [], "chunk_index": 7, "total_chunks": 8, "metadata": {"title": "View search job properties", "section_heading": "See also", "section_id": "id_1725de47_11cd_4386_bf99_1d5111b1db50--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/view-search-job-properties", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:31.627283+00:00", "version": "10.2"}}
{"id": "e08f54334293d31f", "content": "The default lifetime for a search job depends on whether the search job is an artifact of an unscheduled or scheduled search. For example, dashboard panels that are based on an inline search, use unscheduled searches. Panels that are based on a report, use saved searches. The saved search can be unscheduled or scheduled. Default lifetimes for unscheduled searches When you run an ad hoc search and the search is finalized or completes on its own, the resulting search job has a default lifetime of 10 minutes. Other knowledge objects, such as real-time alerts and panels based on inline searches that use unscheduled searches have the same default lifetime. Default lifetimes for scheduled searches Scheduled searches launch search jobs on a regular interval. By default, these jobs are retained for the interval of the scheduled search multiplied by two. For example, if the search runs every 6 hours, the resulting jobs expire in 12 hours.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "Extending job lifetimes", "section_heading": "Default job lifetimes", "section_id": "fc65f90e_149e_4b9f_9685_b1fc111def32--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/extending-job-lifetimes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:48.701238+00:00", "version": "10.2"}}
{"id": "c2b713690baef831", "content": "Whenever you access an active job, such as when you view the results of a search job, the lifetime is reset. The reset happens whether the job lifespan is 10 minutes or 7 days. Here are a few examples of how this works. If the lifetime is set to 10 minutes and you run the search job at 11:00 AM, the job lifetime is set to end at 11:10 AM. If you run the job again at 11:07 AM, the job lifetime is reset to end at 11:17 AM. If you set the lifetime for a job to 7 days and then access the job 4 days later, the job lifetime is reset and will not expire for another 7 days from the current day and time.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "Extending job lifetimes", "section_heading": "Automatic lifetime extensions", "section_id": "ea996799_d75e_4d6c_84b7_efc8c28f79b1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/extending-job-lifetimes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:48.701245+00:00", "version": "10.2"}}
{"id": "18fd927897aec62f", "content": "You change the lifetime setting for the current ad hoc search job in the Search app. Select the Job drop-down. Select Edit Job Settings to display the Job Settings. For Lifetime , select either 10 Minutes or 7 Days.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "Extending job lifetimes", "section_heading": "Changing the lifetime for the current job", "section_id": "e34a2188_a880_4f60_8776_7f0c267876c1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/extending-job-lifetimes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:48.701250+00:00", "version": "10.2"}}
{"id": "dfc82972ecec1879", "content": "You can't change the lifetimes for jobs resulting from previously run unscheduled or scheduled searches. You can only change the lifetimes for active search jobs. See Manage search jobs .", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "Extending job lifetimes", "section_heading": "Changing the lifetime for active jobs", "section_id": "b7581823_bf86_450e_a09e_19492f829eeb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/extending-job-lifetimes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:48.701253+00:00", "version": "10.2"}}
{"id": "53e845c4d7e2afb4", "content": "After the job lifetime ends, the job expires and is deleted from the system. Note: It is possible that while you are looking at the list of jobs that a job will expire. When you try to extend the lifetime of the expired job, a message appears explaining that the job no longer exists. You cannot extend the lifetime of an expired job.", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "Extending job lifetimes", "section_heading": "Expired jobs", "section_id": "id_3b7f21da_8000_48c4_978e_404a3d9cc543--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/extending-job-lifetimes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:48.701258+00:00", "version": "10.2"}}
{"id": "1ac6c246844ef01a", "content": "You can change the default value for the job lifetime for unscheduled and scheduled searches. Change the default lifetime value for unscheduled searches You can change the default value for the job lifetime for unscheduled searches. Splunk Cloud Platform To change the default value for the job lifetime for unscheduled searches, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Enterprise Prerequisites Only users with file system access, such as system administrators, can change the default lifetime values. Review the steps in How to edit a configuration file in the Admin Manual. Note: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make the changes in the local directory. Open the local limits.conf file for the Search app. For example, $SPLUNK_HOME/etc/apps/<app_name>/local. In the [search] stanza, change the default_save_ttl value to a number that is appropriate for your needs. The acronym TTL is an abbreviation for \"time to live.\" The default_save_ttl setting measures search job time to live in seconds, and defaults to 604800 seconds, or one week. Change the default lifetime value for scheduled searches You can change the default lifetime for jobs resulting from a specific scheduled search. Splunk Cloud Platform You can use Splunk Web to make this change. Prerequisites Only system administrators with the sc_admin role should change default lifetime values. Review the information about the dispatch.ttl setting in the Dispatch search options section of the Savedsearches.conf.spec file in the Admin Manual. Steps: Select Settings > Searches, Reports, and Alerts. Locate the name of the scheduled search in the list. In the Actions column, select Edit , Advanced Edit. Locate the dispatch.ttl setting. The default setting is 2p. Change the setting, based on the information provided in the savedsearches.conf.spec file. Splunk Enterprise You can change this default either through Splunk Web, as described previously for Splunk Cloud Platform, or through the configuration files. Prerequisites Only users with file system access, such as system administrators, can change default lifetime values using configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. You can have configuration files with the same name in your default, local, and app directories. Read Where you can place (or find) your modified configuration files in the Splunk Enterprise Admin Manual. Note: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make the changes in the local directory. Steps: Open the local savedsearches.conf file. For example, $SPLUNK_HOME/etc/apps/<app_name>/local. Locate the scheduled search, and change the dispatch.ttl setting to a different interval multiple.", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "Extending job lifetimes", "section_heading": "Changing the default lifetime values", "section_id": "cef9d1d8_4b04_4dab_b535_9972fc4dc523--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/extending-job-lifetimes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:36:48.701262+00:00", "version": "10.2"}}
{"id": "2c8c217423bdc3cb", "content": "When you share a job, you are sharing the results of a specific run of a search. There are several ways that you can share a specific job with other Splunk platform users. You can change the permissions for a search job to share that job with other users. You can also share a job by sending the URL for a search job to another Splunk platform user. You can only change permissions or share a link to the current job. Change job permissions You can share a job by changing the permissions on that job. By default, all jobs are private and have a lifetime of 10 minutes. From the Job menu, select Edit Job Settings to display the Job Settings dialog box. If the Edit Job Settings option is turned off, sharing a job has been turned off by the Splunk administrator. However, you can share the search itself. See Share a search. Change Read Permissions to Everyone. Select Save. The following image shows the menu selections required to change job permissions: Share a link to the job You can share a search job with other Splunk users by sending them a link to the job. This is helpful when you want another user to see the results returned by the job. The users that you send the link to must have permissions to use the app that the job originated from. There are two methods you can use to obtain a job link. You can use the Share icon or the Job menu. Use the Share icon You can share a search job by using the Share icon: Select the Share icon, which is one of the search action icons. When you share a search job, the job lifetime and permissions are automatically extended. The job lifetime is extended to 7 days and that the read permissions is changed to Everyone. There is a link to manage the job in the Job Settings window. There is also a button to copy the link to the search job. Select Copy job link to copy the URL. Send the link to the users that you want to share the job results with. Use the Job menu You can share a search job by using the Job menu: Select Edit Job Settings to display the Job Settings dialog box. If the Edit Job Settings option is turned off, sharing a job has been turned off by the Splunk administrator. However, you can share the search itself. See Share a search. Change Read Permissions to Everyone. If the permissions for a job are set to Private , other users cannot access the job with the link. Change Lifetime to 7 days. Select Copy job link to copy the URL. Send the link to the users that you want to share the job results with.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Share jobs and export results", "section_heading": "Share a job", "section_id": "id_34ab3505_f9e1_49d3_aae2_b3aac3c1833c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/share-jobs-and-export-results", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:37:05.070195+00:00", "version": "10.2"}}
{"id": "b590ea5fabe91035", "content": "Your Splunk administrator controls how searches are shared. If your Splunk administrator has turned off search job sharing, you can share the actual search instead. The link you share copies the search into the Search bar in the Search app. If the user that you share the link with has the proper permissions, such as access to the index and source and the role they are assigned, they can run the search personally. Select the Share icon, which is one of the search action icons. In the Share Job dialog box, select Copy search query link to copy the URL. Send the link to the users that you want to share the search with.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Share jobs and export results", "section_heading": "Share a search", "section_id": "dfd6f701_aa1f_4577_8f7c_5a3d3d71c083--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/share-jobs-and-export-results", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:37:05.070203+00:00", "version": "10.2"}}
{"id": "9b5f7c956f510099", "content": "You can export your job results in a variety of formats such as CSV, JSON, PDF, Raw Events, and XML. You can then archive the file, or use the file with a third-party charting application. The format options depend on the type of job artifact that you are working with. If the search generates calculated data that appears on the Statistics tab, you cannot export using the Raw Events format. If the search is a saved search, such as a report, you can export using the PDF format. The export file is saved in the default download directory for your browser or operating system. There are several methods that you can use to export search results. A few of these methods include Splunk Web, CLI, SDKs, and REST. Some of the methods are optimized for speed, while others are good for extremely large event sets. For a complete list of the export methods and links to the specific steps, see Export search results .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Share jobs and export results", "section_heading": "Export job results to a file", "section_id": "a7ac322f_bebc_4cdf_bef0_5e227116b51f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/share-jobs-and-export-results", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:37:05.070208+00:00", "version": "10.2"}}
{"id": "cc490f32e4350e1a", "content": "By default, a user can share a search job with other users. However, a Splunk administrator can turn off job sharing, which turns on the ability to share the actual search itself. Splunk Cloud Platform To change the setting for job sharing, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Enterprise Prerequisites Have the permissions to change the default job sharing setting. Only users with file system access, such as system administrators, can change the job sharing default using configuration files. Know how to edit configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. Decide which directory to store configuration file changes in. You can have configuration files with the same name in your default, local, and app directories. Read Where you can place (or find) your modified configuration files in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps Open or create a local web-features.conf file at $SPLUNK_HOME/etc/system/local. Under the [feature:share_job] stanza, set enable_share_job_control to false. Changing this setting to false turns off sharing a link to a search job and turns on sharing a link to the actual search itself. Restart your Splunk platform instance.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Share jobs and export results", "section_heading": "Change how a search is shared", "section_id": "acdafe0b_7195_4017_9928_c7fccbfc2cda--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/manage-jobs/share-jobs-and-export-results", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Manage Jobs", "manual": "search-manual", "scraped_at": "2026-01-23T13:37:05.070212+00:00", "version": "10.2"}}
{"id": "33d7975cdfe16303", "content": "Saving searches", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "Scheduling searches", "section_heading": "See Also", "section_id": "id_98f4bfa7_e701_487f_9e7e_ee1e1ea8fd68--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/save-and-schedule-searches/scheduling-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Save and Schedule Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:37:21.648031+00:00", "version": "10.2"}}
{"id": "31d3484a1f52c057", "content": "Scheduling searches", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "Saving searches", "section_heading": "See Also", "section_id": "fa5a677c_6d21_4c29_9ec0_5a7df8a4d0ef--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/save-and-schedule-searches/saving-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Save and Schedule Searches", "manual": "search-manual", "scraped_at": "2026-01-23T13:37:39.519602+00:00", "version": "10.2"}}
{"id": "d9c295b966366935", "content": "After you launch a search, you can access and manage information about the search job without leaving the Search view. After your search is running, paused, or finalized, click Job from the Search actions group. Select an option from the list. Edit job settings. Opens the Job Settings dialog, where you can change the read permissions for the job, extend the job lifespan, and get a URL for the job. You can use the URL to share the job with others or to add a bookmark to the job in your Web browser. Send job to the background. Runs the job on the background. Use this option if the search job is slow to complete. This enables you to work on other activities, including running a new search job. Inspect job. Opens the Search Job Inspector window and displays information and metrics about the search job. You can select this action while the search is running or after the search completes. For more information, see View search job properties. Delete job. Deletes the current job, even if that job is running, paused, or has finalized. After you delete the job you can still save the search as a report. For more information, see About jobs and job management .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "Search actions", "section_heading": "Control search job progress", "section_id": "id_12c0fb9e_e3f9_4d0d_9dba_4abdc99dbe9a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/search-actions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:37:56.355006+00:00", "version": "10.2"}}
{"id": "f381a3ea25ada8ed", "content": "The search mode controls the search experience. The default search mode is Smart Mode. Fast Mode Speeds up searches by cutting down on the amount of event information that the search returns. Verbose Mode Returns as much event information as possible. Smart Mode Automatically toggles the search behavior between Fast Mode and Verbose Mode, based on the type of search that you are running. For more information, see Search modes .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "Search actions", "section_heading": "Change the search mode", "section_id": "ba291524_0ad5_4f41_af75_52e453ec26c6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/search-actions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:37:56.355014+00:00", "version": "10.2"}}
{"id": "99f97a41056808a1", "content": "The Save as menu lists options for saving the results of a search as a report, dashboard panel, alert, and event type. Report Saves a search as a report to use the search again later. You can run the report again from the Reports page. You access the Reports page from the App bar. Read more about how to Create and edit reports in the Reporting Manual. Dashboard Panel Generates a dashboard panel based on your search and add it to a new or existing dashboard. To learn more, see the Dashboard overview in the Dashboards and Visualizations manual. Alert Defines an alert based on your search. An alert runs a report in the background (either on a schedule or in real time ). When the search returns results that meet a condition you have set in the alert definition, the alert is triggered. For more information, see the Alerting Manual. Event Type Classify events that have common characteristics. If the search does not include a pipe operator or a subsearch , you can use this option to save the search as an event type. For more information, see About event types and Define event types in Splunk Web in the Knowledge Manager manual.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "Search actions", "section_heading": "Save the results", "section_id": "id_16e65ecb_fde0_476b_b1f6_198fc46c862b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/search-actions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:37:56.355019+00:00", "version": "10.2"}}
{"id": "db4f28f4cdb2f282", "content": "Between the job progress controls and search mode selector are three buttons which enable you to Share , Export , and Print the results of a search. Click Share to share the job. When you select this, the job's lifetime is extended to 7 days and read permissions are set to Everyone. For more information about jobs, see About jobs and job management in this manual. Click Export to export the results. You can select to output to CSV, raw events, XML, or JSON and specify the number of results to export. Click Print to send the results to a printer that has been configured. Additionally, use the Close button next to Save as menu to cancel the search and return to Splunk Home.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "Search actions", "section_heading": "Other search actions", "section_id": "id_0eb1abc2_7af7_4b0b_8dd5_c5d7eefcbb12--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/search-actions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:37:56.355024+00:00", "version": "10.2"}}
{"id": "e765f3fbd3c14ae6", "content": "About the Search app", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "Search actions", "section_heading": "See also", "section_id": "id_73d77142_9ea6_4596_98d9_ab9f9bbed796--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/search-actions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:37:56.355027+00:00", "version": "10.2"}}
{"id": "050d7dc8b9f40814", "content": "The Fast mode prioritizes the performance of the search and only returns essential event data, such as default fields and fields that were extracted at index time. Because field discovery for events is turned off, the Fast mode doesn't perform any search-time field extraction. However, the Fast mode does return search-time fields that you explicitly add to the search. Since the Fast mode is optimized for speed, there might be disparities in values of fields compared to the Smart or Verbose modes. But, the number of results returned in searches using the Fast mode are consistent with the number of results returned by the other modes. The Fast mode does the following: Extracts default fields such as host , source , and sourcetype. The fast mode only returns information on indexed and default fields, and any search-time extracted fields that you explicitly include in your search. If you're searching on specific indexed fields, those fields are also extracted. For searches with transforming commands , depicts search results by displaying a visualization, such as a chart. For searches that don't include transforming commands, displays event lists and event timelines. Most of the time, your ad hoc searches should use the Smart mode, but there are certain specific cases where it makes sense to use the Fast mode instead. For example, use the Fast mode in the following situations: When you need to do basic field discovery and you don't care about the accuracy of your field extractions. For example, perhaps you just want to see what is in a particular source type, source, or host field. When you don't know what's in your index. For example, perhaps you want to know what indexed fields are in your index. When you want to know how many events are in your index and how frequently they occur in your data. For example, perhaps you want to find out the volume of events and approximately how many events you're getting during a particular slice of time, such as 500,000 events in a day. When you want to run reporting searches that include transforming commands that return results in charts or visualizations. When you don't care whether your events have extracted fields that aren't mentioned in your search. For more information about what the Splunk platform does when field discovery is turned on or off, see When Splunk Enterprise extracts fields in the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "Search modes", "section_heading": "Using the Fast mode", "section_id": "cea2f425_6a46_4f1d_b099_256718125811--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/search-modes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:13.080946+00:00", "version": "10.2"}}
{"id": "1dbe049a4e90d480", "content": "All reports run in Smart mode, the default search mode, after they are first created. The Smart mode returns the best results for whatever search or report you run. If you search on events, field discovery for event searches is turned on and you get all the event information you need. If you run a transforming search, the Splunk platform favors speed over thoroughness and takes you quickly to the report result table or visualization. When you run a Smart mode search that does not include transforming commands , the search behaves as if it were in Verbose mode. All possible fields are discovered. The full event list and event timeline are generated. No event table or visualization will appear because you need transforming commands to make those happen. When you run a Smart mode search that includes transforming commands, the search behaves as if it were in Fast mode. Field discovery is turned off. Your search does not waste time generating the event list and event timeline, and jumps you straight to the report result table or visualization. For more information about transforming commands and transforming searches, see About reporting commands in the Search Manual .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "Search modes", "section_heading": "Using the Smart mode", "section_id": "c087aadb_1603_45ec_b48f_b2995f0910f6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/search-modes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:13.080954+00:00", "version": "10.2"}}
{"id": "e9ecfca71cad1d10", "content": "The Verbose mode returns all of the field and event data it possibly can, even if it means the search takes longer to complete, and even if the search includes reporting commands. Your search discovers all of the fields it can. This includes default fields, automatic search-time field extractions, and all user-defined index-time and search-time field extractions. Discovered fields are displayed in the left-hand fields sidebar in the Events results tab. Your search returns an event list view of results and generates the search timeline. It also generates report tables and visualizations if your search includes reporting commands. You may want to use the Verbose mode if you are putting together a transforming search, but are not exactly sure which fields you need to report on, or if you need to verify that you are summarizing the correct events. Note: Reports cannot benefit from report acceleration when you run them in Verbose mode. If you turn on report acceleration for a report and it has been running faster as a result, be aware that if you switch the mode of the search to Verbose it will run at a slower, non-accelerated pace. Report acceleration is designed to be used with slow-completing searches that have over 100,000 events and which utilize transforming commands. For more information see Accelerate reports, in the Reporting Manual .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "Search modes", "section_heading": "Using the Verbose mode", "section_id": "id_745ad6eb_af9e_4f28_8577_69844ba368d2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/search-modes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:13.080959+00:00", "version": "10.2"}}
{"id": "0335b206cdf73b6c", "content": "The following table compares the key differences between the different search modes.", "code_examples": [], "tables": [{"headers": ["Search mode", "Optimized for", "Field extraction behavior", "Displays"], "rows": [["Fast", "Search PerformanceTransforming searches", "Field Discovery is off.Only returns search-time fields that you explicitly add to the search.", "Results only"], ["Smart", "A healthy compromise between data discoverability and search performance.", "Field Discovery is:On for event searches.Off for transforming searches.", "EventsResults"], ["Verbose", "Data discoverabilityEvent searches", "Field Discovery is on.Your search discovers all of the fields it can.", "EventsResults"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "Search modes", "section_heading": "Comparing search modes", "section_id": "id_1ffdc20a_22be_42fa_9082_e30968717a6f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/search-modes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:13.080966+00:00", "version": "10.2"}}
{"id": "883d6572c01f7ac1", "content": "About the Search app", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "Search modes", "section_heading": "See also", "section_id": "d9b2ecb6_cbea_49a2_a90b_0fe95e8d3438--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/use-the-search-app/search-modes", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Use the Search App", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:13.080970+00:00", "version": "10.2"}}
{"id": "44562687a376580f", "content": "The Splunk platform provides several export methods: Export data using Splunk Web Export data using the CLI Export data using SDKs Export data using REST API The dump search command Data forwarding Splunk apps Deploy and Use Splunk DB Connect Install and Use Splunk ODBC Driver with Microsoft Excel Install and Use Splunk ODBC Driver with Microsoft PowerBI Install and Use Splunk ODBC Driver with Tableau", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Export search results", "section_heading": "What are the available export methods?", "section_id": "id_5d6d6dac_f72e_441a_bc2a_8d1a1116c0fa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-search-results", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:29.502911+00:00", "version": "10.2"}}
{"id": "c74716670292eac5", "content": "The export method you choose depends on the data volumes involved and your level of interactivity. For example, a single on-demand search export through Splunk Web might be appropriate for a low-volume export. Alternatively, if you want to set up a higher-volume, scheduled export, the SDK and REST options work best. For large exports, the most stable method of search data retrieval is the Command Line Interface (CLI). From the CLI, you can tailor your search to external applications using the various Splunk SDKs. The REST API works from the CLI as well, but is recommended only for internal use. In terms of level of expertise, the Splunk Web and CLI methods are significantly more accessible than the SDKs and REST API, which require previous experience working with software development kits or REST API endpoints.", "code_examples": [], "tables": [{"headers": ["Method", "Volume", "Interactivity", "Remarks"], "rows": [["Splunk Web", "Low", "On-Demand, Interactive", "Easy to obtain on-demand exports"], ["CLI", "Medium", "On-Demand, Low Interactive", "Easy to obtain on-demand exports"], ["REST", "High", "Automated, best for computer-to-computer", "Works underneath SDK"], ["SDK", "High", "Automated, best for computer-to-computer", "Best for automation"]]}], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Export search results", "section_heading": "Export options", "section_id": "id_61a63cc1_3141_4500_b018_e755ebd242d7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-search-results", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:29.502922+00:00", "version": "10.2"}}
{"id": "34c43a99e1a150d0", "content": "You can export Splunk data into the following formats: Raw Events (for search results that are raw events and not calculated fields) CSV JSON XML PDF (for saved searches, using Splunk Web)", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Export search results", "section_heading": "Supported export formats", "section_id": "id_86d8aced_8bba_4d94_963e_2a59e26a6c6f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-search-results", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:29.502928+00:00", "version": "10.2"}}
{"id": "1b417af8a26490cd", "content": "The sampling ratio is the likelihood of any event being included in the sample result set. The formula for the ratio is 1/sample_ratio_value. For example, if the sample ratio value is 100, each event has a 1 in 100 chance of being included in the result set. The selection of each event is independent of the selection of all another events. It is possible that many events are included from the first 100 events, or none at all. If a search matches 1,000,000 events when sampling is not used, using a sample ratio value of 100 would result in returning approximately 10,000 events. If you to rerun a sampling search many times, the exact number of returned results is modeled by a binomial distribution with n=1000000 and p=0.01. This distribution looks like a normal distribution, with the mean=10000 and the standard deviation (stdev)=99.5. In Splunk Web, the sampling ratio that you specify must be a positive integer that is greater than 1. To turn off sampling in Splunk Web, set the ratio to 1. Set the default sampling ratio Splunk Cloud Platform To set the default sampling ratio, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Enterprise Prerequisites Only users with file system access, such as system administrators, can set the default sampling ratio. Review the steps in How to edit a configuration file in the Admin Manual. Note: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make the changes in the local directory. Steps Open the local ui-prefs.conf file for the Search app. For example, $SPLUNK_HOME/etc/apps/<app_name>/local. Under the [Preference options] stanza, set display.prefs.customSampleRatio to a positive integer.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "Event sampling", "section_heading": "The event sampling ratio", "section_id": "id_45a65ec9_4410_4da1_915e_92eda8118a9d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/event-sampling", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:46.545730+00:00", "version": "10.2"}}
{"id": "d581a11e075ebacb", "content": "By default, event sampling is not active. When you run a search, every event that matches your criteria is returned. When you specify a ratio, sampling remains in effect for the active search window. Sampling also remains in effect when you save a search as a report or dashboard panel. When you specify a ratio value, your value overrides the default value configured for your Splunk deployment and remains in effect until you change it. If you open a new search window, event sampling is no longer active. However, the last custom ratio that you used appears in the Sampling drop-down.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "Event sampling", "section_heading": "How event sampling works", "section_id": "id_4af6cfae_59f9_4825_98ed_c14c3595adfb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/event-sampling", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:46.545736+00:00", "version": "10.2"}}
{"id": "1cdebb123fdf67a8", "content": "Typically, searches that use the transaction , stats , or streamstats commands are not good candidates for sampling. When you calculate statistics using a sample set of events, the statistical values will not be accurate. To get an approximation of the true statistical values, you must scale the values that the search returns. To get the most accurate value approximation, the scaling factor should be the same as the sampling ratio. For example, you create a report using this search with event sampling enabled. Because you used event sampling, the returned value is not the complete sum of all of the events. It is only the sum of the sample set of events. If the sampling ratio is 100, the true sum is approximately 100 times the value returned by the search. Statistical calculations that fall into this situation are count , sum , and sumsq. Other statistics that are difficult to interpret when event sampling is used include: distinct_count earliest latest max min", "code_examples": [{"language": "spl", "code": "... | stats sum(x)"}], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "Event sampling", "section_heading": "Commands and functions to avoid with event sampling", "section_id": "id_94faaf85_46fb_438f_836d_942c2c6ea3d8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/event-sampling", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:46.545741+00:00", "version": "10.2"}}
{"id": "91d2e7ba98d35e1e", "content": "You activate event sampling for a search by specifying a sampling ratio. 1. In Splunk Web, below the Search bar, click No Event Sampling. 2. You can use one of the default ratios or specify a custom ratio. a. To use one of the default ratios, click the ratio in the Sampling drop-down. b. To specify a custom ratio, click Custom and type the ratio value. Then click Apply. The ratio value must be a positive integer greater than 1.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "Event sampling", "section_heading": "Specify a sampling ratio", "section_id": "a63cee67_d897_4699_9af6_c21b42b73471--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/event-sampling", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:46.545745+00:00", "version": "10.2"}}
{"id": "3f5af5d54dcd36c2", "content": "There are several indicators in the Search & Reporting App window which show that event sampling is active. After you run a search, the Sampling drop-down appears in the event count line. The label for the Sampling drop-down specifies the ratio that is applied to the search. Additionally, if a sampling ratio is being used, the Jobs drop-down specifies the ratio that is applied to the search.", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "Event sampling", "section_heading": "Event sampling indicators", "section_id": "id_398f0214_b81d_486b_86de_cf5f48a7b14b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/event-sampling", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:46.545749+00:00", "version": "10.2"}}
{"id": "6d836adf46133ea8", "content": "You can save a search that uses event sampling as a report or dashboard panel. Use the Save As drop-down to save the search. When the search is saved as a report, the sampling ratio is used when the report is run. When the search saved as a dashboard panel, the panel is powered by an inline search. When the dashboard is refreshed, the sampling ratio that was saved with the inline search is used. If you open a report and add the report to a dashboard panel, you can specify how the panel is powered. You can specify that the panel is powered by the inline search that the report is based on. Or you can specify that the panel is powered by the report itself. Panels powered by reports When you view the source for the panel in Simple XML, there is no indication if the report uses event sampling. Panels powered by inline searches When you view the source for the panel in Simple XML, if the underlying search uses event sampling there is <sampleRatio> entry. For example: Accelerated reports You cannot accelerate reports that are based on event sampling searches. See Accelerate reports in the Reporting Manual .", "code_examples": [{"language": "spl", "code": "<event>\n  <title>sample events</title>\n  <search>\n     <query>buttercupgames</query>\n     <earliest>@d</earliest>\n     <latest>now</latest>\n     <sampleRatio>500</sampleRatio>\n  </search>\n</event>"}], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "Event sampling", "section_heading": "Event sampling with reports and dashboard panels", "section_id": "id_062f1b14_c1b5_4df5_925e_ebe97b227b7c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/event-sampling", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:38:46.545753+00:00", "version": "10.2"}}
{"id": "5b597c9b162baf50", "content": "The time range picker includes many built-in time ranges options that are defined by default in the times.conf file. You can select from a list of Real-time windows, Relative time ranges, and search over All Time. Real-Time Preset time ranges CAUTION: The number of concurrent real-time searches can greatly affect indexing performance. See About real-time searches and reports. Note: Users must have the Admin role to run and save real-time searches. For more information on managing roles and assigning roles to users, see Create and manage roles with Splunk Web in Securing Splunk Enterprise. The Real-Time Preset time ranges apply to real-time searches and are listed in the following table. To learn about relative time modifiers, see Specify time modifiers in your search. Relative Preset time ranges The Relative Preset time ranges are listed in the following table. To learn more about relative time modifiers, see Specify time modifiers in your search .", "code_examples": [], "tables": [{"headers": ["Real-Time Preset time range", "Description", "Equivalent relative time modifiers"], "rows": [["30 second window", "Events in the last 30 second window.", "earliest_time = rt-30slatest_time = rt"], ["1 minute window", "Events in the last 1 minute window.", "earliest_time = rt-1mlatest_time = rt"], ["5 minute window", "Events in the last 5 minute window.", "earliest_time = rt-5mlatest_time = rt"], ["30 minute window", "Events in the last 30 minute window.", "earliest_time = rt-30mlatest_time = rt"], ["1 hour window", "Events in the last 1 hour window.", "earliest_time = rt-1hlatest_time = rt"], ["All time (real-time)", "Total events for all real-time searches.", "earliest_time = rtlatest_time = rt"]]}, {"headers": ["Relative Preset time range", "Description", "Equivalent relative time modifiers"], "rows": [["Today", "Events from today.", "earliest_time = @dlatest_time = now"], ["Week to date", "Events from this week to the current date.", "earliest_time = @w0latest_time = now"], ["Business week to date", "Events from this business week to the current date. Starts from the previous Monday at midnight (00:00:00) to now.", "earliest_time = @w1latest_time = now"], ["Month to date", "Events from this month to the current date.", "earliest_time = @monlatest_time = now"], ["Year to date", "Events from this year to the current date.", "earliest_time = @ylatest_time = now"], ["Yesterday", "Events from yesterday to today.", "earliest_time = -1d@dlatest_time = @d"], ["Previous week", "Events from the previous week.", "earliest_time = -7d@w0latest_time = @w0"], ["Previous business week", "Events from the previous business week. If you run a search with this time range on a Sunday, the earliest time value will be the previous Monday. However, if you run this time range on a Saturday, the earliest time will be Monday 2 weeks ago.", "earliest_time = -6d@w1latest_time = -1d@w6"], ["Previous month", "Events from the previous month.", "earliest_time = -1mon@monlatest_time = @mon"], ["Previous year", "Events from the complete calendar year of the previous year. For example, if you run a search on any day in 2024, search results include events from Jan 1, 2023 at 00:00:00 to Jan 1, 2024 at 00:00:00.", "earliest_time = -1y@ylatest_time = @y"], ["Last 15 minutes", "Events from the last 15 minutes.", "earliest_time = -15mlatest_time = now"], ["Last 60 minutes", "Events from the last 60 minutes.", "earliest_time = -60m@mlatest_time = now"], ["Last 4 hours", "Events from the last 4 hours.", "earliest_time = -4h@mlatest_time = now"], ["Last 24 hours", "Events from the last 24 hours.", "earliest_time = -24h@hlatest_time = now"], ["Last 7 days", "Events from the last 7 days.", "earliest_time = -7d@hlatest_time = now"], ["Last 30 days", "Events from the last 30 days.", "earliest_time = -30d@dlatest_time = now"]]}], "chunk_index": 0, "total_chunks": 9, "metadata": {"title": "Select time ranges to apply to your search", "section_heading": "Select from a list of Preset time ranges", "section_id": "id_965eddb9_8bc6_4d13_8942_0983805d6ffa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/select-time-ranges-to-apply-to-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:03.316871+00:00", "version": "10.2"}}
{"id": "1f554bd6e5065846", "content": "Use Relative time range options to specify a custom time range for your search that is relative to Now or the Beginning of the current hour. You can select from the list of time range units: Seconds Ago, Minutes Ago, and so on. By default, Earliest is set to No Snap-to and Latest is set to Now. If you specify the snap-to option for Earliest or Latest , the time range will snap to beginning of the time frame that you select. For example, if you select Days Ago , the Earliest snap to value is Beginning of today. The preview boxes below the fields update to the time range as you make the selections. To learn more about relative time ranges, see Specify time modifiers in your search .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 9, "metadata": {"title": "Select time ranges to apply to your search", "section_heading": "Define custom Relative time ranges", "section_id": "id_8c19b64a_66a6_4172_8f11_2de5b51138e9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/select-time-ranges-to-apply-to-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:03.316883+00:00", "version": "10.2"}}
{"id": "7614ee9ae36cf621", "content": "Note: Users must have the Admin role to run and save real-time searches. For more information on managing roles and assigning roles to users, see Create and manage roles with Splunk Web in Securing Splunk Enterprise. In Splunk Cloud Platform on Victoria Experience, real-time searches are enabled by default. In Splunk Cloud Platform on Classic Experience, you must open a support ticket to enable real-time search. For more information, see About real-time searches and reports in the Search Manual. Users can use the real-time option to specify a custom Earliest time for a real-time search. Because this time range is for a real-time search, a Latest time is not relevant. To learn more about time ranges for real-time searches, see Specify real-time time range windows in your search .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 9, "metadata": {"title": "Select time ranges to apply to your search", "section_heading": "Define custom Real-time time ranges", "section_id": "id_4fcc0588_fd21_460c_b4f2_75f6b549bfe7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/select-time-ranges-to-apply-to-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:03.316890+00:00", "version": "10.2"}}
{"id": "acdce9d447b6c783", "content": "Use the Date Range option to specify custom calendar dates in your search. You can choose among options to return events: Between a beginning and end date, Before a date, and Since a date. For these fields, you can type the date into the text box or select the date from a calendar.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 9, "metadata": {"title": "Select time ranges to apply to your search", "section_heading": "Define custom Date ranges", "section_id": "ec8e5e18_0edd_4030_8111_d2e37200453d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/select-time-ranges-to-apply-to-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:03.316897+00:00", "version": "10.2"}}
{"id": "97bf633097c4cd8d", "content": "Use the Date & Time Range option to specify custom calendar dates and times for the beginning and ending of your search. You can type the date into the text box or select the date from a calendar.", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 9, "metadata": {"title": "Select time ranges to apply to your search", "section_heading": "Define custom Date & Time ranges", "section_id": "d16ceaed_c73d_4702_b828_c9415ddfd326--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/select-time-ranges-to-apply-to-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:03.316903+00:00", "version": "10.2"}}
{"id": "32c7b2b9f9ad8ce2", "content": "Use the Advanced option to specify the earliest and latest search times. You can write the times in UNIX time or relative time notation, such as -3d@d. The UNIX time value you type is converted to local time. The UNIX time or relative time that you specify is displayed as a timestamp under the text field so that you can verify your entry.", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 9, "metadata": {"title": "Select time ranges to apply to your search", "section_heading": "Use Advanced time range options", "section_id": "d37a2939_2327_41e1_a718_8a7f555a8e62--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/select-time-ranges-to-apply-to-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:03.316909+00:00", "version": "10.2"}}
{"id": "f85a745ec42318cd", "content": "You can customize the set of time ranges that appear in the Presets list the time range picker in Splunk Web. You can create a time range based on an existing time range, or you can hide time ranges. Create a time range based on an existing time range The easiest way to create a new time range is to use an existing time range as the basis for a new time range. For example, the Relative time range list contains the Last 15 minutes time range. You want to create a time range for the last 30 minutes. You start by creating a duplicate, or clone, of the Last 15 minutes time range. In the clone, you change the Earliest setting from -15min to -30min. From the Settings menu, under the Knowledge list select User interface. In the User Interface window, select Time ranges. Locate the time range that you want to use. In the Actions column click Clone. A copy of the specifications for the time range appear. Make the changes to the time range specifications and click Save. The new time range appears in the Relative list in the Presets menu. Create a new Preset time range You can create a new time range for the Presets menu. For example, you want to create a time range that shows searches yesterday from the hours of 12:00 to 15:00. You need to specify relative times in the Earliest and Latest fields. In the Earliest field you specify -1d@d+12h. In the Latest field you specify -1d@d+15h. From the Settings menu, under the Knowledge list select User interface. In the User Interface window, select Time ranges. Click New. Complete the fields in the Add New window and click Save. The new time range appears in the Relative list in the Presets menu. Hide a time range on the Presets list From the Settings menu, under the Knowledge list select User interface. In the User Interface window, select Time ranges. Locate the time range you want to hide. In the Status column click Disable. Setting default time ranges for the API or CLI You can set time ranges manually in the times.conf file when you want to specify a time range for a REST API endpoint or for the command line interface (CLI). Splunk Cloud Platform To set the default time ranges for the API, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Cloud Platform users don't have shell access to the Splunk Cloud Platform deployment and can't use the CLI to set default time ranges. Splunk Enterprise Prerequisites Only users with file system access, such as system administrators, can change time ranges manually in the times.conf file. Review the steps in How to edit a configuration file in the Admin Manual. Note: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make the changes in the local directory. Steps Open the local times.conf file for the Search app. For example, $SPLUNK_HOME/etc/apps/<app_name>/local. Create a stanza for the time range that you want to specify. For examples, see the times.conf reference in the Admin Manual .", "code_examples": [], "tables": [], "chunk_index": 6, "total_chunks": 9, "metadata": {"title": "Select time ranges to apply to your search", "section_heading": "Customize the list of Preset time ranges", "section_id": "id_06043913_de88_4a1d_8678_bcccc4329699--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/select-time-ranges-to-apply-to-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:03.316915+00:00", "version": "10.2"}}
{"id": "4fe7cae76d307884", "content": "The default time range for ad hoc searches in the Search & Reporting App is set to Last 24 hours. In Splunk Enterprise, an administrator can set the default time range globally, across all apps. See Change default values in the Splunk Enterprise Admin Manual. In Splunk Cloud Platform, contact Splunk customer support to request a change to the default time range.", "code_examples": [], "tables": [], "chunk_index": 7, "total_chunks": 9, "metadata": {"title": "Select time ranges to apply to your search", "section_heading": "Change the default time range", "section_id": "id_129dd7ed_9420_490d_a06d_75059138118f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/select-time-ranges-to-apply-to-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:03.316920+00:00", "version": "10.2"}}
{"id": "563f932526491cca", "content": "About searching with time Specify time ranges for real-time searches How time zones are processed", "code_examples": [], "tables": [], "chunk_index": 8, "total_chunks": 9, "metadata": {"title": "Select time ranges to apply to your search", "section_heading": "See also", "section_id": "id_0fe4f029_c698_4b1d_bdbb_c65a90a08dcb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/select-time-ranges-to-apply-to-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:03.316925+00:00", "version": "10.2"}}
{"id": "973d435bb58359fb", "content": "A time range that you specify in the Search bar, or in a saved search, overrides the time range that is selected in the Time Range Picker. For example, if you specify a time range of Last 24 hours in the Time Range Picker and in the Search bar you specify earliest=-30m latest=now , the search only looks at events that have a timestamp within the last 30 minutes. This applies to any of the options you can select in the Time Range Picker, However, this does not apply to subsearches. Time ranges and subsearches Time ranges selected from the Time Range Picker apply to the main search and to subsearches, unless a time range is specified in the Search bar. Time ranges that you specify directly in the Search bar apply only to that portion of the search. The time ranges specified in the main search do not apply to subsearches. Time time ranges specified in a subsearch applies only to that subsearch. The time range does not apply to the main search or any other subsearch.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 9, "metadata": {"title": "Specify time modifiers in your search", "section_heading": "Time modifiers and the Time Range Picker", "section_id": "id_038190ec_7949_4092_88de_2c32e6bf0461--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-modifiers-in-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:20.718849+00:00", "version": "10.2"}}
{"id": "c36b8520583597fe", "content": "For exact time ranges, the syntax for the time modifiers is %m/%d/%Y:%H:%M:%S. For example, the following search specifies a time range from 12 A.M. April 19, 2025 to 12 A.M. April 27, 2025. If you specify only the earliest time modifier, latest is set to the current time now by default. If you specify a latest time modifier, you must also specify an earliest time.", "code_examples": [{"language": "spl", "code": "earliest=04/19/2025:00:00:00 latest=04/27/2025:00:00:00"}], "tables": [], "chunk_index": 1, "total_chunks": 9, "metadata": {"title": "Specify time modifiers in your search", "section_heading": "Specify absolute time ranges", "section_id": "id_073fdf83_1af6_4d1b_80f8_f0b2064291e1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-modifiers-in-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:20.718859+00:00", "version": "10.2"}}
{"id": "bf738d2260ec1848", "content": "You define the relative time in your search by using a string of characters that indicate the amount of time. The syntax is an integer and a time unit. 1. Begin your string with a minus ( - ) or a plus ( + ) to indicate the offset before or after the time amount. 2. Specify the amount of time by using a number and a time unit. When you specify single time amounts, the number is implied. For example s is the same as 1s , m is the same as 1m , and so on. The supported time units are listed in the following table. When specifying relative time, use now to refer to the current time.", "code_examples": [], "tables": [{"headers": ["Time range", "Valid values"], "rows": [["seconds", "s, sec, secs, second, seconds"], ["minutes", "m, min, mins, minute, minutes"], ["hours", "h, hr, hrs, hour, hours"], ["days", "d, day, days"], ["weeks", "w, week, weeks"], ["months", "mon, month, months"], ["quarters", "q, qtr, qtrs, quarter, quarters"], ["years", "y, yr, yrs, year, years"]]}], "chunk_index": 2, "total_chunks": 9, "metadata": {"title": "Specify time modifiers in your search", "section_heading": "Specify relative time ranges", "section_id": "b33de090_913b_4771_a1ba_f530aafd3775--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-modifiers-in-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:20.718874+00:00", "version": "10.2"}}
{"id": "26cc492696a5234b", "content": "With relative time, you can specify a snap to time, which is an offset from the relative time. The snap to time unit rounds down to the nearest or latest time for the time amount that you specify. To do this, separate the time amount from the snap to time unit with an \"@\" character. The syntax for the snap to time unit is [+|-]<time_integer><time_unit>@<time_unit>. When snapping to the nearest or latest time, Splunk software always snaps backwards or rounds down to the latest time that is not after the specified time. For example, the current time is 15:45:00 and the snap to time is earliest=-h@h. The time modifier snaps to 14:00. You can also define the relative time modifier using only the snap to time unit. For example, to snap to a specific day of the week, use @w0 for Sunday, @w1 for Monday, and so forth. For Sunday, you can specify either w0 or w7. If you do not specify a snap to time unit, the search uses seconds as the snap to time unit. The snap to option becomes very useful in a range of situations. For example, if you want to search for events in the previous month, specify earliest=-mon@mon latest=@mon. This example begins at the start of the previous month and ends at the start of the current month. Difference between relative time and relative snap to time On April 28th, you decide to run a search at 14:05. If you specify earliest=-2d , the search goes back exactly two days, starting at 14:05 on April 26th. If you specify earliest=-2d@d , the search goes back to two days and snaps to the beginning of the day. The search looks for events starting from 00:00 on April 26th. Special time units The following abbreviations are reserved for special cases of time units and snap time offsets.", "code_examples": [], "tables": [{"headers": ["Time Unit", "Description"], "rows": [["earliest=1", "If you want to search events from the start of UNIX epoch time, useearliest=1. UNIX epoch time 1 is UTC January 1, 1970 at 12:00:01 AM.earliest=0in the search string indicates that time is not used in the search.Whenearliest=1andlatest=noworlatest=<a_large_number>, the search will run overall time. The difference is that:Specifyinglatest=now(which is the default) does not return future events.Specifyinglatest=<a_large_number>returns future events, which are events that contain timestamps later than the current time,now()."], ["earliest=nowlatest=now", "Specify that the search starts or ends at the current time.Any search that includesearliest=<relative time offset>should also includelatest=now. For example,earliest=-10s latest=now. SeeSpecify earliest relative time offset and latest time in ad hoc searches."], ["@q, @qtr, or @quarter", "Specify a snap to the beginning of the most recent quarter: Jan 1, Apr 1, July 1, or Oct 1."], ["w0, w1, w2, w3, w4, w5, w6, and w7", "Specify \"snap to\"days of the week; where w0 is Sunday, w1 is Monday, etc. When you snap to a week,@wor@week, it is equivalent to snapping to Sunday or@w0. You can use either w0 or w7 for Sunday."]]}], "chunk_index": 3, "total_chunks": 9, "metadata": {"title": "Specify time modifiers in your search", "section_heading": "Relative time modifiers that snap to a time", "section_id": "d6ed2138_2996_4c62_8f14_d79c9efee400--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-modifiers-in-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:20.718884+00:00", "version": "10.2"}}
{"id": "0c3f810f11ee6287", "content": "Ad hoc searches that use the earliest time modifier with a relative time offset should also include latest=now in order to avoid time range inaccuracies. For example, if you want to get all events from the last 10 seconds starting at 01:00:10, the following search returns all events that occur between the time of 01:00:00 and 01:00:10, as expected. Running the same search without including latest=now might produce unpredictable results or impact performance in certain scenarios when the search head is overloaded with ad hoc searches.", "code_examples": [{"language": "spl", "code": "index=main earliest=-10s latest=now"}], "tables": [], "chunk_index": 4, "total_chunks": 9, "metadata": {"title": "Specify time modifiers in your search", "section_heading": "Specify earliest relative time offset and latest time in ad hoc searches", "section_id": "id_4f5e8bf3_422c_45fc_9b73_d12005c62816--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-modifiers-in-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:20.718890+00:00", "version": "10.2"}}
{"id": "8b3b81059307277a", "content": "For the following examples, the current time is Wednesday, 05 June 2024, 01:37:05 P.M. Note: 24h is usually but not always equivalent to 1d because of Daylight Savings Time boundaries.", "code_examples": [], "tables": [{"headers": ["Time modifier", "Description", "Resulting time", "Equivalent modifiers"], "rows": [["now", "Now, the current time", "Wednesday, 04 June 2025, 01:37:05 P.M.", "now()"], ["-60m", "60 minutes ago", "Wednesday, 04 June 2025, 12:37:05 P.M.", "-60m@s"], ["-1h@h", "1 hour ago, to the hour", "Wednesday, 04 June 2025, 12:00:00 P.M.", ""], ["-1d@d", "Yesterday", "Tuesday, 03 June 2025, 12:00:00 A.M.", ""], ["-24h", "24 hours ago (yesterday)", "Tuesday, 03 June 2025, 01:37:05 P.M.", "-24h@s"], ["-7d@d", "7 days ago, 1 week ago today", "Wednesday, 28 May 2025, 12:00:00 A.M.", ""], ["-7d@m", "7 days ago, snap to minute boundary", "Wednesday, 28 May 2025, 01:37:00 P.M.", ""], ["@w0", "Beginning of the current week", "Sunday, 01 June 2025, 12:00:00 A.M.", ""], ["+1d@d", "Tomorrow", "Thursday, 05 June 2025, 12:00:00 A.M.", ""], ["+24h", "24 hours from now, tomorrow", "Thursday, 05 June 2025, 01:37:05 P.M.", "+24h@s"]]}], "chunk_index": 5, "total_chunks": 9, "metadata": {"title": "Specify time modifiers in your search", "section_heading": "Examples of relative time modifiers", "section_id": "id_9385eaac_e8a8_4051_90f8_e81d4424d4bf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-modifiers-in-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:20.718905+00:00", "version": "10.2"}}
{"id": "9e94b777ba2f877f", "content": "You can also specify offsets from the snap-to-time or \"chain\" together the time modifiers for more specific relative time definitions.", "code_examples": [], "tables": [{"headers": ["Time modifier", "Description", "Resulting time"], "rows": [["@d-2h", "Snap to the beginning of today (12 A.M.) and subtract 2 hours from that time.", "10 P.M. last night."], ["-mon@mon+7d", "One month ago, snapped to the first of the month at midnight, and add 7 days.", "The 8th of last month at 12 A.M."]]}], "chunk_index": 6, "total_chunks": 9, "metadata": {"title": "Specify time modifiers in your search", "section_heading": "Examples of chained relative time offsets", "section_id": "id_5d71d61f_e52b_4e3f_b259_9f9ce449afe7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-modifiers-in-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:20.718912+00:00", "version": "10.2"}}
{"id": "5b9241749210570a", "content": "Search from the beginning of the week to the time of your search This example searches for Web access errors from the beginning of the week to the time that you run your search. This search returns matching events starting from 12:00 A.M. of the Sunday of the current week to the current time. Of course, this means that if you run this search on Monday at noon, you will only see events for 36 hours of data. Search the current business week This example searches for Web access errors from the current business week, where w1 is Monday and w6 is Friday. This search returns matching events starting from 12:00 A.M. of the Monday of the current week and ending at 11:59 P.M. of the Friday of the current week. If you run this search on Monday at noon, you will only see events for 12 hours of data. If you run this search on Friday, you will see events from the beginning of the week to the current time on Friday. The timeline however, will display for the full business week. Search the last full business week This example searches Web access errors from the last full business week. This search returns matching events starting from 12:00 A.M. of last Monday and ending at 11:59 P.M. of last Friday. Search the last 24 hours but omit 1 hour This example searches an index for the last 24 hours but omits any events returned from Midnight to 1:00 A.M., when downtime returns false log entries. This search specifies two time ranges: 24 hours before the search is run, up to midnight The beginning of the day that the search is run, starting at 1 hour after midnight or 1:00 A.M. Search for specific windows of time The following table describes several ways to search for specific windows of time:", "code_examples": [{"language": "spl", "code": "eventtype=webaccess error earliest=@w0 latest=now"}, {"language": "spl", "code": "eventtype=webaccess error earliest=@w1 latest=+7d@w6"}, {"language": "spl", "code": "eventtype=webaccess error earliest=-7d@w1 latest=@w6"}, {"language": "spl", "code": "index=myindex ((earliest=-24h latest<@d) OR (earliest>=@d+1h))"}, {"language": "spl", "code": "... earliest=-7d latest=now"}, {"language": "spl", "code": "... earliest=-7d latest=-7d+5h"}, {"language": "spl", "code": "i... earliest=-7d@d latest=-7d@d+2h"}, {"language": "spl", "code": "... earliest=-7d@d-3h latest=-7d@d"}], "tables": [{"headers": ["Description", "Search", "Search runtime", "Search start window", "Search end window"], "rows": [["Search for events from seven days ago until today.", "... earliest=-7d latest=now", "Wednesday May 11th at 3:37 PM", "Wednesday May 4th at 3:37 PM", "Wednesday, May 11th at 3:37 PM"], ["Search for events from seven days ago for a five hour window.", "... earliest=-7d latest=-7d+5h", "Wednesday May 11th at 3:37 PM", "Wednesday May 4th at 3:37 PM", "Wednesday May 4th at 8:37 PM"], ["Search for events from seven days ago, from the beginning of that day for a two hour window.", "i... earliest=-7d@d latest=-7d@d+2h", "Wednesday May 11th at 3:37 PM", "Wednesday May 4th at 00:00 AM", "Wednesday May 4th at 2:00 AM"], ["Search for events from seven days ago. Start three hours before the beginning of that day. End at the beginning of the day, seven days ago.", "... earliest=-7d@d-3h latest=-7d@d", "Wednesday May 11th at 3:37 PM", "Tuesday May 3rd at 9:00 PM", "Wednesday May 4th at 00:00 AM"]]}], "chunk_index": 7, "total_chunks": 9, "metadata": {"title": "Specify time modifiers in your search", "section_heading": "Examples of searches with relative time modifiers", "section_id": "dd9dc82b_22a7_4294_829f_62ee4c004545--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-modifiers-in-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:20.718922+00:00", "version": "10.2"}}
{"id": "6608566d4011da40", "content": "Time modifiers in the Search Reference", "code_examples": [], "tables": [], "chunk_index": 8, "total_chunks": 9, "metadata": {"title": "Specify time modifiers in your search", "section_heading": "See also", "section_id": "b31a71b3_2557_4aa3_acb1_5e15b2600918--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-modifiers-in-your-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:20.718927+00:00", "version": "10.2"}}
{"id": "f82de3a47908e49c", "content": "Transactions returned at search time consist of the raw text of each event, the shared event types, and the field values. Transactions also have additional data that is stored in the fields: duration and transactiontype. duration contains the duration of the transaction (the difference between the timestamps of the first and last events of the transaction). transactiontype is the name of the transaction (as defined in transactiontypes.conf by the transaction's stanza name). You can add transaction to any search. For best search performance, craft your search and then pipe it to the transaction command. For more information see the topic on the transaction command in the Search Reference manual. Follow the transaction command with the following options. Note: Some transaction command options do not work in conjunction with other options. name=<transaction-name> Specifies the name of a stanza from transactiontypes.conf. Use this to invoke a transaction type that you have already configured for reuse. If other arguments are provided, they overule values specified for the same arguments in the transaction rule. For example, if web_purchase , the transaction rule you're invoking, is configured with maxevents=10 , but you'd like to run it with a different value for maxevents , add maxevents to the search string with the value you want: [field-list] This is a comma-separated list of fields, such as ...| transaction host,cookie If set, each event must have the same field(s) to be considered part of the same transaction. Events with common field names and different values will not be grouped. For example, if you add ...| transaction host , then a search result that has host=mylaptop can never be in the same transaction as a search result with host=myserver. A search result that has no host value can be in a transaction with a result that has host=mylaptop. match=closest Specify the matching type to use with a transaction definition. The only value supported currently is closest. maxspan=[<integer>s | m | h | d] Set the maximum duration of one transaction. Can be in seconds, minutes, hours or days. For example: 5s, 6m, 12h or 30d. Defaults to maxspan=-1 , for an \"all time\" timerange. maxpause=[<integer> s|m|h|d] Specifies the maximum pause between transactions. Requires there be no pause between the events within the transaction greater than maxpause. If the value is negative, the maxspause constraint is turned off. Defaults to maxpause=-1. startswith=<string> A search or eval-filtering expression which, if satisfied by an event, marks the beginning of a new transaction. For example: startswith=\"login\" startswith=(username=foobar) startswith=eval(speed_field < max_speed_field) startswith=eval(speed_field < max_speed_field/12) Defaults to \"\". endswith=<transam-filter-string> A search or eval-filtering expression which, if satisfied by an event, marks the end of a transaction. For example: endswith=\"logout\" endswith=(username=foobar) endswith=eval(speed_field < max_speed_field) endswith=eval(speed_field < max_speed_field/12) Defaults to \"\". For startswith and endswith , <transam-filter-string> is defined with the following syntax: \"<search-expression>\" | (<quoted-search-expression>) | eval(<eval-expression> <search-expression> is a valid search expression that does not contain quotes. <quoted-search-expression> is a valid search expression that contains quotes. <eval-expression> is a valid eval expression that evaluates to a boolean. Examples: search expression: (name=\"foo bar\") search expression: \"user=mildred\" search expression: (\"search literal\") eval bool expression: eval(distance/time < max_speed)", "code_examples": [{"language": "spl", "code": "sourcetype=access_*  | transaction name=web_purchase maxevents=5"}], "tables": [], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Identify and group events into transactions", "section_heading": "Transaction search options", "section_id": "id_1a4d505d_6071_4d54_b2c2_dc6fd95346c7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/group-and-correlate-events/identify-and-group-events-into-transactions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Group and Correlate Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:38.116362+00:00", "version": "10.2"}}
{"id": "df99a3ad57abe937", "content": "Run a search that groups together all of the web pages a single user (or client IP address) looked at over a time range. This search takes events from the access logs, and creates a transaction from events that share the same clientip value that occurred within 5 minutes of each other (within a 3 hour time span). Refer to the transaction command topic in the Search Reference Manual for more examples.", "code_examples": [{"language": "spl", "code": "sourcetype=access_combined  | transaction clientip maxpause=5m maxspan=3h"}], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Identify and group events into transactions", "section_heading": "Example transaction search", "section_id": "id_5003074d_50f9_4f36_88eb_ae721e2ec0ef--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/group-and-correlate-events/identify-and-group-events-into-transactions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Group and Correlate Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:38.116371+00:00", "version": "10.2"}}
{"id": "c1dd4e25e2a23b71", "content": "Time is crucial for determining what went wrong. You often know generally when something occurred. Splunk software enables you to identify baseline patterns or trends in your events and compare it against current activity. You can run a series of time-based searches to investigate and identify atypical activity and then use the timeline to drill into specific time periods. Looking at events that happened around the same time can help correlate results and find the root cause. Read more about how to Use the timeline to investigate events in this manual.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "dc22c74b0e12c496cb8be9cc85d3138e5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/group-and-correlate-events/use-time-to-identify-relationships-between-events", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Group and Correlate Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:39:54.789838+00:00", "version": "10.2"}}
{"id": "ad4d9469627fbd5b", "content": "To identify the IP address of the top customer at Buttercup Games with the most purchases, you could run the following search: Then, you could search the customer's purchase history by running the following search on the customer's IP address, which is 87.194.216.51: But, what if the next time you run this search, someone else is the top customer? You would have to run the first search again to find out the new top customer's IP address and then rewrite the second search with that new IP address. Instead of going to all of that trouble, you could get the same results by using a subsearch to correlate the events with the IP address and pass the top customer's IP address to the main search every time you run the search: The search results look something like this:", "code_examples": [{"language": "spl", "code": "sourcetype=access_* status=200 action=purchase\n| toplimit=1 clientip"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase clientip=87.194.216.51\n| stats count, distinct_count(productId), values(productId) by clientip"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase \n    [ search sourcetype=access_* status=200 action=purchase \n    | toplimit=1 clientip \n    | table clientip\n        ] \n| stats count, distinct_count(productId), values(productId) by clientip"}], "tables": [{"headers": ["clientip", "count", "dc(productId)", "values(productId)"], "rows": [["87.194.216.51", "134", "14", "BS-AG-G09CU-PG-G06DB-SG-G01DC-SG-G02FI-AG-G08FS-SG-G03MB-AG-G07MB-AG-T01PZ-SG-G05SC-MG-G10WC-SH-A01WC-SH-A02WC-SH-G04WC-SH-T02"]]}], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "Use subsearch to correlate events", "section_heading": "Example", "section_id": "cb06bb59_9c5e_45ed_8d6c_f8755c34c428--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/subsearches/use-subsearch-to-correlate-events", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Subsearches", "manual": "search-manual", "scraped_at": "2026-01-23T13:40:12.205514+00:00", "version": "10.2"}}
{"id": "43ae78ea58028fd1", "content": "A transaction search is useful for a single observation of any physical event stretching over multiple logged events. Use the transaction command to define a transaction or override transaction options specified in transactiontypes.conf. One common use of a transaction search is to group multiple events into a single meta-event that represents a single physical event. For example, an out of memory problem could trigger several database events to be logged, and they can all be grouped together into a transaction. To learn more, see Identify and group events into transactions in this manual.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "About transactions", "section_heading": "Transaction search", "section_id": "fce32ff3_3f38_4c6c_9458_c48ebc87e710--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/group-and-correlate-events/about-transactions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Group and Correlate Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:40:29.672572+00:00", "version": "10.2"}}
{"id": "28636c5f39670da6", "content": "This example searches for transactions with the same session ID and IP address. This example defines a transaction as a group of events that have the same session ID, JSESSIONID , and come from the same IP address, clientip , and where the first event contains the string, \"view\", and the last event contains the string, \"purchase\". The search defines the first event in the transaction as events that include the string, \"view\", using the startswith=\"view\" argument. The endswith=\"purchase\" argument does the same for the last event in the transaction. This example then pipes the transactions into the where command and the duration field to filter out all of the transactions that took less than a second to complete. The where filter cannot be applied before the transaction command because the duration field is added by the transaction command. The values in the duration field show the difference, in seconds, between the timestamps for the first and last events in the transaction. You might be curious about why the transactions took a long time, so viewing these events might help you to troubleshoot. You won't see it in this data, but some transactions may take a long time because the user is updating and removing items from their shopping cart before completing the purchase. Additionally, this search is run over all events. There is no filtering before the transaction command. Anytime you can filter the search before the first pipe, the faster the search runs. For more examples, see the transaction command.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | transaction JSESSIONID clientip startswith=\"view\"endswith=\"purchase\"|whereduration>0"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "About transactions", "section_heading": "Transaction search example", "section_id": "f0964ad0_e715_4122_9ebe_dd65a189cc7f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/group-and-correlate-events/about-transactions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Group and Correlate Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:40:29.672581+00:00", "version": "10.2"}}
{"id": "e69373809fb22c8d", "content": "Both the stats command and the transaction command are similar in that they enable you to aggregate individual events together based on field values. The stats command is meant to calculate statistics on events grouped by one or more fields and discard the events (unless you are using eventstats or streamstats ). On the other hand, except for the duration between first and last events and the count of events, the transaction command does not compute statistics over the grouped events. Additionally, it retains the raw event and other field values from the original event and enables you to group events using much more complex criteria, such as limiting the grouping by time span or delays and requiring terms to define the start or end of a group. The transaction command is most useful in two specific cases: 1. When a unique ID (from one or more fields) alone is not sufficient to discriminate between two transactions. This is the case when the identifier is reused, for example web sessions identified by cookie or client IP. In this case, time spans or pauses are also used to segment the data into transactions. In other cases, when an identifier is reused, for example in DHCP logs, a particular message may identify the beginning or end of a transaction. 2. When it is desirable to see the raw text of the events combined rather than an analysis on the constituent fields of the events. In other cases, it's usually better to use the stats command, which performs more efficiently, especially in a distributed environment. Often there is a unique ID in the events and you can use the stats command. For example, to compute the statistics on the duration of trades identified by the unique ID trade_id , the following searches will yield the same answer: and If however, the trade_id values are reused but each trade ends with some text, such as \"END\", the only solution is to use this transaction search: On the other hand, if trade_id values are reused, but not within a 10 minute duration, the solution is to use the following transaction search: Read more about \"About event grouping and correlation\" in an earlier chapter in this manual.", "code_examples": [{"language": "spl", "code": "... | transaction trade_id | chart count by duration span=log2"}, {"language": "spl", "code": "... | stats range(_time) as duration by trade_id | chart count by duration span=log2"}, {"language": "spl", "code": "... | transaction trade_id endswith=END | chart count by duration span=log2"}, {"language": "spl", "code": "... | transaction trade_id maxpause=10m | chart count by duration span=log2"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "About transactions", "section_heading": "Using stats instead of transaction", "section_id": "fcbc549f_5395_455e_8cdc_085a0ee6db93--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/group-and-correlate-events/about-transactions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Group and Correlate Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:40:29.672586+00:00", "version": "10.2"}}
{"id": "8c7a81ec1f11b66b", "content": "Transactions and macro searches are a powerful combination that allow substitution into your transaction searches. Make a transaction search and then save it with $field$ to allow substitution. For an example of how to use macro searches and transactions, see Define and use search macros in the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "About transactions", "section_heading": "Transactions and macro search", "section_id": "id_8351c698_6f0a_401b_a36b_96167cfa352a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/group-and-correlate-events/about-transactions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Group and Correlate Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:40:29.672592+00:00", "version": "10.2"}}
{"id": "51527965dd18d07c", "content": "A subsearch is a search within a primary, or outer, search. When a search contains a subsearch, the subsearch typically runs first. Subsearches must be enclosed in square brackets in the primary search. Consider the following search. The subsearch portion of the search is enclosed in square brackets. The first command in a subsearch must be a generating command , such as search , eventcount , inputlookup , and tstats. For a list of generating commands, see Command types in the Search Reference. One exception is the foreach command, which accepts a subsearch that does not begin with a generating command, such as eval. Note: When a search contains a subsearch, the Splunk software processes the subsearch first as a distinct search job. Then it runs the search that contains it as another search job. Keep this in mind if you include subsearches in searches that are run frequently and you are concerned about search concurrency issues or excess load on your search scheduler. How subsearches work A subsearch looks for a single piece of information that is then added as a criteria, or argument, to the primary search. You use a subsearch because the single piece of information that you are looking for is dynamic. The single piece of information might change every time you run the subsearch. For example, you want to return all of the events from the host that was the most active in the last hour. The host that was the most active might be different from hour to hour. You need to identify the most active host before you can return the events from that host. Break this search down into two parts. The most active host in the last hour. This is the subsearch. The events from that host. This is the primary search. You could run two searches to obtain the list of events. The following search identifies the most active host in the last hour. This search returns only one host value. Assume that the result is the host named crashy. To return all of the events from the host crashy , you need to run a second search. The drawback to running two searches is that you cannot set up reports and dashboard panels to run automatically. You must run the first search to identify the piece of information that you need, and then run the second search with that piece of information. You can combine these two searches into one search that includes a subsearch. The subsearch is in square brackets and is run first. The subsearch in this example identifies the most active host in the last hour. The result of the subsearch is then provided as a criteria for the main search. The main search returns the events for the host. Note: By default, subsearches return a maximum of 10,000 results and have a maximum runtime of 60 seconds. If a subsearch runs for more than 60 seconds, its search results are automatically finalized. Time ranges and subsearches Time ranges selected from the Time Range Picker apply to the base search and to subsearches. However, time ranges specified directly in the base search do not apply to subsearches. Likewise, a time range specified directly in a subsearch applies only to that subsearch. The time range does not apply to the base search or any other subsearch. For example, if the Time Range Picker is set to Last 7 days and a subsearch contains earliest=2d@d , then the earliest time modifier applies only to the subsearch and Last 7 days applies to the base search. When to use subsearches Subsearches are mainly used for two purposes: Parameterize one search, using the output of another search. The previous example of searching for the most active host in the last hour is an example of this use of a subsearch. Run a separate search and add the output to the first search using the append command. A subsearch can be used only where the explicit action that you are trying to accomplish is with the search and not a transformation of the data. For example, you cannot use a subsearch with \" sourcetype=top | multikv \", because the multikv command does not expect a subsearch as an argument. Certain commands, such as append and join can accept a subsearch as an argument. Multiple subsearches in a search string You can use more than one subsearch in a search. If a search has a set of nested subsearches, the inner most subsearch is run first, followed by the next inner subsearch, working out to the outermost subsearch and then the primary search. For example, you have the following search. Note: You must have a command after the pipe and before the subsearch. In this example, the search command is used for the second and third subsearches. The first subsearch is used as a predicate to the main search. The order in which this search is processed is: |search [search index=* | stats by ipaddress] |search [search index=* | stats count by user] [search index=* | stats count by component] (An implied search command) index=* OR index=_* (and the results of the nested subsearches) Here is another example. Be certain to analyze your search syntax when you find yourself using subsearches frequently. It is often possible to rewrite the search and omit the subsearch. If the subsearches are sequential instead of nested, the subsearch farthest to the left, or beginning of the search, is run first. Then working towards the right, or end of the search, the next subsearch is run. When all of the subsearches are run, then the primary search is run. For example, you have the following search. The order in which the search is processes is: search index=* | stats by customerID search index=* | stats count by productName (An implied search command) index=* OR index=_* (the results of the subsearches) Initiating subsearches with search commands A subsearch can be initiated through a search command such as the map command. For example, consider the following search: The map command runs a subsearch for the option search=\"search index=* OR index=_* earliest=0 latest=now\". Other commands that can run a subsearch include search , join and union. These commands can start one or more subsearches. What's more, each subsearch, not the command itself, is limited by the maxtime setting in the limits.conf file, which specifies the maximum number of seconds that a subsearch can run before finalizing. In our example, the maxtime setting limits the runtime of the subsearch and is applied to the subsearch, not the map command. However, the subsearch may stop at the map command.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* status=200 action=purchase [search sourcetype=access_* status=200 action=purchase | toplimit=1 clientip | table clientip] | stats count, dc(productId), values(productId) by clientip"}, {"language": "spl", "code": "[search sourcetype=access_* status=200 action=purchase | toplimit=1 clientip | table clientip]"}, {"language": "spl", "code": "sourcetype=syslog earliest=-1h | toplimit=1 host | fields host"}, {"language": "spl", "code": "sourcetype=syslog host=crashy"}, {"language": "spl", "code": "sourcetype=syslog  [search sourcetype=syslog earliest=-1h | toplimit=1 host | fields + host]"}, {"language": "spl", "code": "index=* OR index=_* \n[search index=* | stats count by component \n| search [search index=* | stats count by user \n| search [search index=* | stats by ipaddress]]]"}, {"language": "spl", "code": "index=foo error [ search index=bar baz [search index=* | stats count by user | search count>100] | stats count by host ]"}, {"language": "spl", "code": "index=* OR index=_* | [search index=* | stats count by customerID] | [search index=* | stats by productName]"}, {"language": "spl", "code": "|makeresults \n|map search=\"search index=* OR index=_* earliest=0 latest=now\""}], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "About subsearches", "section_heading": "Using subsearches", "section_id": "id_32e73a62_0f81_4c89_a099_f2556fe612ed--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/subsearches/about-subsearches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Subsearches", "manual": "search-manual", "scraped_at": "2026-01-23T13:40:46.819764+00:00", "version": "10.2"}}
{"id": "b63922d087879eb9", "content": "These examples show you the difference between searching your data with and without a subsearch. Example 1: Without a subsearch, find what the most frequent shopper purchased You want to find the single most frequent shopper on the Buttercup Games online store and what that shopper has purchased. Use the top command to return the most frequent shopper. To find the shopper who accessed the online shop the most, use this search. The limit=1 argument specifies to return 1 value. The clientip argument specifies the field to return. This search returns one clientip value, 87.194.216.51, which you will use to identify the VIP shopper. You now need to run another search to determine how many different products the VIP shopper has purchased. Use the stats command to count the purchases by this VIP customer. This search uses the count() function to return the total count of the purchases for the VIP shopper. The dc() function is the distinct_count function. Use this function to count the number of different, or unique, products that the shopper bought. The values function is used to display the distinct product IDs as a multivalue field. The drawback to this approach is that you have to run two searches each time you want to build this table. The top purchaser is not likely to be the same person at any given time range. Example 2: Using a subsearch, find what the most frequent shopper purchased Let's start with our first requirement, to identify the single most frequent shopper on the Buttercup Games online store. Copy and paste the following search into the Search bar and run the search. Make sure the time range is All time. This search returns the clientip for the most frequent shopper, clientip=87.194.216.51. This search is almost identical to the search in Example 1 Step 1. The difference is the last piped command, | table clientip , which displays the clientip information in a table. To find what this shopper has purchased, you run a search on the same data. You provide the result of the most frequent shopper search as one of the criteria for the purchases search. The most frequent shopper search becomes the subsearch for the purchases search. The purchases search is referred to as the outer or primary search. Because you are searching the same data, the beginning of the outer search is identical to the beginning of the subsearch. A subsearch is enclosed in square brackets [ ] and processed first when the search is parsed. Copy and paste the following search into the Search bar and run the search. Because the top command returns the count and percent fields, the table command is used to keep only the clientip value. These results should match the result of the two searches in Example 1, if you run it on the same time range. If you change the time range, you might see different results because the top purchasing customer will be different. Note: The performance of this subsearch depends on how many distinct IP addresses match status=200 action=purchase. If there are thousands of distinct IP addresses, the top command has to keep track of all of those addresses before the top 1 is returned, impacting performance. By default, subsearches return a maximum of 10,000 results and have a maximum runtime of 60 seconds. In large production environments, it is possible that the subsearch in this example will timeout before it completes. The best option is to rewrite the query to limit the number of events that the subsearch must process. Alternatively, you can increase the maximum results and maximum runtime parameters. For more information about these options, see About performance considerations. Subsearches and long complex searches can be difficult to read. You can apply auto-formatting to the search syntax to make the the search syntax easier to read in the Search bar. Use the following keyboard shortcut to apply auto-formatting to a search. On Linux or Windows use Ctrl + \\. On Mac OSX use Command + \\. You can also use Command + Shift + F , which works well with many non-English keyboards. You can make the information more understandable by renaming the columns. You rename columns by using the AS operator on the fields in your search. If the rename that you want to use contains a space, you must enclose the rename in quotation marks. To rename the fields, copy and paste the following search into the Search bar and run the search.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | toplimit=1 clientip"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase clientip=87.194.216.51 | stats count, dc(productId), values(productId) by clientip"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | toplimit=1 clientip | table clientip"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase [search sourcetype=access_* status=200 action=purchase | toplimit=1 clientip | table clientip] | stats count, distinct_count(productId), values(productId) by clientip"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase [search sourcetype=access_* status=200 action=purchase | toplimit=1 clientip | table clientip] | stats count AS\"Total Purchased\", distinct_count(productId) AS\"Total Products\", values(productId) AS\"Product IDs\"by clientip | rename clientip AS\"VIP Customer\""}], "tables": [{"headers": [], "rows": [["These examples use the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["Column", "Rename"], "rows": [["count", "Total Purchased"], ["dc(productId)", "Total Products"], ["values(productId)", "Product IDs"], ["clientip", "VIP Customer"]]}], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "About subsearches", "section_heading": "Subsearch examples", "section_id": "f81e0e65_7aba_43d1_a57c_0894b710ff12--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/subsearches/about-subsearches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Subsearches", "manual": "search-manual", "scraped_at": "2026-01-23T13:40:46.819775+00:00", "version": "10.2"}}
{"id": "f9f958ca310eca0d", "content": "A subsearch can be a performance drain if the search returns a large number of results. Consider this search. The performance of this subsearch depends on how many distinct IP addresses match status=200 action=purchase. If there are thousands of distinct IP addresses, the top command has to keep track of all of them before the top 1 is returned, impacting performance. Additionally, by default subsearches return a maximum of 10,000 results and have a maximum runtime of 60 seconds. In large production environments it is quite possible that the subsearch in this example will timeout before it completes. There are several alternatives you can use to control the results: Try to rewrite the query to limit the number of events the subsearch must process. You can change the number of results that the format command operates over inline with your search by appending the format command to the end of your subsearch. For more information, see the format command in the Search Reference. If you are using Splunk Enterprise, you can also control the subsearch by editing settings in the [subsearch] stanza in the limits.conf file. For example, you can edit the maxout setting to adjust the size of results that are returned from your subsearches. See How to edit a configuration file. After running a search, you can click the Job menu and select Inspect Job to open the Search Job Inspector. Scroll down to the remoteSearch component to see the actual query that resulted from your subsearch. For more information, see View search job properties in this manual.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* status=200 action=purchase [search sourcetype=access_* status=200 action=purchase | toplimit=1 clientip | table clientip] | stats count, dc(productId), values(productId) by clientip"}, {"language": "spl", "code": "...| format maxresults = <integer>"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "About subsearches", "section_heading": "Subsearch performance considerations", "section_id": "id_98856722_4c42_4e8c_82a0_d1236d95d5e1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/subsearches/about-subsearches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Subsearches", "manual": "search-manual", "scraped_at": "2026-01-23T13:40:46.819784+00:00", "version": "10.2"}}
{"id": "118a9105cde1066e", "content": "By default, subsearches return a maximum of 10,000 results. You will see variations in the actual number of output results because every command can change what the default maxout is when the command invokes a subsearch. Additionally, the default applies to subsearches that are intended to be expanded into a search expression, which is not the case for some commands such as join , append , and appendcols. For example, the append command can override the default maximum if the maxresultrows argument is specified, unless you specify maxout as an argument to the append command. The output limit of the join command is controlled by subsearch_maxout in the limits.conf file. This defaults to 50,000 events.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "About subsearches", "section_heading": "Output settings for subsearch commands", "section_id": "id_4d06d75e_10d1_4d6c_adb8_877eb2100703--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/subsearches/about-subsearches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Subsearches", "manual": "search-manual", "scraped_at": "2026-01-23T13:40:46.819789+00:00", "version": "10.2"}}
{"id": "88e36142135ea668", "content": "Regardless of how time is specified in your events, timestamps are converted to UNIX time and stored in the _time field when your data is indexed. If your data does not have timestamps, the time at which your data is indexed is used as the timestamp for your events. UNIX time is the number of seconds that have elapsed since 00:00:00 Coordinated Universal Time (UTC), 1 January 1970. This moment in time is sometimes referred to as epoch time. GMT and UTC GMT (Greenwich Mean Time) is sometimes confused with UTC (Coordinated Universal Time). However GMT is a time zone and UTC is a time standard. GMT is a time zone officially used in some European and African countries as their local time. The time is displayed in either the 24-hour format (00:00-23:59) or the 12-hour format (00:00-12:00 AM/PM). UTC is a time standard that is the basis for time and time zones worldwide. No country uses UTC as a local time. Neither GMT nor UTC ever change for Daylight Saving Time (DST). However, some of the countries that use GMT switch to different time zones during their DST period. For example, the United Kingdom uses GMT for most of the year, but switches to British Summer Time (BST) during the summer months. BST is one hour ahead of GMT.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "How time zones are processed by the Splunk platform", "section_heading": "Timestamps are stored in UNIX time", "section_id": "id_8ef4f409_01a6_4f9c_969c_d69ab841637b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/how-time-zones-are-processed-by-the-splunk-platform", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:03.308500+00:00", "version": "10.2"}}
{"id": "199015057312b26d", "content": "When data is indexed and added to your Splunk instance, the Splunk indexer assumes that any timestamps in the data are in the same time zone as your Splunk instance. Let's use a set of test data that contains 35 events with various timestamps. The data looks something like this: You can see how the timestamp is processed in the Add Data wizard on the Set Source Type step, for example when you add a CSV file. The values in the timestamp field in the sample data file are converted to UNIX time and stored in the _time field when the data is indexed. However, for display purposes the values in the _time field are shown in a human-readable format. The values in the _time field are the same as the values in the timestamp field in the sample data file because the Splunk indexer assumes that the values in the timestamp field are in the same time zone as the Splunk instance. The default method used to extract timestamps from your data is set to Auto. Timestamp extraction methods Note: Most events do not require special timestamp handling. The Splunk indexer automatically recognizes and extracts the timestamps. Use these other methods only if you discover that the Splunk indexer is not extracting timestamps correctly. In the Add Data wizard on the Set Source Type step, these are the Timestamp options: In addition to the Auto timestamp extraction method, there are other methods for timestamp extraction when you index data: Current time Sets the timestamp to the current clock time. Ignores the timestamps in the fields in the data. Advanced You can specify the Time Zone, the Timestamp format using the strptime() function, and the Timestamp fields when multiple fields comprise the complete timestamp. See the strptime() function in Date and Time Functions in the Search Reference. Configuration file You can specify a configuration file for custom timestamp extractions from your event data. The configuration file must reside in $SPLUNK_HOME and have an XML extension.", "code_examples": [], "tables": [{"headers": ["timestamp", "test_no"], "rows": [["2018/10/1 00:00", "tz_test0"], ["2018/10/1 00:15", "tz_test1"], ["2018/10/1 01:00", "tz_test2"], ["2018/10/1 01:30", "tz_test3"], ["2018/10/1 01:45", "tz_test4"], ["2018/10/1 02:00", "tz_test5"], ["2018/10/1 02:30", "tz_test6"]]}], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "How time zones are processed by the Splunk platform", "section_heading": "Which time zone is used for timestamps", "section_id": "ddc39161_54bd_42e4_94c1_590ea5e31b50--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/how-time-zones-are-processed-by-the-splunk-platform", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:03.308513+00:00", "version": "10.2"}}
{"id": "a8ff94d8baeaf54b", "content": "Maybe you want some data indexed using a different time zone than the default time zone. You can specify the time zone in the Add Data wizard. For example, let's say your Splunk instance is in San Francisco, California, which is in the US Pacific time zone. You are in an office in Tokyo, which is in the Japanese time zone. The data is from October, a time when the US uses daylight saving time. The time zone in the US is referred to as Pacific Daylight Time (PDT). Japan does not have daylight saving time. The time zone for Tokyo is Japan Standard Time (JST). You want to index the data using Japan Standard Time instead of the timezone of the Splunk instance in California. In this example, the data is in a CSV file. To access the Add Data wizard in Splunk Web: From the Settings menu click Upload. In the Set Source Type step of the Add Data wizard, click Timestamp , Advanced , and then Time Zone. Select the time zone that you want to use. In this example, the selected time zone is (GMT+09:00) Osaka, Sapporo, Tokyo. When you select a different time zone, the values in the _time field reflect the timezone you specify when you index the data. In this example, this is the Japanese time zone. These timestamps are equivalent to the timestamps where the Splunk instance is located. In this example, this is the Pacific Daylight Time (PDT). Remember, timestamps are stored in UNIX time. However, time is displayed in the _time field in a human-readable format. In this example, the values in the _time field are 16 hours earlier than the timestamp values in the Tokyo time zone. Notice that the UNIX time does not change. What changes is the time in each time zone that is equivalent to the UNIX time.", "code_examples": [], "tables": [{"headers": ["Location", "Timestamp", "UNIX time", "Description"], "rows": [["Toyko (JST)", "2018/10/1 00:00", "1538352000", "Midnight or 2400 hours. GMT + 09:00 hours."], ["GMT", "2018/09/30 15:00", "1538352000", "Nine hours behind Japan, which is equivalent to 15:00 hours on September 30th. GMT + 00:00 hours."], ["San Francisco (PDT)", "2018/09/30 08:00", "1538352000", "Seven hours behind GMT. 16 hours behind Japan, which is equivalent to 08:00 hours on September 30th. GMT - 07:00 hours."]]}], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "How time zones are processed by the Splunk platform", "section_heading": "Indexing events using a different time zone", "section_id": "f342b661_1e8d_44b2_ac4e_fb6ac04c5498--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/how-time-zones-are-processed-by-the-splunk-platform", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:03.308522+00:00", "version": "10.2"}}
{"id": "6f2131452b3ba8df", "content": "When you specify a time in your search, either by using the time range picker or using time modifiers, the time that you specify is converted into UNIX time for processing. See Select time ranges to apply to your search and Specify time modifiers in your search. Because event timestamps are stored in UNIX time, your searches return a consistent set of results regardless of the time zone you are in. For example, if you search from 12:00 to 14:00 PDT (Pacific Daylight Time), that is the same as searching from 19:00 to 21:00 GMT (Greenwich Mean Time) which is 7 hours ahead of PDT. When daylight saving time is over, Pacific Standard Time (PST) is used. The difference between GMT and PST is 8 hours. Note: In Splunk Web, the values in the _time field appear in a human-readable format in the UI. However, the values in the _time field are actually stored in UNIX time.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "How time zones are processed by the Splunk platform", "section_heading": "How time is interpreted when you search", "section_id": "id_2483a6e4_c5cd_4914_b2c1_487c5ab484ed--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/how-time-zones-are-processed-by-the-splunk-platform", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:03.308527+00:00", "version": "10.2"}}
{"id": "1b70f0318e22e599", "content": "The time range that you specify for a search might return different sets of events in different time zones. This can occur for time ranges that you specify using the time range picker and time ranges that you specify explicitly in the search with the earliest and latest time modifiers, Here are some examples: If you use the default Last 24 hours time range picker setting, the search processes the events using UNIX time. The same set of events are returned for a user in San Francisco and a user in Tokyo. If you use a time range that refers to a time associated with today such as Since 00:00:00 , the search processes events based on midnight of your time zone, not UNIX time. A different set of events are returned for a user in San Francisco and a user in Tokyo, because the time that midnight occurs is different in each timezone. There are several settings in the time range picker that fall into this category, such as the Preset setting Today and the Date Range setting Since <today's date>. If you use a snap-to time, such as @d or @mon , the search processes events based on the beginning of the day or month of your timezone, not UNIX time. A different set of events are returned for a user in San Francisco and a user in Tokyo, because the beginning of a day or month in one time zone is not the same UNIX time as the beginning of a day in another time zone. To mitigate the issues with time zones, specify time based on the time zone where the Splunk indexer resides.", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "How time zones are processed by the Splunk platform", "section_heading": "How time zones impact search results", "section_id": "id_660fc04b_f502_4921_b0cd_0257282a93ce--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/how-time-zones-are-processed-by-the-splunk-platform", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:03.308533+00:00", "version": "10.2"}}
{"id": "1d4d35851d02d24a", "content": "Getting Data In How timestamp assignment works Specify time zones for timestamps The Set Source Type page Securing Splunk Platform Create and manage users with Splunk Web", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "How time zones are processed by the Splunk platform", "section_heading": "See also", "section_id": "bee5410d_c013_43a0_a311_2445ded7a3d2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/how-time-zones-are-processed-by-the-splunk-platform", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:03.308538+00:00", "version": "10.2"}}
{"id": "b3282b22394184df", "content": "To run a search in real time, you can select from predefined Real-time time range windows in the time range range picker list or you can specify a custom real-time window using Custom time... and selecting Real-time. Time ranges for real-time search follow the same syntax as for historical searches, except that you precede the relative time specifier with \"rt\", so that it's rt<time_modifier>: rt[+|-]<time_integer><time_unit>@<time_unit>. See Specify time modifiers in your search. These values are not designed to be used in a search string. If you are a Splunk Enterprise administrator, you can use these values when you edit the times.conf file (to add options to the time range picker), to specify the earliest/latest time ranges in the saved search dialog, or when you use the REST API to access the Splunk search engine. When you use time range windows with real-time searches, some of the events that occur within the latest second may not be displayed. This is expected behavior and is due to the latency between the timestamps within the events and the time when the event arrives. Because the time range window is with respect to the timestamps within the events and not the time when the event arrives, events that arrive after the time window won't display.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Specify time ranges for real-time searches", "section_heading": "Real-time modifier syntax", "section_id": "id_2ed201db_68a8_4d50_81e4_35797acb81f7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-ranges-for-real-time-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:20.599934+00:00", "version": "10.2"}}
{"id": "c1f18de8254f0ee8", "content": "There is a small difference between real-time searches that take place within a set time window (30 seconds, 1 minute, 2 hours) and real-time searches that are set to \"all time.\" In \"windowed\" real time searches, the events in the search can disappear as they fall outside of the window, and events that are newer than the time the search job was created can appear in the window when they occur. In \"all-time\" real-time searches, the window spans all of your events, so events do not disappear once they appear in the window, but events that are newer than the time the search job was created can appear in the window as they occur. In comparison, historical search events never disappear from within the set range of time that you are searching and the latest event is always earlier than the job creation time (with the exception of searches that include events that have future-dated timestamps). Note: When you run large all-time real-time searches that result in more than 1,000 events, you may notice that not all of the search results are displayed in the Events viewer. This is because Splunk software uses a circular buffer to quickly access data in real time and displays only about 1,000 of the most recent events. To see all of the events when you run your searches, select a time range other than All time (real-time) , such as All time .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Specify time ranges for real-time searches", "section_heading": "Real-time searches over \"all time\"", "section_id": "id_024d3b9d_cc39_4fd2_88ab_45271c9378eb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-ranges-for-real-time-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:20.599940+00:00", "version": "10.2"}}
{"id": "611e744c2b42c3af", "content": "For windowed real-time searches, you can backfill the initial window with historical data. This is run as a single search, just in two phases: first, a search on historical data to backfill events; then, a normal real-time search. Real-time backfill ensures that real-time dashboards seeded with data on actual visualizations and statistical metrics over time periods are accurate from the start. You can enable real-time backfill in the limits.conf file in the [realtime] stanza:", "code_examples": [{"language": "spl", "code": "[realtime]\n\ndefault_backfill = <bool>\n* Specifiesifwindowed real-time searches should backfill events\n* Defaults totrue"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Specify time ranges for real-time searches", "section_heading": "Real-time backfill", "section_id": "id_354f9c72_fc70_4abe_a05a_034074cbe4f7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-ranges-for-real-time-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:20.599945+00:00", "version": "10.2"}}
{"id": "f9588daeaa6d5fa1", "content": "About real-time searches and reports Expected performance and known limitations of real-time searches and reports", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Specify time ranges for real-time searches", "section_heading": "See also", "section_id": "id_61247114_29ed_4225_9a87_80d16a394fbd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/specify-time-ranges-for-real-time-searches", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:20.599949+00:00", "version": "10.2"}}
{"id": "ebb11687a1577fcc", "content": "The _time field represents the timestamp of an event. When you run a search to retrieve events, the timestamp for each event is listed under the Time column. You can click the timestamp of an event and open a dialog box containing controls, called a _time accelerator. Use the _time accelerator to run a new search that retrieves events chronologically close to that event. You can search for all events that occurred before or after the event time. The accelerators are Before this time , After this time , and At this time. In addition, you can search for nearby events. For example, you can search for + 30 seconds, - 1 minutes, +/- 5 hours, and so on.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Use time to find nearby events", "section_heading": "Use time accelerators", "section_id": "d26db6b1_a477_48b0_aa49_a2dc11ff3c9a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/use-time-to-find-nearby-events", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:37.083670+00:00", "version": "10.2"}}
{"id": "db366b2ddd1f19bf", "content": "The timeline is a histogram of the number of events returned by a Splunk search over a chosen time range. The time range is broken up into smaller time intervals (such as seconds, minutes, hours, or days), and the count of events for each interval is displayed as a column. The location of each column on the timeline corresponds to an instance when the events that match your search occurred. If there are no columns at a time period, no events were found then. The taller the column, the more events occurred at that time. Spikes in the number of events or no events along the timeline can indicate time periods that you want to investigate. The timeline has drilldown functionality similar to the table and chart drilldown. When you click on a column in the timeline, your search results update to show only the events represented by the column. If you double-click on a column, you re-run the search over the time range represented by the column. Then, you can search for all surrounding events at this time range.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Use time to find nearby events", "section_heading": "Use the timeline", "section_id": "id_1f100159_e0b3_42bd_84f3_216eb16544fd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/specify-time-ranges/use-time-to-find-nearby-events", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Specify Time Ranges", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:37.083677+00:00", "version": "10.2"}}
{"id": "6205453db891c2c9", "content": "Run the following search to use the stats command to determine the number of different page requests, GET and POST, that occurred for each Web server. This example uses eval expressions to specify the different field values for the stats command to count. The first clause uses the count() function to count the Web access events that contain the method field value GET. Then, using the AS keyword, the field that represents these results is renamed GET. The second clause does the same for POST events. The counts of both types of events are then separated by the web server, using the BY clause with the host field. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | stats count(eval(method=\"GET\")) AS GET, count(eval(method=\"POST\")) AS POST BY host"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["host", "GET", "POST"], "rows": [["www1", "8431", "5197"], ["www2", "8097", "4815"], ["www3", "8338", "4654"]]}], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Use stats with eval expressions and functions", "section_heading": "Use eval expressions to count the different types of requests against each Web server", "section_id": "id_8312ecfa_5081_4c84_88f7_3a0fe0a960db--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/calculate-statistics/use-stats-with-eval-expressions-and-functions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Calculate Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:53.537828+00:00", "version": "10.2"}}
{"id": "568ead8860e734bd", "content": "Find out how much of the email in your organization comes from .com, .net, .org or other top level domains. The eval command in this search contains two expressions, separated by a comma. The first part of this search uses the eval command to break up the email address in the mailfrom field. The from_domain is defined as the portion of the mailfrom field after the @ symbol. The split() function is used to break the mailfrom field into a multivalue field called accountname. The first value of accountname is everything before the \"@\" symbol, and the second value is everything after. The mvindex() function is used to set from_domain to the second value in the multivalue field accountname. The results are then piped into the stats command. The stats count() function is used to count the results of the eval expression. The eval expression uses the match() function to compare the from_domain to a regular expression that looks for the different suffixes in the domain. If the value of from_domain matches the regular expression, the count is updated for each suffix, .com , .net , and .org. Other domain suffixes are counted as other. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "sourcetype=\"cisco:esa\"mailfrom=* \n|evalaccountname=split(mailfrom,\"@\"), from_domain=mvindex(accountname,-1) \n| stats count(eval(match(from_domain,\"[^\\n\\r\\s]+\\.com\"))) AS\".com\",\n  count(eval(match(from_domain,\"[^\\n\\r\\s]+\\.net\"))) AS\".net\", \n  count(eval(match(from_domain,\"[^\\n\\r\\s]+\\.org\"))) AS\".org\", \n  count(eval(NOT match(from_domain,\"[^\\n\\r\\s]+\\.(com|net|org)\"))) AS\"other\""}], "tables": [{"headers": [], "rows": [["This example uses sample email data. You should be able to run this search on any email data by replacing thesourcetype=cisco:esawith thesourcetypevalue and themailfromfield with email address field name in your data. For example, the email might beTo,From, orCc)."]]}, {"headers": [".com", ".net", ".org", "other"], "rows": [["4246", "9890", "0", "3543"]]}], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Use stats with eval expressions and functions", "section_heading": "Use eval expressions to categorize and count fields", "section_id": "id_4dddc496_d83b_42cf_af43_4fe08b0fc17f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/calculate-statistics/use-stats-with-eval-expressions-and-functions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Calculate Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:53.537837+00:00", "version": "10.2"}}
{"id": "ee48f37f7f0ed088", "content": "Commands eval in the Search Reference Related information Statistical and charting functions in the Search Reference Evaluation functions in the Search Reference About evaluating and manipulating fields", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Use stats with eval expressions and functions", "section_heading": "See also", "section_id": "id_797d6182_1337_4a37_b840_052fc599bec5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/calculate-statistics/use-stats-with-eval-expressions-and-functions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Calculate Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:41:53.537842+00:00", "version": "10.2"}}
{"id": "bd81c8fa8ec533c5", "content": "The cluster command is a powerful command for detecting patterns in your events. The command groups events based on how similar they are to each other. The cluster command groups events based on the contents of the _raw field, unless you specify another field. When you use the cluster command, two new fields are appended to each event. The cluster_count is the number of events that are part of the cluster. This is the cluster size. The cluster_label specifies which cluster the event belongs to. For example, if the search returns 10 clusters, then the clusters are labeled from 1 to 10. Anomalies come in small or large groups (or clusters) of events. A small group might consist of 1 or 2 login events from a user. An example of a large group of events might be a DDoS attack of thousands of similar events.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "Detecting patterns", "section_heading": "Detecting patterns in events", "section_id": "b7049ff2_1b03_4eeb_a66e_e3a2c458d366--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/detecting-patterns", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:10.105682+00:00", "version": "10.2"}}
{"id": "21abf490817b2a79", "content": "Use the labelonly=true parameter to return all of the events. If you use labelonly=false , which is the default, then only one event from each cluster is returned. Use the showcount=true parameter so that a cluster_count field is added to all of the events. If showcount=false , which is the default, the event count is not added to the event. The threshold parameter t adjusts the cluster sensitivity. The smaller the threshold value, the fewer the number of clusters.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "Detecting patterns", "section_heading": "Use the cluster command parameters wisely", "section_id": "id_9cf81d22_8f58_40de_aeed_5ea7a34334d4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/detecting-patterns", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:10.105690+00:00", "version": "10.2"}}
{"id": "681b0438430d69f5", "content": "Use the dedup command on the cluster_label column to see the most recent grouped events within each cluster. To group the events and make the results more readable, use the sort command with the cluster columns. Sort the cluster_count column based on the number of clusters. For small groups of events, sort the cluster_count column in ascending order. For large groups of events sort the cluster_count column in descending order. Sort the cluster_label column in ascending order. Cluster labels are numeric. Sorting in ascending order organizes the events by label, in numerical order.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "Detecting patterns", "section_heading": "Other commands to use with the cluster command", "section_id": "id_66fc57ef_429b_4653_bb92_fede258a0dcb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/detecting-patterns", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:10.105694+00:00", "version": "10.2"}}
{"id": "57165324e9159ef4", "content": "The following search uses the CustomerID in the sales_entries.log file. Setting showcount=true ensures that all events get a cluster_count. The cluster threshold is set to 0.7. Setting labelonly=true returns the incoming events. The dedup command is used to see the 3 most recent events within each cluster. The results are sorted in descending order to group the events. If you do not set labelonly=true , then only one event from each cluster is returned.", "code_examples": [{"language": "spl", "code": "source=\"/opt/log/ecommsv1/sales_entries.log\"CustomerID \n | cluster showcount=truet=0.7 labelonly=true| table _time, cluster_count, cluster_label, _raw \n | dedup 3 cluster_label \n | sort -cluster_count, cluster_label, - _time"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "Detecting patterns", "section_heading": "Return the 3 most recent events in each cluster", "section_id": "id_9f886bbf_00ab_4192_b950_5aadd574c9ad--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/detecting-patterns", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:10.105699+00:00", "version": "10.2"}}
{"id": "9b3fe10c5c6e433c", "content": "Related information About advanced statistics Commands dedup cluster sort", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "Detecting patterns", "section_heading": "See also", "section_id": "b78ce8cf_683d_4591_a0a5_6ed984ac882d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/detecting-patterns", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:10.105703+00:00", "version": "10.2"}}
{"id": "46f3e7b774c2be8d", "content": "These commands are used to find anomalies in your data. You can search for uncommon or outlying events and fields, or cluster similar events together.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["analyzefields", "Analyzes numeric fields for their ability to predict another discrete field."], ["anomalies", "Computes an \"unexpectedness\" score for an event."], ["anomalousvalue", "Finds and summarizes irregular, or uncommon, search results."], ["anomalydetection", "Computes a probability for each event and detects unusually small probabilities."], ["cluster", "Groups similar events together."], ["kmeans", "Partitions the events into k clusters, with each cluster defined by its mean value. Each event belongs to the cluster with the nearest mean value."], ["outlier", "Removes outlying numerical values."], ["rare", "Displays the least common values of a field."]]}], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Commands for advanced statistics", "section_heading": "Commands that find anomalies", "section_id": "ab4ce14a_9aca_44b7_a721_c24540198762--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/commands-for-advanced-statistics", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:26.571044+00:00", "version": "10.2"}}
{"id": "0908b2f3783f5b6a", "content": "These commands predict future values and calculate trendlines that can be used to create visualizations.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["predict", "Predicts future values of a time series."], ["trendline", "Computes moving averages of fields."], ["x11", "Looks for trends in a time series by removing repeating patterns."]]}], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Commands for advanced statistics", "section_heading": "Commands for predicting and trends", "section_id": "id_45f7f9a5_cc40_4044_bb84_16dad94bc4c1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/commands-for-advanced-statistics", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:26.571054+00:00", "version": "10.2"}}
{"id": "34224750722b414d", "content": "These commands are used to build transforming searches. These commands return statistical data tables that are required for charts and other kinds of data visualizations.", "code_examples": [], "tables": [{"headers": ["Command", "Description"], "rows": [["addtotals", "Computes the sum of all numeric fields for each search result."], ["contingency", "Builds a contingency table, a co-occurrence matrix, for the values of two fields."], ["correlate", "Calculates the correlation between different fields."], ["eval", "Calculates mathematical, string, or boolean expressions. Puts the value into a new field."], ["eventstats", "Adds summary statistics to all search results."], ["geostats", "Generates statistics to display geographic data and summarize the data on maps."], ["outlier", "Removes outlying numerical values."], ["rare", "Displays the least common values of a field."], ["stats", "Calculates aggregate statistics over the results set, such as average, count, and sum."], ["streamstats", "Adds summary statistics about the preceding events to each search result."], ["timechart", "Create a time series chart and corresponding table of statistics. See \"Statistical and charting functions\" in theSearch Reference."], ["trendline", "Computes moving averages of fields."]]}], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Commands for advanced statistics", "section_heading": "Commands for reports, charts, and maps", "section_id": "id_26f0b29d_7c17_414b_af9e_dd1abd1310fd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/commands-for-advanced-statistics", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:26.571062+00:00", "version": "10.2"}}
{"id": "c40e98b37d0c213c", "content": "About advanced statistics Finding and removing outliers Detecting anomalies Detecting patterns About time series forecasting", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Commands for advanced statistics", "section_heading": "See also", "section_id": "a04e7d38_a8f2_4a8b_8c29_88e8564b7959--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/commands-for-advanced-statistics", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:26.571066+00:00", "version": "10.2"}}
{"id": "0368203b85e1aa78", "content": "The stats , streamstats , and eventstats commands each enable you to calculate summary statistics on the results of a search or the events retrieved from an index. The stats command works on the search results as a whole. The streamstats command calculates statistics for each event at the time the event is seen, in a streaming manner. The eventstats command calculates statistics on all search results and adds the aggregation inline to each event for which it is relevant. See more about the differences between these commands in the next section. The chart command returns your results in a data structure that supports visualization as a chart (such as a column, line, area, and pie chart). You can decide what field is tracked on the x-axis of the chart. The timechart command returns your results formatted as a time-series chart, where your data is plotted against an x-axis that is always a time field. Read more about visualization features and options in the Visualization Reference of the Data Visualization Manual. The stats , chart , and timechart commands (and their related commands eventstats and streamstats ) are designed to work in conjunction with statistical functions. The list of statistical functions lets you count the occurrence of a field and calculate sums, averages, ranges, and so on, of the field values. For the list of statistical functions and how they're used, see \"Statistical and charting functions\" in the Search Reference .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Use the stats command and functions", "section_heading": "About the stats commands and functions", "section_id": "id_3285324d_d06d_4122_a97d_b254b2797189--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/calculate-statistics/use-the-stats-command-and-functions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Calculate Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:43.260354+00:00", "version": "10.2"}}
{"id": "b1766e4488ce3695", "content": "The eventstats and streamstats commands are variations on the stats command. The stats command works on the search results as a whole and returns only the fields that you specify. For example, the following search returns a table with two columns and 10 rows. The ASumOfBytes and clientip fields are the only fields that exist after the stats command. For example, the following search returns empty cells in the bytes column because it is not a result field. To see more fields other than ASumOfBytes and clientip in the results, you need to include them in the stats command. Also, if you want to perform calculations on any of the original fields in your raw events, you need to do that before the stats command. The eventstats command computes the same statistics as the stats command, but it also aggregates the results to the original raw data. When you run the following search, it returns an events list instead of a results table, because the eventstats command does not change the raw data. You can use the table command to format the results as a table that displays the fields you want. With the table command you can view the values of the bytes field, or any of the original fields in your raw events. The streamstats command also aggregates the calculated statistics to the original raw event, but it does this at the time the event is seen. To demonstrate this, include the _time field in the earlier search and use the streamstats command. Instead of a total sum for each clientip , which is what the stats and eventstats commands return, this search calculates a sum for each event based on the time that it is seen. The streamstats command is useful for reporting on events at a known time range.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | head 10 | stats sum(bytes) as ASumOfBytes by clientip"}, {"language": "spl", "code": "sourcetype=access_* | head 10 | stats sum(bytes) as ASumOfBytes by clientip | table bytes, ASumOfBytes, clientip"}, {"language": "spl", "code": "sourcetype=access_* | head 10 | eventstats sum(bytes) as ASumOfBytes by clientip"}, {"language": "spl", "code": "sourcetype=access_* | head 10 | eventstats sum(bytes) as ASumOfBytes by clientip | table bytes, ASumOfBytes, clientip"}, {"language": "spl", "code": "sourcetype=access_* | head 10 | sort _time | streamstats sum(bytes) as ASumOfBytes by clientip | table _time, clientip, bytes, ASumOfBytes"}], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Use the stats command and functions", "section_heading": "Stats, eventstats, and streamstats", "section_id": "ee4da1f8_623d_46d1_ac4b_22f0a5820ecd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/calculate-statistics/use-the-stats-command-and-functions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Calculate Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:43.260362+00:00", "version": "10.2"}}
{"id": "d6c1a0b41fdf7e82", "content": "Example 1 This example creates a chart of how many new users go online each hour of the day. The dc , or distinct_count , function returns a count of the unique values of userid and renames the resulting field dcusers. If you don't rename the function, for example dc(userid) as dcusers , the resulting calculation is automatically saved to a field with the same name as the function call, such as dc(userid). The delta command is used to find the difference between the current and previous dcusers value. Then, the sum of this delta is charted over time. Example 2 This example calculates the median for a field, then charts the count of events where the field has a value less than the median. The eventstats command is used to calculate the median for all the values of bytes from the previous search. Example 3 This example calculates the standard deviation and variance of calculated fields. This search returns errors from the last 7 days and creates the new field called warns from extracted fields errorGroup and errorNum. The stats command is used twice. First, to calculate the daily count of warns for each day. Then, to calculate the standard deviation and variance of that count for each warns. Example 4 You can use the calculated fields as filter parameters for your search. In this example, the eventstats command is used to calculate the average and standard deviation of the URI lengths from the useragent field. Then, these numbers are used as filters for the retrieved events.", "code_examples": [{"language": "spl", "code": "... | sort _time | streamstats dc(userid) as dcusers | delta dcusers as deltadcusers | timechart sum(deltadcusers)"}, {"language": "spl", "code": "... | eventstats median(bytes) as medbytes |evalsnap=if(bytes>=medbytes, bytes,\"smaller\") | timechart count by snap"}, {"language": "spl", "code": "sourcetype=log4j ERROR earliest=-7d@d latest=@d |evalwarns=errorGroup+\"-\"+errorNum | stats count as Date_Warns_Count by date_mday,warns | stats stdev(Date_Warns_Count), var(Date_Warns_Count) by warns"}, {"language": "spl", "code": "sourcetype=access_* |evalURILen = len(useragent) | eventstats avg(URILen) as AvgURILen, stdev(URILen) as StdDevURILen|whereURILen > AvgURILen+(2*StdDevURILen) | chart count by URILen span=10 cont=true"}], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Use the stats command and functions", "section_heading": "Examples", "section_id": "id_8dab1beb_c9db_461f_bea0_accb1e3437fc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/calculate-statistics/use-the-stats-command-and-functions", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Calculate Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:43.260366+00:00", "version": "10.2"}}
{"id": "55c8370c87360b4e", "content": "An outlier is a data point that is far removed from the typical distribution of data points. In some situations you might want to simply identify the outliers. In other situations you might want to remove the outliers so that they do not skew your statistical results or create issues displaying data in your charts.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "Finding and removing outliers", "section_heading": "What is an outlier?", "section_id": "bfaac23a_054b_415c_ae0b_f0dd80463203--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/finding-and-removing-outliers", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:59.881910+00:00", "version": "10.2"}}
{"id": "afb87108af7981ed", "content": "A statistical outlier is a data point that is far removed from some measure of centrality. Typical measures of centrality are mean, median, and mode. Mean is the average value. Median is the value at the middle of a sorted list of all values. Mode is the most frequent value. There are different types of statistical outliers. Outliers can be far from the mean, far from the median, or a small number relative to the mode. An outlier can also be a data point that is far removed from the typical range of data points.", "code_examples": [], "tables": [{"headers": ["Centrality measure", "Splunk centrality commands"], "rows": [["Mean", "stats avg(field)"], ["Median", "stats median(field)"], ["Mode", "stats mode(field)"]]}], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "Finding and removing outliers", "section_heading": "Statistical outliers", "section_id": "b5cb43ef_f2ec_44ab_b3de_c547dd712795--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/finding-and-removing-outliers", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:59.881920+00:00", "version": "10.2"}}
{"id": "f9693246390d114b", "content": "There are several methods you can use to identify outliers. Often these methods involve calculating some measure of centrality and then identifying the outliers. In some cases outliers are identified when you notice an anomaly. For example, when you chart the data and you notice that the axis is skewed. An outlier can be the culprit. Use the number of statistical outliers as a bellwether. If you see more statistical outliers than usual, that phenomenon itself is an anomaly. To calculate the centrality measure, you can use the following commands. In many cases, you need additional commands to calculate the information that you are looking for. The following sections contain some of these examples. Calculate the lower and upper boundaries of an acceptable range to identify outliers The following example takes the first 500 events from the quote.csv file. The streamstats command and a moving window of 100 events are used to calculate the average and standard deviation. The average and standard deviation are used with the eval command to calculate the lower and upper boundaries. A sensitivity is added into the calculation by multiplying the stdev by 2. The eval command uses those boundaries to identify the outliers. The outliers are then sorted in descending order. Use the interquartile range (IQR) to identify outliers The following example takes the first 500 events from the quote.csv file. The eventstats command is used to calculate the median, the 25th percentile (p25), and the 75th percentile (p75). The IQR is calculated with the eval command by subtracting the percentiles. The median and the IQR are used with the eval command to calculate the lower and upper boundaries. A sensitivity is added into the calculation by multiplying the IQR by 20.The eval command uses those boundaries to identify the outliers. The outliers are then sorted in descending order.", "code_examples": [{"language": "spl", "code": "| inputlookup quote.csv \n | head 500 \n |eval_time=(round(strptime(time,\"%Y-%m-%dÂ %H:%M:%SZ\")))\n | streamstats window=100 avg(\"price\") as \n   avg stdev(\"price\") as stdev \n |evallowerBound=(avg-stdev*2) \n |evalupperBound=(avg+stdev*2) \n |evalisOutlier=if('price'< lowerBound \n   OR'price'> upperBound, 1, 0) \n | fields\"_time\",\"symbol\",\"sourcetype\",\"time\",\"price\",\"lowerBound\",\"upperBound\",\"isOutlier\"| sort - isOutlier"}, {"language": "spl", "code": "| inputlookup quote.csv \n | head 500 \n |eval_time=(round(strptime(time,\"%Y-%m-%dÂ %H:%M:%SZ\")))\n | eventstats median(\"price\") as median p25(\"price\")\n   as p25 p75(\"price\") as p75 \n |evalIQR=(p75-p25) \n |evallowerBound=(median-IQR*20) \n |evalupperBound=(median+IQR*20) \n |evalisOutlier=if('price'< lowerBound \n   OR'price'> upperBound, 1, 0) \n | fields\"_time\",\"symbol\",\"sourcetype\",\"time\",\"price\",\"lowerBound\",\"upperBound\",\"isOutlier\"| sort - isOutlier"}], "tables": [{"headers": ["Centrality measure", "Splunk centrality commands"], "rows": [["Standard deviation", "stats stdev(field)"], ["Quartiles and percentiles", "stats perc75(field) perc25(field)"], ["Top 3 most frequent values", "top 3field"]]}], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "Finding and removing outliers", "section_heading": "Identifying outliers", "section_id": "e79f68d6_bac2_4d9f_975f_41f354bf13e5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/finding-and-removing-outliers", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:59.881927+00:00", "version": "10.2"}}
{"id": "a34af1bd9488f33d", "content": "You can use the outlier command to remove outlying numerical values from your search results. You have the option to remove or transform the events with outliers. The remove option removes the events. The transform option truncates the outlying value to the threshold for outliers. The threshold is specified with the param option. A value is considered an outlier if the value is outside of the param threshold multiplied by the inter-quartile range (IQR). The default value for param is 2.5. Create a chart of web server events, transform the outlying values For a timechart of web server events, transform the outlying average CPU values. Remove outliers that interfere with displaying the y-axis in a chart Sometimes when you create a chart, a small number of values are so far from the other values that the chart is rendered unreadable. You can remove the outliers so that the chart values are visible. Remove outliers using the three-sigma rule across transactions This example uses the eventstats command to calculate the average and the standard deviation. The three-sigma limit is then calculated. The where command filters search results. Only the events with the duration less than the three-sigma limit are returned.", "code_examples": [{"language": "spl", "code": "host=''web_server''404 \n | timechart avg(cpu_seconds) by host \n | outlier action=transform"}, {"language": "spl", "code": "index=_internalsource=*access* \n | timechart span=1h max(bytes) \n | fillnull \n | outlier"}, {"language": "spl", "code": "... |evaldurationMins = (duration/60) \n | eventstats avg(durationMins) as Avrg, stdev(durationMins) as StDev \n |evalthreeSigmaLimit = (Avrg + (StDev * 3)) \n |wheredurationMins < threeSigmaLimit"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "Finding and removing outliers", "section_heading": "Removing outliers in charts", "section_id": "d109b3cd_3e1d_4500_9eb3_f8cb4bc8abfd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/finding-and-removing-outliers", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:59.881932+00:00", "version": "10.2"}}
{"id": "3504fe8c25a1b240", "content": "When setting up an alert, it is important to review the outlier threshold you have set. * If the threshold value is too low, too many alerts are returned for non-critical outliers * If the threshold value is too high, not enough alerts are returned and you might not identify the outliers Typically a small percentage of events in your data are outliers. If you have 1,000,000 events a day and 5% of the events are outliers, setting an alert would trigger 50,000 alert actions unless you specify a throttle. A throttle suppresses additional alerts that have the same field value in a given time range. For example, your search returns on average 100 events every minute. You only want to be alerted when the status for an event is 404. You can setup the alert to perform the alert action one time every 60 seconds instead of alerting you for every event that has a 404 status in that 60 second window. Set alert throttling You set throttling as part of setting an alert. 1. Determine what percent of your events are outliers. 2. Under Settings , choose Searches, reports, and alerts. 3. Under Schedule and alert, click the Schedule this search check box. The screen expands to display the scheduling and alerting options. 4. Specify the alert condition and mode. 5. Mark the Throttling check box and specify when the throttling expires. 6. Specify the alert action. 7. Click Save .", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "Finding and removing outliers", "section_heading": "Manage alerts for outliers", "section_id": "afd8349b_e2e2_481a_8fdf_212c9e8d2f22--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/finding-and-removing-outliers", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:59.881937+00:00", "version": "10.2"}}
{"id": "fc5c8b389897e1e9", "content": "Related information About advanced statistics Detecting patterns Commands outlier stats top", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "Finding and removing outliers", "section_heading": "See also", "section_id": "id_8170eeb5_9c52_43ef_b948_1b51f32be841--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/finding-and-removing-outliers", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:42:59.881941+00:00", "version": "10.2"}}
{"id": "c935ce676d52f5f2", "content": "The Splunk Machine Learning Toolkit app delivers new SPL commands, custom visualizations, assistants, and examples to explore a variety of machine learning concepts. Each assistant includes end-to-end examples with datasets, plus the ability to apply the visualizations and SPL commands to your own data. You can inspect the assistant panels and underlying code to see how it all works. See the Splunk Machine Learning Toolkit User Guide for information.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "Machine Learning", "section_heading": "Splunk Machine Learning Toolkit", "section_id": "db274bee_e272_4fb9_a806_e10169636d1d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/machine-learning", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:43:16.356197+00:00", "version": "10.2"}}
{"id": "cc0452d7c33b5533", "content": "You want to identify spikes in your data. Spikes can show you where you have peaks, or troughs, that indicate that some metric is rising or falling sharply. There are all sorts of spikes. Traffic spikes, sales spikes, spikes in the number of returns, spikes in database load. Whatever type of spike you are interested in, you want to watch for it and then perhaps take some action to address those spikes. You can use a moving trendline to help you see the spikes. Run a search followed by the trendline command using a field that you want to create a trendline for. For example, on web access data, you could chart an average of the bytes field. To add another line or bar series to the chart for the simple moving average (sma) of the last 5 values of bytes , use this command: If you want to clearly identify spikes, you might add an additional series for spikes. The following search adds a field called \"spike\" that indicates when the average number of bytes exceeds twice the moving average. The 10000 here is arbitrary and you should choose a value relevant to your data that makes the spike noticeable. Changing the formatting of the y-axis to Log scale also helps. Putting this all together, the search is: This search uses a simple moving average for the last 5 results (sma5). Explore with different simple moving average values to determine the best simple moving average to use to identify the spikes. The trendline command also supports the exponential moving average (ema) and the weighted moving average (wma). Alternatively, you can bypass the charting altogether and replace the eval command with the where command to filter your results. And by looking at the table view or setting an alert, you will see when the avg_bytes spiked.", "code_examples": [{"language": "spl", "code": "sourcetype=access* | timechart avg(bytes) as avg_bytes"}, {"language": "spl", "code": "... | trendline sma5(avg_bytes) as moving_avg_bytes"}, {"language": "spl", "code": "... |evalspike=if(avg_bytes > 2 * moving_avg_bytes, 10000, 0)"}, {"language": "spl", "code": "sourcetype=access*\n | timechart avg(bytes) as avg_bytes\n | trendline sma5(avg_bytes) as moving_avg_bytes\n |evalspike=if(avg_bytes > 2 * moving_avg_bytes, 10000, 0)"}, {"language": "spl", "code": "... |whereavg_bytes > 2 * moving_avg_bytes"}], "tables": [], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Detecting anomalies", "section_heading": "Finding spikes in your data", "section_id": "f98e76bc_8f03_452d_9bc2_991e2f9a01f0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/detecting-anomalies", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:43:32.579905+00:00", "version": "10.2"}}
{"id": "3ec955d237464b3c", "content": "Related information About advanced statistics Commands eval trendline", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Detecting anomalies", "section_heading": "See also", "section_id": "id_1b16edde_ed67_4913_89cf_de8ebd3dbe9e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/detecting-anomalies", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:43:32.579914+00:00", "version": "10.2"}}
{"id": "e3529b1b40b25379", "content": "The Splunk search language includes two forecasting commands: predict and x11. The predict command enables you to use different forecasting algorithms to predict future values of single and multivalue fields. The x11 command, which is named after the X11 algorithm, removes seasonal fluctuations in fields to expose the real trend in your underlying data series. Forecasting algorithms You can select from the following algorithms with the predict command: LL, LLP, LLT, LLB, and LLP5. Each of these algorithms are variations of the Kalman filter. For more information, see the predict command in the Search Reference. Forecasting seasonality with the x11 command The seasonal component of your time-series data is either additive or multiplicative, which is reflected in the two types of seasonality that you can calculate with the x11 command: add() for additive and mult() for multiplicative. How do you know which type of seasonality to adjust from your data? The best way to describe the difference between an additive and a multiplicative seasonal component is with an example: The annual sales of flowers will peak on and around certain days of the year, such as Valentine's Day and Mother's Day. During Valentine's Day, the sale of roses might increase by X dollars every year. This dollar amount is independent of the normal level of the series, and you can add X dollars to your forecasts for Valentine's Day every year, making this time series a candidate for an additive seasonal adjustment. In an additive seasonal adjustment, each value of a time series is adjusted by adding or subtracting a quantity that represents the absolute amount by which that value differs from normal in that season. Alternatively, in a multiplicative seasonal component, the seasonal effect expresses itself in percentage terms. The absolute magnitude of the seasonal variations increases as the series grows over time. For example, the number of roses sold during Valentine's Day might increase by 40% or a factor of 1.4. When the sales of roses is generally weak, the absolute (dollar) increase in Valentine's Day sales will also be relatively weak. However, the percentage will be constant. And, if the sales of roses are strong, then the absolute (dollar) increase will be proportionately greater. In a multiplicative seasonal adjustment, this pattern is removed by dividing each value of the time series by a quantity that represents the percentage from normal or divided by a factor that is typically observed in that season. When plotted on a chart, these two types of seasonal components show distinguishing characteristics: The additive seasonal series shows steady seasonal fluctuations, regardless of the overall level of the series. The multiplicative seasonal series shows varying size of seasonal fluctuations that depend on the overall level of the series. For more information, see the \"x11\" command in the Search Reference .", "code_examples": [], "tables": [{"headers": ["Algorithm option", "Algorithm name", "Description"], "rows": [["LL", "Local level", "This is a univariate model with no trends and no seasonality. Requires a minimum of 2 data points."], ["LLP", "Seasonal local level", "This is a univariate model with seasonality. The periodicity of the time series is automatically computed. Requires the minimum number of data points to be twice the period."], ["LLT", "Local level trend", "This is a univariate model with trend but no seasonality. Requires a minimum of 3 data points."], ["LLB", "Bivariate local level", "This is a bivariate model with no trends and no seasonality. Requires a minimum of 2 data points. LLB uses one set of data to make predictions for another. For example, assume it uses dataset Y to make predictions for dataset X. If the holdback=10, the LLB algorithm uses the last 10 data points of Y to make predictions for the last 10 data points of X."], ["LLP5", "", "Combines LLT and LLP models for its prediction."]]}], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "About time series forecasting", "section_heading": "Commands for time series forecasting", "section_id": "id_509c4f24_a391_4a48_a022_fba02c43a2a6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/about-time-series-forecasting", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:43:49.469925+00:00", "version": "10.2"}}
{"id": "dac3e64666d76976", "content": "Related information About advanced statistics Commands predict x11", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "About time series forecasting", "section_heading": "See also", "section_id": "c6e1a8bf_653c_4811_99aa_19a04dffbcb7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/about-time-series-forecasting", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:43:49.469933+00:00", "version": "10.2"}}
{"id": "c56d166d27e9c114", "content": "You always use the sparklines feature in conjunction with chart and stats searches, because it is a function of those two search commands. It is not a command by itself. The functionality of sparklines is the same for both search commands. Sparklines are not available as a dashboard chart visualization by themselves, but you can set up a dashboard panel with a table visualization that displays sparklines. For more information, see the Visualization reference topic in the Splunk Data Visualizations Manual. For more information about the chart and stats commands, including details on the syntax around the sparkline function, see chart and stats in the Search Reference. Example: Stats, sparklines, and earthquake data Here are some examples of stats searches that use sparklines to provide additional information about earthquake data. Let's say you want to use the USGS Earthquakes data to show the locations that had the most earthquakes over the past month, a column that shows the average quake magnitude for each location. You could use the following search: This search returns the following table, with sparklines that illustrate the quake count over the course of the month for each of the top earthquake locations: Right away you can see differences in quake distribution between the different locations. You can click on a sourceLocation to see the actual events that are included in the location calculations. For the avg(mag) column, you can use the Format icon to change the number formatting in that column. You can easily get the minimum and maximum count for a particular region by mousing over the sparkline; in this example you can see that in Southern Alaska, the minimum count of quakes experienced in a single day during the 7-day period was 1, while the maximum count per day was 6. But what if you want your sparkline to represent not only the earthquake count, but also the relative average magnitude of the quakes affecting each region? In other words, how can you make the sparkline line chart represent average quake magnitude for each \"time bucket\" (segment) of the chart? Try a search like this: This search produces a sparkline for each region that shows the average quake magnitude for the quake events that fall into each segment of the sparkline. By specifying a dash ( - ) after sort , the results are sorted in descending order. But it does a bit more than that. It also asks that the sparkline divide itself up into smaller chunks of time. The preceding table had a sparkline that was divided up by day, so each data point in the sparkline represented an event count for a full 24 hour period. This is why those sparklines were so short. The addition of the 6h to the search language overrides this default and displays sparklines that are broken up into discrete six-hour chunks, which makes it easier to see the distribution of events along the sparkline for the chosen time range. The search also renames the sparkline column as magnitude_trend to make it easier to understand. Now you can see that the quakes for the nc location are separated out more evenly than what was shown in the previous search.", "code_examples": [{"language": "spl", "code": "source=\"all_month.csv\"| stats sparkline count, avg(mag) by locationSource | sort count"}, {"language": "spl", "code": "source=\"all_month.csv\"| stats sparkline(avg(mag),6h) as magnitude_trend count, avg(mag) by locationSource | sort - count"}], "tables": [{"headers": [], "rows": [["This example uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand add it as an input."]]}], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "Add sparklines to search results", "section_heading": "Using sparklines with the stats and chart commands", "section_id": "id_1b5b72bf_eb34_4e77_9f1b_77f71f344723--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/calculate-statistics/add-sparklines-to-search-results", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Calculate Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:44:06.294512+00:00", "version": "10.2"}}
{"id": "5c2e1913fbca3e2b", "content": "An anomaly is a deviation from the expected behavior of the system. An anomaly can be: A single event A sequence of events A sequence of transactions Complex patterns Examples of common use cases for anomaly detection include:", "code_examples": [], "tables": [{"headers": ["Industry", "Use case example"], "rows": [["IT", "Identifying a distributed denial of service (DDoS) attack from IP address ranges."], ["Marketing", "Rare but high-value customer purchase patterns."], ["Product", "Rare or previously unknown method of using a product that yields better results or yields results more efficiently than known methods."], ["Security", "Faster-than-human transactions. Detecting when transactions are being performed much more quickly by one user than by others. This could indicate a bot or an attempt to probe security measures."]]}], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "About anomaly detection", "section_heading": "Overview of anomaly detection", "section_id": "id_227fdcef_3160_443a_96b7_f3ddaafb321c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/about-anomaly-detection", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:44:23.850897+00:00", "version": "10.2"}}
{"id": "bf8b3b12461d255a", "content": "To perform effective anomaly detection, put all of the data in one place. If you do not have your machine and business data in the same place, you cannot perform a comprehensive analysis. Begin tracking IT and business performance metrics. Additionally, create a baseline data image which shows the current state of your system.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "About anomaly detection", "section_heading": "Effective anomaly detection", "section_id": "d38e6d15_a8bd_42df_8ff2_9cc5e18d96aa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/about-anomaly-detection", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:44:23.850906+00:00", "version": "10.2"}}
{"id": "6d9aeb6cf9096e1c", "content": "About advanced statistics Finding and removing outliers Detecting anomalies Detecting patterns", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "About anomaly detection", "section_heading": "See also", "section_id": "id_393e9e0c_8a36_4d64_8ab6_e2756934c853--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/advanced-statistics/about-anomaly-detection", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Advanced Statistics", "manual": "search-manual", "scraped_at": "2026-01-23T13:44:23.850911+00:00", "version": "10.2"}}
{"id": "51a142158999fba0", "content": "The timeline is located in the Events tab above the events listing. It shows the count of events over the time range that the search was run. Here, the timeline shows web access events over All time. Format options are located in the Format Timeline menu: You can hide the timeline, or display a Compact or Full view of the timeline. You can also toggle the timeline scale between Linear scale or Log scale (logarithmic). When Full is selected, the timeline view is taller to accommodate the labels on the axis. The count is on the Y-axis and time is on the X-axis.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Use the timeline to investigate events", "section_heading": "Change the timeline format", "section_id": "id_53a3396b_e3a8_4064_9947_8c4d69136859--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/use-the-timeline-to-investigate-events", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:44:37.917275+00:00", "version": "10.2"}}
{"id": "0c44967e763cb957", "content": "Above the timeline are the zoom options. By default, the timeline is zoomed in. The following image shows the timeline display in Full view and zoomed in. The Zoom Out option is available. Timeline legend The timeline legend is on the top right corner of the timeline. This indicates the scale of the timeline. For example, 1 hour per column indicates that each column represents a count of events during that hour. Zooming in and out changes the time scale. Zoom in To zoom in on one or more columns in the timeline, you can either click on the columns and select Zoom to Selection or you can change the time range to a smaller time range in the Time Range Picker. The smallest time unit that you can zoom in to is 1 millisecond. Zoom out When you click Zoom Out , the legend indicates that each column now represents 1 day per column instead of an hour. Zooming out changes not only the timeline but the value in the Time Range Picker. Reset the zoom To reset the zoom or to zoom in, change the value in the Time Range Picker. For example, if you searched using All time and then zoomed out, select All time in the Time Range Picker to return to the original timeline time scale. Zoom to a selection When you mouse over and select bars in the timeline, the Zoom to Selection or Deselect options above the timeline become available. Mouse over and click on one of the bars or drag your mouse over a cluster of bars in the timeline. The events list updates to display only the events that occurred in that selected time range. The time range picker also updates to the selected time range. You can cancel this selection by clicking Deselect. When you select a set of bars on the timeline and click Zoom to Selection , your search results are filtered to show only the selected time period. The timeline and events list update to show the results of your selection. The dates and times that correspond to the bars you selected, along with the number of events in that time range, is reflected in the information just below the Search bar. You cannot Deselect after you zoomed into a selected time range. But, you can Zoom Out again or change the time in the Time Range Picker.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Use the timeline to investigate events", "section_heading": "Zoom in and zoom out to investigate events", "section_id": "a624efe7_c6ee_49cc_a0e8_20597a68b31f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/use-the-timeline-to-investigate-events", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:44:37.917283+00:00", "version": "10.2"}}
{"id": "7ea7b5d44ef88506", "content": "The Splunk Search Processing Language (SPL) encompasses all the search commands and their functions, arguments and clauses. Search commands tell Splunk software what to do to the events you retrieved from the indexes. For example, you need to use a command to filter unwanted information, extract more information, evaluate new fields, calculate statistics, reorder your results, or create a chart. Some search commands have functions and arguments associated with them. Use these functions and their arguments to specify how the commands act on your results and which fields they act on. For example, you can use functions to format the data in a chart, describe what kind of statistics to calculate, and specify what fields to evaluate. Some commands also use clauses to specify how to group your search results. To get familiar with SPL, read these topics in this manual: Types of searches Types of commands Using Splunk Search For more details on SPL syntax, see Understanding SPL syntax , in the Search Reference. For information about functions, see Evaluation functions in the Search Reference Statistical and charting functions in the Search Reference", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "d9d751cb856b54e9895cb61d377362093--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-overview/about-the-search-language", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search Overview", "manual": "search-manual", "scraped_at": "2026-01-23T13:44:54.207493+00:00", "version": "10.2"}}
{"id": "f335bab485f1d698", "content": "As Splunk software processes event data, it extracts and defines fields from that data, first at index time, and again at search time. See Index time versus search time in the Managing Indexers and Clusters manual. Field extraction at index time At index time , Splunk software extracts a small set of fields. This set of fields includes default fields , custom indexed fields, and fields indexed from structured data. Default fields exist in all events. Three important default fields are host, source, and source type. They describe where the event originated. Other default fields include datetime fields, which provide additional searchable granularity to event timestamps. Splunk software also automatically adds default fields classified as internal fields. Custom indexed fields are fields that you have manually configured for index-time extraction. See Create custom fields at index time in the Getting Data In manual. Finally, when Splunk software indexes structured data, it creates index-time field extractions for the fields that it finds. Examples of structured data include: comma-separated value files (CSV) tab-separated value files (TSV) pipe-separated value files JavaScript Object Notation (JSON) data sources When searching for default field values and custom indexed field values you can use the standard <field>=<value> syntax. This syntax matches default fields, custom indexed fields, and search-time fields. However if you are searching specifically for a field that has been extracted at index-time from structured data, you can search more efficiently if you exchange the equal sign for a double colon, as follows: This syntax works best in searches for fields that were indexed from structured data. However, you can use it to search for default and custom indexed fields as well. You cannot use it to search on Search-time fields. For more information about extracting fields from structured data files, see Extract data from files with headers in the Getting Data In manual. Field extraction at search time At search time , Splunk software extracts additional fields, depending on its Search Mode setting and whether or not that setting enables field discovery given the type of search being run.", "code_examples": [{"language": "spl", "code": "<field>::<value>"}], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Use fields to retrieve events", "section_heading": "Index-time and search-time fields", "section_id": "id_71d33605_013f_49b4_b383_ad886cd93ee0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/use-fields-to-retrieve-events", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:45:11.050336+00:00", "version": "10.2"}}
{"id": "59d86573705f4a3f", "content": "Example 1: Search for events on all \"corp\" servers for accesses by the user \"strawsky\". It then reports the 20 most recent events. In this example, host is a default field, while eventtype and user are additional fields that might have been automatically extracted or that you defined. In general, an event type is a user-defined field that simplifies search by letting you categorize events. You can save a search as an event type and quickly retrieve those events using the eventtype field. For more information, read About event types in the Knowledge Manager Manual. Example 2: Search for events from the source \"/var/www/log/php_error.log\". The source of an event is the name of the file, stream, or other input from which the event originates. Example 3: Search for all events that have an Apache web access source type. The source type of an event is the format of the data input from which it originates. In this search uses a wildcard to match any Apache web access log that begins with \"access_\". This includes access_common and access_combined (and you might also see access_combined_wcookie). Example 4: Search indexed information from various CSV files to get a list of Plano-based employees. You have indexed several CSV files of employee records. Each of these CSV files share the same fields. You want to search for the employees from these files that are affiliated with the office in Plano, Texas. This example uses the <field>::<value> syntax to find the fields from those CSV files, which are extracted at index time. This syntax works best for fields extracted from indexed structured data, although it can handle other kinds of index time fields as well. It cannot find fields that are extracted at search time. Example 5: Search corp1 for events that have more than 4 lines, and omit events that contain the term 400. You can use comparison expressions to match field/value pairs. Comparison expressions with \"=\" and \"!=\" work with all field/value pairs. Comparison expressions with < > <= >= work only with fields that have numeric values. This example specifies a search for events that have more than 4 lines, linecount>4. Example 6: Searching with the Boolean \"NOT\" versus the comparison operator \"!=\" is not the same. The following search returns events where field is undefined (or NULL). The following search returns events where field exists and does not have the value \"value\". In the case where the value in question is the wildcard \"*\", NOT field=* will return events where field is null/undefined, and field!=* will never return any events. Example 7: Search for events that match a particular CIDR notation. Suppose the ip field contains these IP address values: 10.10.10.12 50.10.10.17 10.10.10.23 The following search returns the events with the first and last values: 10.10.10.12 and 10.10.10.23", "code_examples": [{"language": "spl", "code": "host=corp* eventtype=access user=strawsky"}, {"language": "spl", "code": "source=\"/var/www/log/php_error.log\""}, {"language": "spl", "code": "sourcetype=\"access_*\""}, {"language": "spl", "code": "employee_office::Plano"}, {"language": "spl", "code": "host=corp1 linecount>4 NOT 400"}, {"language": "spl", "code": "NOT field=\"value\""}, {"language": "spl", "code": "field!=\"value\""}, {"language": "spl", "code": "ip=\"10.10.10.0/24\""}], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Use fields to retrieve events", "section_heading": "Search examples", "section_id": "id_002566be_d39f_41f6_ab95_431c57be9df7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/use-fields-to-retrieve-events", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:45:11.050348+00:00", "version": "10.2"}}
{"id": "4f205f1cd01bd4b4", "content": "This topic only discussed a handful of searches with fields. You can restrict searches to specific indexes and, in distributed topologies, to specific search peers. You can see more search examples Using default fields in the Knowledge Manager Manual. Fields become more important when you start using the Splunk search language to summarize and transform your data into reports. For more information, read About reporting commands .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Use fields to retrieve events", "section_heading": "More about fields", "section_id": "id_2dcc8eef_261b_440e_a56e_28779b77b1fc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/use-fields-to-retrieve-events", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T13:45:11.050353+00:00", "version": "10.2"}}
{"id": "c9dee63e1f5c91ac", "content": "The anomalousvalue command computes an anomaly score for each field of each event, relative to the values of this field across other events. For numerical fields, it identifies or summarizes the values in the data that are anomalous either by frequency of occurrence or number of standard deviations from the mean. For fields that are determined to be anomalous, a new field is added with the following scheme. If the field is numeric, such as size , the new field will be Anomaly_Score_Num(size). If the field is non-numeric, such as name , the new field will be Anomaly_Score_Cat(name). Note: Use current Splunk machine learning (ML) tools to take advantage of the latest algorithms and get the most powerful results. See About the Splunk Machine Learning Toolkit in the Splunk Machine Learning Toolkit .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "anomalousvalue", "section_heading": "Description", "section_id": "id_2b0d227b_16bd_4842_9a63_6cdb912a1bdc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalousvalue", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:45:28.493583+00:00", "version": "10.2"}}
{"id": "fbeb494858492334", "content": "anomalousvalue <av-options>... [action] [pthresh] [field-list] Required arguments None. Optional arguments <av-options> Syntax: minsupcount=<int> | maxanofreq=<float> | minsupfreq=<float> | minnormfreq=<float> Description: Specify one or more option to control which fields are considered for discriminating anomalies. Descriptions for the av-option arguments maxanofreq Syntax: maxanofreq=<float> Description: Maximum anomalous frequency is expressed as a floating point number between 0 and 1. Omits a field from consideration if the field is too frequently anomalous. If the ratio of anomalous occurrences of the field to the total number of occurrences of the field is greater than the maxanofreq value, then the field is removed from consideration. Default 0.05 minnormfreq Syntax: minnormfreq=<float> Description: Minimum normal frequency is expressed as a floating point number between 0 and 1. Omits a field from consideration if the field is not anomalous frequently enough. If the ratio of anomalous occurrences of the field to the total number of occurrences of the field is smaller than p , then the field is removed from consideration. Default: 0.01 minsupcount Syntax: minsupcount=<int> Description: Minimum supported count must be a positive integer. Drops a field that has a small number of occurrences in the input result set. If the field appears fewer than N times in the input events, the field is removed from consideration. Default: 100 minsupfreq Syntax: minsupfreq=<float> Description: Minimum supported frequency is expressed as a floating point number between 0 and 1. Drops a field that has a low frequency of occurrence. The minsupfreq argument checks the ratio of occurrences of the field to the total number of events. If this ratio is smaller than p the field is removed from consideration. Default: 0.05 action Syntax: action=annotate | filter | summary Description: Specify whether to return the anomaly score (annotate), filter out events that are not anomalous values (filter), or return a summary of anomaly statistics (summary). Default: filter Descriptions for the action arguments annotate Syntax: action=annotate Description: The annotate action adds new fields to the events containing anomalous values. The fields that are added are Anomaly_Score_Cat(field) , Anomaly_Score_Num(field) , or both. filter Syntax: action=filter Description: The filter action returns events with anomalous values. Events without anomalous values are removed. The events that are returned are annotated, as described for action=annotate. summary Syntax: action=summary Description: The summary action returns a table summarizing the anomaly statistics for each field generated. The table includes how many events contained this field, the fraction of events that were anomalous, what type of test (categorical or numerical) were performed, and so on. field-list Syntax: <field> ... Description: The List of fields to consider. Default: If no field list is provided, all fields are considered. pthresh Syntax: pthresh=<num> Description: Probability threshold (as a decimal) that has to be met for a value to be considered anomalous. Default: 0.01.", "code_examples": [], "tables": [{"headers": ["Output field", "Description"], "rows": [["fieldname", "The name of the field."], ["count", "The number of times the field appears."], ["distinct_count", "The number of unique values of the field."], ["mean", "The calculated mean of the field values."], ["catAnoFreq%", "The anomalous frequency of the categorical field."], ["catNormFreq%", "The normal frequency of the categorical field."], ["numAnoFreq%", "The anomalous frequency of the numerical field."], ["stdev", "The standard deviation of the field value."], ["supportFreq%", "The support frequency of the field."], ["useCat", "Use categorical anomaly detection. Categorical anomaly detection looks for rare values."], ["useNum", "Use numerical anomaly detection. Numerical anomaly detection looks for values that are far from the mean value. This  anomaly detection is Gaussian distribution based."], ["isNum", "Whether or not the field is numerical."]]}], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "anomalousvalue", "section_heading": "Syntax", "section_id": "id_00adfe96_2e32_4d89_ad1b_416311718134--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalousvalue", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:45:28.493603+00:00", "version": "10.2"}}
{"id": "d52a88fa34296c7e", "content": "By default, a maximum of 50,000 results are returned. This maximum is controlled by the maxresultrows setting in the [anomalousvalue] stanza in the limits.conf file. Increasing this limit can result in more memory usage. Note: Only users with file system access, such as system administrators, can edit the configuration files. Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make the changes in the local directory. See How to edit a configuration file .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "anomalousvalue", "section_heading": "Usage", "section_id": "c6de33b6_eafb_4c0c_b22d_a725bd2e20c0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalousvalue", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:45:28.493608+00:00", "version": "10.2"}}
{"id": "ca31fa76fc6be5a8", "content": "1. Return only uncommon values from the search results This is the same as running the following search: 2. Return uncommon values from the host \"reports\"", "code_examples": [{"language": "spl", "code": "... | anomalousvalue"}, {"language": "spl", "code": "...| anomalousvalue action=filter pthresh=0.01"}, {"language": "spl", "code": "host=\"reports\"| anomalousvalue action=filter pthresh=0.02"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "anomalousvalue", "section_heading": "Basic examples", "section_id": "id_074e0569_bf5d_40ff_9848_8290f57ab8d5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalousvalue", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:45:28.493614+00:00", "version": "10.2"}}
{"id": "4a8f6db9896dfe81", "content": "1. Return a summary of the anomaly statistics for each numeric field Search for anomalous values in the earthquake data. The numeric results are returned with multiple decimals. Use the field formatting icon, which looks like a pencil, to enable number formatting and specify the decimal precision to display.", "code_examples": [{"language": "spl", "code": "source=\"all_month.csv\"| anomalousvalue action=summary pthresh=0.02 | search isNum=YES"}], "tables": [{"headers": [], "rows": [["This search uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance.  This example uses theAll Earthquakesdata from  the past 30 days."]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "anomalousvalue", "section_heading": "Extended example", "section_id": "id_85958bab_db0a_4da9_9a8f_f2bc04bf778e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalousvalue", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:45:28.493620+00:00", "version": "10.2"}}
{"id": "bbe73687bed95e94", "content": "analyzefields , anomalies , cluster , kmeans , outlier", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "anomalousvalue", "section_heading": "See also", "section_id": "id_44e529b3_7240_4c25_9873_84a97132ebfe--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalousvalue", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:45:28.493624+00:00", "version": "10.2"}}
{"id": "a3f35a47e9722d9b", "content": "Converts events into metric data points and inserts the metric data points into a metric index on the search head. A metric index must be present on the search head for mcollect to work properly, unless you are forwarding data to the indexer. Note: If you are forwarding data to the indexer, your data will be inserted on the indexer instead of the search head. You can use the mcollect command only if your role has the run_mcollect capability. See Define roles on the Splunk platform with capabilities in Securing Splunk Enterprise. CAUTION: This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "mcollect", "section_heading": "Description", "section_id": "id_06cd777a_64f9_456e_b928_236ffbb96363--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mcollect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:46:47.298900+00:00", "version": "10.2"}}
{"id": "3b3ab9d947584868", "content": "The required syntax is in bold. | mcollect index=<string> [ file=<string> ] [ split=<true | false | allnums> ] [ spool=<bool> ] [ prefix_field=<string> ] [ host=<string> ] [ source=<string> ] [ sourcetype=<string> ] [ marker=<string> ] [ <field-list> ] Required arguments index Syntax: index=<string> Description: Name of the metric index where the collected metric data is added. field-list Syntax: <field>, ... Description: A list of dimension fields. Required if split=true. Optional if split=false or split=allnums. If unspecified, which implies that split=false , mcollect treats all fields as dimensions for the data point except for the metric_name , prefix_field , and all internal fields. Default: No default value Optional arguments file Syntax: file=<string> Description: The file name where you want the collected metric data to be written. Only applicable when spool=false. You can use a timestamp or a random number for the file name by specifying either file=$timestamp$ or file=$random$. Default: $random$_metrics.csv split Syntax: split=<true | false | allnums> Description: Determines how mcollect identifies the measures in an event. See How to use the split argument. Default: false spool Syntax: spool=<bool> Description: If set to true, the metrics data file is written to the Splunk spool directory, $SPLUNK_HOME/var/spool/splunk , where the file is indexed. Once the file is indexed, it is removed. If set to false, the file is written to the $SPLUNK_HOME/var/run/splunk directory. The file remains in this directory unless further automation or administration is done. Default: true prefix_field Syntax: prefix_field=<string> Description: Only applicable when split=true. If specified, any data point with that field missing is ignored. Otherwise, the field value is prefixed to the metric name. See Set a prefix field Default: No default value host Syntax: host=<string> Description: The name of the host that you want to specify for the collected metrics data. Only applicable when spool=true. Default: No default value source Syntax: source=<string> Description: The name of the source that you want to specify for the collected metrics data. Default: If the search is scheduled, the name of the search. If the search is ad-hoc, the name of the file that is written to the var/spool/splunk directory containing the search results. sourcetype Syntax: sourcetype=<string> Description: The name of the source type that is specified for the collected metrics data. The Splunk platform does not calculate license usage for data indexed with mcollect_stash , the default source type. If you change the value of this setting to a different source type, the Splunk platform calculates license usage for any data indexed by the mcollect command. Default: mcollect_stash CAUTION: Do not change this setting without assistance from Splunk Professional Services or Splunk Support. Changing the source type requires a change to the props.conf file. marker Syntax: marker=<string> Description: A string of one or more comma-separated key/value pairs that mcollect adds as dimensions to the metric data points it generates, for the purpose of searching on those metric data points later. For example, you could add the name of the mcollect search that you are running, like this: marker=savedsearch=firewall_top_src_ip. This allows you to run searches later that isolate the metric data points created by that mcollect search, simply by adding savedsearch=firewall_top_src_ip to the search string.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "mcollect", "section_heading": "Syntax", "section_id": "id_9f941b45_77f3_4c47_b100_e0dc7c7822e1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mcollect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:46:47.298907+00:00", "version": "10.2"}}
{"id": "8520345817313465", "content": "You use the mcollect command to convert events into metric data points to be stored in a metric index on the search head. The metrics data uses a specific format for the metrics fields. See Metrics data format in Metrics. CAUTION: The mcollect command causes new data to be written to a metric index for every run of the search. Note: All metrics search commands are case sensitive. This means, for example, that mcollect treats as the following as three distinct values of metric_name : cap.gear , CAP.GEAR , and Cap.Gear. The Splunk platform cannot index metric data points that contain metric_name fields which are empty or composed entirely of white spaces. If you are upgrading to version 8.0.0 After you upgrade your search head and indexer clusters to version 8.0.x of Splunk Enterprise, edit limits.conf on each search head cluster and set the always_use_single_value_output setting under the [mcollect] stanza to false. This lets these nodes use the \"multiple measures per metric data point\" schema when you convert logs to metrics with the mcollect command or use metrics rollups. This schema increases your data storage capacity and improves metrics search performance. How to use the split argument The split argument determines how mcollect identifies the measurement fields in your search. It defaults to false. When split=false , your search needs to explicitly identify its measurement fields. If necessary it can use rename or eval conversions to do this. If you have single-metric events, your mcollect search must produce results with a metric_name field that provides the name of the measure, and a _value field that provides the measure's numeric value. If you have multiple-metric events, your mcollect search must produce results that follow this syntax: metric_name:<metric_name>=<numeric_value>. mcollect treats each of these fields as a measurement. mcollect treats the remaining fields as dimensions. When you set split=true , you use field-list to identify the dimensions in your search. mcollect converts any field that is not in the field-list into a measurement. The only exceptions are internal fields beginning with an underscore and the prefix_field , if you have set one. When you set split=allnums , mcollect treats all numeric fields as metric measures and all non-numeric fields as dimensions. You can optionally use field-list to declare that mcollect should treat certain numeric fields in the events as dimensions. Set a prefix field Use the prefix_field argument to apply a prefix to the metric fields in your event data. For example, if you have the following data: type=cpu usage=0.78 idle=0.22 You have two metric fields, usage and idle. Say you include the following in an mcollect search of that data: Because you have set split = true the Splunk software automatically converts those fields into measures, because they are not otherwise identified in a <field-list>. Then it applies the value of the specified prefix_field as a prefix to the metric field names. In this case, because you have specified the type field as the prefix field, its value, cpu , becomes the metric name prefix. The results look like this: Time If the _time field is present in the results, the Splunk software uses it as the timestamp of the metric data point. If the _time field is not present, the current time is used. field-list If field-list is not specified, mcollect treats all fields as dimensions for the metric data points it generates, except for the prefix_field and internal fields (fields with an underscore '_' prefix). If field-list is specified, the list must appear at the end of the mcollect command arguments. If field-list is specified, all fields are treated as metric values, except for the fields in field-list , the prefix-field , and internal fields. The name of each metric value is the field name prefixed with the prefix_field value. Effectively, one metric data point is returned for each qualifying field that contains a numerical value. If one search result contains multiple qualifying metric name/value pairs, the result is split into multiple metric data points.", "code_examples": [{"language": "spl", "code": "...split=trueprefix_field=type..."}], "tables": [{"headers": ["metric_name:cpu.usage", "metric_name:cpu.idle"], "rows": [["0.78", "0.22"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "mcollect", "section_heading": "Usage", "section_id": "id_5483f6da_190a_46ef_8413_d9ba5ec4d4ba--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mcollect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:46:47.298912+00:00", "version": "10.2"}}
{"id": "b06cbe7acb3af82f", "content": "The following examples show how to use the mcollect command to convert events into multiple-value metric data points. 1: Generate metric data points that break out jobs and latency metrics by user The following example specifies the metrics that should appear in the resulting metric data points, and splits them by user. Note that it does not use the split argument, so the search has to use a rename conversion to explicitly identify the measurements that will appear in the data points. Here are example results of that search: 2: Generate metric data points that break out event counts and total runtimes by user This search sets split=true so it automatically converts fields not otherwise identified as dimensions by the <field-list> into metrics. The search identifies user as a dimension. Here are example results of that search:", "code_examples": [{"language": "spl", "code": "index=\"_audit\"search_id info total_run_time \n| stats count(search_id) asjobsavg(total_run_time) as latency by user \n| renamejobsas metric_name:jobslatency as metric_name:latency \n| mcollect index=mcollect_test"}, {"language": "spl", "code": "index=\"_audit\"info=completed \n| stats max(total_run_time) as runtime max(event_count) as events by user \n| mcollect index=mcollect_test split=t user"}], "tables": [{"headers": ["_time", "user", "metric_name:jobs", "metric_name:latency"], "rows": [["1563318689", "admin", "25", "3.8105555555555575"], ["1563318689", "splunk-system-user", "129", "0.2951162790697676"]]}, {"headers": ["_time", "user", "metric_name:runtime", "metric_name:events"], "rows": [["1563318968", "admin", "0.29", "293"], ["1563318968", "splunk-system-user", "0.04", "3"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "mcollect", "section_heading": "Examples", "section_id": "id_0e50b27a_24b7_46da_baec_4dd4723f54eb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mcollect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:46:47.298918+00:00", "version": "10.2"}}
{"id": "570d8281eb8e8f32", "content": "Commands collect meventcollect", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "mcollect", "section_heading": "See also", "section_id": "f13b7931_0b2c_4ffd_9e7f_b79341f8d72d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mcollect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:46:47.298921+00:00", "version": "10.2"}}
{"id": "83127d0d7d94f07d", "content": "Displays the least common values in a field. Finds the least frequent tuple of values of all fields in the field list. If the <by-clause> is specified, this command returns rare tuples of values for each distinct tuple of values of the group-by fields. This command operates identically to the top command, except that the rare command finds the least frequent values instead of the most frequent values.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "rare", "section_heading": "Description", "section_id": "id_236a0f57_6ce4_464d_aaa5_15ad44d0c371--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rare", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:03.598163+00:00", "version": "10.2"}}
{"id": "c8be25c0c1359002", "content": "rare [<rare-options>...] <field-list> [<by-clause>] Required arguments <field-list> Syntax: <string>,... Description: Comma-delimited list of field names. Optional arguments <rare-options> Syntax: countfield=<string> | limit=<int> | percentfield=<string> | showcount=<bool> | showperc=<bool> Description: Options that specify the type and number of values to display. These are the same as the <top-options> used by the top command. <by-clause> Syntax: BY <field-list> Description: The name of one or more fields to group by. Rare options countfield Syntax: countfield=<string> Description: The name of a new field to write the value of count into. Default: \"count\" limit Syntax: limit=<int> Description: Specifies how many tuples to return. If you specify limit=0 , all values up to the maxresultrows are returned. Specifying a value larger than the maxresultrows produces an error. See Usage. Default: 10 percentfield Syntax: percentfield=<string> Description: Name of a new field to write the value of percentage. Default: \"percent\" showcount Syntax: showcount=<bool> Description: Specifies whether to add a field to your results with the count of the tuple. The name of the field is controlled by the countield argument. Default: true showperc Syntax: showperc=<bool> Description: Specifies whether to add a field to your results with the relative prevalence of that tuple. The name of the field is controlled by the percentfield argument. Default: true", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "rare", "section_heading": "Syntax", "section_id": "d801ff88_8e0e_4993_803f_36b479242a3b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rare", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:03.598260+00:00", "version": "10.2"}}
{"id": "191fa6e94461c333", "content": "The rare command is a transforming command. See Command types. Limit maximum The number of results returned by the rare command is controlled by the limit argument. The default value for the limit argument is 10. The default maximum is 50,000, which effectively keeps a ceiling on the memory that the rare command uses. You can change this limit up to the maximum value specified in the maxresultrows setting in the [rare] stanza in the limits.conf file. Splunk Cloud Platform To change the maxresultrows setting, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Enterprise To change the the maxresultrows setting in the limits.conf file, follow these steps. Prerequisites Only users with file system access, such as system administrators, can edit configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps Open or create a local limits.conf file in the desired path. For example, use the $SPLUNK_HOME/etc/apps/search/local path to apply this change only to the Search app. Under the [rare] stanza, change the value for the maxresultrows setting.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "rare", "section_heading": "Usage", "section_id": "id_52d3cea0_4149_4327_a2ab_6e80df72c64b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rare", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:03.598271+00:00", "version": "10.2"}}
{"id": "d510c6797e18bbd8", "content": "1. Return the least common values in a field Return the least common values in the url field. Limits the number of values returned to 5. 2. Return the least common values organized by host Find the least common values in the user field for each host value. By default, a maximum of 10 results are returned.", "code_examples": [{"language": "spl", "code": "... | rare urllimit=5"}, {"language": "spl", "code": "... | rare user by host"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "rare", "section_heading": "Examples", "section_id": "id_2cdfe477_8163_4cea_97a0_bc0c2f122f1c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rare", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:03.598276+00:00", "version": "10.2"}}
{"id": "77f20bd3aba31d36", "content": "Related commands top stats sirare", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "rare", "section_heading": "See also", "section_id": "id_8290691a_d38d_453e_8172_fcd76851e607--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rare", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:03.598280+00:00", "version": "10.2"}}
{"id": "b7b38c2232e69e4d", "content": "The geom command adds a field, named geom , to each result. This field contains geographic data structures for polygon geometry in JSON. These geographic data structures are used to create choropleth map visualizations. For more information about choropleth maps, see Mapping data in the Dashboards and Visualizations manual.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "geom", "section_heading": "Description", "section_id": "d7a30f0f_675f_44c3_9d63_f963eaea30ce--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geom", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:18.716025+00:00", "version": "10.2"}}
{"id": "aaaa2b075433e056", "content": "geom [<featureCollection>] [allFeatures=<boolean>] [featureIdField=<string>] [gen=<double>] [min_x=<double>] [min_y=<double>] [max_x=<double>] [max_y=<double>] Required arguments None. Optional arguments featureCollection Syntax: <geo_lookup> Description: Specifies the geographic lookup file that you want to use. Two geographic lookup files are included by default with Splunk software: geo_us_states and geo_countries. You can install your own geographic lookups from KMZ or KLM files. See Usage for more information. allFeatures Syntax: allFeatures=<bool> Description: Specifies that the output include every geometric feature in the feature collection. When a shape has no values, any aggregate fields, such as average or count , display zero when this argument is used. Additional rows are appended for each feature that is not already present in the search results when this argument is used. See Examples. Default: false featureIdField Syntax: featureIdField=<field> Description: If the event contains the featureId in a field named something other than \"featureId\", use this option to specify the field name. gen Syntax: gen=<double> Description: Specifies generalization, in the units of the data. For example, gen=0.1 generalizes, or reduces the size of, the geometry by running the Douglass Puiker Ramer algorithm on the polygons with a parameter of 0.1 degrees. Default: 0.1 min_x Syntax: min_x=<double> Description: The X coordinate for the bottom-left corner of the bounding box for the geometric shape. The range for the coordinate is -180 to 180. See Usage for more information. Default: -180 min_y Syntax: min_y=<double> Description: The Y coordinate for the bottom-left corner of the bounding box for the geometric shape. The range for the coordinate is -90 to 90. Default: -90 max_x Syntax: max_x=<double> Description: The X coordinate for the upper-right corner of the bounding box for the geometric shape. The range for the coordinate -180 to 180. Default: 180 max_y Syntax: max_y=<double> Description: The Y coordinate for the upper-right corner of the bounding box for the geometric shape. The range is -90 to 90. Default: 90", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "geom", "section_heading": "Syntax", "section_id": "id_79e42ba1_5e8d_45f4_b809_116fc3fa284b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geom", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:18.716033+00:00", "version": "10.2"}}
{"id": "3517361aed841a7c", "content": "Specifying a lookup To use your own lookup file in Splunk Enterprise, you can define the lookup in Splunk Web or edit the transforms.conf file. If you use Splunk Cloud Platform, use Splunk Web to define lookups. Define a geospatial lookup in Splunk Web To create a geospatial lookup in Splunk Web, you use the Lookups option in the Settings menu. You must add the lookup file, create a lookup definition, and can set the lookup to work automatically. See Define a geospatial lookup in Splunk Web in the Knowledge Manager Manual. Configure a geospatial lookup in transforms.conf Edit the %SPLUNK_HOME%\\etc\\system\\local\\transforms.conf file, or create a new file named transforms.conf in the %SPLUNK_HOME%\\etc\\system\\local directory, if the file does not already exist. See How to edit a configuration file in the Admin Manual. Specify the name of the lookup stanza in the transforms.conf file for the featureCollection argument. Set external_type=geo in the stanza. See Configure geospatial lookups in the Knowledge Manager Manual. Specifying no optional arguments When no arguments are specified, the geom command looks for a field named featureCollection and a field named featureIdField in the event. These fields are present in the default output from a geoindex lookup. Clipping the geometry The min_x , min_y , max_x , and max_y arguments are used to clip the geometry. Use these arguments to define a bounding box for the geometric shape. You can specify the minimum rectangle corner ( min_x , min_y ) and the maximum rectangle corner ( max_x , max_y ). By specifying the coordinates, you are returning only the data within those coordinates. Testing lookup files You can use the inputlookup command to verify that the geometric features on the map are correct. The syntax is | inputlookup <your_lookup>. For example, to verify that the geometric features in built-in geo_us_states lookup appear correctly on the choropleth map: Run the following search: On the Visualizations tab, change to a Choropleth Map. zoom in to see the geometric features. In this example, the states in the United States. Testing geometric features You can create an arbitrary result to test the geometric features. To show how the output appears with the allFeatures argument, the following search creates a simple set of fields and values. The search uses the stats command, specifying the count field. A single result is created that has a value of zero ( 0 ) in the count field. The eval command is used to add the featureId field with value of California to the result. Another eval command is used to specify the value 10000 for the count field. You now have a single result with two fields, count and featureId. When the geom command is added, two additional fields are added, featureCollection and geom. The following image shows the results of the search on the Statistics tab. The following image shows the results of the search on the Visualization tab. Make sure that the map is a Choropleth Map. This image is zoomed in to show more detail.", "code_examples": [{"language": "spl", "code": "| inputlookup geo_us_states"}, {"language": "spl", "code": "| stats count |evalfeatureId=\"California\"|evalcount=10000 | geom geo_us_states allFeatures=true"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "geom", "section_heading": "Usage", "section_id": "id_04994adf_96ec_463f_a001_c526210226fb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geom", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:18.716040+00:00", "version": "10.2"}}
{"id": "5ba5a4402e18ea0b", "content": "1. Use the default settings When no arguments are provided, the geom command looks for a field named featureCollection and a field named featureId in the event. These fields are present in the default output from a geospatial lookup. 2. Use the built-in geospatial lookup This example uses the built-in geo_us_states lookup file for the featureCollection. 3. Specify a field that contains the featureId This example uses the built-in geo_us_states lookup and specifies state as the featureIdField. In most geospatial lookup files, the feature IDs are stored in a field called featureId. Use the featureIdField argument when the event contains the feature IDs in a field named something other than \"featureId\". 4. Show all geometric features in the output The following example specifies that the output include every geometric feature in the feature collection. If no value is present for a geometric feature, zero is the default value. Using the allFeatures argument causes the choropleth map visualization to render all of the shapes. 5. Use the built-in countries lookup The following example uses the built-in geo_countries lookup. This search uses the lookup command to specify shorter field names for the latitude and longitude fields. The stats command is used to count the feature IDs and renames the featureIdField field as country. The geom command generates the information for the chloropleth map using the renamed field country. 6. Specify the bounding box for the geometric shape This example uses the geom command attributes that enable you to clip the geometry by specifying a bounding box.", "code_examples": [{"language": "spl", "code": "...| geom"}, {"language": "spl", "code": "...| geom geo_us_states"}, {"language": "spl", "code": "...| geom geo_us_states featureIdField=\"state\""}, {"language": "spl", "code": "...| geom geo_us_states allFeatures=true"}, {"language": "spl", "code": "... | lookup geo_countries latitude AS lat, longitude AS long | stats count BY featureIdField AS country | geom geo_countries featureIdField=\"country\""}, {"language": "spl", "code": "... | geom geo_us_states featureIdField=\"state\"gen=0.1 min_x=-130.5 min_y=37.6 max_x=-130.1 max_y=37.7"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "geom", "section_heading": "Examples", "section_id": "id_9d508da5_34e2_468c_bc69_408b826442bb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geom", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:18.716045+00:00", "version": "10.2"}}
{"id": "7eda9bef0cef77fc", "content": "Mapping data in the Dashboards and Visualizations manual.", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "geom", "section_heading": "See also", "section_id": "a460a8ee_49ed_4057_b3d4_0c59af2d1d1f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geom", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:18.716049+00:00", "version": "10.2"}}
{"id": "07bde937f4053b4f", "content": "Converts results from a tabular format to a format similar to stats output. This command is the inverse of the xyseries command. Syntax untable <x-field> <y-name-field> <y-data-field> Required arguments <x-field> Syntax: <field> Description: The field to use for the x-axis labels or row names. This is the first field in the output. <y-name-field> Syntax: <field> Description: A name for the field to contain the labels for the data series. All of the field names, other than <x-field>, are used as the values for the <y-name-field> field. You can specify any name for this field. <y-data-field> Syntax: <field> Description: A name for the field to contain the data to chart. All of the values from the fields, other than <x-field>, are used as the values for the <y-data-field> field. You can specify any name for this field.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "untable", "section_heading": "Description", "section_id": "id_18af3b58_0476_4a47_8f55_4588bd74e513--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/untable", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:35.817410+00:00", "version": "10.2"}}
{"id": "8d764380bd9ae853", "content": "The untable command is a distributable streaming command. See Command types. Results with duplicate field values When you untable a set of results and then use the xyseries command to combine the results, results that contain duplicate values are removed. You can use the streamstats command create unique record numbers and use those numbers to retain all results. See Extended examples .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "untable", "section_heading": "Usage", "section_id": "id_44dcc891_66bc_4a12_b10e_fab1d9ecc3c6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/untable", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:35.817418+00:00", "version": "10.2"}}
{"id": "d944cad24f2a0f51", "content": "To show how to use the untable command, we need results that appear in a table format. Run this search. The results appear on the Statistics tab and look something like this: The top command automatically adds the count and percent fields to the results. For each categoryId, there are two values, the count and the percent. When you untable these results, there will be three columns in the output: The first column lists the category IDs The second column lists the type of calculation: count or percent The third column lists the values for each calculation When you use the untable command to convert the tabular results, you must specify the categoryId field first. You can use any field name you want for the type of calculation and the values. For example: The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | top categoryId"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | top categoryId | untable categoryId calculation value"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["categoryId", "count", "percent"], "rows": [["STRATEGY", "806", "30.495649"], ["ARCADE", "493", "18.653046"], ["TEE", "367", "13.885736"], ["ACCESSORIES", "348", "13.166856"], ["SIMULATION", "246", "9.307605"], ["SHOOTER", "245", "9.269769"], ["SPORTS", "138", "5.221339"]]}, {"headers": ["categoryId", "calculation", "value", ""], "rows": [["STRATEGY", "count", "806"], ["STRATEGY", "percent", "30.495649"], ["ARCADE", "count", "493"], ["ARCADE", "percent", "18.653046"], ["TEE", "count", "367"], ["TEE", "percent", "13.885736"], ["ACCESSORIES", "count", "348"], ["ACCESSORIES", "percent", "13.166856"], ["SIMULATION", "count", "246"], ["SIMULATION", "percent", "9.307605"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "untable", "section_heading": "Basic example", "section_id": "abdba8c8_194b_4daa_ac21_2bdce44e4bbe--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/untable", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:35.817440+00:00", "version": "10.2"}}
{"id": "88a37091c5054381", "content": "The untable command does exactly what the name says, it converts tabular information into individual rows of results. Suppose you have this search: The search produces these results: Notice that this set of events has duplicate values in the _time field for date_time4. We will come back to that in a moment. Use the untable command to remove the tabular format. ...| untable _time FieldName FieldValue Here are the results from the untable command: Events with duplicate timestamps Remember that the original set of events in this example had duplicates for date_time4. If you want to process the events in some way and then put the events back together, you can avoid eliminating the duplicate events by using the streamstats command. Use the streamstats command to give each event a unique record number and use that unique number as the key field for the untable and xyseries commands. For example, you can add the streamstats command to your original search. The search produces these results: You can then add the untable command to your search, using recno as the <x-field>: The search produces these results: These events can be put back together by using the xyseries command, again using the recno field as the <x-field>. For example: The search produces these results: Restoring the timestamps In addition to using the streamstats command to generate a record number, you can use the rename command to restore the timestamp information after the xyseries command. For example: (Thanks to Splunk users DalJeanis and BigCosta for their help with this example.)", "code_examples": [{"language": "spl", "code": "...| table _time EventCode Message"}, {"language": "spl", "code": "...| table _time EventCode Message | streamstats count as recno"}, {"language": "spl", "code": "...| table _time EventCode Message | streamstats count as recno | untable recno FieldName FieldValue"}, {"language": "spl", "code": "...| xyseries recno FieldName FieldValue"}, {"language": "spl", "code": "...| table _time EventCode Message \n| streamstats count as recno \n| rename _time as time\n| untable recno FieldName FieldValue\n| xyseries recno FieldName FieldValue\n| rename time as _time"}], "tables": [{"headers": ["_time", "EventCode", "Message"], "rows": [["date-time1", "4136", "Too late now"], ["date_time2", "1234", "I dont know"], ["date_time3", "3456", "Too busy, ask again later"], ["date_time4", "1256", "Everything is happening at once"], ["date_time4", "1257", "And right now, as well"]]}, {"headers": ["_time", "FieldName", "FieldValue"], "rows": [["date-time1", "EventCode", "4136"], ["date-time1", "Message", "Too late now"], ["date_time2", "EventCode", "1234"], ["date-time2", "Message", "I dont know"], ["date_time3", "EventCode", "3456"], ["date-time3", "Message", "Too busy, ask again later"], ["date_time4", "EventCode", "1256"], ["date-time4", "Message", "Everything is happening at once"], ["date_time4", "EventCode", "1257"], ["date-time4", "Message", "And right now, as well"]]}, {"headers": ["_time", "EventCode", "Message", "recno"], "rows": [["date-time1", "4136", "Too late now", "1"], ["date_time2", "1234", "I dont know", "2"], ["date_time3", "3456", "Too busy, ask again later", "3"], ["date_time4", "1256", "Everything is happening at once", "4"], ["date_time4", "1257", "And right now, as well", "5"]]}, {"headers": ["recno", "FieldName", "FieldValue"], "rows": [["1", "EventCode", "4136"], ["1", "Message", "Too late now"], ["2", "EventCode", "1234"], ["2", "Message", "I dont know"], ["3", "EventCode", "3456"], ["3", "Message", "Too busy, ask again later"], ["4", "EventCode", "1256"], ["4", "Message", "Everything is happening at once"], ["4", "EventCode", "1257"], ["4", "Message", "And right now, as well"]]}, {"headers": ["recno", "EventCode", "Message"], "rows": [["1", "4136", "Too late now"], ["2", "1234", "I dont know"], ["3", "3456", "Too busy, ask again later"], ["4", "1256", "Everything is happening at once"], ["5", "1257", "And right now, as well"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "untable", "section_heading": "Extended example", "section_id": "e9b983f3_da0a_436b_9c67_7444d21a3778--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/untable", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:35.817465+00:00", "version": "10.2"}}
{"id": "9e73fa7a96e4604c", "content": "xyseries", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "untable", "section_heading": "See also", "section_id": "id_272e0e04_3e56_467b_b2de_24f3124aaeb7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/untable", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:35.817469+00:00", "version": "10.2"}}
{"id": "fd6d1a3d44c548c1", "content": "Takes a group of events that are identical except for the specified field, which contains a single value, and combines those events into a single event. The specified field becomes a multivalue field that contains all of the single values from the combined events. Note: The mvcombine command does not apply to internal fields. See Use default fields in the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "mvcombine", "section_heading": "Description", "section_id": "c7eb0270_a567_45e2_8902_a6dcbdfb7984--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mvcombine", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:53.541143+00:00", "version": "10.2"}}
{"id": "2057e7bd83e26bcf", "content": "mvcombine [delim=<string>] <field> Required arguments field Syntax: <field> Description: The name of a field to merge on, generating a multivalue field. Optional arguments delim Syntax: delim=<string> Description: Defines the string to use as the delimiter for the values that get combined into the multivalue field. For example, if the values of your field are \"1\", \"2\", and \"3\", and delim is a semi-colon ( ; ), then the combined multivalue field is \"1\";\"2\";\"3\". Default : a single space, (\" \") Note: To see the output of the delim argument, you must use the nomv command immediately after the mvcombine command. See Usage", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "mvcombine", "section_heading": "Syntax", "section_id": "id_260a9579_ea0e_46f4_95cb_3cdcb0830bbb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mvcombine", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:53.541151+00:00", "version": "10.2"}}
{"id": "c9320115c7d7715f", "content": "The mvcombine command is a transforming command. See Command types. You can use evaluation functions and statistical functions on multivalue fields or to return multivalue fields. The mvcombine command accepts a set of input results and finds groups of results where all field values are identical, except the specified field. All of these results are merged into a single result, where the specified field is now a multivalue field. Because raw events have many fields that vary, this command is most useful after you reduce the set of available fields by using the fields command. The command is also useful for manipulating the results of certain transforming commands, like stats or timechart. Specifying delimiters The mvcombine command creates a multivalue version of the field you specify, as well as a single value version of the field. The multivalue version is displayed by default. The single value version of the field is a flat string that is separated by a space or by the delimiter that you specify with the delim argument. By default the multivalue version of the field is displayed in the results. To display the single value version with the delimiters, add the | nomv command to the end of your search. For example ...| mvcombine delim= \",\" host | nomv host. Some modes of search result investigation prefer this single value representation, such as exporting to CSV in the UI, or running a command line search with splunk search \"...\" -output csv. Some commands that are not multivalue aware might use this single value as well. Most ways of accessing the search results prefer the multivalue representation, such as viewing the results in the UI, or exporting to JSON, requesting JSON from the command line search with splunk search \"...\" -output json or requesting JSON or XML from the REST API. For these forms of, the selected delim has no effect. Other ways of turning multivalue fields into single-value fields If your primary goal is to convert a multivalue field into a single-value field, mvcombine is probably not your best option. mvcombine is mainly meant for the creation of new multivalue fields. Instead, try either the nomv command or the mvjoin eval function.", "code_examples": [], "tables": [{"headers": ["Conversion option", "Description", "For more information"], "rows": [["nomvcommand", "Use for simple multivalue field to single-value field conversions. Provide the name of a multivalue field in your search results andnomvwill convert each instance of the field into a single-value field.", "nomv"], ["mvjoinevalfunction", "Use when you want to perform multivalue field to single-value field conversion where the former multivalues are separated by a delimiter that you supply. For example, you start with a multivalue field that contains the values1,2,3,4,5. You can usemvjointo transform your multivalue field into a single-valued field withORas the delimiter. The new single value of the field is1 OR 2 OR 3 OR 4 OR 5.", "Multivalue eval functions"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "mvcombine", "section_heading": "Usage", "section_id": "id_63e32f51_26e5_4829_9e2c_70f7ce721a80--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mvcombine", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:53.541158+00:00", "version": "10.2"}}
{"id": "467f60f1c2fb0fcf", "content": "1. Creating a multivalue field To understand how mvcombine works, let's explore the data. Set the time range to All time. Run the following search. The results show that the max and min fields have duplicate entries for the hosts that start with www. The other hosts show no results for the max and min fields. To remove the other hosts from your results, modify the search to add host=www* to the search criteria. Because the values in the max and min columns contain the exact same values, you can use the mvcombine to combine the host values into a multivalue result. Add | mvcombine host to your search and run the search again. Instead of three rows, one row is returned. The host field is now a multvalue field. 2. Returning the delimited values As mentioned in the Usage section, by default the delimited version of the results are not returned in the output. To return the results with the delimiters, you must return the single value string version of the field. Add the nomv command to your search. For example: The search results that are returned are shown in the following table. To return the results with a space after each comma, specify delim=\", \". Example 3: In multivalue events: Example 4: Combine the values of \"foo\" with a colon delimiter.", "code_examples": [{"language": "spl", "code": "index=* | stats max(bytes) AS max, min(bytes) AS min BY host"}, {"language": "spl", "code": "index=* host=www* | stats max(bytes) AS max, min(bytes) AS min BY host"}, {"language": "spl", "code": "index=* host=www* | stats max(bytes) AS max, min(bytes) AS min BY host | mvcombine host"}, {"language": "spl", "code": "index=* host=www* | stats max(bytes) AS max, min(bytes) AS min BY host | mvcombine delim=\",\"host | nomv host"}, {"language": "spl", "code": "sourcetype=\"WMI:WinEventLog:Security\"| fields EventCode, Category,RecordNumber | mvcombine delim=\",\"RecordNumber | nomv RecordNumber"}, {"language": "spl", "code": "... | mvcombine delim=\":\"foo"}], "tables": [{"headers": [], "rows": [["This example uses the sample dataset fromthe Search Tutorial. To try this example yourself, download the data set fromGet the tutorial data into Splunkand follow the instructions in the Search Tutorial to upload the data."]]}, {"headers": ["host", "max", "min"], "rows": [["www1,www2,www3", "4000", "200"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "mvcombine", "section_heading": "Examples", "section_id": "cf4fcfdf_2b4b_4cc6_8e67_18fd11ac0078--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mvcombine", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:53.541164+00:00", "version": "10.2"}}
{"id": "764143abc36a51e4", "content": "Commands: makemv mvexpand nomv Functions: Multivalue eval functions Multivalue stats and chart functions split", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "mvcombine", "section_heading": "See also", "section_id": "id_9bd5e659_d74d_4e8c_a0a4_2ea14fc99be6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mvcombine", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:47:53.541174+00:00", "version": "10.2"}}
{"id": "8c20495796735892", "content": "Returns autosuggest information for a specified prefix that is used to autocomplete word candidates in searches. The maximum number of results returned is based on the value you specify for the count argument.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "typeahead", "section_heading": "Description", "section_id": "d2f25908_9d66_406f_b765_acdbe000e492--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typeahead", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:08.034318+00:00", "version": "10.2"}}
{"id": "93a379f4f62c4c74", "content": "The required syntax is in bold. | typeahead prefix=<string> count=<int> [collapse=<bool>] [endtimeu=<int>] [index=<string>] [max_servers=<int>] [max_time=<int>] [starttimeu=<int>] [use_cache=<bool>] Required arguments prefix Syntax: prefix=<string> Description: The full search string to return typeahead information. count Syntax: count=<int> Description: The maximum number of results to return. Optional arguments banned_segments Syntax: banned_segments=<semicolon-separated-list> Description: Specifies a semicolon-separated list of segments. The typeahead search processor filters events with these segments out of the results it returns. A best practice is to bracket each listed segment with wildcard asterisks ('*'). For example, if you set banned_segments = *password*;*SSN* , Splunk software filters any event that contains the string password or SSN from the search results. Default : no default collapse Syntax: collapse=<bool> Description: Specify whether to collapse a term that is a prefix of another term when the event count is the same. Default : true endtimeu Syntax: endtimeu=<int> Description: Set the end time to N seconds, measured in UNIX time. Default : now index-specifier Syntax: index=<string> Description: Search the specified index instead of the default index. max_servers Syntax: max_servers=<int> Description: Specify the maximum number of indexers or remote search peers to be used in addition to the search head for typeahead searches. If the max_servers argument is not specified, the default value is 2 , which means Splunk software uses two search peers in addition to any search heads. Default : 2 max_time Syntax: max_time=<int> Description: The maximum time in seconds that the typeahead can run. If max_time=0 , there is no limit. Default : 1 second startimeu Syntax: starttimeu=<int> Description: Set the start time to N seconds, measured in UNIX time. Default : 0 use_cache Syntax: use_cache = <boolean> Description: Specifies whether the typeahead cache will be used if use_cache is not specified in the command line or endpoint. When use_cache is turned on, Splunk software uses cached search results when running typeahead searches, which may have outdated results for a few minutes after you make changes to .conf files. For more information, see Typeahead and .conf file updates. Default : true or 1", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "typeahead", "section_heading": "Syntax", "section_id": "id_9f68df2b_0f51_4f94_871d_f244ea9c435f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typeahead", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:08.034327+00:00", "version": "10.2"}}
{"id": "a26fbfa263b586ab", "content": "The typeahead command is a generating command and should be the first command in the search. Generating commands use a leading pipe character. The typeahead command can be targeted to an index and restricted by time. When you run the typeahead command, Splunk software runs internal typeahead searches and extracts data from indexes, configurations, and search histories. This information is used to autocomplete word candidates when users type commands in the Search bar in Splunk Web. The typeahead command extracts data from these sources: Terms or tokens from the index lexicon. Settings in configuration files, such as props.conf and savedsearches.conf. The search history from previous searches in Splunk Web. Protect sensitive information in typeahead searches If you have sensitive information, such as Personal Identifiable Information (PII) and Protected Health Information (PHI) data that you don't want to be visible to users when they run typeahead searches, you can use the banned_segments argument to prevent sensitive data from displaying in typeahead searches. For example, to make sure that password or social security information is not visible to users, you can add a new line for the banned_segments setting to the typeahead stanza in the limits.conf file like this: Then, when your users run typeahead searches, any fields containing password , SSN , or ssn are filtered from the search results. Splunk Cloud Platform To add a banned_segments string that you want filtered out of typeahead searches, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Enterprise To add a string to the banned_segments argument in the limits.conf file, follow these steps. Prerequisites Only users with file system access, such as system administrators, can edit configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps Open or create a local limits.conf file at $SPLUNK_HOME/etc/system/local. In the typeahead stanza, set banned_segments to the string that you want filtered out as the prefix in typeahead searches. The impact of typeahead on search results The typeahead command returns the most common terms found in indexed data with the given prefix. If you use the typeahead command with the default settings, the command may not return all search results or the correct search results in the following cases: The time to complete the search takes longer than the value specified by the max_time argument, which is 1 second, by default. Data is indexed on a server that is not randomly chosen, resulting in the exclusion of its data from the search results. This can happen when the value of max_server is less than the number of indexers, for example, if max_server is set to the default, which is 2 , but there are actually 3 indexers. In addition, the typeahead command may not return all of the search results if the count argument is set lower than the actual number of results. For example, if the count argument is set to 10 , the typeahead command returns only the top 10 results, even though more results could actually be returned. Set the number of additional search peers used in a typeahead job The max_servers argument is designed to minimize the workload impact of running typeahead search jobs in an indexer clustering environment. For load balancing, the selection of additional search peers for typeahead is random. A setting of 0 removes all limits, causing all available search peers to be used for typeahead search jobs. The default for the max_servers argument is controlled by the max_servers setting in limits.conf. Typeahead and .conf file updates The typeahead command uses a cache to run fast searches at the expense of accurate results. As a result, sometimes what is in the cache and shows up in typeahead search results may not reflect recent changes to .conf files. This is because it takes 5 or 10 minutes for the cached data to clear, depending on the performance of the server. For example, if you rename a sourcetype in the props.conf file, it may take a few minutes for that change to display in typeahead search results. A typeahead search that is run while the cache is being cleared returns the cached data, which is expected behavior. If you make a change to a .conf file, you can wait a few minutes for the cache to clear to get the most accurate and up-to-date results from your typeahead search. Alternatively, you can turn off the use_cache argument to clear the cache immediately, which fetches more accurate results, but is a little slower. After you manually clear the cache, you should see the changes to your .conf file reflected in your results when you rerun the typeahead search. For more information, see Rename source types in the Splunk Cloud Platform Getting Data In manual. Typeahead and tsidx bucket reduction typeahead searches over indexes that have undergone tsidx bucket reduction will return incorrect results. For more information see Reduce tsidx disk usage in Managing indexers and clusters of indexers .", "code_examples": [{"language": "spl", "code": "[typeahead]\nbanned_segments = *password*;*SSN*;*ssn*"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "typeahead", "section_heading": "Usage", "section_id": "id_25836985_1387_4496_a051_8e7b9440ad0d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typeahead", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:08.034334+00:00", "version": "10.2"}}
{"id": "2f2752330e5be4ec", "content": "Example 1: Return typeahead information for source When you run a typeahead search, Splunk software extracts information about field definitions from indexes, configurations, and search histories, and displays the relevant information for the specified prefix. For example, say you run the following search for the source prefix against the main index: The typeahead command searches the index and shows you what is visible to your users as autocomplete suggestions when they start to type source in their searches in Splunk Web. The results look something like this: Example 2: Return typeahead information for saved searches You can also run typeahead on saved searches. For example, say you run this search: The results look something like this, which tells you what your users see as autocomplete suggestions when they start to type savedsearch in the Search bar in Splunk Web. Example 3: Return typeahead information for sourcetypes in the _internal index When you run the following typeahead search, Splunk software returns typeahead information for sourcetypes in the _internal index. The results look something like this.", "code_examples": [{"language": "spl", "code": "| typeahead index=main prefix=\"source\"count=3"}, {"language": "spl", "code": "|typeahead prefix=\"savedsearch=\"count=3"}, {"language": "spl", "code": "| typeahead prefix=sourcetype count=5 index=_internal"}], "tables": [{"headers": ["content", "count", "operator"], "rows": [["source=\"access_30DAY.log\"", "131645", "false"], ["source=\"data.csv\"", "4", "false"], ["source=\"db_audit_30DAY.csv\"", "44096", "false"]]}, {"headers": ["content", "count", "operator"], "rows": [["savedsearch=\"403_by_clientip\"", "26", "true"], ["savedsearch=\"Errors in the last 24 hours\"", "5", "true"], ["savedsearch=\"Errors in the last hour\"", "2", "true"]]}, {"headers": ["content", "count", "operator"], "rows": [["sourcetype", "373993", "false"], ["sourcetype=\"mongod\"", "711", "false"], ["sourcetype=\"scheduler\"", "2508", "false"], ["sourcetype=\"splunk_btool\"", "3", "false"], ["sourcetype=\"splunk_intro_disk_objects\"", "5", "false"]]}], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "typeahead", "section_heading": "Examples", "section_id": "f1a101f1_9ac8_4c03_9cca_5e01eaf8015e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typeahead", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:08.034346+00:00", "version": "10.2"}}
{"id": "17598c84c00f7236", "content": "Generates suggested event types by taking previous search results and producing a list of potential searches that can be used as event types. By default, the typelearner command initially groups events by the value of the grouping-field. The search then unifies and merges these groups based on the keywords they contain.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "typelearner", "section_heading": "Description", "section_id": "a244b7f8_df08_45f4_b30c_73d22a0546ee--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typelearner", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:24.797221+00:00", "version": "10.2"}}
{"id": "1d046d0f017f448a", "content": "typelearner [<grouping-field>] [<grouping-maxlen>] Optional arguments grouping-field Syntax: <field> Description: The field with values for the typelearner comman to use when initially grouping events. Default: punct , the punctuation seen in _raw grouping-maxlen Syntax: maxlen=<int> Description: Determines how many characters in the grouping-field value to look at. If set to negative, the entire value of the grouping-field value is used to group events. Default: 15", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "typelearner", "section_heading": "Syntax", "section_id": "c4908c9f_32cc_4c0d_bfd9_f727bdfd1a6c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typelearner", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:24.797229+00:00", "version": "10.2"}}
{"id": "ebafe51f460d943c", "content": "Example 1: Have the search automatically discover and apply event types to search results.", "code_examples": [{"language": "spl", "code": "... | typelearner"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "typelearner", "section_heading": "Examples", "section_id": "id_86809ad7_0568_41bf_a3e2_2c95edb404af--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typelearner", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:24.797234+00:00", "version": "10.2"}}
{"id": "c0673f0813348038", "content": "typer", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "typelearner", "section_heading": "See also", "section_id": "e1eccce9_37aa_4725_bb9e_77747e542a94--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typelearner", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:24.797238+00:00", "version": "10.2"}}
{"id": "2f6bacec4d13f5e4", "content": "The iplocation command extracts location information from IP addresses by using 3rd-party databases. This command supports IPv4 and IPv6 addresses and subnets that use CIDR notation. The IP address that you specify in the ip-address-fieldname argument, is looked up in a database. Fields from that database that contain location information are added to each event. The setting of the allfields argument determines which fields are added to the events. Because all the information might not be available for each IP address, an event can have empty field values. For IP addresses which do not have a location, such as internal addresses, no fields are added.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "iplocation", "section_heading": "Description", "section_id": "id_605e3b07_2638_4c1a_a29d_50d8894c8afe--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/iplocation", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:40.449503+00:00", "version": "10.2"}}
{"id": "46fa7e7f98fb5b0c", "content": "The required syntax is in bold. iplocation [prefix=<string>] [allfields=<bool>] [lang=<string>] <ip-address-fieldname> Required arguments ip-address-fieldname Syntax: <field> Description: Specify an IP address field, such as clientip. Optional arguments allfields Syntax: allfields=<bool> Description: Specifies whether to add all of the fields from the database to the search results. If set to true , this argument adds the fields City , Continent , Country , MetroCode , Region , Timezone , _time , lat (latitude), and lon (longitude). Default: false. Only the City , Country , Region , _time , lat , and lon fields are added to the search results. lang Syntax: lang=<string> Description: Renders the resulting strings in different languages. For example, use lang=es for Spanish. The set of languages depends on the geoip database that is used. To specify more than one language, separate them with a comma. This also indicates the priority in descending order. Specify lang=code to return the fields as two letter ISO abbreviations. prefix Syntax: prefix=<string> Description: Specify a string to prefix the field name. With this argument you can add a prefix to the added field names to avoid name collisions with existing fields. For example, if you specify prefix=iploc_ the field names that are added to the events become iploc_City , iploc_County , iploc_lat , and so forth. Default : NULL/empty string", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "iplocation", "section_heading": "Syntax", "section_id": "id_1fb6871c_dd91_4d42_bf52_31c5435c8540--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/iplocation", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:40.449513+00:00", "version": "10.2"}}
{"id": "065cee5b3f6354ba", "content": "The iplocation command is a distributable streaming command. See Command types. The Splunk software ships with a copy of the dbip-city-lite.mmdb IP geolocation database file. This file is located in the $SPLUNK_HOME/share/ directory. Updating the IP geolocation database file Through Splunk Web, you can update the .mmdb file that ships with the Splunk software. The file you update it with can be a copy of one of the following two files. Only those two files are supported. To use these two files, you must have a license for the GeoIP2 City database. Note: Replacing your mmdb file with one of these two files reintroduces the Timezone field that is absent in the default .mmdb file, but does not reintroduce the MetroCode field. Prerequisites You must have a role with the upload_mmdb_files capability. Steps Go online and find a download page for the binary .tar.gz versions of the GeoLite2-City or the GeoIP2-City database files. Download the binary .tar.gz version of the file (GeoLite2-City or GeoIP2-City) that is most appropriate for your needs. Expand the binary .tar.gz version of the file. The .tar.gz file expands into a folder which contains the GeoLite2-City.mmdb file, or the GeoIP2-City.mmdb file, depending on the download you selected. In Splunk Web, go to Settings > Lookups > GeoIP lookups file. On the GeoIP lookups file page, click Choose file. Select the .mmdb file. Click Save. The page displays a success message when the upload completes. An .mmdb file that you upload through this method is treated as a lookup table by the Splunk software. This means it is picked up by knowledge bundle replication in distributed search environments, but that also means it can increase the size of knowledge bundles. See Knowledge bundle replication overview in the Distributed Search manual. Note: If you upload your own .mmdb file in Splunk Web and later decide you want to revert back to the .mmdb file that was shipped with the Splunk software, go to Settings > Lookups > GeoIP lookups file and delete your uploaded file. Impact of upgrading Splunk software When you upgrade your Splunk platform, the .mmdb file in the $SPLUNK_HOME/share/ directory is replaced by the version of the file that ships with the Splunk software. You can avoid this by storing the .mmdb file in a different file path. Storing the .mmdb file in a different file path To store the GeoLite2-City.mmdb or GeoIP2-City.mmdb file in a different file path you must update the path directly in the limits.conf file. This is not possible in Splunk Cloud Platform, only Splunk Enterprise. Note: The Splunk Web .mmdb file upload feature takes precedence over manual updates to the .mmdb file path in limits.conf. Prerequisites Only users with file system access, such as system administrators, can specify a different file path to the .mmdb file in the limits.conf file. Review the steps in How to edit a configuration file in the Admin Manual. You can have configuration files with the same name in your default, local, and app directories. Read Where you can place (or find) your modified configuration files in the Admin Manual. Note: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make the changes in the local directory. Steps Open the local limits.conf file for the Search app. For example, $SPLUNK_HOME/etc/apps/search/local. Add the [iplocation] stanza. Add the db_path setting and specify the absolute path to the .mmdb file. The db_path setting does not support standard Splunk environment variables such as $SPLUNK_HOME. For example: db_path = /Applications/Splunk/mmdb/GeoLite2-City.mmdb specifies a new directory called mmdb. Ensure a copy of the .mmdb file is stored in the ../Applications/Splunk/mmdb/ directory. Because you are editing the path to the .mmdb file, you should restart the Splunk server. Storing the .mmdb file with a different name Alternatively, you can add the updated .mmdb to the $SPLUNK_HOME/share/ directory using a different name and then specify that name in the db_path setting. For example: db_path = /Applications/Splunk/share/GeoLite2-City_paid.mmdb. The .mmdb file and distributed deployments The iplocation command is a distributable streaming command , which means that it can be processed on the indexers. The $SPLUNK_HOME/share/ directory is not part of the knowledge bundle. If you update the .mmdb file in the $SPLUNK_HOME/share/ directory, the updated file is not automatically sent to the indexers in a distributed deployment. To add the .mmdb file to the indexers, use the tools that you typically use to push files to the indexers.", "code_examples": [], "tables": [{"headers": ["File name", "Description"], "rows": [["GeoLite2-City.mmdb", "This is a free IP geolocation database that is updated on its download page on a weekly basis."], ["GeoIP2-City.mmdb", "This is a paid version of the GeoLite2-City IP geolocation database that is more accurate than the free version."]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "iplocation", "section_heading": "Usage", "section_id": "f4edfa7f_9190_428d_bf73_97010812e224--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/iplocation", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:40.449520+00:00", "version": "10.2"}}
{"id": "d91866afa66a2c2d", "content": "1. Add location information to web access events Add location information to web access events. By default, the iplocation command adds the City , Country , lat , lon , and Region fields to the results. 2. Search for client errors and return the first 20 results Search for client errors in web access events, returning only the first 20 results. Add location information and return a table with the IP address, City, and Country for each client error. The results appear on the Statistics tab and look something like this: 3. Add a prefix to the fields added by the iplocation command Prefix the fields added by the iplocation command with iploc_. Add all of the fields in the .mmdb database file to the results. 4. Generate a choropleth map using IP addresses Generate a choropleth map of your data like the one below using the iplocation command. See Use IP addresses to generate a choropleth map in Dashboards and Visualizations. 5. Identify IPv6 address locations The iplocation command supports IPv6 lookup through IP geolocation functionality. In the following example, iplocation looks up the specified IP address in the default geolocation database file to determine where it is located. Search finds the location of the IP address and displays the following results.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | iplocation clientip"}, {"language": "spl", "code": "sourcetype=access_* status>=400 | head 20 | iplocation clientip | table clientip, status, City, Country"}, {"language": "spl", "code": "sourcetype = access_* | iplocation prefix=iploc_ allfields=trueclientip | fields iploc_*"}, {"language": "spl", "code": "| makeresults \n|evalmyip=\"2001:4860:4860::8888\"| iplocation myip"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["clientip", "status", "City", "Country"], "rows": [["182.236.164.11", "408", "Zhengzhou", "China"], ["198.35.1.75", "500", "Princeton", "United States"], ["198.35.1.75", "404", "Princeton", "United States"], ["198.35.1.75", "406", "Princeton", "United States"], ["198.35.1.75", "500", "Princeton", "United States"], ["221.204.246.72", "503", "Taiyuan", "China"], ["1.192.86.205", "503", "Amesbury", "United States"], ["91.205.189.15", "406", "", ""], ["216.221.226.11", "505", "Redwood City", "United States"], ["216.221.226.11", "404", "Redwood City", "United States"], ["195.2.240.99", "400", "", "Russia"]]}, {"headers": ["City", "Country", "Region", "_time", "lat", "lon", "myip"], "rows": [["", "United States", "", "2021-11-22 13:37:07", "37.75100", "-97.82200", "2001:4860:4860::8888"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "iplocation", "section_heading": "Examples", "section_id": "id_31ea0329_f112_43b7_8959_8255919b0938--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/iplocation", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:40.449535+00:00", "version": "10.2"}}
{"id": "58e6bcbe1905ac15", "content": "Commands lookup search Functions cidrmatch", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "iplocation", "section_heading": "See also", "section_id": "id_591ebf63_e2ee_4f56_87a2_d44ed739efbf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/iplocation", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:40.449539+00:00", "version": "10.2"}}
{"id": "24b8ae3a0a75dd5c", "content": "The cluster command groups events together based on how similar they are to each other. Unless you specify a different field, cluster groups events based on the contents of the _raw field. The default grouping method is to break down the events into terms ( match=termlist ) and compute the vector between events. Set a higher threshold value for t , if you want the command to be more discriminating about which events are grouped together. The result of the cluster command appends two new fields to each event. You can specify what to name these fields with the countfield and labelfield parameters, which default to cluster_count and cluster_label. The cluster_count value is the number of events that are part of the cluster, or the cluster size. Each event in the cluster is assigned the cluster_label value of the cluster it belongs to. For example, if the search returns 10 clusters, then the clusters are labeled from 1 to 10.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "cluster", "section_heading": "Description", "section_id": "id_3ef53acf_0e88_48af_a387_362454805d6f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/cluster", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:57.638232+00:00", "version": "10.2"}}
{"id": "ed68423e05ce6b02", "content": "cluster [slc-options]... Optional arguments slc-options Syntax: t=<num> | delims=<string> | showcount=<bool> | countfield=<field> | labelfield=<field> | field=<field> | labelonly=<bool> | match=(termlist | termset | ngramset) Description: Options for configuring simple log clusters (slc). SLC options t Syntax: t=<num> Description: Sets the cluster threshold, which controls the sensitivity of the clustering. This value needs to be a number greater than 0.0 and less than 1.0. The closer the threshold is to 1, the more similar events have to be for them to be considered in the same cluster. Default: 0.8 delims Syntax: delims=<string> Description: Configures the set of delimiters used to tokenize the raw string. By default, everything except 0-9, A-Z, a-z, and '_' are delimiters. showcount Syntax: showcount=<bool> Description: If showcount=false , indexers cluster its own events before clustering on the search head. When showcount=false the event count is not added to the event. When showcount=true , the event count for each cluster is recorded and each event is annotated with the count. Default: showcount=false countfield Syntax: countfield=<field> Description: Name of the field to which the cluster size is to be written if showcount=true is true. The cluster size is the count of events in the cluster. Default: cluster_count. labelfield Syntax: labelfield=<field> Description: Name of the field to write the cluster number to. As the events are grouped into clusters, each cluster is counted and labelled with a number. Default: cluster_label field Syntax: field=<field> Description: Name of the field to analyze in each event. Default: _raw labelonly Description: labelonly=<bool> Syntax: Select whether to preserve incoming events and annotate them with the cluster they belong to (labelonly=true) or output only the cluster fields as new events (labelonly=false). When labelonly=false, outputs the list of clusters with the event that describes it and the count of events that combined with it. Default: false match Syntax: match=(termlist | termset | ngramset) Description: Select the method used to determine the similarity between events. termlist breaks down the field into words and requires the exact same ordering of terms. termset allows for an unordered set of terms. ngramset compares sets of trigram (3-character substrings). ngramset is significantly slower on large field values and is most useful for short non-textual fields, like punct. Default: termlist", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "cluster", "section_heading": "Syntax", "section_id": "id_4819d686_a0b5_4f88_a466_eb1f7035aa60--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/cluster", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:57.638345+00:00", "version": "10.2"}}
{"id": "3854b0e305c630dc", "content": "The cluster command is a streaming command or a dataset processing command, depending on which arguments are specified with the command. See Command types. Use the cluster command to find common or rare events in your data. For example, if you are investigating an IT problem, use the cluster command to find anomalies. In this case, anomalous events are those that are not grouped into big clusters or clusters that contain few events. Or, if you are searching for errors, use the cluster command to see approximately how many different types of errors there are and what types of errors are common in your data.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "cluster", "section_heading": "Usage", "section_id": "d2036965_2013_42c5_919c_4112722303d6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/cluster", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:57.638351+00:00", "version": "10.2"}}
{"id": "4f0a725864d75d9a", "content": "Example 1 Quickly return a glimpse of anything that is going wrong in your Splunk deployment. Your role must have the appropriate capabilities to access the internal indexes. This search takes advantage of what Splunk software logs about its operation in the _internal index. It returns all logs where the log_level is DEBUG, WARN, ERROR, FATAL and clusters them together. Then it sorts the clusters by the count of events in each cluster. The results appear on the Statistics tab and look something like this: Example 2 Search for events that don't cluster into large groups. This returns clusters of events and uses the sort command to display them in ascending order based on the cluster size, which are the values of cluster_count. Because they don't cluster into large groups, you can consider these rare or uncommon events. Example 3 Cluster similar error events together and search for the most frequent type of error. This searches your index for events that include the term \"error\" and clusters them together if they are similar. The sort command is used to display the events in descending order based on the cluster size, cluster_count , so that largest clusters are shown first. The head command is then used to show the twenty largest clusters. Now that you've found the most common types of errors in your data, you can dig deeper to find the root causes of these errors. Example 4 Use the cluster command to see an overview of your data. If you have a large volume of data, run the following search over a small time range, such as 15 minutes or 1 hour, or restrict it to a source type or index. This search helps you to learn more about your data by grouping events together based on their similarity and showing you a few of events from each cluster. It uses labelonly=t to keep each event in the cluster and append them with a cluster_label. The sort command is used to show the results in descending order by its size ( cluster_count ), then its cluster_label , then the indexed timestamp of the event ( _time ). The dedup command is then used to show the first five events in each cluster, using the cluster_label to differentiate between each cluster.", "code_examples": [{"language": "spl", "code": "index=_internalsource=*splunkd.log* log_level!=info | cluster showcount=t | table cluster_count _raw | sort -cluster_count"}, {"language": "spl", "code": "... | cluster showcount=t | sort cluster_count"}, {"language": "spl", "code": "error | cluster t=0.9 showcount=t | sort - cluster_count | head 20"}, {"language": "spl", "code": "... | cluster labelonly=t showcount=t | sort - cluster_count, cluster_label, _time | dedup 5 cluster_label"}], "tables": [{"headers": ["cluster_count", "raw"], "rows": [["303010", "03-20-2018 09:37:33.806 -0700 ERROR HotDBManager - Unable to create directory /Applications/Splunk/var/lib/splunk/_internaldb/db/hot_v1_49427345 because No such file or directory"], ["151506", "03-20-2018 09:37:33.811 -0700 ERROR pipeline - Uncaught exception in pipeline execution (indexer) - getting next event"], ["16390", "04-05-2018 08:30:53.996 -0700 WARN SearchResultsMem - Failed to append to multival. Original value not converted successfully to multival."], ["486", "03-20-2018 09:37:33.811 -0700 ERROR BTreeCP - failed: failed to mkdir /Applications/Splunk/var/lib/splunk/fishbucket/splunk_private_db/snapshot.tmp: No such file or directory"], ["216", "03-20-2018 09:37:33.814 -0700 WARN DatabaseDirectoryManager - idx=_internal Cannot open file='/Applications/Splunk/var/lib/splunk/_internaldb/db/.bucketManifest99454_1652919429_tmp' for writing bucket manifest (No such file or directory)"], ["216", "03-20-2018 09:37:33.814 -0700 ERROR SearchResultsWriter - Unable to open output file: path=/Applications/Splunk/var/lib/splunk/_internaldb/db/.bucketManifest99454_1652919429_tmp error=No such file or directory"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "cluster", "section_heading": "Examples", "section_id": "e15ab26e_d7b0_45f0_a2e3_882c9352390f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/cluster", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:57.638360+00:00", "version": "10.2"}}
{"id": "d88daeb1e3334029", "content": "anomalies , anomalousvalue , kmeans , outlier", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "cluster", "section_heading": "See also", "section_id": "id_5e02e9ae_ff61_4123_aea2_f51166bc362f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/cluster", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:48:57.638364+00:00", "version": "10.2"}}
{"id": "4640c5016bd8aae2", "content": "Removes the events that contain an identical combination of values for the fields that you specify. With the dedup command, you can specify the number of duplicate events to keep for each value of a single field, or for each combination of values among several fields. Events returned by dedup are based on search order. For historical searches , the most recent events are searched first. For real-time searches , the first events that are received are searched, which are not necessarily the most recent events. You can specify the number of events with duplicate values, or value combinations, to keep. You can sort the fields, which determines which event is retained. Other options enable you to retain events with the duplicate fields removed, or to keep events where the fields specified do not exist in the events.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "dedup", "section_heading": "Description", "section_id": "f7516ac7_2a11_443e_a990_5a92340d2dd7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/dedup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:14.691119+00:00", "version": "10.2"}}
{"id": "5c4e02136441be89", "content": "The required syntax is in bold. dedup [<int>] <field-list> [keepevents=<bool>] [keepempty=<bool>] [consecutive=<bool>] [sortby <sort-by-clause>] Required arguments <field-list> Syntax: <string> <string> ... Description: A list of field names to remove duplicate values from. Optional arguments consecutive Syntax: consecutive=<bool> Description: If true, only remove events with duplicate combinations of values that are consecutive. Default: false keepempty Syntax: keepempty=<bool> Description: If set to true, keeps every event where one or more of the specified fields is not present (null). Default: false. All events where any of the selected fields are null are dropped. The keepempty=true argument keeps every event that does not have one or more of the fields in the field list. To keep N representative events for combinations of field values including null values, use the fillnull command to provide a non-null value for these fields. For example: keepevents Syntax: keepevents=<bool> Description: If true, keep all events, but will remove the selected fields from events after the first event containing a particular combination of values. Default: false. Events are dropped after the first event of each particular combination. <N> Syntax: <int> Description: The dedup command retains multiple events for each combination when you specify N. The number for N must be greater than 0. If you do not specify a number, only the first occurring event is kept. All other duplicates are removed from the results. <sort-by-clause> Syntax: sortby ( - | + ) <sort-field> [(- | +) <sort_field> ...] Description: List of the fields to sort by and the sort order. Use the dash symbol ( - ) for descending order and the plus symbol ( + ) for ascending order. You must specify the sort order for each field specified in the <sort-by-clause>. The <sort-by-clause> determines which of the duplicate events to keep. When the list of events is sorted, the top-most event, of the duplicate events in the sorted list, is retained. Sort field options <sort-field> Syntax: <field> | auto(<field>) | str(<field>) | ip(<field>) | num(<field>) Description: The options that you can specify to sort the events. <field> Syntax: <string> Description: The name of the field to sort. auto Syntax: auto(<field>) Description: Determine automatically how to sort the field values. ip Syntax: ip(<field>) Description: Interpret the field values as IP addresses. num Syntax: num(<field>) Description: Interpret the field values as numbers. str Syntax: str(<field>) Description: Order the field values by using the lexicographic order.", "code_examples": [{"language": "spl", "code": "...| fillnull value=\"MISSING\"field1 field2 | dedup field1 field2"}], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "dedup", "section_heading": "Syntax", "section_id": "id_6bfbd2c6_63e4_4e66_88d1_6d3e14af5f39--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/dedup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:14.691127+00:00", "version": "10.2"}}
{"id": "e69aa7718dd5c664", "content": "The dedup command is a streaming command or a dataset processing command, depending on which arguments are specified with the command. For example, if you specify the <sort-by-clause , the dedup command acts as a dataset processing command. All of the results must be collected before sorting. See Command types. Avoid using the dedup command on the _raw field if you are searching over a large volume of data. If you search the _raw field, the text of every event in memory is retained which impacts your search performance. This is expected behavior. This behavior applies to any field with high cardinality and large size. Multivalue fields To use the dedup command on multivalue fields, the fields must match all values to be deduplicated. Lexicographical order Lexicographical order sorts items based on the values used to encode the items in computer memory. In Splunk software, this is almost always UTF-8 encoding, which is a superset of ASCII. Numbers are sorted before letters. Numbers are sorted based on the first digit. For example, the numbers 10, 9, 70, 100 are sorted lexicographically as 10, 100, 70, 9. Uppercase letters are sorted before lowercase letters. Symbols are not standard. Some symbols are sorted before numeric values. Other symbols are sorted before or after letters.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "dedup", "section_heading": "Usage", "section_id": "ce0650ce_4e73_4b67_984b_32be8fd2bfd7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/dedup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:14.691131+00:00", "version": "10.2"}}
{"id": "37a6b55bc85b974d", "content": "1. Remove duplicate results based on one field Remove duplicate search results with the same host value. 2. Remove duplicate results and sort results in ascending order Remove duplicate search results with the same source value and sort the results by the _time field in ascending order. 3. Remove duplicate results and sort results in descending order Remove duplicate search results with the same source value and sort the results by the _size field in descending order. 4. Keep the first 3 duplicate results For search results that have the same source value, keep the first 3 that occur and remove all subsequent results. 5. Keep results that have the same combination of values in multiple fields For search results that have the same source AND host values, keep the first 2 that occur and remove all subsequent results. 6. Remove only consecutive duplicate events Remove only consecutive duplicate events. Keep non-consecutive duplicate events. In this example duplicates must have the same combination of values the source and host fields.", "code_examples": [{"language": "spl", "code": "... | dedup host"}, {"language": "spl", "code": "... | dedupsourcesortby +_time"}, {"language": "spl", "code": "... | dedupsourcesortby -_size"}, {"language": "spl", "code": "... | dedup 3source"}, {"language": "spl", "code": "... | dedup 2sourcehost"}, {"language": "spl", "code": "... | dedup consecutive=truesourcehost"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "dedup", "section_heading": "Examples", "section_id": "b368fab4_17da_4508_b1e1_3d4ae7377c8a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/dedup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:14.691136+00:00", "version": "10.2"}}
{"id": "e9a76cab3fd34268", "content": "uniq", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "dedup", "section_heading": "See also", "section_id": "id_23a8400d_a371_4bc2_8c8d_a82c4aedecc8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/dedup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:14.691140+00:00", "version": "10.2"}}
{"id": "e0427ef90e4eda1f", "content": "The diff command mimics *nix diff output and compares two search results at a time by returning the line-by-line difference, or comparison, of the two. The two search results compared are specified by the two position values position1 and position2. These values default to 1 and 2 to compare the first two results. By default, the text ( _raw field) of the two search results is compared. Other fields can be compared by selecting another field using attribute .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "diff", "section_heading": "Description", "section_id": "id_82de4657_1707_4abe_9b50_19bf48237d1c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/diff", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:32.274455+00:00", "version": "10.2"}}
{"id": "a7ab4d3ca9ef4856", "content": "diff [position1= int ] [position2= int ] [attribute= string ] [diffheader= bool ] [context= bool ] [maxlen= int ] Optional arguments position1 Datatype: <int> Description: Of the table of input search results, selects a specific search result to compare to position2. Default: position1=1 and refers to the first search result. position2 Datatype: <int> Description: Of the table of input search results, selects a specific search result to compare to position1. This value must be greater than position1. Default: position2=2 and refers to the second search result. attribute Datatype: <field> Description: The field name to be compared between the two search results. Default: attribute=_raw , which refers to the text of the event or result. diffheader Datatype: <bool> Description: If true, show the traditional diff header, naming the \"files\" compared. The diff header makes the output a valid diff as would be expected by the programmer command-line patch command. Default: diffheader=false. context Datatype: <bool> Description: If true, selects context-mode diff output as opposed to the default unified diff output. Default: context=false , or unified. maxlen Datatype: <int> Description: Controls the maximum content in bytes diffed from the two events. If maxlen=0 , there is no limit. Default: maxlen=100000 , which is 100KB.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "diff", "section_heading": "Syntax", "section_id": "id_2b41e663_c59d_48c3_ba85_5890e48d4436--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/diff", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:32.274462+00:00", "version": "10.2"}}
{"id": "416ad98c1ad36936", "content": "Example 1: Compare the \"ip\" values of the first and third search results. Example 2: Compare the 9th search results to the 10th.", "code_examples": [{"language": "spl", "code": "... | diff pos1=1 pos2=3 attribute=ip"}, {"language": "spl", "code": "... | diff position1=9 position2=10"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "diff", "section_heading": "Examples", "section_id": "id_28dc6427_56f3_4cc0_8e62_e8d7dcb9462a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/diff", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:32.274467+00:00", "version": "10.2"}}
{"id": "c1804f66bfcc3bf9", "content": "set", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "diff", "section_heading": "See also", "section_id": "id_821274cc_4f4f_4bf3_8822_9b1a8b1c9333--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/diff", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:32.274471+00:00", "version": "10.2"}}
{"id": "ccafd54448c7e626", "content": "The spath command enables you to extract information from the structured data formats XML and JSON. The command stores this information in one or more fields. The command also highlights the syntax in the displayed events list. You can also use the spath() function with the eval command. For more information, see the evaluation functions .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "spath", "section_heading": "Description", "section_id": "f42206e9_a27c_49b1_b79f_129a668fed73--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/spath", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:49.289443+00:00", "version": "10.2"}}
{"id": "039ccea7b7ead3f3", "content": "spath [input=<field>] [output=<field>] [path=<datapath> | <datapath>] Optional arguments input Syntax: input=<field> Description: The field to read in and extract values from. Default: _raw output Syntax: output=<field> Description: If specified, the value extracted from the path is written to this field name. Default: If you do not specify an output argument, the value for the path argument becomes the field name for the extracted value. path Syntax: path=<datapath> | <datapath> Description: The location path to the value that you want to extract. The location path can be specified as path=<datapath> or as just datapath. If you do not specify the path= , the first unlabeled argument is used as the location path. A location path is composed of one or more location steps, separated by periods. An example of this is vendorProductSet.product.desc. A location step is composed of a field name and an optional index surrounded by curly brackets. The index can be an integer, to refer to the position of the data in an array (this differs between JSON and XML), or a string, to refer to an XML attribute. If the index refers to an XML attribute, specify the attribute name with an @ symbol.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "spath", "section_heading": "Syntax", "section_id": "id_8312840a_15d0_4b41_ba70_d9449bb9a32a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/spath", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:49.289450+00:00", "version": "10.2"}}
{"id": "f6c49bebf26eb2fc", "content": "The spath command is a distributable streaming command. See Command types. Location path omitted When used with no path argument, the spath command runs in \"auto-extract\" mode. By default, when the spath command is in \"auto-extract\" mode, it finds and extracts all the fields from the first 5,000 characters in the input field. These fields default to _raw if another input source is not specified. If a path is provided, the value of this path is extracted to a field named by the path or to a field specified by the output argument, if the output argument is provided. A location path contains one or more location steps A location path contains one or more location steps, each of which has a context that is specified by the location steps that precede it. The context for the top-level location step is implicitly the top-level node of the entire XML or JSON document. The location step is composed of a field name and an optional array index The location step is composed of a field name and an optional array index indicated by curly brackets around an integer or a string. Array indices mean different things in XML and JSON. For example, JSON uses zero-based indexing. In JSON, product.desc{3} refers to the fourth element of the desc child of the product element. In XML, this same path refers to the third desc child of product. Using wildcards in place of an array index The spath command lets you use wildcards to take the place of an array index in JSON. Now, you can use the location path entities.hashtags{}.text to get the text for all of the hashtags, as opposed to specifying entities.hashtags{0}.text , entities.hashtags{1}.text , and so on. The referenced path, here entities.hashtags , has to refer to an array for this to make sense. Otherwise, you get an error just like with regular array indices. This also works with XML. For example, catalog.book and catalog.book{} are equivalent. Both get you all the books in the catalog. Overriding the spath extraction character limit By default, the spath command extracts all the fields from the first 5,000 characters in the input field. If your events are longer than 5,000 characters and you want to extract all of the fields, you can override the extraction character limit for all searches that use the spath command. To change this character limit for all spath searches, change the extraction_cutoff setting in the limits.conf file to a larger value. If you change the default extraction_cutoff setting, you must also change the setting to the same value in all limits.conf files across all search head and indexer tiers. Splunk Cloud Platform To change the limits.conf extraction_cutoff setting, use one of the following methods: The Configure limits page in Splunk Web. For more information, see Configure limits using Splunk Web in the Splunk Cloud Platform Admin Manual. The Admin Config Service (ACS) command line interface (CLI). For more information, see Administer Splunk Cloud Platform using the ACS CLI in the Splunk Cloud Platform Admin Config Service Manual. The Admin Config Service (ACS) API. For more information, see Manage limits.conf configurations in Splunk Cloud Platform in the Splunk Cloud Platform Admin Config Service Manual. Splunk Enterprise To change the extraction_cutoff setting, follow these steps. Prerequisites Only users with file system access, such as system administrators, can edit configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps Open or create a local limits.conf file at $SPLUNK_HOME/etc/system/local if you are using *nix, or %SPLUNK_HOME%\\etc\\system\\local if you are using Windows. In the [spath] stanza, add the line extraction_cutoff = <value> set to the value you want as the extraction cutoff. If your deployment includes search head or indexer clusters, repeat the previous steps on every indexer peer node or search head cluster member. See Use the deployer to distribute apps and configuration updates in Splunk Enterprise Distributed Search and Update common peer configurations and apps in Splunk Enterprise Managing Indexers and Clusters of Indexers for information about changing the limits.conf setting across search head and indexer clusters. JSON data used with the spath command must be well-formed To use the spath command to extract JSON data, ensure that the JSON data is well-formed. For example, string literals other than the literal strings true , false and null must be enclosed in double quotation marks ( \" ). For a full reference on the JSON data format, see the JSON Data Interchange Syntax standard at https://www.ecma-international.org/publications-and-standards/standards/ecma-404/. Alternatives to the spath command If you are using autokv or index-time field extractions, the path extractions are performed for you at index time. You do not need to explicitly use the spath command to provide a path. If you are using indexed_extractions=JSON or KV_MODE=JSON in the props.conf file, then you don't need to use the spath command.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "spath", "section_heading": "Usage", "section_id": "id_4dec293a_2f4d_45cf_9928_30718e3aff05--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/spath", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:49.289454+00:00", "version": "10.2"}}
{"id": "91098d091c42f17d", "content": "1. Specify an output field and path This example shows how to specify an output field and path. 2. Specify just the <datapath> For the path argument, you can specify the location path with or without the path=. In this example the <datapath> is server.name. 3. Specify an output field and path based on an array For example, you have this array. To specify the output field and path, use this syntax. 4. Specify an output field and a path that uses a nested array For example, you have this nested array. To specify the output and path from this nested array, use this syntax. 5. Specify the output field and a path for an XML attribute Use the @ symbol to specify an XML attribute. Consider the following XML list of books and authors. Use this search to return the path for the book and the year it was published. In this example, the output is a single multivalue result that lists all of the years the books were published.", "code_examples": [{"language": "spl", "code": "... | spath output=myfield path=vendorProductSet.product.desc"}, {"language": "spl", "code": "... | spath output=myfield server.name"}, {"language": "spl", "code": "{\"vendorProductSet\": [1,2]\n}"}, {"language": "spl", "code": "... | spath output=myfield path=vendorProductSet{1}"}, {"language": "spl", "code": "{\"vendorProductSet\": {\"product\": [\n         {\"desc\": 1},\n         {\"locDesc\": 2}\n      ]\n   }\n}"}, {"language": "spl", "code": "... | spath output=myfield path=vendorProductSet.product{}.locDesc"}, {"language": "spl", "code": "<?xml version=\"1.0\">\n<purchases>\n   <book>\n         <author>Martin, George R.R.</author>\n         <title yearPublished=1996>A Game of Thrones</title>\n         <title yearPublished=1998>A Clash of Kings</title>\n  </book>\n   <book>\n         <author>Clarke, Susanna</author>\n         <title yearPublished=2004>Jonathan Strange and Mr. Norrell</title>\n   </book>\n   <book>\n         <author>Kay, Guy Gavriel</author>\n         <title yearPublished=1990>Tigana</title>\n   </book>\n   <book>\n         <author>Bujold, Lois McMasters</author>\n         <title yearPublished=1986>The Warrior's Apprentice</title>\n   </book>\n</purchases>"}, {"language": "spl", "code": "... | spath output=dates path=purchases.book.title{@yearPublished} | table dates"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "spath", "section_heading": "Basic examples", "section_id": "d3fca037_1dfc_4a14_89b4_6d13c9c3d25d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/spath", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:49.289460+00:00", "version": "10.2"}}
{"id": "a2f08a2954ea036a", "content": "1: GitHub As an administrator of a number of large Git repositories, you want to: See who has committed the most changes and to which repository Produce a list of the commits submitted for each user Suppose you are Indexing JSON data using the GitHub PushEvent webhook. You can use the spath command to extract fields called repository, commit_author, and commit_id: To see who has committed the most changes to a repository, run the search. To see the list of commits by each user, run this search. 2: Extract a subset of a XML attribute This example shows how to extract values from XML attributes and elements. To extract the values of the locDesc elements (Precios, Prix, Preise, etc.), use: To extract the value of the locale attribute (es, fr, de, etc.), use: To extract the attribute of the 4th locDesc (ca), use: 3: Extract and expand JSON events with multi-valued fields The mvexpand command only works on one multivalued field. This example walks through how to expand a JSON event that has more than one multivalued field into individual events for each field value. For example, given this event with sourcetype=json : First, start with a search to extract the fields from the JSON. Because no path argument is specified, the spath command runs in \"auto-extract\" mode and extracts all of the fields from the first 5,000 characters in the input field. The fields are then renamed and placed in a table. Then, use the eval function, mvzip(), to create a new multivalued field named x, with the values of the size and data: Now, use the mvexpand command to create individual events based on x and the eval function mvindex() to redefine the values for data and size. (Thanks to Splunk user G. Zaimi for this example.)", "code_examples": [{"language": "spl", "code": "... | spath output=repository path=repository.url"}, {"language": "spl", "code": "... | spath output=commit_author path=commits{}.author.name"}, {"language": "spl", "code": "... | spath output=commit_id path=commits{}.id"}, {"language": "spl", "code": "... | top commit_author by repository"}, {"language": "spl", "code": "... | stats values(commit_id) by commit_author"}, {"language": "spl", "code": "<vendorProductSet vendorID=\"2\">\n            <product productID=\"17\"units=\"mm\">\n                <prodName nameGroup=\"custom\">\n                    <locName locale=\"all\">APLI 01209</locName>\n                </prodName>\n                <desc descGroup=\"custom\">\n                    <locDesc locale=\"es\">Precios</locDesc>\n                    <locDesc locale=\"fr\">Prix</locDesc>\n                    <locDesc locale=\"de\">Preise</locDesc>\n                    <locDesc locale=\"ca\">Preus</locDesc>\n                    <locDesc locale=\"pt\">PreÃ§os</locDesc> \n                </desc>\n           </product>"}, {"language": "spl", "code": "... | spath output=locDesc path=vendorProductSet.product.desc.locDesc"}, {"language": "spl", "code": "... | spath output=locDesc.locale  path=vendorProductSet.product.desc.locDesc{@locale}"}, {"language": "spl", "code": "... | spath path=vendorProductSet.product.desc.locDesc{4}{@locale}"}, {"language": "spl", "code": "{\"widget\": {\"text\": [ \n        {\"data\":\"Click here\",\"size\": 36\n        },\n       {\"data\":\"Learn more\",\"size\": 37\n       },\n       {\"data\":\"Help\",\"size\": 38\n       },\n       ]\n   }\n}"}, {"language": "spl", "code": "sourcetype=json | spath | rename widget.text.size AS size, widget.text.data AS data | table _time,size,data"}, {"language": "spl", "code": "_time            size    data\n--------------------------- ---- -----------\n2018-10-18 14:45:46.000 BST   36 Click here\n                              37 Learn more\n                              38 Help"}, {"language": "spl", "code": "sourcetype=json | spath | rename widget.text.size AS size, widget.text.data AS data |evalx=mvzip(data,size) | table _time,data,size,x"}, {"language": "spl", "code": "_time                data    size        x\n--------------------------- ----------- ----- --------------\n2018-10-18 14:45:46.000 BST Click here   36   Click here,36\n                            Learn more   37   Learn more,37\n                            Help         38   Help,38"}, {"language": "spl", "code": "sourcetype=json | spath | rename widget.text.size AS size, widget.text.data AS data |evalx=mvzip(data,size)| mvexpand x |evalx = split(x,\",\") |evaldata=mvindex(x,0) |evalsize=mvindex(x,1) | table _time,data, size"}, {"language": "spl", "code": "_time                data   size\n--------------------------- ---------- ----\n2018-10-18 14:45:46.000 BST Click here  36\n2018-10-18 14:45:46.000 BST Learn more  37\n2018-10-18 14:45:46.000 BST Help        38"}], "tables": [], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "spath", "section_heading": "Extended examples", "section_id": "id_4f782ec9_dcda_4ed9_8eee_35b13fd7bf47--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/spath", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:49.289465+00:00", "version": "10.2"}}
{"id": "88bf2043549cec9a", "content": "extract , kvform , multikv , regex , rex , xmlkv , xpath", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "spath", "section_heading": "See also", "section_id": "id_7ab4e1f7_3f6b_4804_aaa9_121a7da335a1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/spath", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:49:49.289469+00:00", "version": "10.2"}}
{"id": "01a495d03d9b0578", "content": "The transaction command finds transactions based on events that meet various constraints. Transactions are made up of the raw text (the _raw field) of each member, the time and date fields of the earliest member, as well as the union of all other fields of each member. Additionally, the transaction command adds two fields to the raw events, duration and eventcount. The values in the duration field show the difference between the timestamps for the first and last events in the transaction. The values in the eventcount field show the number of events in the transaction. See About transactions in the Search Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 7, "metadata": {"title": "transaction", "section_heading": "Description", "section_id": "id_67bb3e85_25a0_42de_9044_a393c64298e0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transaction", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:05.108022+00:00", "version": "10.2"}}
{"id": "1fc69947b6d8f32a", "content": "The required syntax is in bold. transaction [<field-list>] [name=<transaction-name>] [<txn_definition-options>...] [<memcontrol-options>...] [<rendering-options>...] Required arguments None. Optional arguments field-list Syntax: <field> ... Description: One or more field names. The events are grouped into transactions, based on the unique values in the fields. For example, suppose two fields are specified: client_ip and host. For each client_ip value, a separate transaction is returned for each unique host value for that client_ip. memcontrol-options Syntax: <maxopentxn> | <maxopenevents> | <keepevicted> Description: These options control the memory usage for your transactions. They are not required, but you can use 0 or more of the options to define your transaction. See Memory control options. name Syntax: name=<transaction-name> Description: Specify the stanza name of a transaction that is configured in the transactiontypes.conf file. This runs the search using the settings defined in this stanza of the configuration file. If you provide other transaction definition options (such as maxpause) in this search, they overrule the settings in the configuration file. rendering-options Syntax: <delim> | <mvlist> | <mvraw> | <nullstr> Description: These options control the multivalue rendering for your transactions. They are not required, but you can use 0 or more of the options to define your transaction. See Multivalue rendering options. txn_definition-options Syntax: <maxspan> | <maxpause> | <maxevents> | <startswith> | <endswith> | <connected> | <unifyends> | <keeporphans> Description: Specify the transaction definition options to define your transactions. You can use multiple options to define your transaction. Txn definition options connected Syntax: connected=<bool> Description: Only relevant if a field or fields list is specified. If an event contains fields required by the transaction, but none of these fields have been instantiated in the transaction (added with a previous event), this opens a new transaction (connected=true) or adds the event to the transaction (connected=false). If the default value for this argument is changed, the default for the unifyends argument will also change to the same value. For example, if connected=false , then unifyends=false. Default: true endswith Syntax: endswith=<filter-string> Description: A search or eval expression which, if satisfied by an event, marks the end of a transaction. keeporphans Syntax: keeporphans=true | false Description: Specify whether the transaction command should output the results that are not part of any transactions. The results that are passed through as \"orphans\" are distinguished from transaction events with a _txn_orphan field, which has a value of 1 for orphan results. Default: false maxspan Syntax: maxspan=<int>[s | m | h | d] Description: Specifies the maximum length of time in seconds, minutes, hours, or days that the events can span, which is the maximum total time between the earliest and latest events in a transaction. The events in the transaction must span less than the integer specified for maxspan. Events that exceed the maxspan limit are treated as part of a separate transaction. If the value is negative, the maxspan constraint is deactivated and there is no limit. Events must be sorted in descending chronological order before the maxspan argument is used. See the related example later in this topic. Default: -1 (no limit) maxpause Syntax: maxpause=<int>[s | m | h | d] Description: Specifies the maximum length of time in seconds, minutes, hours, or days for the pause between consecutive events in a transaction, which is the maximum total time between events. If the value is negative, the maxpause constraint is deactivated and there is no limit. Events must be sorted in descending chronological order before the maxpause argument is used. See the related example later in this topic. Default: -1 (no limit) maxevents Syntax: maxevents=<int> Description: The maximum number of events in a transaction. This constraint is deactivated if the value is negative. Default: 1000 startswith Syntax: startswith=<filter-string> Description: A search or eval filtering expression which if satisfied by an event marks the beginning of a new transaction. unifyends Syntax: unifyends= true | false Description: Whether to force events that match startswith/endswith constraint(s) to also match at least one of the fields used to unify events into a transaction. The default value for this argument is the same as the connected argument. For example, if connected=false , then unifyends=false. Default: true, set to the same default value as the connected argument Filter string options These options are used with the startswith and endswith arguments. <filter-string> Syntax: <search-expression> | (<quoted-search-expression>) | eval(<eval-expression>) Description: A search or eval filtering expression which if satisfied by an event marks the end of a transaction. <search-expression> Description: A valid search expression that does not contain quotes. <quoted-search-expression> Description: A valid search expression that contains quotes. <eval-expression> Description: A valid eval expression that evaluates to a Boolean. Memory control options If you have Splunk Cloud Platform, Splunk Support administers these settings in the limits.conf file on your behalf. keepevicted Syntax: keepevicted=<bool> Description: Whether to output evicted transactions. Evicted transactions can be distinguished from non-evicted transactions by checking the value of the closed_txn field. The closed_txn field is set to 0 , or false, for evicted transactions and 1 , or true for non-evicted, or closed, transactions. The closed_txn field is set to 1 if one of the following conditions is specified: maxevents , maxspan , maxpause , startswith. For startswith , because the transaction command sees events in reverse chronological order, it closes a transaction when it satisfies the start condition. If none of these conditions is specified, all transactions are output even though all transactions will have closed_txn set to 0. A transaction can also be evicted when the memory limitations are reached. Default: false or 0 maxopenevents Syntax: maxopenevents=<int> Description: Specifies the maximum number of events (which are) part of open transactions before transaction eviction starts happening, using LRU policy. Default: The default value for this argument is read from the transactions stanza in the limits.conf file. maxopentxn Syntax: maxopentxn=<int> Description: Specifies the maximum number of not yet closed transactions to keep in the open pool before starting to evict transactions, using LRU policy. Default: The default value for this argument is read from the transactions stanza in the limits.conf file. Multivalue rendering options delim Syntax: delim=<string> Description: Specify a character to separate multiple values. When used in conjunction with the mvraw=t argument, represents a string used to delimit the values in the _raw field. Default: \" \" (whitespace) mvlist Syntax: mvlist= true | false | <field-list> Description: Flag that controls how multivalued fields are processed. When set to mvlist=true , the multivalued fields in the transaction are a list of the original events ordered in arrival order. When set to mvlist=false , the multivalued fields in the transaction are a set of unique field values ordered alphabetically. If a comma or space delimited list of fields is provided, only those fields are rendered as lists. Default: false mvraw Syntax: mvraw=<bool> Description: Used to specify whether the _raw field of the transaction search result should be a multivalued field. Default: false nullstr Syntax: nullstr=<string> Description: A string value to use when rendering missing field values as part of multivalued fields in a transaction. This option applies only to fields that are rendered as lists. Default: NULL", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 7, "metadata": {"title": "transaction", "section_heading": "Syntax", "section_id": "id_7d4a9cd5_66eb_428c_a49e_0fef685636b9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transaction", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:05.108029+00:00", "version": "10.2"}}
{"id": "53eeff448e385692", "content": "The transaction command is a centralized streaming command. See Command types. In the output, the events in a transaction are grouped together as multiple values in the Events field. Each event in a transaction starts on a new line by default. If there are more than 5 events in a transaction, the remaining events in the transaction are collapsed. A message appears at the end of the transaction which gives you the option to show all of the events in the transaction. Specifying multiple fields The Splunk software does not necessarily interpret the transaction defined by multiple fields as a conjunction ( field1 AND field2 AND field3 ) or a disjunction ( field1 OR field2 OR field3 ) of those fields. If there is a transitive relationship between the fields in the fields list and if the related events appear in the correct sequence, each with a different timestamp, transaction command will try to use it. For example, if you searched for You might see the following events grouped into a transaction: Descending chronological order required The transaction command requires that the incoming events be in descending chronological order. Some commands, such as eval , might change the order or time labeling of events. If one of these commands precedes the transaction command, your search returns an error unless you include a sort command in your search. The sort command must occur immediately before the transaction command to reorder the search results in descending chronological order.", "code_examples": [{"language": "spl", "code": "... | transaction host cookie"}, {"language": "spl", "code": "event=1 host=a\nevent=2 host=a cookie=b\nevent=3 cookie=b"}], "tables": [], "chunk_index": 2, "total_chunks": 7, "metadata": {"title": "transaction", "section_heading": "Usage", "section_id": "c2217cd4_3dae_4005_a08b_cebe0ede405b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transaction", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:05.108035+00:00", "version": "10.2"}}
{"id": "0dee0aa509fd0be0", "content": "1. Transactions with the same host, time range, and pause Group search results that have the same host and cookie value, occur within 30 seconds, and do not have a pause of more than 5 seconds between the events. 2. Transactions with the same \"from\" value, time range, and pause Group search results that have the same value of \"from\", with a maximum span of 30 seconds, and a pause between events no greater than 5 seconds into a transaction. 3. Transactions with the same field values You have events that include an alert_level. You want to create transactions where the level is equal. Using the streamstats command, you can remember the value of the alert level for the current and previous event. Using the transaction command, you can create a new transaction if the alert level is different. Output specific fields to table.", "code_examples": [{"language": "spl", "code": "... | transaction host cookie maxspan=30s maxpause=5s"}, {"language": "spl", "code": "... | transaction from maxspan=30s maxpause=5s"}, {"language": "spl", "code": "... | streamstats window=2 current=t latest(alert_level) AS last earliest(alert_level) AS first | transaction endswith=eval(first!=last) | table _time duration first last alert_level eventcount"}], "tables": [], "chunk_index": 3, "total_chunks": 7, "metadata": {"title": "transaction", "section_heading": "Basic Examples", "section_id": "id_1da6df5b_7ed9_4e4a_9c9c_59c6c9ad24fe--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transaction", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:05.108040+00:00", "version": "10.2"}}
{"id": "7dc4e7ecd508fe78", "content": "1. Transactions of Web access events based on IP address Define a transaction based on Web access events that share the same IP address. The first and last events in the transaction should be no more than thirty seconds apart and each event should not be longer than five seconds apart. This produces the following events list. The clientip for each event in the transaction is highlighted. This search groups events together based on the IP addresses accessing the server and the time constraints. The search results might have multiple values for some fields, such as host and source. For example, requests from a single IP could come from multiple hosts if multiple people are shopping from the same office. For more information, read the topic About transactions in the Knowledge Manager Manual. 2. Transaction of Web access events based on host and client IP Define a transaction based on Web access events that have a unique combination of host and clientip values. The first and last events in the transaction should be no more than thirty seconds apart and each event should not be longer than five seconds apart. This search produces the following events list. Each of these events have a distinct combination of the IP address ( clientip ) values and host values within the limits of the time constraints specified in the search. 3. Purchase transactions based on IP address and time range This search defines a purchase transaction as 3 events from one IP address which occur in a 10 minute span of time. This search defines a purchase event based on Web access events that have the action=purchase value. These results are then piped into the transaction command. This search identifies purchase transactions by events that share the same clientip , where each session lasts no longer than 10 minutes, and includes no more than 3 events. This search produces the following events list: 4. Email transactions based on maxevents and endswith This example defines an email transaction as a group of up to 10 events. Each event contains the same value for the mid (message ID), icid (incoming connection ID), and dcid (delivery connection ID). The last event in the transaction contains a Message done string. This search produces the following list of events: By default, only the first 5 events in a transaction are shown. The first transaction contains 7 events and the last event is hidden. The second and third transactions show the Message done string in the last event in the transaction. 5. Email transactions based on maxevents, maxspan, and mvlist This example defines an email transaction as a group of up to 10 events. Each event contains the same value for the mid (message ID), icid (incoming connection ID), and dcid (delivery connection ID). The first and last events in the transaction should be no more than thirty seconds apart. By default, the values of multivalue fields are suppressed in search results with the default setting for mvlist , which is false. Specifying mvlist=true in this search displays all of the values of the selected fields. This produces the following events list: Here you can see that each transaction has a duration that is less than thirty seconds. Also, if there is more than one value for a field, each of the values is listed. 6. Transactions with the same session ID and IP address Define a transaction as a group of events that have the same session ID, JSESSIONID , and come from the same IP address, clientip , and where the first event contains the string, \"view\", and the last event contains the string, \"purchase\". The search defines the first event in the transaction as events that include the string, \"view\", using the startswith=\"view\" argument. The endswith=\"purchase\" argument does the same for the last event in the transaction. This example then pipes the transactions into the where command and the duration field to filter out all of the transactions that took less than a second to complete. The where filter cannot be applied before the transaction command because the duration field is added by the transaction command. You might be curious about why the transactions took a long time, so viewing these events might help you to troubleshoot. You won't see it in this data, but some transactions might take a long time because the user is updating and removing items from their shopping cart before they completes the purchase. Additionally, this search is run over all events. There is no filtering before the transaction command. Anytime you can filter the search before the first pipe, the faster the search runs. 7. Sort order when using maxspan and maxpause Pay careful attention to the sort order of your events when using the maxspan and maxpause arguments with the transaction command because searches with events sorted in ascending chronological order return incorrect results. For example, the following search returns expected results because the search uses | sort -_time to sort events in descending chronological order before the maxspan argument is used: The results look like this: In contrast, the following search doesn't generate the correct results because events are sorted in ascending chronological order by default before the maxspan argument is used: The results look like this:", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | transaction clientip maxspan=30s maxpause=5s"}, {"language": "spl", "code": "sourcetype=access_* | transaction clientip host maxspan=30s maxpause=5s"}, {"language": "spl", "code": "sourcetype=access_* action=purchase | transaction clientip maxspan=10m maxevents=3"}, {"language": "spl", "code": "sourcetype=\"cisco:esa\"| transaction mid dcid icid maxevents=10 endswith=\"Message done\""}, {"language": "spl", "code": "sourcetype=\"cisco:esa\"| transaction mid dcid icid maxevents=10 maxspan=30s mvlist=true"}, {"language": "spl", "code": "sourcetype=access_* | transaction JSESSIONID clientip startswith=\"view\"endswith=\"purchase\"|whereduration>0"}, {"language": "spl", "code": "|makeresults count=10\n|streamstats count\n|eval_time=now()+10*count, user=\"nobody\"|sort -_time\n|transaction user maxspan=11s"}, {"language": "spl", "code": "|makeresults count=20\n|streamstats count\n|eval_time=now()+10*count, user=\"nobody\"|transaction user maxspan=11s"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}, {"headers": [], "rows": [["This example uses sample email data. You should be able to run this search on any email data by replacing thesourcetype=cisco:esawith thesourcetypevalue and themailfromfield with email address field name in your data. For example, the email might beTo,From, orCc)."]]}, {"headers": [], "rows": [["This example uses sample email data. You should be able to run this search on any email data by replacing thesourcetype=cisco:esawith thesourcetypevalue and themailfromfield with email address field name in your data. For example, the email might beTo,From, orCc)."]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["_raw", "_time", "closed_txn", "count", "duration", "eventcount", "field_match_sum", "linecount", "user"], "rows": [["", "2024-07-16 16:10:30", "1", "109", "10", "2", "2", "2", "nobody"], ["", "2024-07-16 16:10:10", "1", "78", "10", "2", "2", "2", "nobody"], ["", "2024-07-16 16:09:50", "1", "56", "10", "2", "2", "2", "nobody"], ["", "2024-07-16 16:09:30", "1", "34", "10", "2", "2", "2", "nobody"], ["", "2024-07-16 16:09:10", "0", "12", "10", "2", "2", "2", "nobody"]]}, {"headers": ["_raw", "_time", "closed_txn", "count", "duration", "eventcount", "field_match_sum", "linecount", "user"], "rows": [["", "2024-07-16 16:09:17", "0", "11023456789", "90", "10", "10", "10", "nobody"]]}], "chunk_index": 4, "total_chunks": 7, "metadata": {"title": "transaction", "section_heading": "Extended Examples", "section_id": "a02db816_6794_460a_8b5b_709955b3c781--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transaction", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:05.108062+00:00", "version": "10.2"}}
{"id": "2804d25851966b0b", "content": "Reference About transactions Commands stats concurrency", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 7, "metadata": {"title": "transaction", "section_heading": "See also", "section_id": "id_0b7fe69b_6bad_424f_9214_793a31a64cb9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transaction", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:05.108067+00:00", "version": "10.2"}}
{"id": "b942dacb15a4d469", "content": "Have questions? Visit Splunk Answers and see what questions and answers the Splunk community has using the transaction command .", "code_examples": [], "tables": [], "chunk_index": 6, "total_chunks": 7, "metadata": {"title": "transaction", "section_heading": "Answers", "section_id": "cc277e81_e06f_4350_a32e_32d82351fadf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transaction", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:05.108072+00:00", "version": "10.2"}}
{"id": "c42994443d63357f", "content": "Performs set operations on subsearches .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "set", "section_heading": "Description", "section_id": "ff4fdcb6_4005_4ad4_b705_ae5d5a5ed534--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/set", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:22.496163+00:00", "version": "10.2"}}
{"id": "5403f70b556478f4", "content": "The required syntax is in bold. | set (union | diff | intersect) subsearch1 subsearch2 Required arguments union | diff | intersect Syntax: union | diff | intersect Description: Performs two subsearches, then executes the specified set operation on the two sets of search results. subsearch Syntax: \"[\" <string> \"]\" Description: Specifies a subsearch. Subsearches must be enclosed in square brackets. For more information about subsearch syntax, see \"About subsearches\" in the Search Manual .", "code_examples": [], "tables": [{"headers": ["Operation", "Description"], "rows": [["union", "Returns a set that combines the results generated by the two subsearches. Provides results that are common to both subsets only once."], ["diff", "Returns a set that combines the results generated by the two subsearches and excludes the events common to both. Does not indicate which subsearch the results originated from."], ["intersect", "Returns a set that contains results common to both subsearches."]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "set", "section_heading": "Syntax", "section_id": "b6832fc5_2ed2_4e56_95fe_334cab1b4590--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/set", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:22.496175+00:00", "version": "10.2"}}
{"id": "2a5f51159e75cd33", "content": "The set command is an event-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. Results The set command considers results to be the same if all of fields that the results contain match. Some internal fields generated by the search, such as _serial, vary from search to search. You need to filter out some of the fields if you are using the set command with raw events, as opposed to transformed results such as those from a stats command. Typically in these cases, all fields are the same from search to search. Output limitations There is a limit on the quantity of results that come out of the invoked subsearches that the set command receives to operate on. If this limit is exceeded, the input result set to the diff command is silently truncated. If you have Splunk Enterprise, you can adjust this limit by editing the limits.conf file and changing the maxout value in the [subsearch] stanza. If this value is altered, the default quantity of results coming from a variety of subsearch scenarios are altered. Note that very large values might cause extensive stalls during the 'parsing' phase of a search, which is when subsearches run. The default value for this limit is 10000. Note: Only users with file system access, such as system administrators, can edit the configuration files. Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make the changes in the local directory. See How to edit a configuration file. If you have Splunk Cloud Platform and want to edit a configuration file, file a Support ticket. Result rows limitations By default the set command attempts to traverse a maximum of 50000 items from each subsearch. If the number of input results from either search exceeds this limit, the set command silently ignores the remaining events. By default, the maxout setting for subsearches in limits.conf prevents the number of results from exceeding this limit. This maximum is controlled by the maxresultrows setting in the [set] stanza in the limits.conf file. Increasing this limit can result in more memory usage. Note: Only users with file system access, such as system administrators, can edit the configuration files. Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make the changes in the local directory. See How to edit a configuration file. If you have Splunk Cloud Platform and want to edit a configuration file, file a Support ticket.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "set", "section_heading": "Usage", "section_id": "id_71663560_c1d3_4083_bbc6_e8da9e96a0f9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/set", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:22.496181+00:00", "version": "10.2"}}
{"id": "d077d1c15cd24fcb", "content": "Example 1: Return values of \"URL\" that contain the string \"404\" or \"303\" but not both. Example 2: Return all urls that have 404 errors and 303 errors. Note: When you use the fields command in your subsearches, it does not filter out internal fields by default. If you do not want the set command to compare internal fields, such as the _raw or _time fields, you need to explicitly exclude them from the subsearches:", "code_examples": [{"language": "spl", "code": "|setdiff [search 404 | fields url] [search 303 | fields url]"}, {"language": "spl", "code": "|setintersect [search 404 | fields url] [search 303 | fields url]"}, {"language": "spl", "code": "|setintersect [search 404 | fields url | fields - _*] [search 303 | fields url | fields - _*]"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "set", "section_heading": "Examples", "section_id": "id_702222d4_c016_400d_8128_1523da6aa11d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/set", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:22.496187+00:00", "version": "10.2"}}
{"id": "492992833b1dd7f0", "content": "append , appendcols , appendpipe , join , diff", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "set", "section_heading": "See also", "section_id": "id_2ff6378f_5db6_46f8_8072_bce722e172e9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/set", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:22.496191+00:00", "version": "10.2"}}
{"id": "f8daef61d7f7fa5f", "content": "Use the rename command to rename one or more fields. This command is useful for giving fields more meaningful names, such as \"Product ID\" instead of \"pid\". If you want to rename fields with similar names, you can use a wildcard character. See the Usage section.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "rename", "section_heading": "Description", "section_id": "id_06494a9f_690f_4a27_83f9_706737fc7515--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rename", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:37.013065+00:00", "version": "10.2"}}
{"id": "af612a256e8ef218", "content": "rename <wc-field> AS <wc-field>... Required arguments wc-field Syntax: <string> Description: The name of a field and the name to replace it. Field names with spaces must be enclosed in quotation marks. You can use the asterisk ( * ) as a wildcard to specify a list of fields with similar names. For example, if you want to specify all fields that start with \"value\", you can use a wildcard such as value* .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "rename", "section_heading": "Syntax", "section_id": "id_8ee38140_129c_481f_8a85_b7b29bc3efe0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rename", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:37.013072+00:00", "version": "10.2"}}
{"id": "2c6de7e37d137dab", "content": "The rename command is a distributable streaming command. See Command types. Rename with a phrase Use quotation marks when you rename a field with a phrase. Rename multiple, similarly named fields Use wildcards to rename multiple fields with similar names. For example, suppose you have the following field names: EU_UK EU_DE EU_PL You can rename the fields to replace EU with EMEA: The results show these field names: EMEA_UK EMEA_DE EMEA_PL Both the original and renamed fields must include the same number of wildcards, otherwise a wildcard mismatch error is returned. See Examples. You can't rename one field with multiple names You can't rename one field with multiple names. For example if you have field A, you can't specify | rename A as B, A as C. This rule also applies to other commands where you can rename fields, such as the stats command. The following example is not valid: You can't merge multiple fields into one field You can't use the rename command to merge multiple fields into one field because null, or non-present, fields are brought along with the values. For example, if you have events with either product_id or pid fields, ... | rename pid AS product_id would not merge the pid values into the product_id field. It overwrites product_id with Null values where pid does not exist for the event. See the eval command and coalesce() function. You can't match wildcard characters while renaming fields You can use the asterisk ( * ) in your searches as a wildcard character, but you can't use a backslash ( \\ ) to escape an asterisk in search strings. A backslash \\ and an asterisk * match the characters \\* in searches, not an escaped wildcard character. Because the Splunk platform doesn't support escaping wildcards, asterisk ( * ) characters in field names in rename searches can't be matched and replaced. Renaming a field that does not exist Renaming a field can cause loss of data. Suppose you rename fieldA to fieldB, but fieldA does not exist. If fieldB does not exist, nothing happens. If fieldB does exist, the result of the rename is that the data in fieldB is removed. The data in fieldB will contain null values. The original and new field names must have the same number of wildcards The number of asterisks ( * ) in the original name must match the number of asterisks in the new name. For example, the following search fails because there is one wildcard character in the original name, but none in the name that replaces it: The following search completes successfully because the number of wildcard characters in both names is the same. Support for backslash characters ( \\ ) in the rename command To match a backslash character ( \\ ) in a field name when using the rename command, use 2 backslashes for each backslash in the original field name. For example, to rename the field name http\\\\:8000 to localhost:8000 , use the following command in your search: See Backslashes in the Search Manual .", "code_examples": [{"language": "spl", "code": "... | rename SESSIONID AS\"The session ID\""}, {"language": "spl", "code": "... | rename EU* AS EMEA*"}, {"language": "spl", "code": "... | stats first(host) AS site, first(host) AS report"}, {"language": "spl", "code": "... | rename price-a*price-b AS price-a\\price-b"}, {"language": "spl", "code": "... | rename price-a*price-b AS price-a*Newprice-b"}, {"language": "spl", "code": "... | rename  http\\\\\\\\:* AS localhost:*"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "rename", "section_heading": "Usage", "section_id": "dc1f1548_eb80_4bd0_8b29_57f7db6d0144--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rename", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:37.013078+00:00", "version": "10.2"}}
{"id": "d5075b997a405572", "content": "1. Rename a single field Rename the \"_ip\" field to \"IPAddress\". 2. Rename fields with similar names using a wildcard Rename fields that begin with \"usr\" to begin with \"user\". 3. Specifying a field name that contains spaces Rename the \"count\" field. Names with spaces must be enclosed in quotation marks.", "code_examples": [{"language": "spl", "code": "... | rename _ip AS IPAddress"}, {"language": "spl", "code": "... | rename usr* AS user*"}, {"language": "spl", "code": "... | rename count AS\"Count of Events\""}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "rename", "section_heading": "Examples", "section_id": "dbf6ff78_bc53_45ea_a2d7_fb76d673d1d8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rename", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:37.013082+00:00", "version": "10.2"}}
{"id": "6db8c9c695db96bd", "content": "Commands fields replace table", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "rename", "section_heading": "See also", "section_id": "id_6fbe7a3a_68b7_420c_a3db_f07d3d0a3901--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rename", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:37.013086+00:00", "version": "10.2"}}
{"id": "e849be0f55bcbcbd", "content": "A transforming command that identifies anomalous events by computing a probability for each event and then detecting unusually small probabilities. The probability is defined as the product of the frequencies of each individual field value in the event. For categorical fields, the frequency of a value X is the number of times X occurs divided by the total number of events. For numerical fields, we first build a histogram for all the values, then compute the frequency of a value X as the size of the bin that contains X divided by the number of events. The anomalydetection command includes the capabilities of the existing anomalousvalue and outlier commands and offers a histogram-based approach for detecting anomalies. Note: Use current Splunk machine learning (ML) tools to take advantage of the latest algorithms and get the most powerful results. See About the Splunk Machine Learning Toolkit in the Splunk Machine Learning Toolkit .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "anomalydetection", "section_heading": "Description", "section_id": "id_25ba6c8a_9122_4a97_8322_efaa51140693--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalydetection", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:54.140282+00:00", "version": "10.2"}}
{"id": "2b1d4c2485f3d4a2", "content": "anomalydetection [<method-option>] [<action-option>] [<pthresh-option>] [<cutoff-option>] [<field-list>] Optional arguments <method-option> Syntax: method = histogram | zscore | iqr Description: Select the method of anomaly detection. When method=zscore , performs like the anomalousvalue command. When method=iqr , performs like the outlier command. See Usage. Default: method=histogram <action-option> Syntax for method=histogram or method=zscore: action = filter | annotate | summary Syntax for method=iqr: action = remove | transform Description: The actions and defaults depend on the method that you specify. See the detailed descriptions for the actions for each method below. <pthresh-option> Syntax: pthresh=<num> Description: Used with method=histogram or method=zscore. Sets the probability threshold, as a decimal number, that has to be met for an event to be deemed anomalous. Default: For method=histogram , the command calculates pthresh for each data set during analysis. For method=zscore , the default is 0.01. If you try to use this when method=iqr , it returns an invalid argument error. <cutoff-option> Syntax: cutoff=<bool> Description: Sets the upper bound threshold on the number of anomalies. This option applies to only the histogram method. If cutoff=false , the algorithm uses the formula threshold = 1st-quartile - 1.5 * IRQ without modification. If cutoff=true , the algorithm modifies the formula in order to come up with a smaller number of anomalies. Default: true <field-list> Syntax: <string> <string> ... Description: A list of field names. Histogram actions <action-option> Syntax: action=annotate | filter | summary Description: Specifies whether to return all events with additional fields (annotate), to filter out events with anomalous values (filter), or to return a summary of anomaly statistics (summary). Default: filter When action=filter , the command returns anomalous events and filters out other events. Each returned event contains four new fields. When action=annotate , the command returns all the original events with the same four new fields added when action=filter. When action=summary , the command returns a single event containing six fields. Zscore actions <action-option> Syntax: action=annotate | filter | summary Description: Specifies whether to return the anomaly score (annotate), filter out events with anomalous values (filter), or a summary of anomaly statistics (summary). Default: filter When action=filter , the command returns events with anomalous values while other events are dropped. The kept events are annotated, like the annotate action. When action=annotate , the command adds new fields, Anomaly_Score_Cat(field) and Anomaly_Score_Num(field) , to the events that contain anomalous values. When action=summary , the command returns a table that summarizes the anomaly statistics for each field is generated. The table includes how many events contained this field, the fraction of events that were anomalous, what type of test (categorical or numerical) were performed, and so on. IQR actions <action-option> Syntax: action=remove | transform Description: Specifies what to do with outliers. The remove action removes the event containing the outlying numerical value. The transform action transforms the event by truncating the outlying value to the threshold for outliers. If mark=true , the transform action prefixes the value with \"000\". Abbreviations: The abbreviation for remove is rm. The abbreviation for transform is tf. Default: action=transform", "code_examples": [], "tables": [{"headers": ["Field", "Description"], "rows": [["log_event_prob", "The natural logarithm of the event probability."], ["probable_cause", "The name of the field that best explains why the event is anomalous. No one field causes anomaly by itself, but often some field value occurs too rarely to make the event probability small."], ["probable_cause_freq", "The frequency of the value in the probable_cause field."], ["max_freq", "Maximum frequency for all field values in the event."]]}, {"headers": ["Output field", "Description"], "rows": [["num_anomalies", "The number of anomalous events."], ["thresh", "The event probability threshold that separates anomalous events."], ["max_logprob", "The maximum of all log(event_prob)."], ["min_logprob", "The minimum of all log(event_prob)."], ["1st_quartile", "The first quartile of all log(event_prob)."], ["3rd_quartile", "The third quartile of all log(event_prob)."]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "anomalydetection", "section_heading": "Syntax", "section_id": "fa85f411_c634_4670_ad0b_d3f258a8da53--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalydetection", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:54.140296+00:00", "version": "10.2"}}
{"id": "075b73a0b1cf0c16", "content": "The anomalydetection command is a streaming command command. See Command types. The zscore method When you specify method=zscore , the anomalydetection command performs like the anomalousvalue command. You can specify the syntax components of the anomalousvalue command when you use the anomalydetection command with method=zscore. See the anomalousvalue command. The iqr method When you specify method=iqr , the anomalydetection command performs like the outlier command. You can specify the syntax components of the outlier command when you specify method=iqr with the anomalydetection command. For example, you can specify the outlier options <action>, <mark>, <param>, and <uselower>. See the outlier command.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "anomalydetection", "section_heading": "Usage", "section_id": "id_12f95a7c_cad3_4313_a9a8_5cb5bb335b18--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalydetection", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:54.140302+00:00", "version": "10.2"}}
{"id": "7bf4f645dd014664", "content": "Example 1: Return only anomalous events These two searches return the same results. The arguments specified in the second search are the default values. Example 2: Return a short summary of how many anomalous events are there Return a short summary of how many anomalous events are there and some other statistics such as the threshold value used to detect them. Example 3: Return events with anomalous values This example specifies method=zscore to return anomalous values. The search uses the filter action to filter out events that do not have anomalous values. Events must meet the probability threshold pthresh before being considered an anomalous value. Example 4: Return outliers This example uses the outlier options from the outlier command. The abbreviation tf is used for the transform action in this example.", "code_examples": [{"language": "spl", "code": "... | anomalydetection"}, {"language": "spl", "code": "... | anomalydetection method=histogram action=filter"}, {"language": "spl", "code": "... | anomalydetection action=summary"}, {"language": "spl", "code": "... | anomalydetection method=zscore action=filter pthresh=0.05"}, {"language": "spl", "code": "... | anomalydetection method=iqr action=tf param=4 uselower=truemark=true"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "anomalydetection", "section_heading": "Examples", "section_id": "id_11c06706_93cc_4832_a0e1_801a55792d1b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalydetection", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:54.140308+00:00", "version": "10.2"}}
{"id": "0a105e3605a3f8d3", "content": "analyzefields , anomalies , anomalousvalue , cluster , kmeans , outlier", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "anomalydetection", "section_heading": "See also", "section_id": "id_526ec7f7_21d3_4c0c_a1aa_973079b05fef--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalydetection", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:50:54.140311+00:00", "version": "10.2"}}
{"id": "ae48c2fd613d5d3c", "content": "Use mpreview to get an idea of the kinds of metric time series that are stored in your metrics indexes and to troubleshoot your metrics data. mpreview returns a preview of the raw metric data points in a specified metric index that match a provided filter. By default, mpreview retrieves a target of five metric data points per metric time series from each metrics time-series index file ( .tsidx file) associated with the search. You can change this target amount with the target_per_timeseries argument. By design, mpreview returns metric data points in JSON format. Note: The mpreview command cannot search data that was indexed prior to your upgrade to the 8.0.x version of the Splunk platform. You can use the mpreview command only if your role has the run_msearch capability. See Define roles on the Splunk platform with capabilities in Securing Splunk Enterprise. Note: Certain restricted search commands, including mpreview and mstats might stop working if your organization uses field filters to protect sensitive data. See Plan for field filters in your organization in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "mpreview", "section_heading": "Description", "section_id": "id_87bb7d9b_c795_4461_aac4_f5405b22629b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mpreview", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:11.966040+00:00", "version": "10.2"}}
{"id": "276ed884163ca40c", "content": "The required syntax is in bold. | mpreview [filter=<string>] [<index-opt>]... [splunk_server=<wc-string>] [splunk_server_group=<wc-string>]... [earliest=<time-specifier>] [latest=<time-specifier>] : [chunk_size=<unsigned-integer>] [target_per_timeseries=<unsigned-integer>] Required arguments None. By default all types of terms are returned. Optional arguments chunk_size Syntax: chunk_size=<unsigned-integer> Description: Advanced option. This argument controls how many metric time series are retrieved at a time from a single time-series index file ( .tsidx file) when the Splunk software processes searches. Lower this setting from its default only when you find a particular mpreview search is using too much memory, or when it infrequently returns events. This can happen when a search groups by excessively high-cardinality dimensions (dimensions with very large amounts of distinct values). In such situations, a lower chunk_size value can make mpreview searches more responsive, but potentially slower to complete. A higher chunk_size , on the other hand, can help long-running searches to complete faster, with the potential tradeoff of causing the search to be less responsive. For mpreview , chunk_size cannot be set lower than 10. For more information about this setting, see Use chunk_size to regulate mpreview performance. Default: 1000 Note: For Splunk Enterprise: The default value for the the chunk_size argument is set by the chunk_size setting for the [msearch] stanza in limits.conf. earliest Syntax: earliest=<time-specifier> Description: Specify the earliest _time for the time range of your search. You can specify the time in the following formats: Exact time (in ISO8601 format). For example, earliest=\"2023-09-05T11:52:43-05:00\" or earliest=\"2023-09-05T11:52:43.123-05:00\". Relative time. For example, earliest=-h or earliest=@w0. UNIX time. For example, earliest=1696812434. For more information about setting exact times see Date and time format variables. For more information about setting relative times, see Time modifiers. Subsecond options are available only if you are searching over a metrics index with millisecond timestamp resolution. filter Syntax: filter= \"<string>\" Description: An arbitrary boolean expression over the dimension or metric_name. index-opt Syntax: index=<index-name> (index=<index-name>)... Description: Limits the search to results from one or more indexes. You can use wildcard characters (*). To match non-internal indexes, use index=*. To match internal indexes, use index=_*. latest Syntax: latest=<time-specifier> Description: Specify the latest time for the _time range of your search. You can specify the time in the following formats: Exact time (in ISO8601 format). For example, latest=\"2023-09-12T11:52:43-05:00\" or earliest=\"2023-09-12T11:52:43.123-05:00\". Relative time. For example, latest=-30m or latest=@w6. UNIX time. For example, latest=1699490848. For more information about setting exact times see Date and time format variables. For more information about setting relative times, see Time modifiers. Subsecond options are available only if you are searching over a metrics index with millisecond timestamp resolution. splunk_server Syntax: splunk_server=<wc-string> Description: Specifies the distributed search peer from which to return results. If you are using Splunk Enterprise, you can specify only one splunk_server argument. However, you can use a wildcard when you specify the server name to indicate multiple servers. For example, you can specify splunk_server=peer01 or splunk_server=peer*. Use local to refer to the search head. splunk_server_group Syntax: splunk_server_group=<wc-string> Description: Limits the results to one or more server groups. If you are using Splunk Cloud Platform, omit this parameter. You can specify a wildcard character in the string to indicate multiple server groups. target_per_timeseries Syntax: target_per_timeseries=<unsigned-integer> Description Determines the target number of metric data points to retrieve per metric time series from each metrics time-series index file ( .tsidx file) associated with the mpreview search. If a time series has less than the target_per_timeseries of data points within a .tsidx file, the search head retrieves all of the data points for that time series within that particular .tsidx file. CAUTION: If you set target_per_timeseries to 0 it returns all data points available within the given time range for each time series. This search will likely be very large in scale and therefore very slow to complete. If you must search on a large number of metric data points, use mstats instead. For more information about this setting, see How the target_per_timeseries argument works. Default: 5 Note: The default value for the the target_per_timeseries argument is set by the target_per_timeseries setting for the [msearch] stanza in limits.conf", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "mpreview", "section_heading": "Syntax", "section_id": "id_66a013ed_608f_4a38_a8a9_5d4c26731326--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mpreview", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:11.966047+00:00", "version": "10.2"}}
{"id": "27fd12aeebffce2b", "content": "This search command generates a list of individual metric data points from a specified metric index that match a provided filter. The filter can be any arbitrary boolean expression over the dimensions or the metric_name. Specify earliest and latest to override the time range picker settings. For more information about setting earliest and latest , see Time modifiers. The mpreview command is designed to display individual metric data points in JSON format. If you want to aggregate metric data points, use the mstats command. Note: All metrics search commands are case sensitive. This means, for example, that mpreview treats as the following as three distinct values of metric_name : cap.gear , CAP.GEAR , and Cap.Gear. How the target_per_timeseries argument works Unfiltered mpreview searches can cover extremely large numbers of raw metric data points. In some cases the sheer number of data points covered by the search can cause such searches to be slow or unresponsive. The target_per_timeseries argument makes the mpreview command more responsive while giving you a relatively broad preview of your metric data. It limits the number of metric data points that mpreview can return from each metric time series in each .tsidx file covered by the search. For example, if you have 10 metrics tsidx files that each contain 100 metric time series, and each time series has >=5 data points. If you set target_per_timeseries=5 in the search, you should expect a maximum of 10 x 100 x 5 = 5000 metric data points to be returned by the search. On the other hand, say you have 10 metrics tsidx files that each contain 100 metric time series, but in this case, 50 of those time series have 3 data points and the other 50 of those time series have >=5 data points. If you set target_per_timeseries=5 in the search, you should expect to get 10 x ((50 x 3) + (50 x 5)) = 4000 data points. Note: The target_per_timeseries argument is especially useful when the number of metric data points covered by your mpreview search is significantly larger than the number of metric time series covered by the search. It's not particularly helpful if the number of data points in your search are slightly larger than or equal to the number of metric time series in the search. You can run this search to determine the number of metric data points that could potentially be covered by an mpreview search: You can run this search to determine the number of metric time series that could potentially be covered by an mpreview search: Use chunk_size to regulate mpreview performance If you find that mpreview is slow or unresponsive despite the target_per_timeseries argument you can also use chunk_size to regulate mpreview behavior. Reduce the chunk_size to make the search more responsive with the potential tradeoff of making the search slower to complete. Raise the chunk_size to help the mpreview search to complete faster, with the potential tradeoff of making it less responsive.", "code_examples": [{"language": "spl", "code": "| metadata index=<metric_index_name>type=hosts datatype=metric | fields totalCount"}, {"language": "spl", "code": "| mstats count(*) WHERE index=<metric_index_name>  by _timeseries | stats count"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "mpreview", "section_heading": "Usage", "section_id": "id_843e9090_4027_48c6_920b_423cc825055b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mpreview", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:11.966052+00:00", "version": "10.2"}}
{"id": "a047757f3e384d3b", "content": "1. Return data points that match a specific filter This search returns individual data points from the _metrics index that match a specific filter. Here is an example of a JSON-formatted result of the above search. 2. Return individual data points from the metrics index 3. Lower chunk_size to improve mpreview performance The following search lowers chunk_size so that it returns 100 metric time series worth of metric data points in batches from tsidx files that belong to the _metrics index. Ordinarily it would return 1000 metric time series in batches. 4. Speed up an mpreview search with target_per_timeseries The following search uses target_per_timeseries to return a maximum of five metric data points per time series in each tsidx file searched in the _metrics index.", "code_examples": [{"language": "spl", "code": "| mpreview index=_metrics filter=\"group=queue name=indexqueue metric_name=*.current_size\""}, {"language": "spl", "code": "| mpreview index=_metrics"}, {"language": "spl", "code": "| mpreview index=_metrics chunk_size=100"}, {"language": "spl", "code": "| mpreview index=_metrics target_per_timeseries=5"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "mpreview", "section_heading": "Examples", "section_id": "id_8b588207_a374_4de8_ac86_f94510a57af1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mpreview", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:11.966056+00:00", "version": "10.2"}}
{"id": "bc28d417fffd5920", "content": "Commands mcatalog mcollect mstats", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "mpreview", "section_heading": "See also", "section_id": "e3058fb5_db36_46ec_be2c_a86c82e48bd4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mpreview", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:11.966060+00:00", "version": "10.2"}}
{"id": "4486e9f09658d43a", "content": "Use the geomfilter command to specify points of a bounding box for clipping choropleth maps. For more information about choropleth maps, see \"Mapping data\" in the Dashboards and Visualizations Manual.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "geomfilter", "section_heading": "Description", "section_id": "id_4ea22953_8200_44f0_b49d_6d92decd8eee--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geomfilter", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:28.941171+00:00", "version": "10.2"}}
{"id": "967eceb2fd57e4ec", "content": "geomfilter [min_x=<float>] [min_y=<float>] [max_x=<float>] [max_y=<float>] Optional arguments min_x Syntax: min_x=<float> Description: The x coordinate of the bounding box's bottom-left corner, in the range [-180, 180]. Default: -180 min_y Syntax: min_y=<float> Description: The y coordinate of the bounding box's bottom-left corner, in the range [-90, 90]. Default: -90 max_x Syntax: max_x=<float> Description: The x coordinate of the bounding box's up-right corner, in the range [-180, 180]. Default: 180 max_y Syntax: max_y=<float> Description: The y coordinate of the bounding box's up-right corner, in the range [-90, 90]. Default: max_y=90", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "geomfilter", "section_heading": "Syntax", "section_id": "id_4dd4a42e_0e20_4d9c_898c_1a72a7dcca91--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geomfilter", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:28.941179+00:00", "version": "10.2"}}
{"id": "bf715a5f47dcb369", "content": "The geomfilter command accepts two points that specify a bounding box for clipping choropleth maps. Points that fall outside of the bounding box will be filtered out.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "geomfilter", "section_heading": "Usage", "section_id": "id_5c6a0597_7cfb_40ad_a592_6bc2bc8494e5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geomfilter", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:28.941185+00:00", "version": "10.2"}}
{"id": "58b9932eaed5c877", "content": "Example 1: This example uses the default bounding box, which will clip the entire map. Example 2: This example clips half of the whole map.", "code_examples": [{"language": "spl", "code": "...| geomfilter"}, {"language": "spl", "code": "...| geomfilter min_x=-90 min_y=-90 max_x=90 max_y=90"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "geomfilter", "section_heading": "Examples", "section_id": "d3577916_9592_4639_a2e6_e8c64e41b913--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geomfilter", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:28.941190+00:00", "version": "10.2"}}
{"id": "26bcc1dac46f69bd", "content": "geom", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "geomfilter", "section_heading": "See also", "section_id": "a17faee9_d27b_48a8_a515_64b9e3f25a57--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/geomfilter", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:28.941195+00:00", "version": "10.2"}}
{"id": "000b51c874102dbe", "content": "Use the search command to retrieve events from indexes or filter the results of a previous search command in the pipeline. You can retrieve events from your indexes, using keywords, quoted phrases, wildcards, and field-value expressions. The search command is implied at the beginning of any search. You do not need to specify the search command at the beginning of your search criteria. You can also use the search command later in the search pipeline to filter the results from the previous command in the pipeline. The search command can also be used in a subsearch. See about subsearches in the Search Manual. After you retrieve events , you can apply commands to transform, filter, and report on the events. Use the vertical bar ( | ) , or pipe character, to apply a command to the retrieved events. The search command supports IPv4 and IPv6 addresses and subnets that use CIDR notation.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "search", "section_heading": "Description", "section_id": "id_4ba71b85_00e0_4451_84d6_65048bb1a53f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/search", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:45.390783+00:00", "version": "10.2"}}
{"id": "e1e003993be13876", "content": "search <logical-expression> Required arguments <expression> Syntax: <logical-expression> | <time-opts> | <search-modifier> | NOT <logical-expression> | <index-expression> | <comparison-expression> | <logical-expression> [OR] <logical-expression> Description: Includes all keywords or field-value pairs used to describe the events to retrieve from the index. Include parenthesis as necessary. Use Boolean expressions, comparison operators, time modifiers, search modifiers, or combinations of expressions for this argument. The AND operator is always implied between terms and expressions. For example, web error is the same as web AND error. Specifying clientip=192.0.2.255 earliest=-1h@h is the same as clientip=192.0.2.255 AND earliest=-1h@h. So unless you want to include it for clarity reasons, you do not need to specify the AND operator. Logical expression options <comparison-expression> Syntax: <field><comparison-operator><value> | <field> IN (<value-list>) Description: Compare a field to a literal value or provide a list of values that can appear in the field. <index-expression> Syntax: \"<string>\" | <term> | <search-modifier> Description: Describe the events you want to retrieve from the index using literal strings and search modifiers. <time-opts> Syntax: [<timeformat>] (<time-modifier>)... Description: Describe the format of the starttime and endtime terms of the search. See Time options. Comparison expression options <comparison-operator> Syntax: = | != | < | <= | > | >= Description: You can use comparison operators when searching field/value pairs. Comparison expressions with the equal ( = ) or not equal ( != ) operator compare string values. For example, \"1\" does not match \"1.0\". Comparison expressions with greater than or less than operators < > <= >= numerically compare two numbers and lexicographically compare other values. See Usage. <field> Syntax: <string> Description: The name of a field. <value> Syntax: <literal-value> Description: In comparison-expressions, the literal number or string value of a field. <value-list> Syntax: (<literal-value>, <literal-value>, ...) Description: Used with the IN operator to filter events by specifying two or more values. For example use error IN (400, 402, 404, 500) instead of error=400 OR error=402 OR error=404 OR error=500. You can also use a wildcard character ( * ) to specify values that are similar, such as error IN (40*). See the \"Multiple field-value comparisons with the IN operator\" section in Usage. Index expression options <string> Syntax: \"<string>\" Description: Specify keywords or quoted phrases to match. When searching for strings and quoted strings (anything that's not a search modifier), Splunk software searches the _raw field for the matching events or results. <search-modifier> Syntax: <sourcetype-specifier> | <host-specifier> | <hosttag-specifier> | <source-specifier> | <savedsplunk-specifier> | <eventtype-specifier> | <eventtypetag-specifier> | <splunk_server-specifier> Description: Search for events from specified fields or field tags. For example, search for one or a combination of hosts, sources, source types, saved searches, and event types. Also, search for the field tag, with the format: tag::<field>=<string>. Read more about searching with default fields in the Knowledge Manager manual. Read more about using tags and field aliases in the Knowledge Manager manual. <sourcetype-specifier> Syntax: sourcetype=<string> Description: Search for events from the specified sourcetype field. <host-specifier> Syntax: host=<string> Description: Search for events from the specified host field. <hosttag-specifier> Syntax: hosttag=<string> Description: Search for events that have hosts that are tagged by the string. <eventtype-specifier> Syntax: eventtype=<string> Description: Search for events that match the specified event type. <eventtypetag-specifier> Syntax: eventtypetag=<string> Description: Search for events that would match all eventtypes tagged by the string. <savedsplunk-specifier> Syntax: savedsearch=<string> | savedsplunk=<string> Description: Search for events that would be found by the specified saved search. <source-specifier> Syntax: source=<string> Description: Search for events from the specified source field. <splunk_server-specifier> Syntax: splunk_server=<string> Description: Search for events from a specific server. Use \"local\" to refer to the search head. Time options For a list of time modifiers, see Time modifiers for search. <timeformat> Syntax: timeformat=<string> Description: Set the time format for starttime and endtime terms. Default: timeformat=%m/%d/%Y:%H:%M:%S. <time-modifier> Syntax: starttime=<string> | endtime=<string> | earliest=<time_modifier> | latest=<time_modifier> Description: Specify start and end times using relative or absolute time. Note: You can also use the earliest and latest attributes to specify absolute and relative time ranges for your search. For more about this time modifier syntax, see Specify time modifiers in your search in the Search Manual. starttime Syntax: starttime=<string> Description: Events must be later or equal to this time. Must match timeformat. endtime Syntax: endtime=<string> Description: All events must be earlier or equal to this time.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "search", "section_heading": "Syntax", "section_id": "id_2a75b3dc_f9a6_4111_a69d_a89901208920--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/search", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:45.390790+00:00", "version": "10.2"}}
{"id": "2312d707f8c28ae9", "content": "The search command is an event-generating command when it is the first command in the search, before the first pipe. When the search command is used further down the pipeline, it is a distributable streaming command. See Command types. A subsearch can be initiated through a search command such as the search command. See Initiating subsearches with search commands in the Splunk Cloud Platform Search Manual. The implied search command The search command is implied at the beginning of every search. When search is the first command in the search, you can use terms such as keywords, phrases, fields, boolean expressions, and comparison expressions to specify exactly which events you want to retrieve from Splunk indexes. If you don't specify a field, the search looks for the terms in the the _raw field. Some examples of search terms are: keywords: error login , which is the same as specifying for error AND login quoted phrases: \"database error\" boolean operators: login NOT (error OR fail) wildcards: fail* field-value pairs: status=404, status!=404, or status>200 Note: To search field values that are SPL operators or keywords, such as country=IN , country=AS , iso=AND , or state=OR , you must enclose the operator or keyword in quotation marks. For example: country=\"IN\". See Use the search command in the Search Manual. Using the search command later in the search pipeline In addition to the implied search command at the beginning of all searches, you can use the search command later in the search pipeline. The search terms that you can use depend on which fields are passed into the search command. If the _raw field is passed into the search command, you can use the same types of search terms as you can when the search command is the first command in a search. However, if the _raw field is not passed into the search command, you must specify field-values pairs that match the fields passed into the search command. Transforming commands, such as stats and chart , do not pass the _raw field to the next command in the pipeline. Boolean expressions The order in which Boolean expressions are evaluated with the search command is: Expressions within parentheses NOT clauses OR clauses AND clauses This evaluation order is different than the order used with the eval and where commands, which evaluate AND before OR clauses. The search command doesn't support XOR. See Boolean expressions with logical operators in the Splunk platform Search Manual. Comparing two fields To compare two fields, do not specify index=myindex fieldA=fieldB or index=myindex fieldA!=fieldB with the search command. When specifying a comparison_expression, the search command expects a <field> compared with a <value>. The search command interprets fieldB as the value, and not as the name of a field. Use the where command to compare two fields. For not equal comparisons, you can specify the criteria in several ways. or See Difference between NOT and != in the Search Manual. Filter using the IN operator Use the IN operator when you want to determine if a field contains one of several values. For example, use this syntax: Instead of this syntax: When used with the search command, you can use a wildcard character ( * ) in the list of values for the IN operator. For example: You can use the NOT operator with the IN operator. For example: There is also an IN function that you can use with the eval and where commands. Wild card characters are not allowed in the values list when the IN function is used with the eval and where commands. See Comparison and Conditional functions. CIDR matching The search command can perform a CIDR match on a field that contains IPv4 and IPv6 addresses. Suppose the ip field contains these values: 10.10.10.12 50.10.10.17 10.10.10.23 If you specify ip=\"10.10.10.0/24\" , the search returns the events with the first and last values: 10.10.10.12 and 10.10.10.23. Lexicographical order Lexicographical order sorts items based on the values used to encode the items in computer memory. In Splunk software, this is almost always UTF-8 encoding, which is a superset of ASCII. Numbers are sorted before letters. Numbers are sorted based on the first digit. For example, the numbers 10, 9, 70, 100 are sorted lexicographically as 10, 100, 70, 9. Uppercase letters are sorted before lowercase letters. Symbols are not standard. Some symbols are sorted before numeric values. Other symbols are sorted before or after letters. You can specify a custom sort order that overrides the lexicographical order. See the blog Order Up! Custom Sort Orders. Quotes and escaping characters In general, you need quotation marks around phrases and field values that include white spaces, commas, pipes, quotations, and brackets. Quotation marks must be balanced. An opening quotation must be followed by an unescaped closing quotation. For example: A search such as error | stats count will find the number of events containing the string error. A search such as ... | search \"error | stats count\" would return the raw events containing error, a pipe, stats, and count, in that order. Additionally, use quotation marks around keywords and phrases if you don't want to search for their default meaning, such as Boolean operators and field/value pairs. For example: A search for the keyword AND without meaning the Boolean operator: error \"AND\" A search for this field/value phrase: error \"startswith=foo\" The backslash character (&nbsp\\&nbsp) is used to escape quotes, pipes, and the backslash character itself. Backslash escape sequences are expanded inside quotation marks. For example: The sequence \\| as part of a search sends a pipe character to the command, instead of using the pipe as a split between commands. The sequence \\\" sends a literal quotation mark to the command. For example, this is useful if you want to search for a literal quotation mark or insert a literal quotation mark into a field using regular expressions. The \\\\ sequence sends a literal backslash to the command. Unrecognized backslash sequences are not altered: For example, \\s in a search string is available as \\s to the command, because \\s is not a known escape sequence. However, the search string \\\\s is available as \\s to the command, because \\\\ is a known escape sequence that is converted to \\. See Backslashes in the Search Manual. Search with TERM() You can use the TERM() directive to force Splunk software to match whatever is inside the parentheses as a single term in the index. TERM is more useful when the term contains minor segmenters, such as periods, and is bounded by major segmenters, such as spaces or commas. In fact, TERM does not work for terms that are not bounded by major breakers. See Use CASE and TERM to match phrases in the Search Manual. Search with CASE() You can use the CASE() directive to search for terms and field values that are case-sensitive. See Use CASE and TERM to match phrases in the Search Manual .", "code_examples": [{"language": "spl", "code": "index=myindex |wherefieldA=fieldB"}, {"language": "spl", "code": "index=myindex |wherefieldA!=fieldB"}, {"language": "spl", "code": "index=myindex |whereNOT fieldA=fieldB"}, {"language": "spl", "code": "... error_code IN (400, 402, 404, 500) | ..."}, {"language": "spl", "code": "... error_code=400 OR error_code=402 OR error_code=404 OR error_code=500 | ..."}, {"language": "spl", "code": "... error_code IN (40*, 500) | ..."}, {"language": "spl", "code": "... NOT clientip IN (211.166.11.101, 182.236.164.11, 128.241.220.82) | ..."}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "search", "section_heading": "Usage", "section_id": "id_6df4df05_8fdf_48fc_9c96_97915b2eea74--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/search", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:45.390795+00:00", "version": "10.2"}}
{"id": "7101e2428758e790", "content": "These examples demonstrate how to use the search command. You can find more examples in the Start Searching topic of the Search Tutorial. 1. Field-value pair matching This example demonstrates field-value pair matching for specific values of source IP (src) and destination IP (dst). 2. Using boolean and comparison operators This example demonstrates field-value pair matching with boolean and comparison operators. Search for events with code values of either 10 or 29, and any host that isn't \"localhost\", and an xqp value that is greater than 5. In this example you could also use the IN operator since you are specifying two field-value pairs on the same field. The revised search is: 3. Using wildcards This example demonstrates field-value pair matching with wildcards. Search for events from all the web servers that have an HTTP client or server error status. In this example you could also use the IN operator since you are specifying two field-value pairs on the same field. The revised search is: 4. Using the IN operator This example shows how to use the IN operator to specify a list of field-value pair matchings. In the events from an access.log file, search the action field for the values addtocart or purchase. 5. Specifying a secondary search This example uses the search command twice. The search command is implied at the beginning of every search with the criteria eventtype=web-traffic. The search command is used again later in the search pipeline to filter out the results. This search defines a web session using the transaction command and searches for the user sessions that contain more than three events. 6. Using the NOT or != comparisons Searching with the boolean \"NOT\"comparison operator is not the same as using the \"!=\" comparison. The following search returns everything except fieldA=\"value2\", including all other fields. The following search returns events where fieldA exists and does not have the value \"value2\". If you use a wildcard for the value, NOT fieldA=* returns events where fieldA is null or undefined, and fieldA!=* never returns any events. See Difference between NOT and != in the Search Manual. 7. Using search to perform CIDR matching You can use the search command to match IPv4 and IPv6 addresses and subnets that use CIDR notation. For example, this search identifies whether the specified IPv4 address is located in the subnet. The IP address is located in the subnet, so search displays it in the search results, which look like this. Note that you can get identical results using the eval command with the cidrmatch(\"X\",Y) function, as shown in this example. Alternatively, if you're using IPv6 addresses, you can use the search command to identify whether the specified IPv6 address is located in the subnet. The IP address is in the subnet, so the search results look like this. See also Commands iplocation lookup Functions cidrmatch", "code_examples": [{"language": "spl", "code": "src=\"10.9.165.*\"OR dst=\"10.9.165.8\""}, {"language": "spl", "code": "(code=10 OR code=29) host!=\"localhost\"xqp>5"}, {"language": "spl", "code": "code IN(10, 29) host!=\"localhost\"xqp>5"}, {"language": "spl", "code": "host=webserver* (status=4* OR status=5*)"}, {"language": "spl", "code": "host=webserver* status IN(4*, 5*)"}, {"language": "spl", "code": "sourcetype=access_combined_wcookie action IN (addtocart, purchase)"}, {"language": "spl", "code": "eventtype=web-traffic | transaction clientip startswith=\"login\"endswith=\"logout\"| search eventcount>3"}, {"language": "spl", "code": "NOT fieldA=\"value2\""}, {"language": "spl", "code": "fieldA!=\"value2\""}, {"language": "spl", "code": "| makeresults \n|evalip=\"192.0.2.56\"| search ip=\"192.0.2.0/24\""}, {"language": "spl", "code": "| makeresults \n|evalip=\"192.0.2.56\"|wherecidrmatch(\"192.0.2.0/24\", ip)"}, {"language": "spl", "code": "| makeresults \n|evalip=\"2001:0db8:ffff:ffff:ffff:ffff:ffff:ff99\"| search ip=\"2001:0db8:ffff:ffff:ffff:ffff:ffff:ff00/120\""}], "tables": [{"headers": ["time", "ip"], "rows": [["2020-11-19 16:43:31", "192.0.2.56"]]}, {"headers": ["time", "ip"], "rows": [["2020-11-19 16:43:31", "2001:0db8:ffff:ffff:ffff:ffff:ffff:ff99"]]}], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "search", "section_heading": "Examples", "section_id": "id_42410c6c_9738_46eb_87d4_2da4221e097a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/search", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:51:45.390801+00:00", "version": "10.2"}}
{"id": "fab3662f35cc3c19", "content": "Prevents subsequent commands from being executed on remote peers. Tells the search to run subsequent commands locally, instead. The localop command forces subsequent commands to be part of the reduce step of the mapreduce process.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "localop", "section_heading": "Description", "section_id": "c34c2d99_da5b_4ba1_adc2_70afa1bf6298--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/localop", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:02.322664+00:00", "version": "10.2"}}
{"id": "91c082e1616585e6", "content": "localop", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "localop", "section_heading": "Syntax", "section_id": "b2c001ef_4e14_404b_a7a2_005f53138718--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/localop", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:02.322671+00:00", "version": "10.2"}}
{"id": "3df4d2b6a0b0244a", "content": "Example 1: The iplocation command in this case will never be run on remote peers. All events from remote peers that originate from the initial search, which was for the terms FOO and BAR, are forwarded to the search head. The search head is where the iplocation command is run.", "code_examples": [{"language": "spl", "code": "FOO BAR | localop | iplocation clientip"}], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "localop", "section_heading": "Examples", "section_id": "id_18383c93_f19d_4861_8fae_b2c0668f5ee3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/localop", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:02.322677+00:00", "version": "10.2"}}
{"id": "2a7f8afe4d31a57e", "content": "Calculates the correlation between different fields. You can use the correlate command to see an overview of the co-occurrence between fields in your data. The results are presented in a matrix format, where the cross tabulation of two fields is a cell value. The cell value represents the percentage of times that the two fields exist in the same events. The field the result is specific to is named in the value of the RowField field, while the fields it is compared against are the names of the other fields. Note: This command looks at the relationship among all the fields in a set of search results. If you want to analyze the relationship between the values of fields, refer to the contingency command, which counts the co-ocurrence of pairs of field values in events.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "correlate", "section_heading": "Description", "section_id": "id_547b1a1d_1e68_4fe3_a04e_707813a1a4f9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/correlate", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:20.310154+00:00", "version": "10.2"}}
{"id": "a70a85841648d563", "content": "correlate", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "correlate", "section_heading": "Syntax", "section_id": "id_49902719_176a_42df_b228_48e7fb2ecf3e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/correlate", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:20.310161+00:00", "version": "10.2"}}
{"id": "77ee150d3d73bcb1", "content": "There is a limit on the number of fields that correlate considers in a search. From limits.conf, stanza [correlate], the maxfields sets this ceiling. The default is 1000. If more than this many fields are encountered, the correlate command continues to process data for the first N (eg thousand) field names encountered, but ignores data for additional fields. If this occurs, the notification from the search or alert contains a message \"correlate: input fields limit (N) reached. Some fields may have been ignored.\" As with all designed-in limits, adjusting this might have significant memory or cpu costs.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "correlate", "section_heading": "Limits", "section_id": "id_72e2426a_480f_435e_8f5d_60b579de02f4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/correlate", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:20.310165+00:00", "version": "10.2"}}
{"id": "a938ed574823e940", "content": "Example 1: Look at the co-occurrence between all fields in the _internal index. Here is a snapshot of the results. Because there are different types of logs in the _internal , you can expect to see that many of the fields do not co-occur. Example 2: Calculate the co-occurrences between all fields in Web access events. You expect all Web access events to share the same fields: clientip, referer, method, and so on. But, because the sourcetype=access_* includes both access_common and access_combined Apache log formats, you should see that the percentages of some of the fields are less than 1.0. Example 3: Calculate the co-occurrences between all the fields in download events. The more narrow your search is before you pass the results into correlate , the more likely it is that all the field value pairs have a correlation of 1.0. A correlation of 1.0 means the values co-occur in 100% of the search results. For these download events, you might be able to spot an issue depending on which pairs have less than 1.0 co-occurrence.", "code_examples": [{"language": "spl", "code": "index=_internal | correlate"}, {"language": "spl", "code": "sourcetype=access_* | correlate"}, {"language": "spl", "code": "eventtype=download | correlate"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "correlate", "section_heading": "Examples", "section_id": "b46495ac_2dc6_4852_bc15_38f66da2f7ce--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/correlate", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:20.310169+00:00", "version": "10.2"}}
{"id": "1c90740672b0b3f1", "content": "associate , contingency", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "correlate", "section_heading": "See also", "section_id": "cbac9e16_9ab2_42d5_97a7_8ed6fde54711--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/correlate", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:20.310172+00:00", "version": "10.2"}}
{"id": "55bffc50105f192f", "content": "Creates an eventtype field for search results that match known event types. You must create event types to use this command. See About event types in the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "typer", "section_heading": "Description", "section_id": "id_6d7b8c48_bd4d_4b9e_bfa9_81697ed94b45--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typer", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:37.613128+00:00", "version": "10.2"}}
{"id": "f751f81041bd5390", "content": "The required syntax is in bold. typer [eventtypes=<string>] [maxlen=<unsigned_integer>] Required arguments None. Optional arguments eventtypes Syntax: eventtypes=<string> Description: Provide a comma-separated list of event types to filter the set of event types that typer can return in the eventtype field. The eventtypes argument filters out all event types except the valid event types in its list. If all of the event types listed for eventtypes are invalid, or if no event types are listed, typer is turned off and will not return any event types. The eventtypes argument accepts wildcards. Default: No default (by default typer returns all available event types) maxlen Syntax: maxlen=<unsigned_integer> Description: By default, the typer command looks at the first 10000 characters of an event to determine its event type. Use maxlen to override this default. For example, maxlen=300 restricts typer to determining event types from the first 300 characters of events.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "typer", "section_heading": "Syntax", "section_id": "id_054e194c_522f_4948_a5f6_8330a4069773--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typer", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:37.613137+00:00", "version": "10.2"}}
{"id": "4d0e16b3e5b60961", "content": "The typer command is a distributable streaming command. See Command types. Changing the default for maxlen Users with file system access, such as system administrators, can change the default setting for maxlen. Splunk Cloud Platform To change the maxlen default setting, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Enterprise To change the maxlen default setting, follow these steps. Prerequisites Only users with file system access, such as system administrators, can change the maxlen default setting using configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps Open or create a local limits.conf file for the Search app at $SPLUNK_HOME/etc/apps/search/local. Under the [typer] stanza, specify the default for the maxlen setting.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "typer", "section_heading": "Usage", "section_id": "id_3c7ea99d_5021_4868_8fdf_d8ec4830723f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typer", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:37.613143+00:00", "version": "10.2"}}
{"id": "321a4c578df6946b", "content": "Example 1: Returns a field called eventtype which lists the names of the event types associated with the search results.", "code_examples": [{"language": "spl", "code": "... | typer"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "typer", "section_heading": "Examples", "section_id": "e0a779b1_1642_4835_a12f_00f5562b7837--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typer", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:37.613148+00:00", "version": "10.2"}}
{"id": "a07f0879fe711a30", "content": "Commands findtypes Related information About event types in the Knowledge Manager Manual", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "typer", "section_heading": "See also", "section_id": "id_6c40d176_0e47_4102_9e5f_f77b72709502--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/typer", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:37.613153+00:00", "version": "10.2"}}
{"id": "0c050f875c7aac56", "content": "If you have Splunk Enterprise, this command saves search results to the specified CSV file on the local search head in the $SPLUNK_HOME/var/run/splunk/csv directory. Updates to $SPLUNK_HOME/var/run/*.csv using the outputcsv command are not replicated across the cluster. If you have Splunk Cloud Platform, you cannot use this command. Instead, you have these options: Export search results using Splunk Web. See Export data using Splunk Web in the Search Manual. Export search results using REST API. See Export data using the REST APIs in the Search Manual. Create an alert action that includes a CSV file as an email attachment. See Email notification action in the Alerting Manual. CAUTION: This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "outputcsv", "section_heading": "Description", "section_id": "id_9434e55a_5996_4941_9b6c_a3f556864a76--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputcsv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:53.128842+00:00", "version": "10.2"}}
{"id": "4aa4b497d438b5a6", "content": "outputcsv [append=<bool>] [create_empty=<bool>] [override_if_empty=<bool>] [dispatch=<bool>] [usexml=<bool>] [singlefile=<bool>] [<filename>] Optional arguments append Syntax: append=<bool> Description: If append is true, the command attempts to append to an existing CSV file, if the file exists. If the CSV file does not exist, a file is created. If there is an existing file that has a CSV header already, the command only emits the fields that are referenced by that header. The command cannot append to .gz files. Default: false create_empty Syntax: create_empty=<bool> Description: If set to true and there are no results, a zero-length file is created. When set to false and there are no results, no file is created. If the file previously existed, the file is deleted. Default: false dispatch Syntax: dispatch=<bool> Description: If set to true, refers to a file in the job directory in $SPLUNK_HOME/var/run/splunk/dispatch/<job id>/. filename Syntax: <filename> Description: Specify the name of a CSV file to write the search results to. This file should be located in $SPLUNK_HOME/var/run/splunk/csv. Directory separators are not permitted in the filename. If no filename is specified, the command rewrites the contents of each result as a CSV row into the _xml field. Otherwise the command writes into a file. The .csv file extension is appended to the filename if the filename has no file extension. override_if_empty Syntax: override_if_empty=<bool> Description: If override_if_empty=true and no results are passed to the output file, the existing output file is deleted, If override_if_empty=false and no results are passed to the output file, the command does not delete the existing output file. Default: true singlefile Syntax: singlefile=<bool> Description: If singlefile is set to true and the output spans multiple files, collapses it into a single file. Default: true usexml Syntax: usexml=<bool> Description: If there is no filename, specifies whether or not to encode the CSV output into XML. This option should not be used when invoking the outputcsv from the UI.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "outputcsv", "section_heading": "Syntax", "section_id": "id_390804bc_f3ec_4203_9ba7_58bf943f0fd2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputcsv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:53.128849+00:00", "version": "10.2"}}
{"id": "dfcbca63b1999c17", "content": "There is no limit to the number of results that can be saved to the CSV file. Internal fields and the outputcsv command When the outputcsv command is used there are internal fields that are automatically added to the CSV file. The internal fields that are added to the output in the CSV file are: _raw _time _indextime _serial _sourcetype _subsecond To exclude internal fields from the output, use the fields command and specify the fields that you want to exclude. For example: Multivalued fields The outputcsv command merges values in a multivalued field into single space-delimited value. Distributed deployments The outputcsv command is not compatible with search head pooling and search head clustering. The command saves the *.csv file on the local search head in the $SPLUNK_HOME/var/run/splunk/ directory. The *.csv files are not replicated on the other search heads.", "code_examples": [{"language": "spl", "code": "... | fields - _indextime _sourcetype _subsecond _serial | outputcsv MyTestCsvFile"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "outputcsv", "section_heading": "Usage", "section_id": "id_7e655502_2554_47d8_a989_87bfd12e73d6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputcsv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:53.128853+00:00", "version": "10.2"}}
{"id": "2bf4b74aeeee38c8", "content": "1. Output search results to a CSV file Output the search results to the mysearch.csv file. The CSV file extension is automatically added to the file name if you don't specify the extension in the search. 2. Add a dynamic timestamp to the file name You can add a timestamp to the file name by using a subsearch. 3. Exclude internal fields from the output CSV file You can exclude unwanted internal fields from the output CSV file. In this example, the fields to exclude are _indextime , _sourcetype , _subsecond , and _serial. 4. Do not delete the CSV file if no search results are returned Output the search results to the mysearch.csv file if results are returned from the search. Do not delete the mysearch.csv file if no results are returned.", "code_examples": [{"language": "spl", "code": "... | outputcsv mysearch"}, {"language": "spl", "code": "... | outputcsv [stats count |evalsearch=strftime(now(),\"mysearch-%y%m%d-%H%M%S.csv\")]"}, {"language": "spl", "code": "index=_internal sourcetype=\"splunkd\"| head 5 | fields _raw _time | fields - _indextime _sourcetype _subsecond _serial | outputcsv MyTestCsvfile"}, {"language": "spl", "code": "... | outputcsv mysearch.csv override_if_empty=false"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "outputcsv", "section_heading": "Examples", "section_id": "bab1a1df_78bf_4660_a639_9c55e78d79a5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputcsv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:53.128858+00:00", "version": "10.2"}}
{"id": "7a2d266baf88b6d2", "content": "inputcsv", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "outputcsv", "section_heading": "See also", "section_id": "id_4080a939_90ac_4355_807a_3615a8145753--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputcsv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:52:53.128861+00:00", "version": "10.2"}}
{"id": "587b8eefe592ae7c", "content": "The addtotals command computes the arithmetic sum of all numeric fields for each search result. The results appear in the Statistics tab. You can specify a list of fields that you want the sum for, instead of calculating every numeric field. The sum is placed in a new field. If col=true , the addtotals command computes the column totals, which adds a new result at the end that represents the sum of each field. labelfield , if specified, is a field that will be added to this summary event with the value set by the 'label' option. Alternately, instead of using the addtotals col=true command, you can use the addcoltotals command to calculate a summary event.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "addtotals", "section_heading": "Description", "section_id": "e67ff6f8_3433_415c_bb27_d0261a899e64--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addtotals", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:10.369711+00:00", "version": "10.2"}}
{"id": "50014ec227082f85", "content": "addtotals [row=<bool>] [col=<bool>] [labelfield=<field>] [label=<string>] [fieldname=<field>] [<field-list>] Required arguments None. Optional arguments field-list Syntax: <field> ... Description: One or more numeric fields, delimited with a space. Only the fields specified in the <field-list> are summed. If a <field-list> is not specified, all numeric fields are included in the sum. Usage: You can use wildcards in the field names. For example, if the field names are count1 , count2 , and count3 you can specify count* to indicate all fields that begin with 'count'. Default: All numeric fields are included in the sum. row Syntax: row=<bool> Description: Specifies whether to calculate the sum of the <field-list> for each event. This is similar to calculating a total for each row in a table. The sum is placed in a new field. The default name of the field is Total. If you want to specify a different name for the field, use the fieldname argument. Usage: Because the default is row=true , specify the row argument only when you do not want the event totals to appear row=false. Default: true col Syntax: col=<bool> Description: Specifies whether to add a new event, referred to as a summary event, at the bottom of the list of events. The summary event displays the sum of each field in the events, similar to calculating column totals in a table. Default: false fieldname Syntax: fieldname=<field> Description: Used to specify the name of the field that contains the calculated sum of the field-list for each event. The fieldname argument is valid only when row=true. Default: Total labelfield Syntax: labelfield=<field> Description: Used to specify a field for the summary event label. The labelfield argument is valid only when col=true. To use an existing field in your result set, specify the field name for the labelfield argument. For example if the field name is IP , specify labelfield=IP. If there is no field in your result set that matches the labelfield , a new field is added using the labelfield value. Default: none label Syntax: label=<string> Description: Used to specify a row label for the summary event. If the labelfield argument is an existing field in your result set, the label value appears in that row in the display. If the labelfield argument creates a new field, the label appears in the new field in the summary event row. Default: Total", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "addtotals", "section_heading": "Syntax", "section_id": "id_62ba092e_c8f9_4765_9acc_e72ed6bfdadd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addtotals", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:10.369719+00:00", "version": "10.2"}}
{"id": "c2bbaa13201aec3b", "content": "The addtotals command is a distributable streaming command, except when is used to calculate column totals. When used to calculate column totals, the addtotals command is a transforming command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "addtotals", "section_heading": "Usage", "section_id": "id_123a3e79_ca2e_440a_8c85_55756db85f5b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addtotals", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:10.369724+00:00", "version": "10.2"}}
{"id": "8946c4c1480ca0f9", "content": "1: Calculate the sum of the numeric fields of each event This example uses events that list the numeric sales for each product and quarter, for example: Use the chart command to summarize data To summarize the data by product for each quarter, run this search: In this example, there are two fields specified in the BY clause with the chart command. The products field is referred to as the <row-split> field. The quarter field is referred to as the <column-split> field. The results appear on the Statistics tab and look something like this: To add a column that generates totals for each row, run this search: The results appear on the Statistics tab and look something like this: Use the stats command to calculate totals If all you need are the totals for each product, a simpler solution is to use the stats command: The results appear on the Statistics tab and look something like this: 2. Specify a name for the field that contains the sums for each event Instead of accepting the default name added by the addtotals command, you can specify a name for the field. 3. Use wildcards to specify the names of the fields to sum Calculate the sums for the fields that begin with amount or that contain the text size in the field name. Save the sums in the field called TotalAmount. 4. Calculate the sum for a specific field In this example, the row calculations are turned off and the column calculations are turned on. The total for only a single field, sum(quota) , is calculated. The labelfield argument specifies in which field the label for the total appears. The default label is Total. The results appear on the Statistics tab and look something like this: 5. Calculate the field totals and add custom labels to the totals Calculate the sum for each quarter and product, and calculate a grand total. The labelfield argument specifies in which field the label for the total appears, which in this example is products. The label argument is used to specify the label Quarterly Totals for the labelfield , instead of using the default label Total. The fieldname argument is used to specify the label Product Totals for the row totals. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "source=\"addtotalsData.csv\"| chart sum(sales) BY products quarter"}, {"language": "spl", "code": "source=\"addtotalsData.csv\"| chart sum(sales) BY products quarter | addtotals"}, {"language": "spl", "code": "source=\"addtotalsData.csv\"| stats sum(sales) BY products"}, {"language": "spl", "code": "... | addtotals fieldname=sum"}, {"language": "spl", "code": "... | addtotals fieldname=TotalAmount amount* *size*"}, {"language": "spl", "code": "source=\"addtotalsData.csv\"| stats sum(quota) by quarter| addtotals row=f col=t labelfield=quarter sum(quota)"}, {"language": "spl", "code": "source=\"addtotalsData.csv\"| chart sum(sales) by products quarter| addtotals col=t labelfield=products label=\"Quarterly Totals\"fieldname=\"Product Totals\""}], "tables": [{"headers": ["products", "quarter", "sales", "quota"], "rows": [["ProductA", "QTR1", "1200", "1000"], ["ProductB", "QTR1", "1400", "1550"], ["ProductC", "QTR1", "1650", "1275"], ["ProductA", "QTR2", "1425", "1300"], ["ProductB", "QTR2", "1175", "1425"], ["ProductC", "QTR2", "1550", "1450"], ["ProductA", "QTR3", "1300", "1400"], ["ProductB", "QTR3", "1250", "1125"], ["ProductC", "QTR3", "1375", "1475"], ["ProductA", "QTR4", "1550", "1300"], ["ProductB", "QTR4", "1700", "1225"], ["ProductC", "QTR4", "1625", "1350"]]}, {"headers": ["products", "QTR1", "QTR2", "QTR3", "QTR4"], "rows": [["ProductA", "1200", "1425", "1300", "1550"], ["ProductB", "1400", "1175", "1250", "1700"], ["ProductC", "1650", "1550", "1375", "1625"]]}, {"headers": ["products", "QTR1", "QTR2", "QTR3", "QTR4", "Total"], "rows": [["ProductA", "1200", "1425", "1300", "1550", "5475"], ["ProductB", "1400", "1175", "1250", "1700", "5525"], ["ProductC", "1650", "1550", "1375", "1625", "6200"]]}, {"headers": ["products", "sum(sales)"], "rows": [["ProductA", "5475"], ["ProductB", "5525"], ["ProductC", "6200"]]}, {"headers": ["quarter", "sum(quota)"], "rows": [["QTR1", "3825"], ["QTR2", "4175"], ["QTR3", "4000"], ["QTR4", "3875"], ["Total", "15875"]]}, {"headers": ["products", "QTR1", "QTR2", "QTR3", "QTR4", "Product Totals"], "rows": [["ProductA", "1200", "1425", "1300", "1550", "5475"], ["ProductB", "1400", "1175", "1250", "1700", "5525"], ["ProductC", "1650", "1550", "1375", "1625", "6200"], ["Quarterly Totals", "4250", "4150", "3925", "4875", "17200"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "addtotals", "section_heading": "Examples", "section_id": "id_0107f464_97f8_464c_84b1_c23b60efaea2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addtotals", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:10.369747+00:00", "version": "10.2"}}
{"id": "ba0914c1486bd156", "content": "stats", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "addtotals", "section_heading": "See also", "section_id": "ad63eefe_87c6_4b1f_afcf_e736aaddc73b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addtotals", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:10.369751+00:00", "version": "10.2"}}
{"id": "7be4a5db96f87418", "content": "In statistics, contingency tables are used to record and analyze the relationship between two or more (usually categorical) variables. Many metrics of association or independence, such as the phi coefficient or the Cramer's V , can be calculated based on contingency tables. You can use the contingency command to build a contingency table, which in this case is a co-occurrence matrix for the values of two fields in your data. Each cell in the matrix displays the count of events in which both of the cross-tabulated field values exist. This means that the first row and column of this table is made up of values of the two fields. Each cell in the table contains a number that represents the count of events that contain the two values of the field in that row and column combination. If a relationship or pattern exists between the two fields, you can spot it easily just by analyzing the information in the table. For example, if the column values vary significantly between rows (or vice versa), there is a contingency between the two fields (they are not independent). If there is no contingency, then the two fields are independent.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "contingency", "section_heading": "Description", "section_id": "id_7a1f1b04_af09_45ec_acc6_b259e2db55f3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/contingency", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:27.846646+00:00", "version": "10.2"}}
{"id": "9c83f3bb1e1fc723", "content": "contingency [<contingency-options>...] <field1> <field2> Required arguments <field1> Syntax: <field> Description: Any field. You cannot specify wildcard characters in the field name. <field2> Syntax: <field> Description: Any field. You cannot specify wildcard characters in the field name. Optional arguments contingency-options Syntax: <maxopts> | <mincover> | <usetotal> | <totalstr> Description: Options for the contingency table. Contingency options maxopts Syntax: maxrows=<int> | maxcols=<int> Description: Specify the maximum number of rows or columns to display. If the number of distinct values of the field exceeds this maximum, the least common values are ignored. A value of 0 means a maximum limit on rows or columns. This limit comes from the maxvalues setting in the [ctable] stanza in the limits.conf file. Default: 1000 mincover Syntax: mincolcover=<num> | minrowcover=<num> Description: Specify a percentage of values per column or row that you would like represented in the output table. As the table is constructed, enough rows or columns are included to reach this ratio of displayed values to total values for each row or column. The maximum rows or columns take precedence if those values are reached. Default: 1.0 usetotal Syntax: usetotal=<bool> Description: Specify whether or not to add row, column, and complete totals. Default: true totalstr Syntax: totalstr=<field> Description: Field name for the totals row and column. Default: TOTAL", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "contingency", "section_heading": "Syntax", "section_id": "id_06888d92_d17c_495e_be16_a64b3be8307e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/contingency", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:27.846653+00:00", "version": "10.2"}}
{"id": "679995ef9989b3e9", "content": "The contingency command is a transforming command. See Command types. This command builds a contingency table for two fields. If you have fields with many values, you can restrict the number of rows and columns using the maxrows and maxcols arguments. Totals By default, the contingency table displays the row totals, column totals, and a grand total for the counts of events that are represented in the table. If you don't want the totals to appear in the results, include the usetotal=false argument with the contingency command. Empty values Values which are empty strings (\"\") will be represented in the results table as EMPTY_STR. Limits There is a limit on the value of maxrows or maxcols , which means more than 1000 values for either field will not be used.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "contingency", "section_heading": "Usage", "section_id": "id_1c79d749_c87b_4e3f_9137_cdf55391ceb0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/contingency", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:27.846657+00:00", "version": "10.2"}}
{"id": "3803d8918e2f3475", "content": "1. Build a contingency table of recent data You want to build a contingency table to look at the relationship between the magnitudes and depths of recent earthquakes. You start with a simple search. There are quite a range of values for the Magnitude and Depth fields, which results in a very large table. The magnitude values appear in the first column. The depth values appear in the first row. The list is sorted by magnitude. The results appear on the Statistics tab. The following table shows only a small portion of the table of results returned from the search. As you can see, earthquakes can have negative magnitudes. Only where an earthquake occurred that matches the magnitude and depth will a count appear in the table. To build a more usable contingency table, you should reformat the values for the magnitude and depth fields. Group the magnitudes and depths into ranges. This search uses the eval command with the case() function to redefine the values of Magnitude and Depth, bucketing them into a range of values. For example, the Depth values are redefined as \"Shallow\", \"Mid\", or \"Deep\". Use the sort command to sort the results by magnitude. Otherwise the results are sorted by the row totals. The results appear on the Statistics tab and look something like this: There were a lot of quakes in this month. Do higher magnitude earthquakes have a greater depth than lower magnitude earthquakes? Not really. The table shows that the majority of the recent earthquakes in all of magnitude ranges were shallow. There are significantly fewer earthquakes in the mid-to-deep range. In this data set, the deep-focused quakes were all in the mid-range of magnitudes. 2. Identify potential component issues in the Splunk deployment Determine if there are any components that might be causing issues in your Splunk deployment. Build a contingency table to see if there is a relationship between the values of log_level and component. Run the search using the time range All time and limit the number of columns returned. Your results should appear something like this: These results show you any components that might be causing issues in your Splunk deployment. The component field has more than 50 values. In this search, the maxcols argument is used to show 5 components with the highest values.", "code_examples": [{"language": "spl", "code": "source=all_month.csv | contingency mag depth | sort mag"}, {"language": "spl", "code": "source=all_month.csv  \n|evalMagnitude=case(mag<=1,\"0.0 - 1.0\", mag>1 AND mag<=2,\"1.1 - 2.0\", mag>2 \n  AND mag<=3,\"2.1 - 3.0\", mag>3 AND mag<=4,\"3.1 - 4.0\", mag>4 \n  AND mag<=5,\"4.1 - 5.0\", mag>5 AND mag<=6,\"5.1 - 6.0\", mag>6 \n  AND mag<=7,\"6.1 - 7.0\", mag>7,\"7.0+\") \n|evalDepth=case(depth<=70,\"Shallow\", depth>70 AND depth<=300,\"Mid\", depth>300 \n  AND depth<=700,\"Deep\") \n| contingency Magnitude Depth \n| sort Magnitude"}, {"language": "spl", "code": "index=_internal | contingency maxcols=5 log_level component"}], "tables": [{"headers": [], "rows": [["This search uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance.  This example uses theAll Earthquakesdata from  the past 30 days. Use the time rangeAll timewhen you run the searches."]]}, {"headers": ["mag", "10", "0", "5", "35", "8", "12", "15", "11.9", "11.8", "6.4", "5.4", "8.2", "6.5", "8.1", "5.6", "10.1", "9", "8.5", "9.8", "8.7", "7.9"], "rows": [["-0.81", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0"], ["-0.59", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0"], ["-0.56", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0"], ["-0.45", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0"], ["-0.43", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0"]]}, {"headers": ["Magnitude", "Shallow", "Mid", "Deep", "TOTAL"], "rows": [["0.0 - 1.0", "3579", "33", "0", "3612"], ["1.1 - 2.0", "3188", "596", "0", "3784"], ["2.1 - 3.0", "1236", "131", "0", "1367"], ["3.1 - 4.0", "320", "63", "1", "384"], ["4.1 - 5.0", "400", "157", "43", "600"], ["5.1 - 6.0", "63", "12", "3", "78"], ["6.1 - 7.0", "2", "2", "1", "5"], ["TOTAL", "8788", "994", "48", "9830"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "contingency", "section_heading": "Examples", "section_id": "cd8fec44_4e9f_4659_9da0_df3c9bfee5a3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/contingency", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:27.846667+00:00", "version": "10.2"}}
{"id": "41f9b2e3becced7a", "content": "associate , correlate", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "contingency", "section_heading": "See also", "section_id": "fa1213f0_960e_4033_8dbd_2c27cff8cf40--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/contingency", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:27.846671+00:00", "version": "10.2"}}
{"id": "a2b8cd5c45beb995", "content": "Using the delete command marks all of the events returned by the search as deleted. Subsequent searches do not return the marked events. No user, not even a user with admin permissions, is able to view this data after deletion. The delete command does not reclaim disk space. CAUTION: Removing data is irreversible. If you want to get your data back after the data is deleted, you must re-index the applicable data sources. You cannot run the delete command in a real-time search to delete events as they arrive. CAUTION: This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "delete", "section_heading": "Description", "section_id": "id_3dcdf8ee_1fa9_4f1e_9504_e55bd0af2684--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/delete", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:43.587577+00:00", "version": "10.2"}}
{"id": "7585d9485467d158", "content": "delete", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "delete", "section_heading": "Syntax", "section_id": "id_8bd3521c_9be2_44b9_9ac8_0b2a810b15ef--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/delete", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:43.587585+00:00", "version": "10.2"}}
{"id": "5ca9d730c8eb4f2e", "content": "The delete command can be accessed only by a user with the \"delete_by_keyword\" capability. By default, only the \"can_delete\" role has the ability to delete events. No other role, including the admin role, has this ability. You should create a special userid that you log on with when you intend to delete indexed data. To use the delete command, run a search that returns the events you want deleted. Make sure that the search returns ONLY the events that you want to delete, and no other events. After you confirm that the results contain the data that you want to delete, pipe the search to the delete command. The delete command does not trigger a roll of hot buckets to warm in the affected indexes. The output of the delete command is a table of the quantity of events removed by the fields splunk_server (the name of the indexer or search head), and index, as well as a rollup record for each server by index \"__ALL__\". The quantity of deleted events is in the deleted field. An errors field is also emitted, which will normally be 0. Delete command restrictions The delete command does not work in all situations: Searches with centralized streaming commands. You cannot use the delete command after a centralized streaming command. For example, you can't delete events using a search like this: Centralized streaming commands include: head , streamstats , some modes of dedup , and some modes of cluster. See Command types. Events with an index field. If your events contain a field named index aside from the default index field that is applied to all events. If your events do contain an additional index field, you can use eval before invoking delete , as in this example: Permanently removing data from an index The delete command does not remove the data from your disk space. You must use the clean command from the CLI to permanently remove the data. The clean command removes all of the data in an index. You cannot select the specific data that you want to remove. See Remove indexes and indexed data in Managing Indexers and Clusters of Indexers .", "code_examples": [{"language": "spl", "code": "index=myindex ... | head 100 | delete"}, {"language": "spl", "code": "index=fbus_summary latest=1417356000 earliest=1417273200 |evalindex =\"fbus_summary\"| delete"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "delete", "section_heading": "Usage", "section_id": "id_26d6805f_08ec_4933_b89d_e3e383e7d8b8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/delete", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:43.587590+00:00", "version": "10.2"}}
{"id": "8eb78d02ebc7c80f", "content": "1. Delete events with Social Security numbers Delete the events from the insecure index that contain strings that look like Social Security numbers. Use the regex command to identify events that contain the strings that you want to match. Run the following search to ensure that you are retrieving the correct data from the insecure index. If necessary, adjust the search to retrieve the correct data. Then add the delete command to the end of the search to delete the events. 2. Delete events that contain a specific word Delete events from the imap index that contain the word invalid. 3. Remove the Search Tutorial events Remove all of the Splunk Search Tutorial events from your index. Login as a user with an administrator role: For Splunk Cloud Platform, the role is sc_admin. For Splunk Enterprise, the role is admin. Click Settings > Users and create a new user with the can_delete role. Log out as the administrator and log back in as the user with the can_delete role. Set the time range picker to All time. Run the following search to retrieve all of the Search Tutorial events. Confirm that the search is retrieving the correct data. Add the delete command to the end of the search criteria and run the search again. The events are removed from the index. Log out as the user with the can_delete role.", "code_examples": [{"language": "spl", "code": "index=insecure | regex _raw =\"\\d{3}-\\d{2}-\\d{4}\""}, {"language": "spl", "code": "index=insecure | regex _raw =\"\\d{3}-\\d{2}-\\d{4}\"| delete"}, {"language": "spl", "code": "index=imap invalid | delete"}, {"language": "spl", "code": "source=tutorialdata.zip:*"}, {"language": "spl", "code": "source=tutorialdata.zip:* | delete"}], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "delete", "section_heading": "Examples", "section_id": "id_42affa20_6162_4a1a_8892_4a81d35ccc3c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/delete", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:43.587594+00:00", "version": "10.2"}}
{"id": "87601fe3502174fa", "content": "The convert command converts field values in your search results into numerical values. Unless you use the AS clause, the original values are replaced by the new values. Alternatively, you can use evaluation functions such as strftime() , strptime() , or tonumber() to convert field values.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "convert", "section_heading": "Description", "section_id": "af6fe769_7f30_4389_9a9a_bf2fd9494921--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/convert", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:58.704881+00:00", "version": "10.2"}}
{"id": "c3b5e4f3b75326e0", "content": "convert [timeformat=string] (<convert-function> [AS <field>] )... Required arguments <convert-function> Syntax: auto() | ctime() | dur2sec() | memk() | mktime() | mstime() | none() | num() | rmcomma() | rmunit() Description: Functions to use for the conversion. Optional arguments timeformat Syntax: timeformat=<string> Description: Specify the output format for the converted time field. The timeformat option is used by ctime and mktime functions. For a list and descriptions of format options, see Common time format variables in the Search Reference. Default: %m/%d/%Y %H:%M:%S. Note that this default does not conform to the locale settings. <field> Syntax: <string> Description: Creates a new field with the name you specify to place the converted values into. The original field and values remain intact. Convert functions auto() Syntax: auto(<wc-field>) Description: Automatically convert the fields to a number using the best conversion. Note that if not all values of a particular field can be converted using a known conversion type, the field is left untouched and no conversion at all is done for that field. You can use a wildcard ( * ) character to specify all fields. ctime() Syntax: ctime(<wc-field>) Description: Convert a UNIX time to an ASCII human readable time. Use the timeformat option to specify the exact format to convert to. You can use a wildcard ( * ) character to specify all fields. dur2sec() Syntax: dur2sec(<wc-field>) Description: Convert a duration format \"[D+]HH:MM:SS\" to seconds. You can use a wildcard ( * ) character to specify all fields. memk() Syntax: memk(<wc-field>) Description: Accepts a positive number (integer or float) followed by an optional \"k\", \"m\", or \"g\". The letter k indicates kilobytes, m indicates megabytes, and g indicates gigabytes. If no letter is specified, kilobytes is assumed. The output field is a number expressing quantity of kilobytes. Negative values cause data incoherency. You can use a wildcard ( * ) character to specify all fields. mktime() Syntax: mktime(<wc-field>) Description: Convert a human readable time string to an epoch time. Use timeformat option to specify exact format to convert from. You can use a wildcard ( * ) character to specify all fields. mstime() Syntax: mstime(<wc-field>) Description: Convert a [MM:]SS.SSS format to seconds. You can use a wildcard ( * ) character to specify all fields. none() Syntax: none(<wc-field>) Description: In the presence of other wildcards, indicates that the matching fields should not be converted. You can use a wildcard ( * ) character to specify all fields. num() Syntax: num(<wc-field>) Description: Like auto(), except non-convertible values are removed. You can use a wildcard ( * ) character to specify all fields. rmcomma() Syntax: rmcomma(<wc-field>) Description: Removes all commas from value, for example rmcomma(1,000,000.00) returns 1000000.00. You can use a wildcard ( * ) character to specify all fields. rmunit() Syntax: rmunit(<wc-field>) Description: Looks for numbers at the beginning of the value and removes trailing text. You can use a wildcard ( * ) character to specify all fields.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "convert", "section_heading": "Syntax", "section_id": "c85541ff_b135_4446_9a43_4ca5be9e4062--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/convert", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:58.704888+00:00", "version": "10.2"}}
{"id": "f171eed41277f296", "content": "The convert command is a distributable streaming command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "convert", "section_heading": "Usage", "section_id": "id_91aef6a6_9a5f_40d5_81fd_71c1e3c74a02--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/convert", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:58.704892+00:00", "version": "10.2"}}
{"id": "371cf6193c4ff4ab", "content": "1. Convert all field values to numeric values Use the auto convert function to convert all field values to numeric values. 2. Convert field values except for values in specified fields Convert every field value to a number value except for values in the field src_ip. Use the none convert function to specify fields to ignore. 3. Change the duration values to seconds for the specified fields Change the duration values to seconds for the specified fields 4. Change the sendmail syslog duration format to seconds Change the sendmail syslog duration format (D+HH:MM:SS) to seconds. For example, if delay=\"00:10:15\" , the resulting value is delay=\"615\". This example uses the dur2sec convert function. 5. Convert field values that contain numeric and string values Convert the values in the duration field, which contain numeric and string values, to numeric values by removing the string portion of the values. For example, if duration=\"212 sec\" , the resulting value is duration=\"212\". This example uses the rmunit convert function. 6. Change memory values to kilobytes Change all memory values in the virt field to KBs. This example uses the memk convert function.", "code_examples": [{"language": "spl", "code": "... | convert auto(*)"}, {"language": "spl", "code": "... | convert auto(*) none(src_ip)"}, {"language": "spl", "code": "... | convert dur2sec(xdelay) dur2sec(delay)"}, {"language": "spl", "code": "... | convert dur2sec(delay)"}, {"language": "spl", "code": "... | convert rmunit(duration)"}, {"language": "spl", "code": "... | convert memk(virt)"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "convert", "section_heading": "Basic examples", "section_id": "c13df9dd_e165_4881_a738_516141928d0d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/convert", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:58.704897+00:00", "version": "10.2"}}
{"id": "8231f3a79ab257d0", "content": "1. Convert a UNIX time to a more readable time format Convert a UNIX time to a more readable time formatted to show hours, minutes, and seconds. The ctime() function converts the _time value in the CSV file events to the format specified by the timeformat argument. The timeformat=\"%H:%M:%S\" argument tells the search to format the _time value as HH:MM:SS. The converted time ctime field is renamed c_time. The table command is used to show the original _time value and the ctime field. The results appear on the Statistics tab and look something like this: The ctime() function changes the timestamp to a non-numerical value. This is useful for display in a report or for readability in your events list. 2. Convert a time in MM:SS.SSS to a number in seconds Convert a time in MM:SS.SSS (minutes, seconds, and subseconds) to a number in seconds. The mstime() function converts the _time field values from a minutes and seconds to just seconds. The converted time field is renamed ms_time. The table command is used to show the original _time value and the converted time. The mstime() function changes the timestamp to a numerical value. This is useful if you want to use it for more calculations. 3. Convert a string time in HH:MM:SS into a number Convert a string field time_elapsed that contains times in the format HH:MM:SS into a number. Sum the time_elapsed by the user_id field. This example uses the eval command to convert the converted results from seconds into minutes.", "code_examples": [{"language": "spl", "code": "source=\"all_month.csv\"| convert timeformat=\"%H:%M:%S\"ctime(_time) AS c_time | table _time, c_time"}, {"language": "spl", "code": "sourcetype=syslog | convert mstime(_time) AS ms_time | table _time, ms_time"}, {"language": "spl", "code": "...| convert num(time_elapsed) | stats sum(eval(time_elapsed/60)) AS Minutes BY user_id"}], "tables": [{"headers": ["_time", "c_time"], "rows": [["2018-03-27 17:20:14.839", "17:20:14"], ["2018-03-27 17:21:05.724", "17:21:05"], ["2018-03-27 17:27:03.790", "17:27:03"], ["2018-03-27 17:28:41.869", "17:28:41"], ["2018-03-27 17:34:40.900", "17:34:40"], ["2018-03-27 17:38:47.120", "17:38:47"], ["2018-03-27 17:40:10.345", "17:40:10"], ["2018-03-27 17:41:55.548", "17:41:55"]]}, {"headers": ["_time", "ms_time"], "rows": [["2018-03-27 17:20:14.839", "1522196414.839"], ["2018-03-27 17:21:05.724", "1522196465.724"], ["2018-03-27 17:27:03.790", "1522196823.790"], ["2018-03-27 17:28:41.869", "1522196921.869"], ["2018-03-27 17:34:40.900", "1522197280.900"], ["2018-03-27 17:38:47.120", "1522197527.120"], ["2018-03-27 17:40:10.345", "1522197610.345"], ["2018-03-27 17:41:55.548", "1522197715.548"]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "convert", "section_heading": "Extended Examples", "section_id": "id_249bdff7_034b_499c_8c07_498e6f5dfd9e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/convert", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:58.704907+00:00", "version": "10.2"}}
{"id": "72079b8bd0b81001", "content": "Commands eval fieldformat Functions tonumber strptime", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "convert", "section_heading": "See also", "section_id": "id_5394c392_d6cf_43f5_9c23_ed928d6ab5f9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/convert", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:53:58.704911+00:00", "version": "10.2"}}
{"id": "cb09e8d44b9bebd0", "content": "Extracts field-value pairs from the search results. The extract command works only on the _raw field. If you want to extract from another field, you must perform some field renaming before you run the extract command.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "extract", "section_heading": "Description", "section_id": "eb5d9422_2ca3_4b0a_b235_f0abe1ac67ae--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/extract", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:13.874783+00:00", "version": "10.2"}}
{"id": "421b98a223588b5b", "content": "The required syntax is in bold. extract [<extract-options>... ] [<extractor-name>...] Required arguments None. Optional arguments <extract-options> Syntax: auto=f | clean_keys=<bool> | kvdelim=<string> | limit=<int> | maxchars=<int> | mv_add=<bool> | pairdelim=<string> | reload=<bool> | segment=<bool> Description: Options for defining the extraction. See the Extract_options section in this topic. <extractor-name> Syntax: <string> Description: A stanza in the transforms.conf file. This is used when the props.conf file does not explicitly cause an extraction for this source, sourcetype, or host. Extract options auto Syntax: auto=f Description: Specifies whether automatic key-value field extraction is turned off. When you include auto=f in a search with the extract command, you are explicitly telling Splunk software not to perform automatic key-value field extraction by default on the _raw field for that specific search. Using this option gives you more granular control over how fields are extracted. Default: None. clean_keys Syntax: clean_keys=<bool> Description: Specifies whether to clean keys. Overrides CLEAN_KEYS in the transforms.conf file. Default: The value specified in the CLEAN_KEYS in the transforms.conf file. kvdelim Syntax: kvdelim=<string> Description: A list of character delimiters that separate the key from the value. If the delimiter appears in the value, that value is not extracted. For example, if the delimiter is a colon ( : ) and a key-value pair is Referer: https://buttercupgames.com , the key-value pair is not extracted. limit Syntax: limit=<int> Description: Specifies how many automatic key-value pairs to extract. Default: 50 maxchars Syntax: maxchars=<int> Description: Specifies how many characters to look into the event. Default: 10240 mv_add Syntax: mv_add=<bool> Description: Specifies whether to create multivalued fields. Overrides the value for the MV_ADD parameter in the transforms.conf file. Default: false pairdelim Syntax: pairdelim=<string> Description: A list of character delimiters that separate the key-value pairs from each other. reload Syntax: reload=<bool> Description: Specifies whether to force reloading of the props.conf and transforms.conf files. Default: false segment Syntax: segment=<bool> Description: Specifies whether to note the locations of the key-value pairs with the results. Default: false", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "extract", "section_heading": "Syntax", "section_id": "id_52f4ee4c_b755_4dad_b09c_accba849136f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/extract", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:13.874791+00:00", "version": "10.2"}}
{"id": "91851e09099e359d", "content": "The extract command is a distributable streaming command. See Command types. Alias The alias for the extract command is kv .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "extract", "section_heading": "Usage", "section_id": "id_42282a0a_d972_4545_8dc7_99a5d5e1ad7e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/extract", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:13.874795+00:00", "version": "10.2"}}
{"id": "f9b59418cbd66e51", "content": "1. Specify the delimiters to use for the field and value extractions Extract field-value pairs that are delimited by the pipe ( | ) or semicolon ( ; ) characters. Extract values of the fields that are delimited by the equal ( = ) or colon ( : ) characters. The delimiters are individual characters. In this example the \"=\" or \":\" character is used to delimit the key value. Similarly, a \"|\" or \";\" is used to delimit the field-value pair itself. 2. Extract field-value pairs and reload the field extraction settings Extract field-value pairs and reload field extraction settings from disk. 3. Rename a field to _raw to extract from that field Rename the _raw field to a temporary name. Rename the field you want to extract from, to _raw. In this example the field name is uri_query. 4. Extract field-value pairs from a stanza in the transforms.conf file Extract field-value pairs that are defined in the my-access-extractions stanza in the transforms.conf file. The transforms.conf stanza for this example looks something like this.", "code_examples": [{"language": "spl", "code": "... | extract pairdelim=\"|;\", kvdelim=\"=:\""}, {"language": "spl", "code": "... | extract reload=true"}, {"language": "spl", "code": "... | rename _raw AS temp uri_query AS _raw | extract pairdelim=\"?&\"kvdelim=\"=\"| rename _raw AS uri_query temp AS _raw"}, {"language": "spl", "code": "... | extract my-access-extractions"}, {"language": "spl", "code": "[my-access-extractions]\nREGEX=\\[(?!(?:headerName|headerValue))([^\\s\\=]+)\\=([^\\]]+)\\]\nFORMAT=$1::$2"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "extract", "section_heading": "Examples", "section_id": "id_0cb9f1b2_5ed4_4516_90f9_144d91d6bb44--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/extract", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:13.874799+00:00", "version": "10.2"}}
{"id": "9a4d3463c2a74139", "content": "kvform , multikv , rex , spath , xmlkv , xpath", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "extract", "section_heading": "See also", "section_id": "e2f0a879_2cca_4a5b_9641_8440a76afd3e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/extract", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:13.874804+00:00", "version": "10.2"}}
{"id": "13138ba3f18fda9b", "content": "Adds the results of a search to a summary index that you specify. You must create the summary index before you invoke the collect command. You do not need to know how to use collect to create and use a summary index, but it can help. For an overview of summary indexing, see Use summary indexing for increased reporting efficiency in the Knowledge Manager Manual. CAUTION: This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "collect", "section_heading": "Description", "section_id": "id_4b334a56_8f55_4b93_8bb7_891e00734e5a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/collect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:32.260859+00:00", "version": "10.2"}}
{"id": "cef4f08fd58ce32f", "content": "The required syntax is in bold. collect index=<string> [<arg-options>...] Required arguments index Syntax: index=<string> Description: Name of the summary index where the events are added. The index must exist before the events are added. The index is not created automatically. Optional arguments arg-options Syntax: addinfo=<bool> | addtime=<bool> | file=<string> | spool=<bool> | marker=<string> | uselb=<bool> | output_format [raw | hec] | testmode=<bool> | run_in_preview=<bool> | host=<string> | source=<string> | sourcetype=<string> | timeformat=<string> Description: Optional arguments for the collect command. See the arg-options section for the descriptions for each option. arg-options addinfo Syntax: addinfo=<bool> Description: Use this option to specify whether to prefix search time and time-range information fields on to each summary index event. If set to true , adds fields to each event in the following format: info_min_time=<search_earliest_time>, info_max_time=<search_latest_time>, info_search_time=<search_exec_time> Default: True when summary events are destined for an events index or when output_format=raw. False when summary events are destined for a metrics index. addtime Syntax: addtime=<bool> Description: Use this option to specify whether to prefix a time field on to each event. Some commands return results that do not have a _raw field, such as the stats , chart , timechart commands. If you specify addtime=false , the Splunk software uses its generic date detection against fields in whatever order they happen to be in the summary rows. If you specify addtime=true , the Splunk software uses the search time range info_min_time. This time range is added by the sistats command or _time. Splunk software adds the time field based on the first field that it finds: info_min_time , _time , or now(). This option is not valid when output_format=hec. Default: True when summary events are destined for an events index. False when summary events are destined for a metrics index. file Syntax: file=<string> Description: The file name where you want the events to be written. You can use a timestamp or a random number for the file name by specifying either file=$timestamp$ or file=$random$. Usage: \".stash\" needs to be added at the end of the file name when used with \"index=\". Otherwise, the data is added to the main index. Default: <random-number>_events.stash host Syntax: host=<string> Description: The name of the host that you want to specify for the events. This option is not valid when output_format=hec. marker Syntax: marker=<string> Description: A string, usually of key-value pairs, to append to each event written out. Each key-value pair must be separated by a comma and a space. If the value contains spaces or commas, it must be escape quoted. For example if the key-value pair is search_name=vpn starts and stops , you must change it to search_name=\\\"vpn starts and stops\\\". This option is not valid when output_format=hec. output_format Syntax: output_format=[raw | hec] Description: Specifies the output format for the summary indexing. If set to raw , uses the traditional non-structured log style summary indexing stash output format. If set to hec , the collect command generates HTTP Event Collector (HEC) JSON formatted output. By default, license usage is not counted in the following cases: When output_format=raw and the source type is the internal stash source type ( stash ). When output_format=hec and the source type is the internal stash source type ( stash_hec ). To confirm whether the source type is stash or stash_hec , expand the event in the search results after you run your search. License usage is counted when output_format=hec and the original source type is used instead of stash_hec. When using output_format=hec , note that: All fields are automatically indexed when the stash file is indexed. The file that is written to the var/spool/splunk path ends in .stash_hec instead of .stash. The source, source type, and host from the original data are used directly in the summary index. These fields are not remapped to the extract_host/extracted_sourcetype/... path. The index and splunk_server fields in the original data are ignored. You can't use the addtime , host , marker , source , sourcetype , or uselb options. Default: raw run_in_preview Syntax: run_in_preview=<bool> Description: Controls whether the collect command is enabled during preview generation. Generally, you do not want to insert preview results into the summary index, run_in_preview=false. In some cases, such as when a custom search command is used as part of the search, you might want to turn this on to ensure correct summary indexable previews are generated. Default: false spool Syntax: spool=<bool> Description: If set to true, the summary indexing file is written to the Splunk spool directory, where it is indexed automatically. If set to false, the file is written to the $SPLUNK_HOME/var/run/splunk/collect directory. The file remains in this directory unless some form of further automation or administration is done. If you have Splunk Enterprise, you can use this command to troubleshoot summary indexing by dumping the output file to a location on disk where it will not be ingested as data. Default: true source Syntax: source=<string> Description: The name of the source that you want to specify for the events. This option is not valid when output_format=hec. sourcetype Syntax: sourcetype=<string> Description: The name of the source type that you want to specify for the events. If you specify a source type other than stash, the ingested summary data will count against your license usage. This option is not valid when output_format=hec. Default: stash testmode Syntax: testmode=<bool> Description: Toggle between testing and real mode. In testing mode the results are not written into the new index but the search results are modified to appear as they would if sent to the index. Default: false timeformat Syntax: timeformat=<string> Description: Controls the format of the timestamp that is written to the stash file before it is indexed. The addtime argument must be set to true for the same invocation of the command in order to take advantage of this functionality. Use this argument only if you need precise control over the format of output files that the collect command generates. This option is not valid when output_format=hec. Default: %m/%d/%Y %H:%M:%S %z uselb Syntax: uselb=<bool> Description: Controls how line breaks are used to split events. When set to true , the data that is ingested using the collect command is split into individual events. A string identical to the LINE_BREAKER setting defined for the stash_new source type in the props.conf file is used. When set to false , a simple line break is used to split events. Do not use this argument unless you are intentionally generating events with the collect command in a line-oriented format. This option is not valid when output_format=hec. NOTE: While the default behavior of the collect command is to use a LINE_BREAKER setting identical to that used in the props.conf file, the default LINE_BREAKER for the collect command is hardcoded. Changes to props.conf setting do NOT affect the behavior of the uselb option. Default: true", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "collect", "section_heading": "Syntax", "section_id": "id_3c8887e3_292d_42ab_9dcb_dc0335782ecb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/collect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:32.260867+00:00", "version": "10.2"}}
{"id": "5cc4908f720b1828", "content": "The events are written to a file whose name format is: random-num _events.stash , unless overwritten, in a directory that your Splunk deployment is monitoring. If the events contain a _raw field, then this field is saved. If the events do not have a _raw field, one is created by concatenating all the fields into a comma-separated list of key=value pairs. The collect command also works with real-time searches that have a time range of All time. Events without timestamps If you apply the collect command to events that do not have timestamps, the command designates a time for all of the events using the earliest (or minimum) time of the search range. For example, if you use the collect command over the past four hours (range: -4h to +0h), the command assigns a timestamp that is four hours prior to the time that the search was launched. The timestamp is applied to all of the events without a timestamp. If you use the collect command with a time range of All time and the events do not have timestamps, the current system time is used for the timestamps. For more information on summary indexing of data without timestamps, see Use summary indexing for increased reporting efficiency in the Knowledge Manager Manual. Copying events to a different index You can use the collect command to copy search results to another index. Construct a search that returns the data you want to copy, and pipe the results to the collect command. For example: This search writes the results into the bar index. The sourcetype is changed to stash. You can specify a sourcetype with the collect command. However, specifying a sourcetype counts against your license, as if you indexed the data again. Change how collect summarizes multivalue fields on Splunk Enterprise By default, the collect command summarizes multivalue fields as multivalue fields. For example, when collect summarizes the multivalue field alphabet = a, b, c , it adds the following field to the summary index: However, you might prefer the collect command to break multivalue fields into separate field-value pairs when it adds them to a _raw field in a summary index. For example, if given the multivalue field alphabet = a,b,c , you can have the collect command add the following fields to a _raw event in the summary index: alphabet = \"a\", alphabet = \"b\", alphabet = \"c\" If you are using Splunk Enterprise and you prefer to have collect follow this multivalue field summarization format, set the limits.conf setting format_multivalue_collect to true. To change the format_multivalue_collect setting in your local limits.conf file and enable collect to break multivalue fields into separate fields, follow these steps. Prerequisites Only users with file system access, such as system administrators, can edit configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps Open or create a local limits.conf file at $SPLUNK_HOME/etc/system/local. Under the [collect] stanza, set format_multivalue_collect to true. The collect and tstats commands The collect command does not segment data by major breakers and minor breakers , such as characters like spaces, square or curly brackets, parenthesis, semicolons, exclamation points, periods, and colons. As a result, if either major or minor breakers are found in value strings, Splunk software places quotation marks around field values when it adds events to the summary index. These extra quotation marks can cause problems for subsequent searches. In particular, field values that have quotation marks around them can't be used in tstats searches with the PREFIX() directive. This is because PREFIX() does not support major breakers like quotation marks. For example, in the following search with the collect command, the field values in quotes include periods as minor breakers. The search results look something like this. So far, that looks fine, right? Not exactly. Although there aren't any extra quotation marks around the field values buttercupgames.com and 2.0 that are displayed in the search results, you will see them if you look in summary index. To see what is in the summary index, run the following search: Now you can see version=\"2.0\" and application=\"buttercupgames.com\". The results look something like this: If you want to run a tstats search with the PREFIX() directive using those field values with quotation marks that are collected in a summary index like our previous example, you will need to edit your limits.conf file. You can do this by changing the collect_ignore_minor_breakers setting in the [collect] stanza from the default to true. Splunk Cloud Platform To change the collect_ignore_minor_breakers setting in your limits.conf file, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Enterprise To change the collect_ignore_minor_breakers setting in your local limits.conf file, follow these steps. Prerequisites Only users with file system access, such as system administrators, can edit configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps: Open or create a local limits.conf file at $SPLUNK_HOME/etc/system/local. Under the [collect] stanza, add the line collect_ignore_minor_breakers=true .", "code_examples": [{"language": "spl", "code": "index=foo | ... | collect index=bar"}, {"language": "spl", "code": "alphabet:\"a\n           b \n           c\""}, {"language": "spl", "code": "| makeresults |evalapplication=\"buttercupgames.com\", version=\"2.0\"| collect index=summarysource=devtest"}, {"language": "spl", "code": "index=summarysource=devtest"}], "tables": [{"headers": ["_time", "application", "version"], "rows": [["2021-12-07 11:43:48", "buttercupgames.com", "2.0"]]}, {"headers": ["Time", "Event"], "rows": [["12/7/2111:43:48.000 AM", "12/07/2021 11:43:48 -0800,  info_search_time=1638906228.401,  version=\"2.0\",  application=\"buttercupgames.com\"host = PF32198D     |     source = devtest     |     sourcetype = stash"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "collect", "section_heading": "Usage", "section_id": "id_67d15c22_1fb3_4071_9025_e5ce632e1c37--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/collect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:32.260878+00:00", "version": "10.2"}}
{"id": "2d2b3f3511dbe861", "content": "1. Put \"download\" events into an index named \"download count\" 2. Collect statistics on VPN connects and disconnects You want to collect hourly statistics on VPN connects and disconnects by country. The addinfo command ensures that the search results contain fields that specify when the search was run to populate these particular index values. 3. Ingest fields using the collect command and HEC formatted output Say you want to create a few fields in your index by running the following search: The results look like this: To see what the event we've just generated looks like in the index, run the following search: The following image shows that all of the fields that were specified in the search appear as fields in the index. Note: This search counts against your license usage because the search assigned a value to the source type and didn't use the default stash_hec sourcetype.", "code_examples": [{"language": "spl", "code": "eventtypetag=\"download\"| collect index=downloadcount"}, {"language": "spl", "code": "index=mysummary \n | geoip REMOTE_IP \n |evalcountry_source=if(REMOTE_IP_country_code=\"US\",\"domestic\",\"foreign\") \n | bin _time span=1h \n | stats count by _time,vpn_action,country_source \n | addinfo\n | collect index=mysummary marker=\"summary_type=vpn, summary_span=3600, \n   summary_method=bin, search_name=\\\"vpn starts and stops\\\"\""}, {"language": "spl", "code": "| makeresults \n|evalsource=\"mysource\", sourcetype=\"mysourcetype\", host=\"myhost\", sentinel=\"4\", _raw=\"this is an event with a key=value pair\"| collect index=main output_format=hec"}, {"language": "spl", "code": "index=main"}], "tables": [{"headers": ["_raw", "_time", "host", "sentinel", "source", "sourcetype"], "rows": [["this is an event with a key=value pair", "2024-01-12T19:22:55.000-08:00", "myhost", "4", "mysource", "mysourcetype"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "collect", "section_heading": "Examples", "section_id": "id_0882307c_0932_4dcd_a43b_198d3f257fa1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/collect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:32.260887+00:00", "version": "10.2"}}
{"id": "35cd08b17bce0fc3", "content": "Commands overlap sichart sirare sistats sitimechart sitop tscollect", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "collect", "section_heading": "See also", "section_id": "a9ad7188_be5a_44fe_b811_7d4d99d25f5a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/collect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:32.260892+00:00", "version": "10.2"}}
{"id": "fd52238d7953e8f2", "content": "Loads events or results of a previously completed search job. The artifacts to load are identified either by the search job id <sid> or a scheduled search name and the time range of the current search. If a saved search name is provided and multiple artifacts are found within that range, the latest artifacts are loaded. Note: You cannot run the loadjob command on real-time searches.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "loadjob", "section_heading": "Description", "section_id": "f57fecf7_e88a_4db7_bfe4_ee1b5c75e9f2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/loadjob", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:46.819063+00:00", "version": "10.2"}}
{"id": "5a0d8aa33f94f23b", "content": "The required syntax is in bold. | loadjob (<sid> | <savedsearch>) [<events>] [<job_delegate>] [<artifact_offset>] [<ignore_running>] Required arguments You must specify either sid or savedsearch. sid Syntax: <string> Description: The search ID of the job whose artifacts need to be loaded, for example: 1233886270.2. You can locate the sid through the Job Inspector or the addinfo command. savedsearch Syntax: savedsearch=\"<user-string>:<app-string>:<search-name-string>\" Description: The unique identifier of a saved search whose artifacts need to be loaded. A saved search is uniquely identified by the triplet {user, app, savedsearch name}, for example: savedsearch=\"admin:search:my Saved Search\" There is no method to specify a wildcard or match-all behavior. All portions of the triplet must be provided. Optional arguments artifact_offset Syntax: artifact_offset=<int> Description: Selects a search artifact other than the most recent matching one. For example, if artifact_offset=1 , the second most recent artifact will be used. If artifact_offset=2 , the third most recent artifact will be used. If artifact_offset=0 , selects the most recent. A value that selects past all available artifacts will result in an error. Default: 0 job_delegate Syntax: job_delegate=<string> Description: When specifying a saved search, this option selects search jobs that were started by the given user. Scheduled jobs will be run by the delegate \"scheduler\". Dashboard-embedded searches are run in accordance with the saved search's dispatchAs parameter, typically the owner of the search. Defaults: scheduler ignore_running Syntax: ignore_running=<bool> Description: Skip over artifacts whose search is still running. Default: true events Syntax: events=<bool> Description: Specifies whether to load events or results of a previously completed search job. To load events, set events=true. To load results, set events=false. Default: false", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "loadjob", "section_heading": "Syntax", "section_id": "ff453413_71f3_471e_be44_5dd7adbf6175--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/loadjob", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:46.819071+00:00", "version": "10.2"}}
{"id": "7163cd590c396d3e", "content": "The loadjob command is an event-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. The loadjob command can be used for a variety of purposes, but one of the most useful is to run a fairly expensive search that calculates statistics. You can use loadjob searches to display those statistics for further aggregation, categorization, field selection and other manipulations for charting and display. After a search job has completed and the results are cached, you can use this command to access or load the results. Search head clusters A search head cluster can run the loadjob command only on scheduled saved searches. A search head cluster runs searches on results or artifacts that the search head cluster replicates. For more information on artifact replication, see Search head clustering architecture in the Distributed Search manual. Controlling truncation in search results To improve the speed of searches, Splunk software truncates the output of a search by default. For example, if the full output set is 10,000 events, the loadjob command might return only 1,000 events. Note: When a successful search is run, it returns either events or results. Events are returned if the commands in the search only filter the data. Results are returned if one of the commands in the search is a transforming command, such as the table command. If your search returns events and you don't want the output truncated, you can add the table command to the end of your search. The table command returns results instead of events. For example: Because the search returns results and not events, when you use the loadjob command all of the results are returned. Splunk Enterprise If search performance is not a concern, you can use the read_final_results_from_timeliner setting in the limits.conf file to control whether results are truncated when running the loadjob command. When read_final_results_from_timeliner is set to 'true', which is the default, the loadjob search returns the sample of the final results, not the full result set. For example, if the full result set is 10,000 results, the search might return only 1,000 results. When read_final_results_from_timeliner is set to 'false', the loadjob search returns the full set of search results. For example, if the full result set is 10,000 results, the search returns 10,000 results. To change the read_final_results_from_timeliner setting, follow these steps. Prerequisites Only users with file system access, such as system administrators, can edit configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps Open or create a local limits.conf file at $SPLUNK_HOME/etc/system/local. In the [search] stanza, add the line read_final_results_from_timeliner = true to truncate search results, or read_final_results_from_timeliner = false to output the full set of search results.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | table host,source, event_message, node_path"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "loadjob", "section_heading": "Usage", "section_id": "id_2bc0b489_190c_4ce3_a4fc_39cb27800be3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/loadjob", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:46.819076+00:00", "version": "10.2"}}
{"id": "0540a42df87e254a", "content": "1. Load the results of a saved search Loads the results of the latest scheduled execution of saved search MySavedSearch in the 'search' application owned by the user admin. 2. Specifying a saved search with a space in the name Loads the results of the latest scheduled execution of saved search Potential Threats in the 'search' application owned by the user maria. 3. Load the results from a specific search job Loads the events that were generated by the search job with id=1233886270.2.", "code_examples": [{"language": "spl", "code": "| loadjob savedsearch=\"admin:search:MySavedSearch\""}, {"language": "spl", "code": "| loadjob savedsearch=\"maria:search:Potential Threats\""}, {"language": "spl", "code": "| loadjob 1233886270.2 events=true"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "loadjob", "section_heading": "Examples", "section_id": "id_7b7ef5ec_2520_4a78_b3c1_f02204e39074--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/loadjob", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:46.819081+00:00", "version": "10.2"}}
{"id": "57aaf931328c7544", "content": "Commands addinfo inputcsv savedsearch Related information Manage search jobs", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "loadjob", "section_heading": "See also", "section_id": "ba464b59_7b01_443d_8a80_b2ed6794f4ef--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/loadjob", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:54:46.819084+00:00", "version": "10.2"}}
{"id": "c177e0553bd0dc60", "content": "Creates one or more relative time fields and adds the field or fields to returned events. Each added relative time field provides a human-readable value of the difference between \"now\" (the start time of the search) and the timestamp value of a corresponding field in the returned event. Human-readable values look like 5 days ago , 1 minute ago , 2 years ago , and so on.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "reltime", "section_heading": "Description", "section_id": "f5e04bac_03d3_45e7_a760_95f169be9d3f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/reltime", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:01.910499+00:00", "version": "10.2"}}
{"id": "cbdbab428796c011", "content": "The required syntax is in bold. | reltime [timefield=<field-list>] [prefix=<string>] Optional arguments timefield Syntax: timefield=<field-list> Description: Specifies one or more time fields in the events returned by the search. The reltime command uses these fields as the basis for the relative time field that it adds to the events. timefield can specify only fields with values that are valid timestamps. timefield can specify multiple time fields as a comma-separated list bounded by double quotation marks. Default: _time prefix Syntax: prefix=<string> Description: Sets a prefix string for relative time field names. Use it to help others identify fields added by reltime or to provide unique field names when you identify multiple timefield values. If you specify multiple values for timefield but do not specify a prefix , the reltime command prefixes the relative time fields that it adds with reltime_ .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "reltime", "section_heading": "Syntax", "section_id": "id_2136c713_b6e8_4e14_af25_969f289fe54d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/reltime", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:01.910506+00:00", "version": "10.2"}}
{"id": "c7b7a1b5938aa3da", "content": "The reltime command adds one or more relative time fields to your events. Each field added provides a human-readable value that represents the difference between now (the start time of the search) and the timestamp value of a field in the event. For example, say you tie reltime to the _time fields in your events. If you run a search at 6 a.m., and the search returns an event with a _time value that translates to 5 a.m., reltime adds a field to that event named reltime with the value 1 hour ago. If you use reltime without arguments, the command adds a relative time field to your events named reltime. This new field will be based on the _time field in each of your events. The following table explains how reltime defines and names the fields that it adds. The reltime command is a distributable streaming command. See Command types .", "code_examples": [], "tables": [{"headers": ["Customtimefieldspecified?", "Customprefixspecified?", "Basis for field(s) added byreltime", "Name(s) of field(s) added byreltime"], "rows": [["None", "No", "_time", "reltime"], ["Onetimefieldspecified", "No", "The time field you specified fortimefield", "reltime"], ["Onetimefieldspecified", "Yes", "The time field you specified fortimefield", "reltime, prefixed by your customprefixstring"], ["Multiple time fields specified", "No", "The list of time fields you specified fortimefield", "The names of the fields you specified fortimefield, prefixed byreltime_"], ["Multiple time fields specified", "Yes", "The list of time fields you specified fortimefield", "The names of the fields you specified fortimefield, prefixed by your customprefixstring"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "reltime", "section_heading": "Usage", "section_id": "id_444f14bb_e9dc_467e_b5fd_0fc3bd6dd4fa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/reltime", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:01.910514+00:00", "version": "10.2"}}
{"id": "68609a12efae3e35", "content": "Example 1: Adds a field called reltime to the events returned by the search, based on the _time field in those events. Example 2: Adds a field called reltime to events returned by the search, based on the earliest_time field in those events. Example 3: Adds a field called reltime_now_current_time to events, based on the current_time field in those events. Example 4: Adds three new relative time fields called reltime_max_time , reltime_min_time , and reltime_current_time to returned events with max_time , min_time , and current_time fields. Example 5: Adds two new relative time fields called usr_prefix_max_time and usr_prefix_min_time to returned events with max_time and min_time fields.", "code_examples": [{"language": "spl", "code": "... | reltime"}, {"language": "spl", "code": "... | reltime timefield=earliest_time"}, {"language": "spl", "code": "... | reltime timefield=current_time prefix=reltime_now_"}, {"language": "spl", "code": "... | reltime timefield=\"max_time,min_time,current_time\""}, {"language": "spl", "code": "... | reltime timefield=\"max_time,min_time\"prefix=usr_prefix_"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "reltime", "section_heading": "Examples", "section_id": "id_9c2f3156_8c05_45e1_8c6e_e3c0c5f7cb42--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/reltime", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:01.910519+00:00", "version": "10.2"}}
{"id": "66ca2d448080fa9d", "content": "convert", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "reltime", "section_heading": "See also", "section_id": "b575647d_f70b_400b_87e5_1bbf85e5d022--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/reltime", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:01.910522+00:00", "version": "10.2"}}
{"id": "4ba45ea302133a5f", "content": "The redistribute command implements parallel reduce search processing to shorten the search runtime of a set of supported SPL commands. Apply the redistribute command to high-cardinality dataset searches that aggregate large numbers of search results. The redistribute command requires a distributed search environment where indexers have been configured to operate as intermediate reducers. You can use the redistribute command only once in a search.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "redistribute", "section_heading": "Description", "section_id": "id_58152b35_5632_4e73_bc38_0ea263f59d46--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/redistribute", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:19.058662+00:00", "version": "10.2"}}
{"id": "937b4d6cede6455d", "content": "redistribute [num_of_reducers=<int>] [<by-clause>] Required arguments None. Optional arguments num_of_reducers Syntax: num_of_reducers=<int> Description: Specifies the number of indexers in the indexer pool that are repurposed as intermediate reducers. Default: The default value for num_of_reducers is controlled by three settings in the limits.conf file: maxReducersPerPhase , winningRate , and reducers. If these settings are not changed, by default the Splunk software sets num_of_reducers to 50 percent of your indexer pool, with a maximum of 4 indexers. See Usage for more information. by-clause Syntax: BY <field-list> Description: The name of one or more fields to group by. You cannot use a wildcard character to specify multiple fields with similar names. You must specify each field separately. See Using the by-clause for more information.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "redistribute", "section_heading": "Syntax", "section_id": "a35bb80c_edbe_4704_a7e1_4e7f33e128a9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/redistribute", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:19.058670+00:00", "version": "10.2"}}
{"id": "c64ba663415d06ec", "content": "In Splunk deployments that have distributed search, a two-phase map-reduce process is typically used to determine the final result set for the search. Search results are mapped at the indexer layer and then reduced at the search head. The redistribute command inserts an intermediary reduce phase to the map-reduce process, making it a three-phase map-reduce-reduce process. This three-phase process is parallel reduce search processing. In the intermediary reduce phase, a subset of the indexers become intermediate reducers. The intermediate reducers perform reduce operations for the search commands and then pass the results on to the search head, where the final result reduction and aggregation operations are performed. This parallelization of reduction work that otherwise would be done entirely by the search head can result in faster completion times for high-cardinality searches that aggregate large numbers of search results. For information about managing parallel reduce processing at the indexer level, including configuring indexers to operate as intermediate reducers, see Overview of parallel reduce search processing , in the Distributed Search manual. Note: If you use Splunk Cloud Platform, use redistribute only when your indexers are operating with a low to medium average load. You do not need to perform any configuration tasks to use the redistribute command. Supported commands The redistribute command supports only streaming commands and the following nonstreaming commands: stats tstats streamstats eventstats sichart sitimechart The redistribute command also supports the transaction command, when the transaction command is operating on only one field. For example, the redistribute command cannot support the transaction command when the following conditions are true: The redistribute command has multiple fields in its <by-clause> argument. The transaction command has multiple fields in its <field-list> argument. You use the transaction command in a mode where no field is specified. For best performance, place redistribute immediately before the first supported nonstreaming command that has high-cardinality input. When search processing moves to the search head The redistribute command moves the processing of a search string from the intermediate reducers to the search head in the following circumstances: It encounters a nonstreaming command that it does not support. It encounters a command that it supports but that does not include a split-by field. It encounters a command that it supports and that includes split-by fields, but the split-by fields are not a superset of the fields that are specified in the by-clause argument of the redistribute command. It detects that a command modifies values of the fields specified in the by-clause of the redistribute command. Using the by-clause to determine how results are partitioned on the reducers At the start of the intermediate reduce phase, the redistribute command takes the mapped search results and redistributes them into partitions on the intermediate reducers according to the fields specified by the by-clause argument. If you do not specify any by-clause fields, the search processor uses the field or fields that work best with the commands that follow the redistribute command in the search string. Command type The redistribute command is an orchestrating command , which means that it controls how a search runs. It does not focus on the events processed by the search. The redistribute command instructs the distributed search query planner to convert centralized streaming data into distributed streaming data by distributing it across the intermediate reducers. For more information about command types, see Types of commands in the Search Manual. Setting the default number of intermediate reducers The default value for the num_of_reducers argument is controlled by three settings in the limits.conf file: maxReducersPerPhase , winningRate , and reducers. If you decide to add 7 of your indexers to the reducers list, the winningRate setting ceases to be applied, and the num_of_reducers argument defaults to 4 indexers. The Splunk platform randomly selects four indexers from the reducers list to act as intermediate reducers each time you run a valid redistribute search. Note: If you provide a value for the num_of_reducers argument that exceeds the limit set by the maxReducersPerPhase setting, the Splunk platform sets the number of reducers to the maxReducersPerPhase value. The redistribute command and search head data Searches that use the redistribute command ignore all data on the search head. If you plan to use the redistribute command, the best practice is to forward all search head data to the indexer layer. See Best Practice: Forward search head data to the indexer layer in the Distributed Search manual. Using the redistribute command in chart and timechart searches If you want to add the redistribute command to a search that uses the chart or timechart commands to produce statistical results that can be used for chart visualizations, include either the sichart command or the sitimechart command in the search as well. The redistribute command uses these si- commands to perform the statistical calculations for the reporting commands on the intermediate reducers. When the redistribute command moves the results to the search head, the chart or timechart command transforms the results into a format that can be used for chart visualizations. A best practice is to use the same syntax and values for both commands. For example, if you want to have | timechart count by referrer_domain in your redistribute search, insert | sitimechart count by referrer_domain into the search string: If an order-sensitive command is present in the search Certain commands that the redistribute command supports explicitly return results in a sorted order. As a result of the partitioning that takes place when the redistribute command is run, the Splunk platform loses the sorting order. If the Splunk platform detects that an order-sensitive command, such as streamstats , is used in a redistribute search, it automatically inserts sort into the search as it processes it. For example, the following search includes the streamstats command, which is order-sensitive: The Splunk platform adds a sort segment before the streamstats segment when it processes the search. You can see the sort segment in the search string if you inspect the search job after you run it. The stats and streamstats segments are processed on the intermediate reducers because they both split by the host field, the same field that the redistribute command is distributing on. The work of the sort segment is split between the indexers during the map phase of the search and the search head during the final reduce phase of the search. If you require sorted results from a redistribute search If you require the results of a redistribute search to be sorted in that exact order, use sort to perform the sorting at the search head. There is an additional performance cost to event sorting after the redistribute command partitions events on the intermediate reducers. The following search provides ordered results: If you want to get that same event ordering while also adding redistribute to the search to speed it up, add sort to the search: The stats segment of this search is processed on the intermediate reducers. The work of the sort segment is split between the indexers during the map phase of the search and the search head during the final reduce phase of the search. Redistribute and virtual indexes The redistribute command does not support searches of virtual indexes. The redistribute command also does not support unified searches if their time ranges are long enough that they run across virtual archive indexes.", "code_examples": [{"language": "spl", "code": "index=main | redistribute | transaction referer_domain | search eventcount>500 | sitimechart count by referer_domain | search referer_domain=*.net | timechart count by referer_domain"}, {"language": "spl", "code": "... | redistribute by host | stats count by host | streamstats count by host,source"}, {"language": "spl", "code": "... | redistribute by host | stats count by host | sort 0 str(host) | streamstats count by host,source"}, {"language": "spl", "code": "search * | stats count by foo"}, {"language": "spl", "code": "search * | redistribute | stats count by foo | sort 0 str(foo)"}], "tables": [{"headers": ["Setting name", "Definition", "Default value"], "rows": [["maxReducersPerPhase", "The maximum number of indexers that can be used as intermediate reducers in the intermediate reduce phase.", "4"], ["winningRate", "The percentage of indexers that can be selected from the total pool of indexers and used as intermediate reducers in a parallel reduce search process. This setting applies only when thereducerssetting is not configured.", "50"], ["reducers", "A list of valid indexers that are to be used as dedicated intermediate reducers for parallel reduce search processing. When you run a search with theredistributecommand, the valid indexers in thereducerslist are the only indexers that are used for parallel reduce operations. If the number of valid indexers in thereducerslist exceeds themaxReducersPerPhasevalue, the Splunk platform randomly selects a set of indexers from thereducerslist that meets themaxReducersPerPhaselimit.", "\" \" (empty list)"]]}], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "redistribute", "section_heading": "Usage", "section_id": "f7614217_e135_420b_92c4_52e167c92fee--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/redistribute", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:19.058679+00:00", "version": "10.2"}}
{"id": "449214e48ab78862", "content": "1. Speed up a search on a large high-cardinality dataset In this example, the redistribute command is applied to a stats search that is running over an extremely large high-cardinality dataset. The redistribute command reduces the completion time for the search. The intermediate reducers process the | stats count by ip portion of the search in parallel, lowering the completion time for the search. The search head aggregates the results. 2. Speed up a timechart search without declaring a by-clause field to redistribute on This example uses a search over an extremely large high-cardinality dataset. The search string includes the eventstats command, and it uses the sitimechart command to perform the statistical calculations for a timechart operation. The search uses the redistribute command to reduce the completion time for the search. A by-clause field is not specified, so the search processor selects one. When this search runs, the intermediate reducers process the eventstats and sitimechart segments of the search in parallel, reducing the overall completion time of the search. On the search head, the timechart command takes the reduced sitimechart calculations and transforms them into a format that can be used for for charts and visualizations. Because a by-clause field is not identified in the search string, the intermediate reducers redistribute and partition events on the source field. 3. Speed up a search that uses tstats to generate events This example uses a search over an extremely large high-cardinality dataset. This search uses the tstats command in conjunction with the sitimechart and timechart commands. The redistribute command reduces the completion time for the search. You have to place the tstats command at the start of the search string with a leading pipe character. When you use the redistribute command in conjunction with tstats , you must place the redistribute command after the tstats segment of the search. In this example, the tstats command uses the prestats=t argument to work with the sitimechart and timechart commands. The redistribute command causes the intermediate reducers to process the sitimechart segment of the search in parallel, reducing the overall completion time for the search. The reducers then push the results to the search head, where the timechart command processes them into a format that you can use for charts and visualizations. 4. Speed up a search that includes a mix of supported and unsupported commands This example uses a search over an extremely large high-cardinality dataset. The search uses the redistribute command to reduce the search completion time. The search includes commands that are both supported and unsupported by the redistribute command. It uses the sort command to sort of the results after the rest of the search has been processed. You need the sort command for event sorting because the redistribute process undoes the sorting naturally provided by commands in the stats command family. In this example, the intermediate reducers process the eventstats and where segments in parallel. Those portions of the search complete faster than they would when the redistribute command is not used. The Splunk platform divides the work of processing the sort portion of the search between the indexer and the search head. 5. Speed up a search where a supported command splits by fields that are not in the redistribute command by-clause argument In this example, the redistribute command redistributes events across the intermediate reducers by the source field. The search includes two commands that are supported by the redistribute command but only one of them is processed on the intermediate reducers. In this case, the eventstats segment of the search is processed in parallel by the intermediate reducers because it includes source as a split-by field. The where segment is also processed on the intermediate reducers. The stats portion of the search, however, is processed on the search head because its split-by fields are not a superset of the set of fields that the events have been redistributed by. In other words, the stats split-by fields do not include source .", "code_examples": [{"language": "spl", "code": "... | redistribute by ip | stats count by ip"}, {"language": "spl", "code": "... | redistribute | eventstats count by user,source|wherecount>10 | sitimechart max(count) bysource| timechart max(count) bysource"}, {"language": "spl", "code": "| tstats prestats=t count BY _time span=1d | redistribute by _time | sitimechart span=1d count | timechart span=1d count"}, {"language": "spl", "code": "... | redistribute | eventstats count by user,source|wherecount >10  | sort 0 -num(count)"}, {"language": "spl", "code": "... | redistribute bysource| eventstats count bysource, host |wherecount > 10 | stats count by userid, host"}], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "redistribute", "section_heading": "Examples", "section_id": "e0d61ad0_06fc_44da_a2f2_797f2497fa76--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/redistribute", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:19.058684+00:00", "version": "10.2"}}
{"id": "f38b946b7d3dde29", "content": "The xmlkv command automatically extracts key-value pairs from XML-formatted data. For JSON-formatted data, use the spath command.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "xmlkv", "section_heading": "Description", "section_id": "c3b6ebe1_1aee_4907_a395_a14c66a566d9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xmlkv", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:34.678736+00:00", "version": "10.2"}}
{"id": "7533e0d33787e8c5", "content": "The required syntax is in bold. xmlkv [<field>] maxinputs=<int> Required arguments None. Optional arguments field Syntax: <field> Description: The field from which to extract the key and value pairs. Default: The _raw field. maxinputs Syntax: maxinputs=<int> Description: Sets the maximum number of events or search results that can be passed as inputs into the xmlkv command per invocation of the command. The xmlkv command is invoked repeatedly in increments according to the maxinputs argument until the search is complete and all of the results have been displayed. Do not change the value of maxinputs unless you know what you are doing. Default: 50000", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "xmlkv", "section_heading": "Syntax", "section_id": "id_08d3907d_4132_4320_8815_436932f12ae3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xmlkv", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:34.678743+00:00", "version": "10.2"}}
{"id": "0b32319332864bf0", "content": "The xmlkv command is a distributable streaming command. See Command types. Keys and values in XML elements From the following XML, name is the key and Settlers of Catan is the value in the first element.", "code_examples": [{"language": "spl", "code": "<game>\n   <name>Settlers of Catan</name>\n   <category>competitive</category>\n</game>\n<game>\n   <name>Ticket to Ride</name>\n   <category>competitive</category>\n</game>"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "xmlkv", "section_heading": "Usage", "section_id": "a7fc56af_50b8_4fd3_b6d5_81df5275568b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xmlkv", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:34.678747+00:00", "version": "10.2"}}
{"id": "6354d4f5d2aaefa2", "content": "1. Automatically extract key-value pairs Extract key-value pairs from XML tags in the _raw field. Processes a maximum of 50000 events. 2. Extract key-value pairs in a specific number of increments Extract the key-value pairs from events or search results in increments of 10,000 per invocation of the xmlkv command until the search has finished and all of the results are displayed.", "code_examples": [{"language": "spl", "code": "... | xmlkv"}, {"language": "spl", "code": "... | xmlkv maxinputs=10000"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "xmlkv", "section_heading": "Examples", "section_id": "ad1e4c3c_5ced_473f_9139_dde215a0702a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xmlkv", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:34.678752+00:00", "version": "10.2"}}
{"id": "95b18e15dc8d31fc", "content": "Commands extract kvform multikv rex spath xpath", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "xmlkv", "section_heading": "See also", "section_id": "id_2fcaafbd_18e9_488a_8fcf_3fd4baa91af7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xmlkv", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:34.678755+00:00", "version": "10.2"}}
{"id": "ba93ab45f0e14e2c", "content": "Converts events generated by streaming search commands into metric data points and inserts the data into a metric index on the indexers. You can use the meventcollect command only if your role has the run_mcollect capability. See Define roles on the Splunk platform with capabilities in Securing Splunk Enterprise. CAUTION: This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "meventcollect", "section_heading": "Description", "section_id": "d78978c7_56bc_4e2a_9dd1_a3904a1ffda4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/meventcollect", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:44.939455+00:00", "version": "10.2"}}
{"id": "a65e361bdf1f6aa1", "content": "The required syntax is in bold. | meventcollect index=<string> [ file=<string> ] [ split=<bool> ] [ spool=<bool> ] [ prefix_field=<string> ] [ host=<string> ] [ source=<string> ] [ sourcetype=<string> ] [ <field-list> ] Required arguments index Syntax: index=<string> Description: Name of the metric index where the collected metric data is added. field-list Syntax: <field>, ... Description: A list of dimension fields. Required if split=true. Optional if split=false. If unspecified (which implies that split=false ), meventcollect treats all fields as dimensions for the data point, except for the metric_name , prefix_field , and all internal fields. Default: No default value Optional arguments file Syntax: file=<string> Description: The file name where you want the collected metric data to be written. Only applicable when spool=false. You can use a timestamp or a random number for the file name by specifying either file=$timestamp$ or file=$random$. Default: $random$_metrics.csv split Syntax: split=<bool> Description: Determines how meventcollect identifies the measures in an event. See How to use the split argument. Default: false spool Syntax: spool=<bool> Description: If set to true, meventcollect writes the metrics data file to the Splunk spool directory, $SPLUNK_HOME/var/spool/splunk , where the file is indexed automatically. If set to false, meventcollect writes the file to the $SPLUNK_HOME/var/run/splunk directory. The file remains in this directory unless further automation or administration is done. Default: true prefix_field Syntax: prefix_field=<string> Description: Only applicable when split=true. If specified, meventcollect ignores any data point with that field missing. Otherwise, meventcollect prefixes the field value to the metric name. See Set a prefix field. Default: No default value host Syntax: host=<string> Description: The name of the host that you want to specify for the collected metrics data. Only applicable when spool=true. Default: No default value source Syntax: source=<string> Description: The name of the source that you want to specify for the collected metrics data. Default: If the search is scheduled, the name of the search. If the search is ad-hoc, meventcollect writes the name of the file to the var/spool/splunk directory containing the search results. sourcetype Syntax: sourcetype=<string> Description: The name of the source type that you want to specify for the collected metrics data. Default: metrics_csv CAUTION: Do not change this setting without assistance from Splunk Professional Services or Splunk Support. Changing the source type requires a change to the props.conf file.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "meventcollect", "section_heading": "Syntax", "section_id": "id_44dc1895_e187_456b_964d_a7845a9d5a46--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/meventcollect", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:44.939461+00:00", "version": "10.2"}}
{"id": "2fdc6542dc5af517", "content": "You use the meventcollect command to convert streaming events into metric data to be stored in a metric index on the indexers. The metrics data uses a specific format for the metrics fields. See Metrics data format in Metrics. Only streaming commands can precede the meventcollect command so that results can be ingested on the indexers. If you would like to run a search that uses transforming commands to generate metric data points, use mcollect instead of meventcollect. CAUTION: The meventcollect command causes new data to be written to a metric index for every run of the search. In addition, if you run an meventcollect search over large amounts of data, it potentially can overwhelm indexers and indexer clusters that do not have a significant amount of capacity. Note: All metrics search commands are case sensitive. This means, for example, that meventcollect treats as the following as three distinct values of metric_name : cap.gear , CAP.GEAR , and Cap.Gear. The Splunk platform cannot index metric data points that contain metric_name fields which are empty or composed entirely of white spaces. How to use the split argument The split argument determines how meventcollect identifies the measurement fields in your search. It defaults to false. When split=false , your search needs to explicitly identify its measurement fields. If necessary it can use rename or eval conversions to do this. If you have single-metric events, your meventcollect search must produce results with a metric_name field that provides the name of the measure, and a _value field that provides the measure's numeric value. If you have multiple-metric events, your meventcollect search must produce results that follow this syntax: metric_name:<metric_name>=<numeric_value>. Each of these fields will be treated as a measurement. meventcollect treats the remaining fields as dimensions. When you set split=true , you use field-list to identify the dimensions in your search. meventcollect converts any field that is not in the field-list into a measurement. The only exceptions are internal fields beginning with an underscore and the prefix_field , if you have set one. When you set split=allnums , meventcollect treats all numeric fields as metric measures and all non-numeric fields as dimensions. You can optionally use field-list to declare that meventcollect should treat certain numeric fields in the events as dimensions. Set a prefix field Use the prefix_field argument to apply a prefix to the metric fields in your event data. For example, if you have the following data: type=cpu usage=0.78 idle=0.22 You have two metric fields, usage and idle. Say you include the following in an mcatalog search of that data: Because you have set split = true the Splunk software automatically converts those fields into measures, because they are not otherwise identified in a <field-list>. Then it applies the value of the specified prefix_field as a prefix to the metric field names. In this case, because you have specified the type field as the prefix field, its value, cpu , becomes the metric name prefix. The results look like this:", "code_examples": [{"language": "spl", "code": "...split=trueprefix_field=type..."}], "tables": [{"headers": ["metric_name:cpu.usage", "metric_name:cpu.idle"], "rows": [["0.78", "0.22"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "meventcollect", "section_heading": "Usage", "section_id": "e3a578e3_8253_4430_a5d5_86705f566820--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/meventcollect", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:44.939467+00:00", "version": "10.2"}}
{"id": "afb28d1ee267f762", "content": "1: Collect metrics.log data into a metrics index The following example shows you how to collect metrics log data into a metric index called 'my_metric_index'.", "code_examples": [{"language": "spl", "code": "index=_internalsource=*/metrics.log \n|evalprefix = group +\".\"+ name \n| meventcollect index=my_metric_index split=trueprefix_field=prefix name group"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "meventcollect", "section_heading": "Examples", "section_id": "id_148bcadc_3114_482b_b3d2_b84d181ab752--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/meventcollect", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:44.939471+00:00", "version": "10.2"}}
{"id": "f524948ac6070539", "content": "Commands collect mcollect", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "meventcollect", "section_heading": "See also", "section_id": "e95d7e90_7182_4289_bb56_eb7a33d8b521--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/meventcollect", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:44.939474+00:00", "version": "10.2"}}
{"id": "be407c1503a998f4", "content": "Extracts field-values from table-formatted search results, such as the results of the top , tstats , and so on. The multikv command creates a new event for each table row and assigns field names from the title row of the table. An example of the type of data the multikv command is designed to handle: The key properties here are: Each line of text represents a conceptual record. The columns are aligned. The first line of text provides the names for the data in the columns. The multikv command can transform this table from one event into three events with the relevant fields. It works more easily with the fixed-alignment though can sometimes handle merely ordered fields. The general strategy is to identify a header, offsets, and field counts, and then determine which components of subsequent lines should be included into those field names. Multiple tables in a single event can be handled (if multitable=true), but might require ensuring that the secondary tables have capitalized or ALLCAPS names in a header row. Auto-detection of header rows favors rows that are text, and are ALLCAPS or Capitalized. Note: For Splunk Cloud Platform, you must create a private app to extract field-value pairs from table-formatted search results. If you are a Splunk Cloud administrator with experience creating private apps, see Manage private apps in your Splunk Cloud deployment in the Splunk Cloud Admin Manual. If you have not created private apps, contact your Splunk account representative for help with this customization.", "code_examples": [{"language": "spl", "code": "Name     Age   Occupation\nJosh     42    SoftwareEngineer\nFrancine 35    CEO\nSamantha 22    ProjectManager"}], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "multikv", "section_heading": "Description", "section_id": "id_403dc3ea_2ac7_41ba_a609_e14590e87515--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/multikv", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:54.668802+00:00", "version": "10.2"}}
{"id": "94f5d603283efec2", "content": "multikv [conf=<stanza_name>] [<multikv-option>...] Optional arguments conf Syntax: conf=<stanza_name> Description: If you have a field extraction defined in multikv.conf , use this argument to reference the stanza in your search. For more information, refer to the configuration file reference for multikv.conf in the Admin Manual. <multikv-option> Syntax: copyattrs=<bool> | fields <field-list> | filter <term-list> | forceheader=<int> | multitable=<bool> | noheader=<bool> | rmorig=<bool> Description: Options for extracting fields from tabular events. Descriptions for multikv options copyattrs Syntax: copyattrs=<bool> Description: When true, multikv copies all fields from the original event to the events generated from that event. When false, no fields are copied from the original event. This means that the events will have no _time field and the UI will not know how to display them. Default: true fields Syntax: fields <field-list> Description: Limit the fields set by the multikv extraction to this list. Ignores any fields in the table which are not on this list. filter Syntax: filter <term-list> Description: If specified, multikv skips over table rows that do not contain at least one of the strings in the filter list. Quoted expressions are permitted, such as \"multiple words\" or \"trailing_space \". forceheader Syntax: forceheader=<int> Description: Forces the use of the given line number (1 based) as the table's header. Does not include empty lines in the count. Default: The multikv command attempts to determine the header line automatically. multitable Syntax: multitable=<bool> Description: Controls whether or not there can be multiple tables in a single _raw in the original events. Default: true noheader Syntax: noheader=<bool> Description: Handle a table without header row identification. The size of the table will be inferred from the first row, and fields will be named Column_1, Column_2, ... noheader=true implies multitable=false. Default: false rmorig Syntax: rmorig=<bool> Description: When true, the original events will not be included in the output results. When false, the original events are retained in the output results, with each original emitted after the batch of generated results from that original. Default: true", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "multikv", "section_heading": "Syntax", "section_id": "ddf5dd83_eb52_4b74_abd5_7e5868bfb49e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/multikv", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:54.668811+00:00", "version": "10.2"}}
{"id": "198e23fc064accfa", "content": "The multikv command is a distributable streaming command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "multikv", "section_heading": "Usage", "section_id": "id_8af9f9c2_628d_4be8_9a1b_28ef24daedfc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/multikv", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:54.668815+00:00", "version": "10.2"}}
{"id": "3f9fcfc5a115bcff", "content": "Example 1: Extract the \"COMMAND\" field when it occurs in rows that contain \"splunkd\". Example 2: Extract the \"pid\" and \"command\" fields.", "code_examples": [{"language": "spl", "code": "... | multikv fields COMMAND filter splunkd"}, {"language": "spl", "code": "... | multikv fields pidcommand"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "multikv", "section_heading": "Examples", "section_id": "id_80a3b226_623a_4a48_b999_05b46c228187--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/multikv", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:54.668820+00:00", "version": "10.2"}}
{"id": "d59392d22b66f1b4", "content": "extract , kvform , rex , spath , xmlkv ,", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "multikv", "section_heading": "See also", "section_id": "ef7aa99e_6dd4_442f_86f8_8c3ecd9ece0b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/multikv", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:55:54.668824+00:00", "version": "10.2"}}
{"id": "fc3c48524e3432d6", "content": "Use the return command to return values from a subsearch. return replaces the incoming events with one event, with one attribute: \"search\". To improve performance, the return command automatically limits the number of incoming results with the head command and the resulting fields with the fields command. By default, the return command uses only the first row of results. Use the count argument to specify the number of results to use.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "return", "section_heading": "Description", "section_id": "e08f23f5_8a23_4d06_bebf_2df8dbd3edaa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/return", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:04.236476+00:00", "version": "10.2"}}
{"id": "51192c64ebacd2d3", "content": "return [<count>] [<alias>=<field>...] [<field>...] [$<field>...] Required arguments None. Optional arguments <count> Syntax: <int> Description: Specify the number of rows. Default: 1, which is the first row of results passed into the command. <alias> Syntax: <alias>=<field>... Description: Specify the field alias and value to return. You can specify multiple pairs of aliases and values, separated by spaces. The <alias> argument does not support spaces before and after the '=' sign. <field> Syntax: <field>... Description: Specify one or more fields to return, separated by spaces. <$field> Syntax: <$field> Description: Specify one or more field values to return, separated by spaces.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "return", "section_heading": "Syntax", "section_id": "a5d37d4f_276c_4e9c_9218_db8c95760064--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/return", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:04.236485+00:00", "version": "10.2"}}
{"id": "71a4d2534e565632", "content": "The command is convenient for outputting a field name, an alias-value pair, or just a field value. By default, the return command uses only the first row of results. You can specify multiple rows, for example ' return 2 ip '. Each row is viewed as an OR clause, that is, output might be ' (ip=10.1.11.2) OR (ip=10.2.12.3) '. Multiple values can be specified and are placed within OR clauses. So, ' return 2 user ip ' might output ' (user=bob ip=10.1.11.2) OR (user=fred ip=10.2.12.3) '. In most cases, using the return command at the end of a subsearch removes the need for head , fields , rename , format , and dedup. Duplicate values Suppose you have the following search: You might logically expect the command to return the first two distinct users. Instead the command looks at the first two events, based on the ordering from the implied head command. The return command returns the users within those two events. The command does not determine if the user value is unique. If the same user is listed in these events, the command returns only the one user. To return unique values, you need to include the dedup command in your search. For example: When the input for 'return' is 0 events When the input to the return command is 0 events, the results of the search can be misleading. To avoid this, test your subsearches outside of the main search to verify that they return events. For example, say you have the following search: If index=A test_errror returns 0 events, the subsearch returns an empty string. The final result of the full search is all events from index=B. If you are unaware of the result of the subsearch, this could lead you to believe that all events from index=B satisfied the condition of having test_error for their clientip , when in fact none did. Quotations in returned fields The return command does not escape quotation marks that are in the fields that are returned. You must use an eval command to escape the quotation marks before you use the return command. For example: If you encounter problems with the return command If you encounter difficulties when running the return command, consider running the oldreturn command instead. oldreturn is a deprecated version of return that does not require spaces around the = symbol for the alias argument. The tradeoff is that oldreturn searches complete slower than return searches.", "code_examples": [{"language": "spl", "code": "sourcetype=WinEventLog:Security |return2 user"}, {"language": "spl", "code": "sourcetype=WinEventLog:Security | dedup user |return2 user"}, {"language": "spl", "code": "index=B [index=A test_error |returnclientip]"}, {"language": "spl", "code": "...[searchevalfield2=replace(field1,\"\\\"\",\"\\\\\\\"\") |returnfield2]"}], "tables": [{"headers": ["Output", "Example"], "rows": [["Field name", "return source"], ["Alias=value", "return ip=srcip"], ["Value", "return $srcip"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "return", "section_heading": "Usage", "section_id": "id_147d5c1e_9f53_4e96_aa67_1972dcbfbf77--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/return", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:04.236494+00:00", "version": "10.2"}}
{"id": "a1dedc8b8ec7bfdc", "content": "Example 1: Search for ' error ip=<someip> ', where <someip> is the most recent ip used by user 'boss'. Example 2: Search for ' error (user=user1 ip=ip1) OR (user=user2 ip=ip2) ', where the users and IPs come from the two most-recent logins. Example 3: Return to eval the userid of the last user, and increment it by 1.", "code_examples": [{"language": "spl", "code": "error [ search user=boss |returnip ]"}, {"language": "spl", "code": "error [ search login |return2 user ip ]"}, {"language": "spl", "code": "... |evalnextid = 1 + [ search user=* |return$id] | ..."}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "return", "section_heading": "Examples", "section_id": "id_9eb9fac8_a49e_490d_8e89_84d0db31e918--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/return", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:04.236499+00:00", "version": "10.2"}}
{"id": "fe14843cf06fdd73", "content": "format , search", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "return", "section_heading": "See also", "section_id": "id_7a16c4c6_e7a1_4d29_869c_5eee2ca24035--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/return", "breadcrumb": "Splunk Enterprise (10.2) > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:04.236503+00:00", "version": "10.2"}}
{"id": "6ea4c181b45c5c54", "content": "The associate command identifies correlations between fields. The command tries to find a relationship between pairs of fields by calculating a change in entropy based on their values. This entropy represents whether knowing the value of one field helps to predict the value of another field. In Information Theory , entropy is defined as a measure of the uncertainty associated with a random variable. In this case if a field has only one unique value, the field has an entropy of zero. If the field has multiple values, the more evenly those values are distributed, the higher the entropy. The associate command uses Shannon entropy (log base 2). The unit is in bits .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "associate", "section_heading": "Description", "section_id": "b1e5fd8a_4f6b_40db_8d65_36968caaced3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/associate", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:20.632280+00:00", "version": "10.2"}}
{"id": "ed229d3164a9d2ed", "content": "associate [<associate-options>...] [field-list] Required arguments None. Optional arguments associate-option Syntax: supcnt | supfreq | improv Description: Options for the associate command. See the Associate-options section. field-list Syntax: <field> ... Description: A list of one or more fields. You cannot use wildcard characters in the field list. If you specify a list of fields, the analysis is restricted to only those fields. Default: All fields are analyzed. Associate-options supcnt Syntax: supcnt=<num> Description: Specifies the minimum number of times that the \"reference key=reference value\" combination must appear. Must be a non-negative integer. Default: 100 supfreq Syntax: supfreq=<num> Description: Specifies the minimum frequency of \"reference key=reference value\" combination as a fraction of the number of total events. Default: 0.1 improv Syntax: improv=<num> Description: Specifies a limit, or minimum entropy improvement, for the \"target key\". The calculated entropy improvement must be greater than or equal to this limit. Default: 0.5 Columns in the output table The associate command outputs a table with columns containing the following fields.", "code_examples": [], "tables": [{"headers": ["Field", "Description"], "rows": [["Reference_Key", "Thenameof the first field in a pair of fields."], ["Reference_Value", "Thevaluein the first field in a pair of fields."], ["Target_Key", "The name of the second field in a pair of fields."], ["Unconditional_Entropy", "The entropy of the target key."], ["Conditional_Entropy", "The entropy of the target key when the reference key is the reference value."], ["Entropy_Improvement", "The difference between the unconditional entropy and the conditional entropy."], ["Description", "A message that summarizes the relationship between the field values that is based on the entropy calculations. TheDescriptionis a textual representation of the result. It is written in the format: \"When the 'Reference_Key' has the value 'Reference_Value', the entropy of 'Target_Key' decreases fromUnconditional_EntropytoConditional_Entropy.\""], ["Support", "Specifies how often the reference field is the reference value, relative to the total number of events. For example, how often field A is equal to value X, in the total number of events."]]}], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "associate", "section_heading": "Syntax", "section_id": "b99520bc_35d4_4d1b_ba33_c3382b08631e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/associate", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:20.632299+00:00", "version": "10.2"}}
{"id": "d04c14513674f193", "content": "1. Analyze the relationship between fields in web access log files This example demonstrates one way to analyze the relationship of fields in your web access logs. The first part of this search retrieves web access events that returned a status that is not 200. Web access data contains many fields. You can use the associate command to see a relationship between all pairs of fields and values in your data. To simplify this example, restrict the search to two fields: method and status. Because the associate command adds many columns to the output, this search uses the table command to display only select columns. The results appear on the Statistics tab and look something like this: In the results you can see that there is one method and five status values in the results. From the first row of results, you can see that when method=POST , the status field is 503 for those events. The associate command concludes that, if method=POST , the Top_Conditional_Value is likely to be 503 as much as 33% of the time. The Reference_Key and Reference_Value are being correlated to the Target_Key. The Top_Conditional_Value field states three things: The most common value for the given Reference_Value. The frequency of the Reference_Value for that field in the dataset, sometimes referred to as FRV. The frequency of the most common associated value in the Target_Key for the events that have the specific Reference_Value in that Reference Key. Sometimes referred to as the FCV. The values in the Top_Conditional_Value field are formatted as \"CV (FRV% -> FCV%)\", for example GET (76.37% -> 83.45%). 2. Return results that have at least 3 references to each other Return results associated with each other (that have at least 3 references to each other). 3. Analyze events from a host Analyze all events from host \"reports\" and return results associated with each other.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* status!=200 | fields method, status | associate improv=0.05  | table Reference_Key, Reference_Value, Target_Key, Top_Conditional_Value, Description"}, {"language": "spl", "code": "index=_internal sourcetype=splunkd | associate supcnt=3"}, {"language": "spl", "code": "host=\"reports\"| associate supcnt=50 supfreq=0.2 improv=0.5"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}, {"headers": ["Reference_Key", "Reference_Value", "Target_Key", "Top_Conditional_Value", "Description"], "rows": [["method", "POST", "status", "503 (17.44% -> 33.96%)", "When 'method' has the value 'POST', the entropy of 'status' decreases from 2.923 to 2.729."], ["status", "400", "method", "GET (76.37% -> 83.45%)", "When 'status' has the value '400', the entropy of 'method' decreases from 0.789 to 0.647."], ["status", "404", "method", "GET (76.37% -> 81.27%)", "When 'status' has the value '404', the entropy of 'method' decreases from 0.789 to 0.696."], ["status", "406", "method", "GET (76.37% -> 81.69%)", "When 'status' has the value '406', the entropy of 'method' decreases from 0.789 to 0.687."], ["status", "408", "method", "GET (76.37% -> 80.00%)", "When 'status' has the value '408', the entropy of 'method' decreases from 0.789 to 0.722."], ["status", "500", "method", "GET (76.37% -> 80.73%)", "When 'status' has the value '500', the entropy of 'method' decreases from 0.789 to 0.707."]]}], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "associate", "section_heading": "Examples", "section_id": "id_4383d23a_e916_4a86_a59e_347ef6245631--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/associate", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:20.632312+00:00", "version": "10.2"}}
{"id": "27cb90dc2fe6fce7", "content": "arules , correlate , contingency", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "associate", "section_heading": "See also", "section_id": "c2965775_eb9e_4c13_b86f_eff04e6a080c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/associate", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:20.632318+00:00", "version": "10.2"}}
{"id": "cadf209c9400eb2d", "content": "Returns information about the buckets in the specified index. If you are using Splunk Enterprise, this command helps you understand where your data resides so you can optimize disk usage as required. Searches on an indexer cluster return results from the primary buckets and replicated copies on other peer nodes. The Splunk index is the repository for data ingested by Splunk software. As incoming data is indexed and transformed into events , Splunk software creates files of rawdata and metadata ( index files ). The files reside in sets of directories organized by age. These directories are called buckets. For more information, see Indexes, indexers, and clusters and How the indexer stores indexes in Managing Indexers and Clusters of Indexers .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "dbinspect", "section_heading": "Description", "section_id": "f79458ef_8cd4_44be_bdcd_44a0763a0390--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/dbinspect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:39.179674+00:00", "version": "10.2"}}
{"id": "339063e04385709c", "content": "The required syntax is in bold. | dbinspect [index=<wc-string>]... [<span> | <timeformat>] [corruptonly=<bool>] [cached=<bool>] Required arguments None. Optional arguments index Syntax: index=<wc-string>... Description: Specifies the name of an index to inspect. You can specify more than one index. For all internal and non-internal indexes, you can specify an asterisk ( * ) in the index name. Default: The default index, which is typically main. <span> Syntax: span=<int> | span=<int><timescale> Description: Specifies the span length of the bucket. If using a timescale unit (second, minute, hour, day, month, or subseconds), this is used as a time range. If not, this is an absolute bucket \"length\". When you invoke the dbinspect command with a bucket span, a table of the spans of each bucket is returned. When span is not specified, information about the buckets in the index is returned. See Information returned when no span is specified. <timeformat> Syntax: timeformat=<string> Description: Sets the time format for the modTime field. Default: timeformat=%m/%d/%Y:%H:%M:%S <corruptonly> Syntax: corruptonly=<bool> Description: Specifies that each bucket is checked to determine if any buckets are corrupted and displays only the corrupted buckets. A bucket is corrupt when some of the files in the bucket are incorrect or missing such as Hosts.data or tsidx. A corrupt bucket might return incorrect data or render the bucket unsearchable. In most cases the software will auto-repair corrupt buckets. When corruptonly=true , each bucket is checked and the following informational message appears. Not supported on Splunk SmartStore indexes. INFO: The \"corruptonly\" option will check each of the specified buckets. This search might be slow and will take time. Default: false cached Syntax: cached=<bool> Description: If set to cached=true , the dbinspect command gets the statistics from the bucket's manifest. If set to cached=false , the dbinspect command examines the bucket itself. For SmartStore buckets, cached=false examines an indexer's local copy of the bucket. However, specifying cached=true examines instead the bucket's manifest, which contains information about the canonical version of the bucket that resides in the remote store. For more information see Troubleshoot SmartStore in Managing Indexers and Clusters of Indexers. Default: For non-SmartStore indexes, the default is false. For SmartStore indexes, the default is true. Time scale units These are options for specifying a timescale as the bucket span. <timescale> Syntax: <sec> | <min> | <hr> | <day> | <month> | <subseconds> Description: Time scale units. Information returned when no span is specified When you invoke the dbinspect command without the span argument, the following information about the buckets in the index is returned.", "code_examples": [], "tables": [{"headers": ["Time scale", "Syntax", "Description"], "rows": [["<sec>", "s | sec | secs | second | seconds", "Time scale in seconds."], ["<min>", "m | min |  mins |  minute |  minutes", "Time scale in minutes."], ["<hr>", "h | hr |  hrs |  hour | hours", "Time scale in hours."], ["<day>", "d |  day | days", "Time scale in days."], ["<month>", "mon | month |  months", "Time scale in months."], ["<subseconds>", "us | ms |  cs |  ds", "Time scale in microseconds (us), milliseconds (ms), centiseconds (cs), or deciseconds (ds)"]]}, {"headers": ["Field name", "Description"], "rows": [["bucketId", "A string comprised of<index>~<id>~<guId>, where the delimiters are tilde characters. For example,summary~2~4491025B-8E6D-48DA-A90E-89AC3CF2CE80."], ["endEpoch", "The timestamp for the last event in the bucket, which is the time-edge of the bucket furthest towards the future. Specify the timestamp in the number of seconds from the UNIX epoch."], ["eventCount", "The number of events in the bucket."], ["guId", "The globally unique identifier (GUID) of the server that hosts the index. This is relevant for index replication."], ["hostCount", "The number of unique hosts in the bucket."], ["id", "The local ID number of the bucket, generated on the indexer on which the bucket originated."], ["index", "The name of the index specified in your search. You can specifyindex=*to inspect all of the indexes, and the index field will vary accordingly."], ["modTime", "The timestamp for the last time the bucket was modified or updated, in a format specified by thetimeformatflag."], ["path", "The location to the bucket. The naming convention for the bucketpathvaries slightly, depending on whether the bucket rolled to warm while its indexer was functioning as a cluster peer:For non-clustered buckets:db_<newest_time>_<oldest_time>_<localid>For clustered original bucket copies:db_<newest_time>_<oldest_time>_<localid>_<guid>For clustered replicated bucket copies:rb_<newest_time>_<oldest_time>_<localid>_<guid>For more information, read\"How Splunk stores indexes\"and\"Basic cluster architecture\"inManaging Indexers and Clusters of Indexers."], ["rawSize", "The volume in bytes of the raw data files in each bucket. This value represents the volume before compression and the addition of index files."], ["sizeOnDiskMB", "The size in MB of disk space that the bucket takes up expressed as a floating point number. This value represents the volume of the compressed raw data files and the index files."], ["sourceCount", "The number of unique sources in the bucket."], ["sourceTypeCount", "The number of unique sourcetypes in the bucket."], ["splunk_server", "The name of the Splunk server that hosts the index in a distributed environment."], ["startEpoch", "The timestamp for the first event in the bucket (the time-edge of the bucket furthest towards the past), in number of seconds from the UNIX epoch."], ["state", "Specifies whether the bucket is warm, hot, cold."], ["tsidxState", "Specifies whether each bucket contains full-size or reduced tsidx files. If the value of this field in the results isfull, the tsidx files are full-size. If the value ismini, the tsidx files are reduced. SeeDetermine whether a bucket is reducedin Splunk EnterpriseManaging Indexers and Clusters of Indexers."], ["corruptReason", "Specifies the reason why the bucket is corrupt. The corruptReason field appears only whencorruptonly=true."]]}], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "dbinspect", "section_heading": "Syntax", "section_id": "id_1d42f03c_7d67_4860_8263_e491243138e4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/dbinspect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:39.179692+00:00", "version": "10.2"}}
{"id": "eca341f6f1b38d58", "content": "The dbinspect command is a generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. Accessing data and security If no data is returned from the index that you specify with the dbinspect command, it is possible that you do not have the authorization to access that index. The ability to access data in the Splunk indexes is controlled by the authorizations given to each role. See Use access control to secure Splunk data in Securing Splunk Enterprise. Non-searchable bucket copies For hot non-searchable bucket copies on target peers, tsidx and other metadata files are not maintained. Because accurate information cannot be reported, the following fields show NULL: eventCount hostCount sourceCount sourceTypeCount startEpoch endEpoch", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "dbinspect", "section_heading": "Usage", "section_id": "id_029df858_f40e_4859_94d5_e16702aac3a9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/dbinspect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:39.179697+00:00", "version": "10.2"}}
{"id": "1f7c7769b67ffc68", "content": "1. CLI use of the dbinspect command Display a chart with the span size of 1 day, using the command line interface (CLI). The results look like this: 2. Default dbinspect output Default dbinspect output for a local _internal index. The results look like this: This screenshot does not display all of the columns in the output table. On your computer, scroll to the right to see the other columns. 3. Check for corrupt buckets Use the corruptonly argument to display information about corrupted buckets, instead of information about all buckets. The output fields that display are the same with or without the corruptonly argument. 4. Count the number of buckets for each Splunk server Use this command to verify that the Splunk servers in your distributed environment are included in the dbinspect command. Counts the number of buckets for each server. 5. Find the index size of buckets in GB Use dbinspect to find the index size of buckets in GB. For current numbers, run this search over a recent time range. 6. Determine whether a bucket is reduced Run the dbinspect search command: If the value of the tsidxState field for each bucket is full , the tsidx files are full-size. If the value is mini , the tsidx files are reduced.", "code_examples": [{"language": "spl", "code": "myLaptop $ splunk search\"| dbinspect index=_internal span=1d\""}, {"language": "spl", "code": "_time            hot-3 warm-1 warm-2\n--------------------------- ----- ------ ------\n2015-01-17 00:00:00.000 PST            0       \n2015-01-17 14:56:39.000 PST            0       \n2015-02-19 00:00:00.000 PST            0      1\n2015-02-20 00:00:00.000 PST     2             1"}, {"language": "spl", "code": "| dbinspect index=_internal"}, {"language": "spl", "code": "| dbinspect index=_internal corruptonly=true"}, {"language": "spl", "code": "| dbinspect index=_internal | stats count by splunk_server"}, {"language": "spl", "code": "| dbinspect index=_internal |evalGB=sizeOnDiskMB/1024| stats sum(GB)"}, {"language": "spl", "code": "| dbinspect index=_internal"}], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "dbinspect", "section_heading": "Examples", "section_id": "id_9dfe70fa_b2f3_49f1_8615_aaffc66b83de--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/dbinspect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:39.179703+00:00", "version": "10.2"}}
{"id": "b60859b2c969935d", "content": "This command is used implicitly by subsearches. This command takes the results of a subsearch , formats the results into a single result and places that result into a new field called search. The format command performs similar functions as the return command.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "format", "section_heading": "Description", "section_id": "cd7c8477_01c7_491a_beca_f5922b1f6aff--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/format", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:56.207994+00:00", "version": "10.2"}}
{"id": "f131b78c19fac193", "content": "The required syntax is in bold. format [mvsep=\"<mv separator>\"] [maxresults=<int>] [\"<row prefix>\" \"<column prefix>\" \"<column separator>\" \"<column end>\" \"<row separator>\" \"<row end>\"] [emptystr=\"<string>\"] If you want to specify a row or column options, you must specify all of the row and column options. Required arguments None. Optional arguments mvsep Syntax: mvsep=\"<string>\" Description: The separator to use for multivalue fields. Default: OR maxresults Syntax: maxresults=<int> Description: The maximum results to return. Default: 0, which means no limitation on the number of results returned. <row prefix> Syntax: \"<string>\" Description: The value to use for the row prefix. Default: The open parenthesis character \"(\" <column prefix> Syntax: \"<string>\" Description: The value to use for the column prefix. Default: The open parenthesis character \"(\" <column separator> Syntax: \"<string>\" Description: The value to use for the column separator. Default: AND <column end> Syntax: \"<string>\" Description: The value to use for the column end. Default: The close parenthesis character \")\" <row separator> Syntax: \"<string>\" Description: The value to use for the row separator. Default: OR <row end> Syntax: \"<string>\" Description: The value to use for the column end. Default: The close parenthesis character \")\" emptystr Syntax: emptystr=\"<string>\" Description: The value that the format command outputs instead of the default empty string NOT( ) if the results generated up to that point are empty and no fields or values other than internal fields are returned. You can set this argument to a custom string that is displayed instead of the default empty string whenever your search results are empty. Default: NOT( )", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "format", "section_heading": "Syntax", "section_id": "ef3a6590_b422_4702_bb75_cfb60f57f191--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/format", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:56.208003+00:00", "version": "10.2"}}
{"id": "7c53c7750e50d1e0", "content": "By default, when you do not specify any of the optional row and column arguments, the output of the format command defaults to: \"(\" \"(\" \"AND\" \")\" \"OR\" \")\". Specifying row and column arguments There are several reasons to specify the row and column arguments: Subsearches There is an implicit format at the end of a subsearch that uses the default values for column and row arguments. For example, you can specify OR for the column separator by including the format command at the end of the subsearch. Export the search to a different system Specify the row and column arguments when you need to export the search to another system that requires different formatting.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "format", "section_heading": "Usage", "section_id": "e5454a0a_c67f_4be9_b03c_95518aea65fe--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/format", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:56.208009+00:00", "version": "10.2"}}
{"id": "e65c1f525d24c8f1", "content": "1. Example with no optional parameters Suppose that you have results that look like this: The following search returns the top 2 results, and creates a search based on the host, source, and sourcetype fields. The default format settings are used. This search returns the syntax for a search that is based on the field values in the top 2 results. The syntax is placed into a new field called search. 2. Example using the optional parameters You want to produce output that is formatted to use on an external system. Using the data in Example 1, the result is: 3. Multivalue separator example The following search uses the eval command to create a field called \"foo\" that contains one value \"eventtype,log_level\". The makemv command is used to make the foo field a mulitvalue field and specifies the comma as the delimiter between the values. The search then outputs only the foo field and formats that field. This results in the following output: 4. Use emptystr to indicate empty results When a search generates empty results, the format command returns internal fields and the contents of emptystr. You can change the value of emptystr from the default to a custom string. For example, the results in the following search are empty, so format returns a customized string \"Error Found\" in a new field called search. The results look something like this. If your search doesn't include emptystr like the following example, the format command displays the default empty string to indicate that the results are empty. The results look like this. 5. Use emptystr in a subsearch as a failsafe Customizing your empty string as shown in the last example is one way to use emptystr. However, it is more typical to use the format command as a subsearch that is operating as a search filter, and then use emptystr as a failsafe in case your search returns empty results. For example, perhaps your index isn't generating results because one of the fields you're specifying in the subsearch doesn't exist or there's a typo or some other error in your search. You can include the emptystr argument and set it to a default source type that you know is always present, such as splunkd. Then, instead of returning nothing, your search will return some results that you can use for further filtering. You can use the following sample search to make sure you get results even if your search contains errors. The results look something like this.", "code_examples": [{"language": "spl", "code": "... | head 2 | fieldssource, sourcetype, host | format"}, {"language": "spl", "code": "...  | format\"[\"\"[\"\"&&\"\"]\"\"||\"\"]\""}, {"language": "spl", "code": "index=_internal |head 1 |evalfoo=\"eventtype,log_level\"| makemv delim=\",\"foo | fields foo | format mvsep=\"mvseparator\"\"{\"\"[\"\"AND\"\"]\"\"AND\"\"}\""}, {"language": "spl", "code": "| makeresults count=1 | format emptystr=\"Error Found\""}, {"language": "spl", "code": "| makeresults count=1 | format"}, {"language": "spl", "code": "index=_internal sourcetype=\n    [search index=does_not_exist | head 1 \n    | fields sourcetype \n    | format emptystr=\"splunkd\"]"}], "tables": [{"headers": ["source", "sourcetype", "host"], "rows": [["syslog.log", "syslog", "my_laptop"], ["bob-syslog.log", "syslog", "bobs_laptop"], ["laura-syslog.log", "syslog", "lauras_laptop"]]}, {"headers": ["source", "sourcetype", "host", "search"], "rows": [["", "", "", "( ( host=\"mylaptop\" AND source=\"syslog.log\" AND sourcetype=\"syslog\" ) OR ( host=\"bobslaptop\" AND source=\"bob-syslog.log\" AND sourcetype=\"syslog\" ) )"]]}, {"headers": ["source", "sourcetype", "host", "search"], "rows": [["", "", "", "[ [ host=\"mylaptop\" && source=\"syslog.log\" && sourcetype=\"syslog\" ] | |  [ host=\"bobslaptop\" && source=\"bob-syslog.log\" && sourcetype=\"syslog\" ] ]"]]}, {"headers": ["foo", "search"], "rows": [["", "{ [ ( foo=\"eventtype\" mvseparator foo=\"log_level\" ) ] }"]]}, {"headers": ["search"], "rows": [["Error Found"]]}, {"headers": ["search"], "rows": [["NOT ( )"]]}, {"headers": ["i", "Time", "Event"], "rows": [[">", "11/16/213:11:33.745 PM", "11-16-2021 15:11:33.745 -0800 INFO  Metrics - group=thruput, name=thruput, instantaneous_kbps=4.984, instantaneous_eps=20.935, average_kbps=1.667, total_k_processed=182447.000, kb=154.505, ev=649host = PF32198Dsource = C:\\Program Files\\Splunk\\var\\log\\splunk\\metrics.logsourcetype = splunkd"], [">", "11/16/213:11:33.745 PM", "11-16-2021 15:11:33.745 -0800 INFO  Metrics - group=thruput, name=syslog_output, instantaneous_kbps=0.000, instantaneous_eps=0.000, average_kbps=0.000, total_k_processed=0.000, kb=0.000, ev=0 host = PF32198Dsource = C:\\Program Files\\Splunk\\var\\log\\splunk\\metrics.logsourcetype = splunkd"], [">", "11/16/213:11:33.745 PM", "11-16-2021 15:11:33.745 -0800 INFO  Metrics - group=thruput, name=index_thruput, instantaneous_kbps=4.971, instantaneous_eps=19.355, average_kbps=1.667, total_k_processed=182424.000, kb=154.094, ev=600 host = PF32198Dsource = C:\\Program Files\\Splunk\\var\\log\\splunk\\metrics.logsourcetype = splunkd"], [">", "11/16/213:11:33.745 PM", "11-16-2021 15:11:33.745 -0800 INFO  Metrics - group=queue, name=winparsing, max_size_kb=500, current_size_kb=0, current_size=0, largest_size=0, smallest_size=0 host = PF32198Dsource = C:\\Program Files\\Splunk\\var\\log\\splunk\\metrics.logsourcetype = splunkd"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "format", "section_heading": "Examples", "section_id": "b458d998_4df8_4327_b48d_c45d635223ef--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/format", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:56.208028+00:00", "version": "10.2"}}
{"id": "ab984f1d3d81c0d5", "content": "search , return", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "format", "section_heading": "See also", "section_id": "id_16f6eb8e_9dc9_4da4_8f73_94b88fb57706--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/format", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:56:56.208033+00:00", "version": "10.2"}}
{"id": "30ead83cab788dd4", "content": "Returns the first N number of specified results in search order. This means the most recent N events for a historical search, or the first N captured events for a real-time search. The search results are limited to the first results in search order. There are two types of limits that can be applied: an absolute number of results, or an expression where all results are returned until the expression becomes false.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "head", "section_heading": "Description", "section_id": "id_1cc6f048_7fd0_4da0_839b_61a1bcfaa1e9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/head", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:13.513333+00:00", "version": "10.2"}}
{"id": "e18a4c61fdd79d41", "content": "The required syntax is in bold. head [<N> | (<eval-expression>)] [limit=<int>] [null=<bool>] [keeplast=<bool>] Required arguments None. If no options or limits are specified, the head command returns the first 10 results. Optional arguments <N> Syntax: <int> Description: The number of results to return. Default: 10 limit Syntax: limit=<int> Description: Another way to specify the number of results to return. Default: 10 eval-expression Syntax: <eval-compare-exp> | <eval-bool-exp> Description: A valid <eval-expression> that evaluates to a Boolean. The search returns results until this expression evaluates to false. For more information, see the evaluation functions in the Search Reference. keeplast Syntax: keeplast=<bool> Description: You must specify a eval-expression to use the keeplast argument. Controls whether the last result in the result set is retained. The last result returned is the result that caused the eval-expression to evaluate to false or NULL. Set keeplast to true to retain the last result in the result set. Set keeplast to false to discard the last result. Default: false null Syntax: null=<bool> Description: You must specify an <eval-expression> for the null argument to have any effect. Controls how an <eval-expression> that evaluates to NULL is handled. For example, if the <eval-expression> is (x > 10) and a value in field x does not exist, the <eval-expression> evaluates to NULL instead of true or false. If null=true , the results of the head command include events for which <eval-expression> evaluates to NULL in the output. The head command continues to process the remaining events. If null=false , the head command treats the <eval-expression> that evaluates to NULL as if the <eval-expression> evaluated to false. The head command stops processing events. If keeplast=true , the event for which the <eval-expression> evaluated to NULL is also included in the output. Default : false", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "head", "section_heading": "Syntax", "section_id": "add48c46_6068_4bd6_a8a1_d1c1cc98f74d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/head", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:13.513342+00:00", "version": "10.2"}}
{"id": "7c6bbf0216f2c8ef", "content": "The head command is a centralized streaming command. See Command types. Setting limits If a numeric limit such as a numeric literal or the argument limit=<int> is used, the head command returns the first N results where N is the selected number. Using both the numeric limit and limit=<int> results in an error. Using an <eval-expression> If an <eval-expression> is used, all initial results are returned until the first result where the expression evaluates to false. The result where the <eval-expression> evaluates to false is kept or dropped based on the keeplast argument. If both a numeric limit and an <eval-expression> are used, the smaller of the two constraints applies. For example, the following search returns up to the first 10 results, because the <eval-expression> is always true. However, this search returns no results because the <eval-expression> is always false.", "code_examples": [{"language": "spl", "code": "... |headlimit=10 (1==1)"}, {"language": "spl", "code": "... |headlimit=10 (0==1)"}], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "head", "section_heading": "Usage", "section_id": "id_1346d642_df7c_4712_b863_4f16bf829b9a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/head", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:13.513350+00:00", "version": "10.2"}}
{"id": "e620c6dfb2eaef09", "content": "1. Return a specific number of results Return the first 20 results. 2. Return results based on a specified limit Return events until the time span of the data is >= 100 seconds", "code_examples": [{"language": "spl", "code": "... | head 20"}, {"language": "spl", "code": "... | streamstats range(_time) as timerange | head (timerange<100)"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "head", "section_heading": "Basic examples", "section_id": "id_436fed2a_f4dc_4b1e_a6fb_f44e8ad06a77--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/head", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:13.513355+00:00", "version": "10.2"}}
{"id": "c61080c4a875e14f", "content": "1. Using the keeplast and null arguments The following example shows the search results when an <eval-expression> evaluates to NULL, and the impact of the keeplast and null arguments on those results. Let's start with creating a set of events. The eval command replaces the value 3 with NULL in the count field. The results look something like this: When null is set to true , the head command continues to process the results. In this example the command processes the results, ignoring NULL values, as long as the count is less than 5. Because keeplast=true the event that stopped the processing, count 5, is also included in the output. The results look something like this: When null is set to false , the head command stops processing the results when it encounters a NULL value. The events with count 1 and 2 are returned. Because keeplast=true the event with the NULL value that stopped the processing, the third event, is also included in the output. The results look something like this:", "code_examples": [{"language": "spl", "code": "| makeresults count=7\n| streamstats count \n|evalcount=if(count=3,null(), count)"}, {"language": "spl", "code": "| makeresults count=7\n| streamstats count \n|evalcount=if(count=3,null(), count) \n| head count<5 keeplast=truenull=true"}, {"language": "spl", "code": "| makeresults count=7 \n| streamstats count \n|evalcount=if(count=3,null(), count) \n| head count<5 keeplast=truenull=false"}], "tables": [{"headers": ["_time", "count"], "rows": [["2020-05-18 12:46:51", "1"], ["2020-05-18 12:46:51", "2"], ["2020-05-18 12:46:51", ""], ["2020-05-18 12:46:51", "4"], ["2020-05-18 12:46:51", "5"], ["2020-05-18 12:46:51", "6"], ["2020-05-18 12:46:51", "7"]]}, {"headers": ["_time", "count"], "rows": [["2020-05-18 12:46:51", "1"], ["2020-05-18 12:46:51", "2"], ["2020-05-18 12:46:51", ""], ["2020-05-18 12:46:51", "4"], ["2020-05-18 12:46:51", "5"]]}, {"headers": ["_time", "count"], "rows": [["2020-05-18 12:46:51", "1"], ["2020-05-18 12:46:51", "2"], ["2020-05-18 12:46:51", ""]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "head", "section_heading": "Extended example", "section_id": "id_78fb87ed_eeb8_4305_8235_951bfab07a30--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/head", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:13.513370+00:00", "version": "10.2"}}
{"id": "f9bedf9a670faa8d", "content": "Commands reverse tail", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "head", "section_heading": "See also", "section_id": "a4c50e98_315e_4177_be90_9efdd35b071d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/head", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:13.513376+00:00", "version": "10.2"}}
{"id": "fc4a7279140fd69b", "content": "For each event where field is a number, the accum command calculates a running total or sum of the numbers. The accumulated sum can be returned to either the same field, or a newfield that you specify.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "accum", "section_heading": "Description", "section_id": "id_64f0c4cf_430b_4217_9183_ae6bb6b6ca1b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/accum", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:28.469005+00:00", "version": "10.2"}}
{"id": "4a675c2aec27e843", "content": "accum <field> [AS <newfield>] Required arguments field Syntax: <string> Description: The name of the field that you want to calculate the accumulated sum for. The field must contain numeric values. Optional arguments newfield Syntax: <string> Description: The name of a new field where you want the results placed.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "accum", "section_heading": "Syntax", "section_id": "b0bb3e35_149b_4abd_8e9f_6d3940f3061e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/accum", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:28.469014+00:00", "version": "10.2"}}
{"id": "21225b7fab92854d", "content": "1. Create a running total of a field The following search looks for events from web access log files that were successful views of strategy games. A count of the events by each product ID is returned. The results appear on the Statistics tab and look something like this: You can use the accum command to generate a running total of the views and display the running total in a new field called \"TotalViews\". The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "sourcetype=access_* status=200 categoryId=STRATEGY | chart count AS views by productId"}, {"language": "spl", "code": "sourcetype=access_* status=200 categoryId=STRATEGY | chart count AS views by productId | accum views as TotalViews"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["productId", "views"], "rows": [["DB-SG-G01", "1796"], ["DC-SG-G02", "1642"], ["FS-SG-G03", "1482"], ["PZ-SG-G05", "1300"]]}, {"headers": ["productId", "views", "TotalViews"], "rows": [["DB-SG-G01", "1796", "1796"], ["DC-SG-G02", "1642", "3438"], ["FS-SG-G03", "1482", "4920"], ["PZ-SG-G05", "1300", "6220"]]}], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "accum", "section_heading": "Basic example", "section_id": "cdbe01c5_75d9_4c34_83c8_1f36a046a96a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/accum", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:28.469025+00:00", "version": "10.2"}}
{"id": "566fad88e9030ef7", "content": "autoregress , delta , streamstats , trendline", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "accum", "section_heading": "See also", "section_id": "cd9f406b_f100_4d15_ad36_03a8b78e9508--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/accum", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:28.469030+00:00", "version": "10.2"}}
{"id": "3f07fa10ab9d7f6f", "content": "Join search result rows with other search result rows in the same result set, based on one or more fields that you specify.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "selfjoin", "section_heading": "Description", "section_id": "id_4610ca7e_6704_4cff_be26_3a1fb4786575--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/selfjoin", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:44.090314+00:00", "version": "10.2"}}
{"id": "d101e0131dff56be", "content": "selfjoin [<selfjoin-options>...] <field-list> Required arguments <field-list> Syntax: <field>... Description: The field or list of fields to join on. Optional arguments <selfjoin-options> Syntax: overwrite=<bool> | max=<int> | keepsingle=<bool> Description: Options that control the search result set that is returned. You can specify one or more of these options. Selfjoin options keepsingle Syntax: keepsingle=<bool> Description: Controls whether or not to retain results that have a unique value in the join fields and no matching values to join with. When keepsingle=true , search results that have no other results to join with are kept in the output. For example, if you're joining results matching employees to their managers, and one of the employees is the CEO who doesn't have a manager, the field for that employee is included in the results when keepsingle=true. Default : false max Syntax: max=<int> Description: Indicates the maximum number of 'other' results to join with each main result. If max=0 , there is no limit. This argument sets the maximum for the 'other' results. The maximum number of main results is 100,000. Default : 1 overwrite Syntax: overwrite=<bool> Description: When overwrite=true , causes fields from the 'other' results to overwrite fields of the main results. The main results are used as the basis for the join. Default : true", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "selfjoin", "section_heading": "Syntax", "section_id": "id_0a78562c_5af7_454e_acb1_e13f995d900b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/selfjoin", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:44.090323+00:00", "version": "10.2"}}
{"id": "7da8dc084d62c312", "content": "Self joins are more commonly used with relational database tables. They are used less commonly with event data. An example of an events usecase is with events that contain information about processes, where each process has a parent process ID. You can use the selfjoin command to correlate information about a process with information about the parent process. See the Extended example .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "selfjoin", "section_heading": "Usage", "section_id": "b1cffdd8_5cab_4758_a6c6_013f3739dd74--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/selfjoin", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:44.090329+00:00", "version": "10.2"}}
{"id": "359b65f7d5d5003c", "content": "1: Use a single field to join results Join the results with itself on the 'id' field.", "code_examples": [{"language": "spl", "code": "... | selfjoin id"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "selfjoin", "section_heading": "Basic example", "section_id": "id_1efc3387_1934_4656_9a4a_3f7d19191229--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/selfjoin", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:44.090335+00:00", "version": "10.2"}}
{"id": "161975a8abef8dae", "content": "The following example shows how the selfjoin command works against a simple set of results. You can follow along with this example on your own Splunk instance. Note: This example builds a search incrementally. With each addition to the search, the search is rerun and the impact of the additions are shown in a results table. The values in the _time field change each time you rerun the search. However, in this example the values in the results table are not changed so that we can focus on how the changes to the search impact the results. 1. Start by creating a simple set of 5 results by using the makeresults command. There are 5 results created, each with the same timestamp. 2. To keep better track of each result use the streamstats command to add a field that numbers each result. The a field is added to the results. 3. Additionally, use the eval command to change the timestamps to be 60 seconds apart. Different timestamps make this example more realistic. The minute portion of the timestamp is updated. 4. Next use the eval command to create a field to use as the field to join the results on. The new field is added. 5. Use the eval command to create some fields with data. An if function is used with a modulo (modulus) operation to add different data to each of the new fields. A modulo operation finds the remainder after the division of one number by another number: The eval b command processes each result and performs a modulo operation. If the remainder of a/2 is 0, put \"something\" into the field \"b\", otherwise put \"nada\" into field \"b\". The eval c command processes each result and performs a modulo operation. If the remainder a/2 is 1, put \"something else\" into the field \"c\", otherwise put nothing (NULL) into field \"c\". The new fields are added and the fields are arranged in alphabetical order by field name, except for the _time field. 6. Use the selfjoin command to join the results on the joiner field. The results are joined. 7. To understand how the selfjoin command joins the results together, remove the | selfjoin joiner portion of the search. Then modify the search to append the values from the a field to the values in the b and c fields. The results now have the row number appended to the values in the b and c fields. 8. Now add the selfjoin command back into the search. The results of the self join. If there are values for a field in both rows, the last result row, based on the _time value, takes precedence. The joins performed are shown in the following table. (Thanks to Splunk user Alacercogitatus for helping with this example.)", "code_examples": [{"language": "spl", "code": "| makeresults count=5"}, {"language": "spl", "code": "| makeresults count=5 | streamstats count as a"}, {"language": "spl", "code": "| makeresults count=5 | streamstats count as a |eval_time = _time + (60*a)"}, {"language": "spl", "code": "| makeresults count=5 | streamstats count as a |eval_time = _time + (60*a) |evaljoiner=\"x\""}, {"language": "spl", "code": "| makeresults count=5 | streamstats count as a |eval_time = _time + (60*a) |evaljoiner=\"x\"|evalb =if(a%2==0,\"something\",\"nada\"), c =if(a%2==1,\"somethingelse\",null())"}, {"language": "spl", "code": "| makeresults count=5 | streamstats count as a |eval_time = _time + (60*a) |evaljoiner=\"x\"|evalb =if(a%2==0,\"something\",\"nada\"), c =if(a%2==1,\"somethingelse\",null()) | selfjoin joiner"}, {"language": "spl", "code": "| makeresults count=5 | streamstats count as a |eval_time = _time + (60*a) |evaljoiner=\"x\"|evalb =if(a%2==0,\"something\"+a,\"nada\"+a), c =if(a%2==1,\"somethingelse\"+a,null())"}, {"language": "spl", "code": "| makeresults count=5 | streamstats count as a |eval_time = _time + (60*a) |evaljoiner=\"x\"|evalb =if(a%2==0,\"something\"+a,\"nada\"+a), c =if(a%2==1,\"somethingelse\"+a,null()) | selfjoin joiner"}], "tables": [{"headers": ["_time"], "rows": [["2018-01-18 14:38:59"], ["2018-01-18 14:38:59"], ["2018-01-18 14:38:59"], ["2018-01-18 14:38:59"], ["2018-01-18 14:38:59"]]}, {"headers": ["_time", "a"], "rows": [["2018-01-18 14:38:59", "1"], ["2018-01-18 14:38:59", "2"], ["2018-01-18 14:38:59", "3"], ["2018-01-18 14:38:59", "4"], ["2018-01-18 14:38:59", "5"]]}, {"headers": ["_time", "a"], "rows": [["2018-01-18 14:38:59", "1"], ["2018-01-18 14:39:59", "2"], ["2018-01-18 14:40:59", "3"], ["2018-01-18 14:41:59", "4"], ["2018-01-18 14:42:59", "5"]]}, {"headers": ["_time", "a", "joiner"], "rows": [["2018-01-18 14:38:59", "1", "x"], ["2018-01-18 14:39:59", "2", "x"], ["2018-01-18 14:40:59", "3", "x"], ["2018-01-18 14:41:59", "4", "x"], ["2018-01-18 14:42:59", "5", "x"]]}, {"headers": ["_time", "a", "b", "c", "joiner"], "rows": [["2018-01-18 14:38:59", "1", "nada", "somethingelse", "x"], ["2018-01-18 14:39:59", "2", "something", "", "x"], ["2018-01-18 14:40:59", "3", "nada", "somethingelse", "x"], ["2018-01-18 14:41:59", "4", "something", "", "x"], ["2018-01-18 14:42:59", "5", "nada", "somethingelse", "x"]]}, {"headers": ["_time", "a", "b", "c", "joiner"], "rows": [["2018-01-18 14:39:59", "2", "something", "somethingelse", "x"], ["2018-01-18 14:40:59", "3", "nada", "somethingelse", "x"], ["2018-01-18 14:41:59", "4", "something", "somethingelse", "x"], ["2018-01-18 14:42:59", "5", "nada", "somethingelse", "x"]]}, {"headers": ["_time", "a", "b", "c", "joiner"], "rows": [["2018-01-18 14:38:59", "1", "nada1", "somethingelse1", "x"], ["2018-01-18 14:39:59", "2", "something2", "", "x"], ["2018-01-18 14:40:59", "3", "nada3", "somethingelse3", "x"], ["2018-01-18 14:41:59", "4", "something4", "", "x"], ["2018-01-18 14:42:59", "5", "nada5", "somethingelse5", "x"]]}, {"headers": ["_time", "a", "b", "c", "joiner"], "rows": [["2018-01-18 14:39:59", "2", "something2", "somethingelse1", "x"], ["2018-01-18 14:40:59", "3", "nada3", "somethingelse3", "x"], ["2018-01-18 14:41:59", "4", "something4", "somethingelse3", "x"], ["2018-01-18 14:42:59", "5", "nada5", "somethingelse5", "x"]]}, {"headers": ["Result row", "Output", "Description"], "rows": [["1", "Row 1 is joined with row 2 and returned as row 2.", "In fieldb, the valuenada1is discarded because the valuesomething2in row 2 takes precedence. In fieldc, there is no value in row 2. The valuesomethingelse1from row 1 is returned."], ["2", "Row 2 is joined with row 3 and returned as row 3.", "Since row 3 contains values for both fieldband fieldc, the values in row 3 take precedence and the values in row 2  are discarded."], ["3", "Row 3 is joined with row 4 and returned as row 4.", "In fieldb, the valuenada3is discarded because the valuesomething4in row 4 takes precedence. In fieldc, there is no value in row 4. The valuesomethingelse3from row 3 is returned."], ["4", "Row 4 is joined with row 5 and returned as row 5.", "Since row 5 contains values for both fieldband fieldc, the values in row 5 take precedence and the values in row 4  are discarded."], ["5", "Row 5 has no other row to join with.", "No additional results are returned."]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "selfjoin", "section_heading": "Extended example", "section_id": "fa8ead6b_f34c_4c80_9bb1_4ef930e48b69--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/selfjoin", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:44.090389+00:00", "version": "10.2"}}
{"id": "334d55bf3475d2ae", "content": "join", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "selfjoin", "section_heading": "See also", "section_id": "id_5478d7aa_79e7_46f6_8b22_8400ae999570--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/selfjoin", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:57:44.090394+00:00", "version": "10.2"}}
{"id": "836e5f10031e7cfa", "content": "Appends the results of a subsearch to the current results. The append command runs only over historical data and does not produce correct results if used in a real-time search. By default, subsearches return a maximum of 10,000 results and have a maximum runtime of 60 seconds. If a subsearch runs for more than 60 seconds, its search results are automatically finalized. For more information about when to use the append command, see the flowchart in the topic About event grouping and correlation in the Search Manual. If you are familiar with SQL but new to SPL, see Splunk SPL for SQL users .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "append", "section_heading": "Description", "section_id": "id_4ed122a2_5d2c_47d3_89bd_cbc51adeec72--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/append", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:01.240840+00:00", "version": "10.2"}}
{"id": "725facba7b5b88ed", "content": "append [<subsearch-options>...] <subsearch> Required arguments subsearch Syntax: [subsearch] Description: A secondary search where you specify the source of the events that you want to append. The subsearch must be enclosed in square brackets. See About subsearches in the Search Manual. Optional arguments subsearch-options Syntax: extendtimerange=<boolean> | maxtime=<int> | maxout=<int> Description: Controls how the subsearch is processed. Subsearch options extendtimerange Syntax: extendtimerange=<boolean> Description: Specifies whether to include the subsearch time range in the time range for the entire search. Use the extendtimerange argument when the time range in the subsearch extends beyond the time range for the main search. Use this argument when a transforming command , such as chart , timechart , or stats , follows the append command in the search and the search uses time based bins. Default: false maxtime Syntax: maxtime=<int> Description: The maximum time, in seconds, to spend on the subsearch before automatically finalizing. Default: 60 maxout Syntax: maxout=<int> Description: The maximum number of result rows to return from the subsearch within the append command. The default value for maxout affects only subsearches used with the append command and not subsearches for other commands. This means that changing the maxout setting only changes the number of rows the append commandâ€™s subsearch returns, leaving other subsearches unaffected. Default: 50000", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "append", "section_heading": "Syntax", "section_id": "d57c5a71_903b_4a5c_9d1e_ede5b19bb5e0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/append", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:01.240847+00:00", "version": "10.2"}}
{"id": "290b11edd3d86540", "content": "The append command is a transforming command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "append", "section_heading": "Usage", "section_id": "id_362567a3_d159_460b_adfd_a5c0fbc809d6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/append", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:01.240851+00:00", "version": "10.2"}}
{"id": "9b345580feb83b98", "content": "1. Use the append command to add column totals. Count the number of earthquakes that occurred in and around California yesterday and then calculate the total number of earthquakes. This example uses a subsearch to count all the earthquakes in the California regions ( place=\"*California\" ), then uses the main search to count the number of earthquakes based on the magnitude type of the search. You cannot use the stats command to simultaneously count the total number of events and the number of events for a specified field. The subsearch is used to count the total number of earthquakes that occurred. This count is added to the results of the previous search with the append command. Because both searches share the count field, the results of the subsearch are listed as the last row in the count column. The results appear on the Statistics tab and look something like this: This search demonstrates how to use the append command in a way that is similar to using the addcoltotals command to add the column totals. 2. Count the number of different customers who purchased items. Append the top purchaser for each type of product. Count the number of different customers who purchased something from the Buttercup Games online store yesterday, and break this count down by the type of product (accessories, t-shirts, and type of games) they purchased. Also, list the top purchaser for each type of product and how much that person bought of that product. This example first searches for purchase events ( action=purchase ). These results are piped into the stats command and the dc() , or distinct_count() function is used to count the number of different users who make purchases. The BY clause is used to break up this number based on the different category of products ( categoryId ). This example contains a subsearch as an argument for the append command. The subsearch is used to search for purchase events and count the top purchaser (based on clientip ) for each category of products. These results are added to the results of the previous search using the append command. Here, the table command is used to display only the category of products ( categoryId ), the distinct count of users who bought each type of product ( dc(clientip) ), the actual user who bought the most of a product type ( clientip ), and the number of each product that user bought ( count ). You can see that the append command just tacks on the results of the subsearch to the end of the previous search, even though the results share the same field values. It does not let you manipulate or reformat the output. 3. Use the append command to determine the number of unique IP addresses that accessed the Web server. Use the append command, along with the stats , count , and top commands to determine the number of unique IP addresses that accessed the Web server. Find the user who accessed the Web server the most for each type of page request. Count the number of different IP addresses that accessed the Web server and also find the user who accessed the Web server the most for each type of page request ( method ). The Web access events are piped into the stats command and the dc() or distinct_count() function is used to count the number of different users who accessed the site. The count() function is used to count the total number of times the site was accessed. These numbers are separated by the page request ( method ). The subsearch is used to find the top user for each type of page request ( method ). The append command is used to add the result of the subsearch to the bottom of the table. The results appear on the Statistics tab and look something like this: The first two rows are the results of the first search. The last two rows are the results of the subsearch. Both result sets share the method and count fields. 4. Specify the maximum time for the subsearch to run and the maximum number of result rows from the subsearch Use the append command, to determine the number of unique IP addresses that accessed the Web server. Find the user who accessed the Web server the most for each type of page request. Count the number of different IP addresses that accessed the Web server and also find the user who accessed the Web server the most for each type of page request ( method ). Limit the subsearch to 30 seconds and the maximum number of subsearch results to 1000. 5. Use the extendtimerange argument Use the extendtimerange argument to ensure that the time range used for the search includes both the time range of the main search and the time range of the subsearch. The time range used for the search is from 11/1/2017:00:00:00, the earliest time in the subsearch, to 11/30/2017:00:00:00, the latest time in the main search.", "code_examples": [{"language": "spl", "code": "source=usgs place=*California* | stats count by magType | append [search index=usgs_*source=usgs place=*California* | stats count]"}, {"language": "spl", "code": "sourcetype=access_* action=purchase | stats dc(clientip) BY categoryId | append [search sourcetype=access_* action=purchase | top 1 clientip BY categoryId] | table categoryId, dc(clientip), clientip, count"}, {"language": "spl", "code": "...[search sourcetype=access_* action=purchase | top 1 clientip BY categoryId]"}, {"language": "spl", "code": "sourcetype=access_* | stats dc(clientip), count by method | append [search sourcetype=access_* | top 1 clientip by method]"}, {"language": "spl", "code": "sourcetype=access_* | stats dc(clientip), count by method | append maxtime=30 maxout=1000 [search sourcetype=access_* | top 1 clientip by method]"}, {"language": "spl", "code": "index=_internal earliest=11/20/2017:00:00:00 latest=11/30/2017:00:00:00 \n|append extendtimerange=true[search index=_audit earliest=11/1/2017:00:00:00 latest=11/25/2017:00:00:00] \n|timechart span=1d count"}], "tables": [{"headers": [], "rows": [["This search uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance.  This example uses theAll Earthquakesdata from the past 30 days."]]}, {"headers": ["magType", "count"], "rows": [["H", "123"], ["MbLg", "1"], ["Md", "1565"], ["Me", "2"], ["Ml", "1202"], ["Mw", "6"], ["ml", "10"], ["", "2909"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}, {"headers": ["method", "dc(clientip)", "count", "clientip", "percent"], "rows": [["GET", "173", "2666", "", ""], ["POST", "168", "1727", "", ""], ["GET", "", "83", "87.194.216.51", "3.113278"], ["POST", "", "64", "87.194.216.51", "3.705848"]]}, {"headers": [], "rows": [["This example uses the sample dataset fromthe Search Tutorialbut should work with any format of Apache web access log. Download the data set fromthis topic in the Search Tutorialand follow the instructions to upload it to your Splunk deployment. Use the time rangeYesterdaywhen you run this search."]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "append", "section_heading": "Examples", "section_id": "id_8cbbcecc_17ea_4d28_bb98_0066493d6755--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/append", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:01.240861+00:00", "version": "10.2"}}
{"id": "1777e9744fb95995", "content": "appendcols , appendpipe , join , set", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "append", "section_heading": "See also", "section_id": "id_6cafa506_fd5d_42c7_9f13_6776c078c22c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/append", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:01.240865+00:00", "version": "10.2"}}
{"id": "d01f48757a81deef", "content": "Removes results that match or do not match the specified regular expression.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "regex", "section_heading": "Description", "section_id": "f084dafa_d1d9_4d47_94ed_0ea1b4620c5b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/regex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:17.959020+00:00", "version": "10.2"}}
{"id": "b1da002b0eaa9876", "content": "The required syntax is in bold. regex (<field>=<regex-expression> | <field>!=<regex-expression> | <regex-expression> ) Required arguments <regex-expression> Syntax: \"<string>\" Description: An unanchored regular expression. The regular expression must be a Perl Compatible Regular Expression supported by the PCRE library. Quotation marks are required. Optional arguments <field> Syntax: <field> Description: Specify the field name from which to match the values against the regular expression. You can specify that the regex command keeps results that match the expression by using <field>=<regex-expression>. To keep results that do not match, specify <field>!=<regex-expression>. Default: _raw", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "regex", "section_heading": "Syntax", "section_id": "id_0285d2c4_078e_457f_bdfd_3112ae2daeb8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/regex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:17.959029+00:00", "version": "10.2"}}
{"id": "d81d84c29ad1813f", "content": "The regex command is a distributable streaming command. See Command types. When you use regular expressions in searches, you need to be aware of how characters such as pipe ( | ) and backslash ( \\ ) are handled. See SPL and regular expressions in the Search Manual. Although != is valid within a regex command, NOT is not valid. For general information about regular expressions, see About Splunk regular expressions in the Knowledge Manager Manual. The difference between the regex and rex commands Use the regex command to remove results that match or do not match the specified regular expression. Use the rex command to either extract fields using regular expression named groups, or replace or substitute characters in a field using sed expressions. Using the regex command with != If you use regular expressions in conjunction with the regex command, note that != behaves differently for the regex command than for the search command. You can use a regex command with != to filter for events that don't have a field value matching the regular expression, or for which the field is null. For example, this search will include events that do not define the field Location. The search command behaves the opposite way. You can use a search command with != to filter for events that don't contain a field matching the search string, and for which the field is defined. For example, this search will not include events that do not define the field Location. If you use != in the context of the regex command, keep this behavior in mind and make sure you want to include null fields in your results.", "code_examples": [{"language": "spl", "code": "... | regex Location!=\"Calaveras Farms\""}, {"language": "spl", "code": "... | search Location!=\"Calaveras Farms\""}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "regex", "section_heading": "Usage", "section_id": "a9a63380_6be9_4e82_b801_9d58ebbf804e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/regex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:17.959037+00:00", "version": "10.2"}}
{"id": "61c69e49121b4df1", "content": "1. Keep only results that contain IP addresses in a non-routable class This example keeps only search results whose \"_raw\" field contains IP addresses in the non-routable class A (10.0.0.0/8). This example uses a negative lookbehind assertion at the beginning of the expression. 2. Keep only the results that match a valid email address This example keeps only the results that match a valid email address. For example, buttercup@example.com. Note: This regular expression is for example purposes only and isn't a fully RFC-compliant email address validator. The following table explains each part of the expression. 3. Filter out zip codes with a specific format Filter out zip codes that are formatted like a United States zip code or zip+4 code. For example, this search would return a Canadian zip code. 4. Filter events where a field has no value The search with regex and != in the following example creates 5 events with Country=\"Canada\" and 5 events with City=\"Toronto\", and filters on events where Country does not equal \"Canada\". This search returns the union of two groups of events: events where the field Country is defined and has a value not equal to \"Canada\"; and events where the field Country is not defined. As a result, 5 events are displayed for the City field, even though a Country field was not defined for those events. Also, the Country field is displayed, but the values are null. The results look something like this. In contrast, the search with search and != in the following example doesn't return any events because all of the events with field City where the field Country is null are excluded.", "code_examples": [{"language": "spl", "code": "... | regex _raw=\"(?<!\\d)10\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}(?!\\d)\""}, {"language": "spl", "code": "...| regex  email=\"^([a-z0-9_\\.-]+)@([\\da-z\\.-]+)\\.([a-z\\.]{2,6})$\""}, {"language": "spl", "code": "... | regex not_usa_zip!=\"[0-9]{5}(-[0-9]{4})?\""}, {"language": "spl", "code": "| makeresults count=5 |evalCountry=\"Canada\"| append [\n| makeresults count=5 |evalCity=\"Toronto\"] \n| regex Country!=\"Canada\""}, {"language": "spl", "code": "| makeresults count=5 |evalCountry=\"Canada\"| append [\n| makeresults count=5 |evalCity=\"Toronto\"] \n| search Country!=\"Canada\""}], "tables": [{"headers": ["Part of the expression", "Description"], "rows": [["^", "Specifies the beginning of the string."], ["([a-z0-9_\\.-]+)", "This is the first group in the expression. Specifies to match one or more lowercase letters, numbers, underscores, dots, or hyphens. The backslash ( \\ ) character is used to escape the dot ( . ) character. The dot character is escaped, because a non-escaped dot matches any character. The plus ( + ) sign specifies to match from 1 to unlimited characters in this group. In this example this part of the expression matchesbuttercupin the email address buttercup@example.com."], ["@", "Matches theatsymbol."], ["([\\da-z\\.-]+)", "This is the second group in the expression. Specifies to match the domain name, which can be one or more lowercase letters, numbers, underscores, dots, or hyphens. This is followed by another escaped dot character. The plus ( + ) sign specifies to match from 1 to unlimited characters in this group. In this example this part of the expression matchesexamplein the email address buttercup@example.com."], ["([a-z\\.]{2,6})", "This is the third group. Specifies to match the top-level domain (TLD), which can be 2 to 6 letters or dots. This group matches all types of TLDs, such as.co.uk,.edu, or.asia. In this example it matches.comin the email address buttercup@example.com."], ["$", "Specifies the end of the string."]]}, {"headers": ["_time", "City", "Country"], "rows": [["Toronto", "", "2025-11-02 15:48:47"], ["Toronto", "", "2025-11-02 15:48:47"], ["Toronto", "", "2025-11-02 15:48:47"], ["Toronto", "", "2025-11-02 15:48:47"], ["Toronto", "", "2025-11-02 15:48:47"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "regex", "section_heading": "Examples", "section_id": "b1e8512b_6809_4422_8771_2009db9c2c2c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/regex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:17.959054+00:00", "version": "10.2"}}
{"id": "70ccc061979f0162", "content": "Commands rex search", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "regex", "section_heading": "See also", "section_id": "id_0ef80e65_102d_4d74_bace_7b90d705a804--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/regex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:17.959059+00:00", "version": "10.2"}}
{"id": "218912f1da004dfb", "content": "The uniq command works as a filter on the search results that you pass into it. This command removes any search result if that result is an exact duplicate of the previous result. This command does not take any arguments. Note: We do not recommend running this command against a large dataset.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "uniq", "section_heading": "Description", "section_id": "d3e3ad97_6296_41b7_a0c7_156f5a2afef0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/uniq", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:32.242335+00:00", "version": "10.2"}}
{"id": "d7a60448724aa852", "content": "uniq", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "uniq", "section_heading": "Syntax", "section_id": "id_7151bf2a_7daa_4088_bb62_745f4a780195--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/uniq", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:32.242343+00:00", "version": "10.2"}}
{"id": "b91d3599be09de32", "content": "Example 1: Keep only unique results from all web traffic in the past hour.", "code_examples": [{"language": "spl", "code": "eventtype=webtraffic earliest=-1h@s | uniq"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "uniq", "section_heading": "Examples", "section_id": "id_5e156142_4c42_4ac8_ba18_ef327d1cdbbe--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/uniq", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:32.242349+00:00", "version": "10.2"}}
{"id": "6c0233950813348f", "content": "dedup", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "uniq", "section_heading": "See also", "section_id": "id_91f11770_3cbb_4318_a752_df355ff7137f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/uniq", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:32.242353+00:00", "version": "10.2"}}
{"id": "909b82b9c6e15152", "content": "Expands the values of a multivalue field into separate events, one event for each value in the multivalue field. For each result, the mvexpand command creates a new result for every multivalue field. Note: The mvexpand command can't be applied to internal fields. See Use default fields in the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "mvexpand", "section_heading": "Description", "section_id": "a8829077_fe74_46ef_acc6_04c1d9486fcb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mvexpand", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:48.971291+00:00", "version": "10.2"}}
{"id": "ce042c080f1275ea", "content": "mvexpand <field> [limit=<int>] Required arguments field Syntax: <field> Description: The name of a multivalue field. Optional arguments limit Syntax: limit=<int> Description: Specify the number of values of <field> to use for each input event. Default: 0, or no limit", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "mvexpand", "section_heading": "Syntax", "section_id": "e8911042_a19f_4d4b_a985_e2707ee20708--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mvexpand", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:48.971298+00:00", "version": "10.2"}}
{"id": "8aca3f0c141c98a2", "content": "The mvexpand command is a distributable streaming command. See Command types. You can use evaluation functions and statistical functions on multivalue fields or to return multivalue fields. Limits A limit exists on the amount of RAM that the mvexpand command is permitted to use while expanding a batch of results. By default the limit is 500MB. The input chunk of results is typically maxresultrows or smaller in size, and the expansion of all these results resides in memory at one time. The total necessary memory is the average result size multiplied by the number of results in the chunk multiplied by the average size of the multivalue field being expanded. If this attempt exceeds the configured maximum on any chunk, the chunk is truncated and a warning message is emitted. If you have Splunk Enterprise, you can adjust the limit by editing the max_mem_usage_mb setting in the limits.conf file. Prerequisites Have the permissions to increase the maxresultrows and max_mem_usage_mb settings. Only users with file system access, such as system administrators, can increase the maxresultrows and max_mem_usage_mb settings using configuration files. Know how to edit configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. Decide which directory to store configuration file changes in. There can be configuration files with the same name in your default, local, and app directories. See Where you can place (or find) your modified configuration files in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. If you use Splunk Cloud Platform and encounter problems because of this limit, file a Support ticket.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "mvexpand", "section_heading": "Usage", "section_id": "id_5530f284_1d8a_4c63_b5bd_c389b4905afa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mvexpand", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:48.971303+00:00", "version": "10.2"}}
{"id": "ff9f9373e58484c4", "content": "Example 1: Create new events for each value of multivalue field, \"foo\". Example 2: Create new events for the first 100 values of multivalue field, \"foo\". Example 3: The mvexpand command only works on one multivalue field. This example walks through how to expand an event with more than one multivalue field into individual events for each field value. For example, given these events, with sourcetype=data: First, use the rex command to extract the field values for a and b. Then use the eval command and mvzip function to create a new field from the values of a and b. The results appear on the Statistics tab and look something like this: Use the mvexpand command and the rex command on the new field, fields , to create new events and extract the alpha and beta values: Use the table command to display only the _time, alpha, and beta fields in a results table. The results appear on the Statistics tab and look something like this: (Thanks to Splunk user Duncan for this example.)", "code_examples": [{"language": "spl", "code": "... | mvexpand foo"}, {"language": "spl", "code": "... | mvexpand foolimit=100"}, {"language": "spl", "code": "2018-04-01 00:11:23 a=22 b=21 a=23 b=32 a=51 b=24\n2018-04-01 00:11:22 a=1 b=2 a=2 b=3 a=5 b=2"}, {"language": "spl", "code": "source=\"mvexpandData.csv\"| rex field=_raw\"a=(?<a>\\d+)\"max_match=5 \n| rex field=_raw\"b=(?<b>\\d+)\"max_match=5 \n|evalfields = mvzip(a,b)  \n| table _time fields"}, {"language": "spl", "code": "source=\"mvexpandData.csv\"| rex field=_raw\"a=(?<a>\\d+)\"max_match=5 \n| rex field=_raw\"b=(?<b>\\d+)\"max_match=5 \n|evalfields = mvzip(a,b) \n| mvexpand fields \n| rex field=fields\"(?<alpha>\\d+),(?<beta>\\d+)\"| table _time alpha beta"}], "tables": [{"headers": ["_time", "fields"], "rows": [["2018-04-01 00:11:23", "22,2123,3251,24"], ["2018-04-01 00:11:22", "1,22,35,2"]]}, {"headers": ["_time", "alpha", "beta"], "rows": [["2018-04-01 00:11:23", "23", "32"], ["2018-04-01 00:11:23", "51", "24"], ["2018-04-01 00:11:22", "1", "2"], ["2018-04-01 00:11:22", "2", "3"], ["2018-04-01 00:11:22", "5", "2"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "mvexpand", "section_heading": "Examples", "section_id": "id_4b1541cf_f810_46e4_87ec_ad52b4ba3cc4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mvexpand", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:48.971310+00:00", "version": "10.2"}}
{"id": "f638cf09aa75481a", "content": "Commands: makemv mvcombine nomv Functions: Multivalue eval functions Multivalue stats and chart functions split", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "mvexpand", "section_heading": "See also", "section_id": "ca10b737_132b_4e03_9c01_844b1c7004de--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/mvexpand", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:58:48.971314+00:00", "version": "10.2"}}
{"id": "3373ff8212caa444", "content": "The arules command looks for associative relationships between field values. The command returns a table with the following columns: Given fields, Implied fields, Strength, Given fields support, and Implied fields support. The given and implied field values are the values of the fields you supply. The Strength value indicates the relationship between (among) the given and implied field values. Implements the arules algorithm as discussed in Michael Hahsler, Bettina Gruen and Kurt Hornik (2012). arules: Mining Association Rules and Frequent Itemsets. R package version 1.0-12. This algorithm is similar to the algorithms used for online shopping websites which suggest related items based on what items other customers have viewed or purchased.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "arules", "section_heading": "Description", "section_id": "id_02ce12e2_b344_401f_902b_32acfd3541de--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/arules", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:05.579939+00:00", "version": "10.2"}}
{"id": "d7a3dc0e5a669e88", "content": "arules [<arules-option>... ] <field-list>... Required arguments field-list Syntax: <field> <field> ... Description: The list of field names. At least two fields must be specified. Optional arguments <arules-option> Syntax: <support> | <confidence> Description: Options for arules command. arules options support Syntax: sup=<int> Description: Specify a support limit. Associations with computed support levels smaller than this value are not included in the output results. The support option must be a positive integer. Default: 3 confidence Syntax: conf=<float> Description: Specify a confidence limit. Associations with a confidence (expressed as Strength field) are not included in the output results. Must be between 0 and 1. Default: .5", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "arules", "section_heading": "Syntax", "section_id": "id_5b0c1554_e238_4351_8aa9_023b2def6412--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/arules", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:05.579945+00:00", "version": "10.2"}}
{"id": "22467ba3ca7d048b", "content": "The arules command is a streaming command that is both distributable streaming and centralized streaming. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "arules", "section_heading": "Usage", "section_id": "id_8582e414_4ffa_4bb5_9c54_2a9472736c69--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/arules", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:05.579949+00:00", "version": "10.2"}}
{"id": "14585585ec75365a", "content": "Example 1: Search for the likelihood that the fields are related. Example 2:", "code_examples": [{"language": "spl", "code": "... | arules field1 field2 field3"}, {"language": "spl", "code": "... | arules sup=3 conf=.6 field1 field2 field3"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "arules", "section_heading": "Examples", "section_id": "id_5520e2c3_1ee0_4ce1_902d_cf871f9207e0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/arules", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:05.579953+00:00", "version": "10.2"}}
{"id": "ca734cbe15953d72", "content": "associate , correlate", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "arules", "section_heading": "See also", "section_id": "d4dda4ba_92fb_4ffc_917e_a6f46333f7d1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/arules", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:05.579956+00:00", "version": "10.2"}}
{"id": "6ccc73c4887d2713", "content": "Use this command to either extract fields using regular expression named groups, or replace or substitute characters in a field using sed expressions. The rex command matches the value of the specified field against the unanchored regular expression and extracts the named groups into fields of the corresponding names. When mode=sed , the given sed expression used to replace or substitute characters is applied to the value of the chosen field. This sed-syntax is also used to mask, or anonymize, sensitive data at index-time. Read about using sed to anonymize data in the Getting Data In Manual. Note: If a field is not specified, the regular expression or sed expression is applied to the _raw field. Running the rex command against the _raw field might have a performance impact. Use the rex command for search-time field extraction or string replacement and character substitution.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "rex", "section_heading": "Description", "section_id": "d944f87b_d8e7_4413_be0b_53780e923912--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:23.192977+00:00", "version": "10.2"}}
{"id": "577f4e4d4247100c", "content": "The required syntax is in bold. rex [field=<field>] ( <regex-expression> [max_match=<int>] [offset_field=<string>] ) | ( mode=sed <sed-expression> ) Required arguments You must specify either <regex-expression> or mode=sed <sed-expression>. regex-expression Syntax: \"<string>\" Description: The PCRE regular expression that defines the information to match and extract from the specified field. mode Syntax: mode=sed Description: Specify to indicate that you are using a sed (UNIX stream editor) expression. sed-expression Syntax: \"<string>\" Description: When mode=sed, specify whether to replace strings (s) or substitute characters (y) in the matching regular expression. No other sed commands are implemented. Sed mode supports the following flags: global (g) and Nth occurrence (N), where N is a number that is the character location in the string. Optional arguments field Syntax: field=<field> Description: The field that you want to extract information from. Default: _raw max_match Syntax: max_match=<int> Description: Controls the number of times the regex is matched. If greater than 1, the resulting fields are multivalued fields. Use 0 to specify unlimited matches. Multiple matches apply to the repeated application of the whole pattern. If your regex contains a capture group that can match multiple times within your pattern, only the last capture group is used for multiple matches. Default: 1 offset_field Syntax: offset_field=<string> Description: Creates a field that lists the position of certain values in the field argument, based on the regular expression specified in regex-expression. For example, if the rex expression is \"(?<tenchars>.{10})\" the first ten characters of the field argument are matched. The offset_field shows tenchars=0-9. The offset calculation always uses zero ( 0 ) for the first position. For another example, see Examples. Default: No default", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "rex", "section_heading": "Syntax", "section_id": "df078ad5_3ced_46a3_908f_35570793c13b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:23.192984+00:00", "version": "10.2"}}
{"id": "12b542a56f7fb8b9", "content": "The rex command is a distributable streaming command. See Command types. rex command or regex command? Use the rex command to either extract fields using regular expression named groups, or replace or substitute characters in a field using sed expressions. Use the regex command to remove results that do not match the specified regular expression. Regular expressions Splunk SPL supports perl-compatible regular expressions (PCRE). When you use regular expressions in searches, you need to be aware of how characters such as pipe ( | ) and backslash ( \\ ) are handled. See SPL and regular expressions in the Search Manual. For general information about regular expressions, see About Splunk regular expressions in the Knowledge Manager Manual. Sed expressions When using the rex command in sed mode, you have two options: replace (s) or character substitution (y). The syntax for using sed to replace (s) text in your data is: \"s/<regex>/<replacement>/<flags>\" <regex> is a PCRE regular expression, which can include capturing groups. <replacement> is a string to replace the regex match. Use \\n for back references, where \"n\" is a single digit. <flags> can be either g to replace all matches, or a number to replace a specified match. The syntax for using sed to substitute characters is: \"y/<string1>/<string2>/\" This substitutes the characters that match <string1> with the characters in <string2>. When using the rex command in sed mode, the rex command supports the same sed expressions as the SEDCMD setting in the props.conf.in file. Anonymize multiline text using sed expressions The Splunk platform doesn't support applying sed expressions in multiline mode. To use a sed expression to anonymize multiline events, use 2 sed expressions in succession by first removing the newlines and then performing additional replacements. For example, the following search uses the rex command to replace all newline characters in a multiline event containing HTML content, and then redacts all of the HTML content.", "code_examples": [{"language": "spl", "code": "index=main html \n| rex mode=sed field=_raw\"s/\\\\n/NEWLINE_REMOVED/g\"| rex mode=sed field=_raw\"s/<html.*html>/REDACTED/g\""}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "rex", "section_heading": "Usage", "section_id": "id_98a9b369_e84c_44e3_a2f8_f1811fba11a9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:23.192990+00:00", "version": "10.2"}}
{"id": "c8025c96b6034c75", "content": "1. Extract email values using regular expressions Extract email values from events to create from and to fields in your events. For example, you have events such as: When the events were indexed, the From and To values were not identified as fields. You can use the rex command to extract the field values and create from and to fields in your search results. The from and to lines in the _raw events follow an identical pattern. Each from line is From: and each to line is To:. The email addresses are enclosed in angle brackets. You can use this pattern to create a regular expression to extract the values and create the fields. You can remove duplicate values and return only the list of address by adding the dedup and table commands to the search. The results look something like this: 2. Extract from multi-valued fields using max_match You can use the max_match argument to specify that the regular expression runs multiple times to extract multiple values from a field. For example, use the makeresults command to create a field with multiple values: The results look something like this: To extract each of the values in the test field separately, you use the max_match argument with the rex command. For example: The results look something like this: 3. Extract values from a field in scheduler.log events Extract \"user\", \"app\" and \"SavedSearchName\" from a field called \"savedsearch_id\" in scheduler.log events. If savedsearch_id=bob;search;my_saved_search then user=bob , app=search and SavedSearchName=my_saved_search 4. Use a sed expression Use sed syntax to match the regex to a series of numbers and replace them with an anonymized string. 5. Use a sed expression with capture replace for strings This example shows how to use the rex command sed expression with capture replace using \\1, \\2 to reuse captured pieces of a string. This search creates an event with three fields, _time , search , and orig_search. The regular expression removes the quotation marks and any leading or trailing spaces around the quotation marks. The results look like this: 6. Use an offset_field To identify the position of certain values in a field, use the rex command with the offset_field argument and a regular expression. The following example starts with the makeresults command to create a field with a value: The results look something like this: Add the rex command with the offset_field argument to the search to create a field called off. You can identify the position of the first five values in the field list using the regular expression \"(?<firstfive>abcde)\". For example: The results look something like this: You can identify the position of several of the middle values in the field list using the regular expression \"(?<middle>fgh)\". For example: The results look something like this: 7. Display IP address and ports of potential attackers Display IP address and ports of potential attackers. This search uses the rex command to extract the port field and values. The search returns a table that lists the top source IP addresses (src_ip) and ports of the potential attackers.", "code_examples": [{"language": "spl", "code": "Mon Mar 19 20:16:27 2018 Info: Bounced: DCID 8413617 MID 19338947 From: <MariaDubois@example.com> To: <zecora@buttercupgames.com> RID 0 - 5.4.7 - Delivery expired (message too old) ('000', ['timeout']) \n\nMon Mar 19 20:16:03 2018 Info: Delayed: DCID 8414309 MID 19410908 From: <WeiZhang@example.com> To: <mcintosh@buttercupgames.com> RID 0 - 4.3.2 - Not accepting messages at this time ('421', ['4.3.2 try again later']) \n\nMon Mar 19 20:16:02 2018 Info: Bounced: DCID 0 MID 19408690 From: <Exit_Desk@sample.net> To: <lyra@buttercupgames.com> RID 0 - 5.1.2 - Bad destination host ('000', ['DNS Hard Error looking up mahidnrasatyambsg.com (MX):  NXDomain']) \n\nMon Mar 19 20:15:53 2018 Info: Delayed: DCID 8414166 MID 19410657 From: <Manish_Das@example.com> To: <dash@buttercupgames.com> RID 0 - 4.3.2 - Not accepting messages at this time ('421', ['4.3.2 try again later'])"}, {"language": "spl", "code": "source=\"cisco_esa.txt\"| rex field=_raw\"From: <(?<from>.*)> To: <(?<to>.*)>\""}, {"language": "spl", "code": "source=\"cisco_esa.txt\"| rex field=_raw\"From: <(?<from>.*)> To: <(?<to>.*)>\"| dedup from to | table from to"}, {"language": "spl", "code": "| makeresults \n|evaltest=\"a$1,b$2\""}, {"language": "spl", "code": "...| rex field=testmax_match=0\"((?<field>[^$]*)\\$(?<value>[^,]*),?)\""}, {"language": "spl", "code": "... | rex field=savedsearch_id\"(?<user>\\w+);(?<app>\\w+);(?<SavedSearchName>\\w+)\""}, {"language": "spl", "code": "... | rex field=ccnumber mode=sed\"s/(\\d{4}-){3}/XXXX-XXXX-XXXX-/g\""}, {"language": "spl", "code": "|makeresults\n|evalorig_search=\"src_ip=TERM( \\\"10.8.2.33\\\" ) OR src_ip=TERM( \\\"172.17.154.197\\\" )\", search=orig_search\n|rex mode=sed field=search\"s/\\s\\\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\\\"\\s/\\1/g\""}, {"language": "spl", "code": "| makeresults\n|evallist=\"abcdefghijklmnopqrstuvwxyz\""}, {"language": "spl", "code": "| makeresults\n|evallist=\"abcdefghijklmnopqrstuvwxyz\"| rex offset_field=off field=list\"(?<firstfive>abcde)\""}, {"language": "spl", "code": "| makeresults\n|evallist=\"abcdefghijklmnopqrstuvwxyz\"| rex offset_field=off field=list\"(?<middle>fgh)\""}, {"language": "spl", "code": "sourcetype=linux_secure port\"failed password\"| rex\"\\s+(?<ports>port \\d+)\"| top src_ip ports showperc=0"}], "tables": [{"headers": ["_time", "test"], "rows": [["2019-12-05 11:15:28", "a$1,b$2"]]}, {"headers": ["_time", "field", "test", "value"], "rows": [["2019-12-05 11:36:57", "ab", "a$1,b$2", "12"]]}, {"headers": ["_time", "orig_search", "search"], "rows": [["2021-05-31 23:36:29", "src_ip=TERM( \"10.8.2.33\" ) OR src_ip=TERM( \"172.17.154.197\" )", "src_ip=TERM(10.8.2.33) OR src_ip=TERM(172.17.154.197)"]]}, {"headers": ["_time", "list"], "rows": [["2022-05-21 11:36:57", "abcdefghijklmnopqrstuvwxyz"]]}, {"headers": ["_time", "firstfive", "list", "off"], "rows": [["2022-05-21 11:36:57", "abcde", "abcdefghijklmnopqrstuvwxyz", "firstfive=0-4"]]}, {"headers": ["_time", "list", "middle", "off"], "rows": [["2022-05-21 11:36:57", "abcdefghijklmnopqrstuvwxyz", "fgh", "middle=5-7"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "rex", "section_heading": "Examples", "section_id": "id_9ea82255_82e5_4400_950a_41363c6170f7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:23.193001+00:00", "version": "10.2"}}
{"id": "62a1d9e897851f1f", "content": "Commands extract kvform multikv regex spath xmlkv", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "rex", "section_heading": "See also", "section_id": "id_46229f42_a0b6_4d39_93dc_51d8cdd73302--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:23.193004+00:00", "version": "10.2"}}
{"id": "8cfdbd28f3adefc6", "content": "The predict command forecasts values for one or more sets of time-series data. The command can also fill in missing data in a time-series and provide predictions for the next several time steps. The predict command provides confidence intervals for all of its estimates. The command adds a predicted value and an upper and lower 95th percentile range to each event in the time-series. See the Usage section in this topic. Note: Use current Splunk machine learning (ML) tools to take advantage of the latest algorithms and get the most powerful results. See About the Splunk Machine Learning Toolkit in the Splunk Machine Learning Toolkit .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "predict", "section_heading": "Description", "section_id": "id_87ac22c8_c489_4a7f_96b9_112661c9221f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/predict", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:37.921005+00:00", "version": "10.2"}}
{"id": "fd0879db86aaa8dd", "content": "predict <field-list> [AS <newfield>] [<predict_options>] Required arguments <field-list> Syntax: <field>... Description: The names of the fields for the variable that you want to predict. You can specify one or more fields. Optional arguments <newfield> Syntax: <string> Description: Renames the fields that are specified in the <field-list>. You do not need to rename every field that you specify in the <field-list>. However, for each field that you want to rename, you must specify a separate AS <newfield> clause. <predict_options> Syntax: algorithm=<algorithm_name> | correlate_field=<field> | future_timespan=<number> | holdback=<number> | period=<number> | suppress=<bool> | lowerXX=<field> | upperYY=<field> Description: Options you can specify to control the predictions. You can specify one or more options, in any order. Each of these options is described in the Predict options section. Predict options algorithm Syntax: algorithm= LL | LLT | LLP | LLP5 | LLB | BiLL Description: Specify the name of the forecasting algorithm to apply. LL, LLT, LLP, and LLP5 are univariate algorithms. LLB and BiLL are bivariate algorithms. All the algorithms are variations based on the Kalman filter. Each algorithm expects a minimum number of data points. If not enough effective data points are supplied, an error message is displayed. For instance, the field itself might have more than enough data points, but the number of effective data points might be small if the holdback value that you specify is large. Default: LLP5 correlate Syntax: correlate=<field> Description: Specifies the time series that the LLB algorithm uses to predict the other time series. Required when you specify the LLB algorithm. Not used for any other algorithm. Default: None future_timespan Syntax: future_timespan=<num> Description: Specifies how many future predictions the predict command will compute. This number must be a non-negative number. You would not use the future_timespan option if algorithm=LLB. Default: 5 holdback Syntax: holdback=<num> Description: Specifies the number of data points from the end that are not to be used by the predict command. Use in conjunction with the future_timespan argument. For example, 'holdback=10 future_timespan=10' computes the predicted values for the last 10 values in the data set. You can then judge how accurate the predictions are by checking whether the actual data point values fall into the predicted confidence intervals. Default: 0 lowerXX Syntax: lower<int>=<field> Description: Specifies a percentage for the confidence interval and a field name to use for the lower confidence interval curve. The <int> value is a percentage that specifies the confidence level. The integer must be a number between 0 and 100. The <field> value is the field name. Default: The default confidence interval is 95%. The default field name is 'lower95(prediction(X))' where X is the name of the field to be predicted. period Syntax: period=<num> Description: Specifies the length of the time period, or recurring cycle, in the time series data. The number must be at least 2. The LLP and LLP5 algorithms attempt to compute the length of time period if no value is specified. If you specify the span argument with the timechart command, the unit that you specify for span is the unit used for period. For example, if your search is ...|timechart span=1d foo2| predict foo2 period=3. The spans are 1 day and the period for the predict is 3 days. Otherwise, the unit for the time period is a data point. For example, if there are a thousand events, then each event is a unit. If you specify period=7 , that means the data recycles after every 7 data points, or events. Default: None suppress Syntax: suppress=<field> Description: Used with the multivariate algorithms. Specifies one of the predicted fields to hide from the output. Use suppress when it is difficult to see all of the predicted visualizations at the same time. Default: None upperYY Syntax: upper<int>=<field> Description: Specifies a percentage for the confidence interval and a field name to use for the upper confidence interval curve. The <int> value is a percentage that specifies the confidence level. This must be a number between 0 and 100. The <field> value is the field name. Default: The default confidence interval is 95%. The default field name is 'upper95(prediction(X))' where X is the name of the field to be predicted. Confidence intervals The lower and upper confidence interval parameters default to lower95 and upper95. These values specify a confidence interval where 95% of the predictions are expected fall. It is typical for some of the predictions to fall outside the confidence interval. The confidence interval does not cover 100% of the predictions. The confidence interval is about a probabilistic expectation and results do not match the expectation exactly.", "code_examples": [], "tables": [{"headers": ["Algorithm option", "Algorithm type", "Description"], "rows": [["LL", "Local level", "A univariate model with no trends and no seasonality. Requires a minimum of 2 data points.  The LL algorithm is the simplest algorithm and computes the levels of the time series. For example, each new state equals the previous state, plus the Gaussian noise."], ["LLT", "Local level trend", "A univariate model with trend, but no seasonality. Requires a minimum of 3 data points."], ["LLP", "Seasonal local level", "A univariate model with seasonality. The number of data points must be at least twice the number of periods, using theperiodattribute. The LLP algorithm takes into account the cyclical regularity of the data, if it exists. If you know the number of periods, specify theperiodargument. If you do not set theperiod, this algorithm tries to calculate it. LLP returns an error message if the data is not periodic."], ["LLP5", "Combines LLT and LLP models for its prediction.", "If the time series is periodic, LLP5 computes two predictions, one using LLT and the other using LLP. The algorithm then takes a weighted average of the two values and outputs that as the prediction. The confidence interval is also based on a weighted average of the variances of LLT and LLP."], ["LLB", "Bivariate local level", "A bivariate model with no trends and no seasonality. Requires a minimum of 2 data points. LLB uses one set of data to make predictions for another. For example, assume it uses dataset Y to make predictions for dataset X. Ifholdback=10,  LLB takes the last 10 data points of Y to make predictions for the last 10 data points of X."], ["BiLL", "Bivariate local level", "A bivariate model  that predicts both time series simultaneously. The covariance of the two series is taken into account."]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "predict", "section_heading": "Syntax", "section_id": "id_19c908a5_937f_4ce2_a76f_31bdef757619--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/predict", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:37.921024+00:00", "version": "10.2"}}
{"id": "ddb5d50c642e12c9", "content": "Command sequence requirement The predict command must be preceded by the timechart command. The predict command requires time series data. See the Examples section for more details. How it works The predict command models the data by stipulating that there is an unobserved entity which progresses through time in different states. To predict a value, the command calculates the best estimate of the state by considering all of the data in the past. To compute estimates of the states, the command hypothesizes that the states follow specific linear equations with Gaussian noise components. Under this hypothesis, the least-squares estimate of the states are calculated efficiently. This calculation is called the Kalman filter, or Kalman-Bucy filter. For each state estimate, a confidence interval is obtained. The estimate is not a point estimate. The estimate is a range of values that contain the observed, or predicted, values. The measurements might capture only some aspect of the state, but not necessarily the whole state. Missing values The predict command can work with data that has missing values. The command calculates the best estimates of the missing values. Do not remove events with missing values, Removing the events might distort the periodicity of the data. Do not specify cont=false with the timechart command. Specifying cont=false removes events with missing values. Specifying span The unit for the span specified with the timechart command must be seconds or higher. The predict command cannot accept subseconds as an input when it calculates the period .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "predict", "section_heading": "Usage", "section_id": "id_223214e5_97c8_416d_b79c_e0d6d5e0f545--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/predict", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:37.921029+00:00", "version": "10.2"}}
{"id": "04da02918fe99897", "content": "1. Predict future access Predict future access based on the previous access numbers that are stored in Apache web access log files. Count the number of access attempts using a span of 1 day. The results appear on the Statistics tab. Click the Visualization tab. If necessary change the chart type to a Line Chart. 2. Predict future purchases for a product Chart the number of purchases made daily for a specific product. This example searches for all purchases events, defined by the action=purchase for the arc, and pipes those results into the timechart command. The span=1day argument buckets the count of purchases into daily chunks. The results appear on the Statistics tab and look something like this: Add the predict command to the search to calculate the prediction for the number of purchases of the Arcade games that might be sold in the near future. The results appear on the Statistics tab. Click the Visualization tab. If necessary change the chart type to a Bar Chart. 3. Predict the values using the default algorithm Predict the values of foo using the default LLP5 algorithm, an algorithm that combines the LLP and LLT algorithms. 4. Predict multiple fields using the same algorithm Predict multiple fields using the same algorithm. The default algorithm in this example. 5. Specifying different upper and lower confidence intervals When specifying confidence intervals, the upper and lower confidence interval values do not need to match. This example predicts 10 values for a field using the LL algorithm, holding back the last 20 values in the data set. 6. Predict the values using the LLB algorithm This example illustrates the LLB algorithm. The foo3 field is predicted by correlating it with the foo2 field. 7. Omit the last 5 data points and predict 5 future values In this example, the search abstains from using the last 5 data points and makes 5 future predictions. The predictions correspond to the last 5 values in the data. You can judge how accurate the predictions are by checking whether the observed values fall into the predicted confidence intervals. 8. Predict multiple fields using the same algorithm and the same future_timespan and holdback Predict multiple fields using the same algorithm and same future_timespan and holdback. 9. Specify aliases for fields Use aliases for the fields by specifying the AS keyword for each field. 10. Predict multiple fields using different algorithms and options Predict multiple fields using different algorithms and different options for each field. 11. Predict multiple fields using the BiLL algorithm Predict values for foo1 and foo2 together using the bivariate algorithm BiLL.", "code_examples": [{"language": "spl", "code": "sourcetype=access_combined_* | timechart span=1d count(file) as count | predict count"}, {"language": "spl", "code": "sourcetype=access_* action=purchase arcade | timechart span=1d count"}, {"language": "spl", "code": "sourcetype=access_* action=purchase arcade | timechart span=1d count | predict count"}, {"language": "spl", "code": "... | timechart span=\"1m\"count AS foo | predict foo"}, {"language": "spl", "code": "... | timechart ... |  predict foo1 foo2 foo3"}, {"language": "spl", "code": "... | timechart span=\"1m\"count AS foo | predict foo AS foobar algorithm=LL upper90=high lower97=low future_timespan=10 holdback=20"}, {"language": "spl", "code": "... | timechart span=\"1m\"count(x) AS foo2 count(y) AS foo3 | predict foo3 AS foobar algorithm=LLB correlate=foo2 holdback=100"}, {"language": "spl", "code": "... | timechart ... |  predict foo holdback=5 future_timespan=5"}, {"language": "spl", "code": "... | timechart ... |  predict foo1 foo2 foo3 algorithm=LLT future_timespan=15 holdback=5"}, {"language": "spl", "code": "... | timechart ... |  predict foo1 AS foobar1 foo2 AS foobar2 foo3 AS foobar3 algorithm=LLT future_timespan=15 holdback=5"}, {"language": "spl", "code": "... | timechart ... |  predict foo1 algorithm=LL future_timespan=15 foo2 algorithm=LLP period=7 future_timespan=7"}, {"language": "spl", "code": "... | timechart ... |  predict foo1 foo2 algorithm=BiLL future_timespan=10"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["_time", "count"], "rows": [["2018-06-11", "17"], ["2018-06-12", "63"], ["2018-06-13", "94"], ["2018-06-14", "82"], ["2018-06-15", "63"], ["2018-06-16", "76"], ["2018-06-17", "70"], ["2018-06-18", "72"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "predict", "section_heading": "Examples", "section_id": "id_7ec8a793_434b_4e40_b31a_1cbf0ca4e209--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/predict", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:37.921044+00:00", "version": "10.2"}}
{"id": "8c28de5849e386b8", "content": "trendline , x11", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "predict", "section_heading": "See also", "section_id": "id_091dc984_3f77_4172_ab4a_f1b15cd2ffec--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/predict", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:37.921049+00:00", "version": "10.2"}}
{"id": "1836b2330ed1bc34", "content": "Adds fields to each event that contain global, common information about the search. This command is primarily an internally-used component of Summary Indexing.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "addinfo", "section_heading": "Description", "section_id": "id_2a0b2b80_eb28_4c52_aed6_9f482aeaf4ad--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addinfo", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:55.323410+00:00", "version": "10.2"}}
{"id": "da962fe5100a7936", "content": "addinfo The following fields are added to each event when you use the addinfo command.", "code_examples": [], "tables": [{"headers": ["Field", "Description"], "rows": [["info_min_time", "The earliest time boundary for the search."], ["info_max_time", "The latest time boundary for the search."], ["info_sid", "The ID of the search that generated the event."], ["info_search_time", "The time when the search was run."]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "addinfo", "section_heading": "Syntax", "section_id": "id_20d4e16e_0dd5_464b_95e6_977438efb1db--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addinfo", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:55.323420+00:00", "version": "10.2"}}
{"id": "98edffdbc9d48ed7", "content": "The addinfo command is a distributable streaming command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "addinfo", "section_heading": "Usage", "section_id": "id_827dbaa7_8943_4b8f_b576_07b742092644--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addinfo", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:55.323424+00:00", "version": "10.2"}}
{"id": "1386b8504f49d97e", "content": "1. Add information to each event Add information about the search to each event. 2. Determine which heartbeats are later than expected You can use this example to track heartbeats from hosts, forwarders, tcpin_connections on indexers, or any number of system components. This example uses hosts. You have a list of host names in a lookup file called expected_hosts. You want to search for heartbeats from your hosts that are after an expected time range. You use the addinfo command to add information to each event that will help you evaluate the time range. Use the stats command to calculate the latest heartbeat by host. The addinfo command adds information to each result. This search uses info_max_time , which is the latest time boundary for the search. The eval command is used to create a field called latest_age and calculate the age of the heartbeats relative to end of the time range. This allows for a time range of -11m@m to -m@m. This is the previous 11 minutes, starting at the beginning of the minute, to the previous 1 minute, starting at the beginning of the minute. The search does not work if you specify latest=null / all time because info_max_time would be set to +infinity. Using the lookup file, expected_hosts , append the list of hosts to the results. Using this list you can determine which hosts are not sending a heartbeat in the expected time range. For any hosts that have a null value in the latest_age field, fill the field with the value 9999. Remove any duplicated host events with the dedup command. Use the where command to filter the results and return any heartbeats older than 42 seconds. Note: In this example, you could use the tstats command, instead of the stats command, to improve the performance of the search.", "code_examples": [{"language": "spl", "code": "... | addinfo"}, {"language": "spl", "code": "... | stats latest(_time) AS latest_time BY host\n| addinfo |evallatest_age = info_max_time - latest_time | fields - info_*\n| inputlookup append=t expected_hosts | fillnull value=9999 latest_age\n| dedup host\n|wherelatest_age > 42"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "addinfo", "section_heading": "Examples", "section_id": "id_5b188172_4fa9_433d_8b3a_531bfe77066e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addinfo", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:55.323429+00:00", "version": "10.2"}}
{"id": "854cadbd623b1678", "content": "search", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "addinfo", "section_heading": "See also", "section_id": "id_3d366ba2_0a06_4d8f_8cd6_0e2f6b1a7718--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addinfo", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T13:59:55.323432+00:00", "version": "10.2"}}
{"id": "ea3e90fac6f955a7", "content": "Appends the result of the subpipeline to the search results. Unlike a subsearch, the subpipeline is not run first. The subpipeline is run when the search reaches the appendpipe command. The appendpipe command is used to append the output of transforming commands , such as chart , timechart , stats , and top .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "appendpipe", "section_heading": "Description", "section_id": "id_96fd84e9_a340_4cbb_aab3_295009a5bc49--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/appendpipe", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:12.480520+00:00", "version": "10.2"}}
{"id": "e3b9a639354d1530", "content": "appendpipe [run_in_preview=<bool>] [<subpipeline>] Optional Arguments run_in_preview Syntax: run_in_preview=<bool> Description: Specifies whether or not display the impact of the appendpipe command in the preview. When set to FALSE, the search runs and the preview shows the results as if the appendpipe command is not part of the search. However, when the search finishes, the results include the impact of the appendpipe command. Default: True subpipeline Syntax: <subpipeline> Description: A list of commands that are applied to the search results from the commands that occur in the search before the appendpipe command.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "appendpipe", "section_heading": "Syntax", "section_id": "id_2f504320_910a_4756_bd20_03abdbf5ba12--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/appendpipe", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:12.480527+00:00", "version": "10.2"}}
{"id": "ad22d1e40a3b9348", "content": "The appendpipe command can be useful because it provides a summary, total, or otherwise descriptive row of the entire dataset when you are constructing a table or chart. This command is also useful when you need the original results for additional calculations.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "appendpipe", "section_heading": "Usage", "section_id": "id_9b692f4f_ae75_4f63_8475_a341751b6767--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/appendpipe", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:12.480532+00:00", "version": "10.2"}}
{"id": "cc96125e1de13b33", "content": "Example 1: Append subtotals for each action across all users. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "index=_audit | stats count by action user | appendpipe [stats sum(count) as count by action |evaluser =\"TOTAL - ALL USERS\"] | sort action"}], "tables": [{"headers": ["action", "user", "count"], "rows": [["accelerate_search", "admin", "209"], ["accelerate_search", "buttercup", "345"], ["accelerate_search", "can-delete", "6"], ["accelerate_search", "TOTAL - ALL USERS", "560"], ["add", "n/a", "1"], ["add", "TOTAL - ALL USERS", "1"], ["change_authentication", "admin", "50"], ["change_authentication", "buttercup", "9"], ["change_authentication", "can-delete", "24"], ["change_authentication", "TOTAL - ALL USERS", "83"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "appendpipe", "section_heading": "Examples", "section_id": "e6208262_8e8f_4195_9450_3a1677e843db--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/appendpipe", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:12.480541+00:00", "version": "10.2"}}
{"id": "c154e21b2f257cd9", "content": "append , appendcols , join , set", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "appendpipe", "section_heading": "See also", "section_id": "b6e3bee3_b9df_4411_bfa7_77563bd7f927--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/appendpipe", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:12.480545+00:00", "version": "10.2"}}
{"id": "dc7876528c3b0bad", "content": "The table command returns a table that is formed by only the fields that you specify in the arguments. Columns are displayed in the same order that fields are specified. Column headers are the field names. Rows are the field values. Each row represents an event. The table command is similar to the fields command in that it lets you specify the fields you want to keep in your results. Use table command when you want to retain data in tabular format. With the exception of a scatter plot to show trends in the relationships between discrete values of your data, you should not use the table command for charts. See Usage. To optimize searches, avoid putting the table command in the middle of your searches and instead, put it at the end of your searches.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "table", "section_heading": "Description", "section_id": "a56595a4_6c4c_44cd_bb5c_f9bea81a2613--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/table", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:29.459001+00:00", "version": "10.2"}}
{"id": "06904a5cde1a293a", "content": "table <wc-field-list> Arguments <wc-field-list> Syntax: <wc-field> ... Description: A list of valid field names. The list can be space-delimited or comma-delimited. You can use the asterisk ( * ) as a wildcard to specify a list of fields with similar names. For example, if you want to specify all fields that start with \"value\", you can use a wildcard such as value* .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "table", "section_heading": "Syntax", "section_id": "f81fd65d_0d65_4511_9327_5d024f509e4d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/table", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:29.459009+00:00", "version": "10.2"}}
{"id": "62c5779528b21d63", "content": "The table command is a transforming command. See Command types. Visualizations To generate visualizations, the search results must contain numeric, datetime, or aggregated data such as count, sum, or average. Command type The table command is a non-streaming command. If you are looking for a streaming command similar to the table command, use the fields command. Field renaming The table command doesn't let you rename fields, only specify the fields that you want to show in your tabulated results. If you're going to rename a field, do it before piping the results to table. Truncated results The table command truncates the number of results returned based on settings in the limits.conf file. In the [search] stanza, if the value for the truncate_report parameter is 1, the number of results returned is truncated. The number of results is controlled by the max_count parameter in the [search] stanza. If truncate_report is set to 0, the max_count parameter is not applied.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "table", "section_heading": "Usage", "section_id": "id_39f586ee_dba7_469a_8e0d_0923417c0b83--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/table", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:29.459027+00:00", "version": "10.2"}}
{"id": "82d3decf40bf8409", "content": "Example 1 Search for recent earthquakes in and around California and display only the time of the quake ( time ), where it occurred ( place ), and the quake's magnitude ( mag ) and depth ( depth ). This search reformats your events into a table and displays only the fields that you specified as arguments. The results look something like this: Example 2 Show the date, time, coordinates, and magnitude of each recent earthquake in Northern California. This example begins with a search for all recent earthquakes in Northern California ( place=\"Northern California\" ). Then the events are piped into the rename command to change the names of the coordinate fields, from latitude and longitude to lat and lon. The locationSource field is also renamed to locSource. (The table command doesn't let you rename or reformat fields, only specify the fields that you want to show in your tabulated results.) Finally, the results are piped into the table command, which specifies both coordinate fields with lat and lon , the date and time with time , and locSource using the asterisk wildcard. The results look something like this: Example 3 Search for IP addresses and classify the network they belong to. This example searches for Web access data and uses the dedup command to remove duplicate values of the IP addresses ( clientip ) that access the server. These results are piped into the eval command, which uses the cidrmatch() function to compare the IP addresses to a subnet range (192.0.0.0/16). This search also uses the if() function, which specifies that if the value of clientip falls in the subnet range, then the network field is given the value local. Otherwise, the network field is other. The results are then piped into the table command to show only the distinct IP addresses ( clientip ) and the network classification ( network ). The results look something like this: Example 4 Create a table with the fields host, action, and all fields that start with date_m. The results look something like this:", "code_examples": [{"language": "spl", "code": "source=all_month.csv place=*California | table time, place, mag, depth"}, {"language": "spl", "code": "source=all_month.csv place=\"Northern California\"| rename latitude as lat longitude as lon locationSource as locSource | table time, place, lat, lon, locS*"}, {"language": "spl", "code": "sourcetype=access_* | dedup clientip |evalnetwork=if(cidrmatch(\"192.0.0.0/16\", clientip),\"local\",\"other\") | table clientip, network"}, {"language": "spl", "code": "sourcetype=access_* | table host action date_m*"}], "tables": [{"headers": [], "rows": [["This example uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), and so forth, for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance if you want follow along with this example."]]}, {"headers": ["time", "place", "mag", "depth"], "rows": [["2023-03-06T06:45:17.427Z", "0 km S of Carnelian Bay, California", "0.2", "8"], ["2023-03-06T12:49:26.451Z", "35 km NE of Independence, California", "0.7", "0"], ["2023-03-07T09:22:15.281Z", "16 km ENE of Doyle, California", "0.4", "11"], ["2023-03-07T09:37:03.042Z", "Northern California", "0.4", "0"], ["2023-03-07T16:41:29.557Z", "27 km ENE of Herlong, California", "1", "0"], ["2023-03-07T20:57:11.181Z", "259 km W of Ferndale, California", "3.3", "16.554"]]}, {"headers": [], "rows": [["This example uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), and so forth, for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand upload the file to your Splunk instance if you want follow along with this example."]]}, {"headers": ["time", "place", "lat", "lon", "locSource"], "rows": [["2023-03-03T13:32:16.019Z", "Northern California", "39.3547", "-120.0101", "nn"], ["2023-03-07T09:37:03.042Z", "Northern California", "39.6117", "-120.7116", "nn"], ["2023-03-09T03:56:40.162Z", "Northern California", "39.3561", "-120.0133", "nn"], ["2023-03-01T09:37:57.283Z", "Northern California", "39.5293", "-120.3513", "nn"], ["2023-02-21T05:18:39.039Z", "Northern California", "39.6726", "-120.642", "nn"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["clientip", "network"], "rows": [["192.0.1.51", "other"], ["192.168.11.33", "other"], ["192.168.11.44", "other"], ["192.168.11.35", "other"], ["192.1.2.40", "other"], ["192.1.2.35", "other"], ["192.0.1.39", "local"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["host", "action", "date_mday", "date_minute", "date_month"], "rows": [["www1", "", "20", "51", "july"], ["www1", "", "20", "48", "july"], ["www1", "", "20", "48", "july"], ["www1", "addtocart", "20", "48", "july"], ["www1", "", "20", "48", "july"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "table", "section_heading": "Examples", "section_id": "badfe0d5_043b_4b6e_bd69_27652bf7f89f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/table", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:29.459045+00:00", "version": "10.2"}}
{"id": "e7879303ef5a515d", "content": "fields", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "table", "section_heading": "See Also", "section_id": "c228b704_5e38_4205_a9e2_bd78c4e776d5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/table", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:29.459049+00:00", "version": "10.2"}}
{"id": "9687c0c6f3ff66bc", "content": "Makes a field on the x-axis numerically continuous by adding empty buckets for periods where there is no data and quantifying the periods where there is data. This x-axis field can then be invoked by the chart and timechart commands.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "makecontinuous", "section_heading": "Description", "section_id": "id_48df3dde_6b58_43ec_9fad_67f8e9efa930--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makecontinuous", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:44.742855+00:00", "version": "10.2"}}
{"id": "a291dfd488fe4cac", "content": "The required syntax is in bold. makecontinuous <field> [<bin-options>...] Required arguments <bins-options> Datatype: bins | span | start-end Description: Discretization options. See \"Bins options\" for details. Optional arguments <field> Datatype: <field> Description: Specify a field name. Bins options bins Syntax: bins=<int> Description: Sets the maximum number of bins to discretize into. span Syntax: <log-span> | <span-length> Description: Sets the size of each bin, using a span length based on time or log-based span. <start-end> Syntax: end=<num> | start=<num> Description: Sets the minimum and maximum extents for numerical bins. Data outside of the [start, end] range is discarded. Span options <log-span> Syntax: [<num>]log[<num>] Description: Sets to log-based span. The first number is a coefficient. The second number is the base. If the first number is supplied, it must be a real number >= 1.0 and < base. Base, if supplied, must be real number > 1.0, meaning it must be strictly greater than 1. span-length Syntax: <span>[<timescale>] Description: A span length based on time. <span> Syntax: <int> Description: The span of each bin. If using a timescale, this is used as a time range. If not, this is an absolute bin \"length.\" <timescale> Syntax: <sec> | <min> | <hr> | <day> | <month> | <subseconds> Description: Time scale units.", "code_examples": [], "tables": [{"headers": ["Time scale", "Syntax", "Description"], "rows": [["<sec>", "s | sec | secs | second | seconds", "Time scale in seconds."], ["<min>", "m | min |  mins |  minute |  minutes", "Time scale in minutes."], ["<hr>", "h | hr |  hrs |  hour | hours", "Time scale in hours."], ["<day>", "d |  day | days", "Time scale in days."], ["<month>", "mon | month |  months", "Time scale in months."], ["<subseconds>", "us | ms |  cs |  ds", "Time scale in microseconds (us), milliseconds (ms), centiseconds (cs), or deciseconds (ds)"]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "makecontinuous", "section_heading": "Syntax", "section_id": "id_61746fbe_1bf5_4043_a575_0ce9084101e6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makecontinuous", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:44.742866+00:00", "version": "10.2"}}
{"id": "81c53d2f8beb9bca", "content": "The makecontinuous command is a transforming command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "makecontinuous", "section_heading": "Usage", "section_id": "id_74dd258b_7bdd_4051_8dd7_478289e2574b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makecontinuous", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:44.742871+00:00", "version": "10.2"}}
{"id": "60a14a83ca8937e8", "content": "Example 1: Make the _time field continuous with a span of 10 minutes.", "code_examples": [{"language": "spl", "code": "... | makecontinuous _time span=10m"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "makecontinuous", "section_heading": "Examples", "section_id": "id_5cda52a3_a48e_42ee_b764_9eb35c06f113--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makecontinuous", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:44.742876+00:00", "version": "10.2"}}
{"id": "0d838794d5ce33bb", "content": "chart , timechart", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "makecontinuous", "section_heading": "See also", "section_id": "id_151431e1_cba0_49e0_a9a5_3905231ef478--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makecontinuous", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:00:44.742880+00:00", "version": "10.2"}}
{"id": "69216ef09a323fc6", "content": "The metadata command returns a list of sources, sourcetypes, or hosts from a specified index or distributed search peer. The metadata command returns information accumulated over time. You can view a snapshot of an index over a specific timeframe, such as the last 7 days, by using the time range picker. See Usage .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "metadata", "section_heading": "Description", "section_id": "id_806338d2_2785_4264_be5b_913a4af6ff6e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/metadata", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:01.791481+00:00", "version": "10.2"}}
{"id": "8ac4bda0b749227b", "content": "| metadata type=<metadata-type> [<index-specifier>]... [splunk_server=<wc-string>] [splunk_server_group=<wc-string>]...<datatype> Required arguments type Syntax: type= hosts | sources | sourcetypes Description: The type of metadata to return. This must be one of the three literal strings: hosts, sources, or sourcetypes. Optional arguments index-specifier Syntax: index=<index_name> Description: Specifies the index from which to return results. You can specify more than one index. Wildcard characters (*) can be used. To match non-internal indexes, use index=*. To match internal indexes, use index=_*. Example: | metadata type=hosts index=cs* index=na* index=ap* index=eu* Default: The default index, which is usually the main index. splunk_server Syntax: splunk_server=<wc-string> Description: Specifies the distributed search peer from which to return results. If you are using Splunk Cloud Platform, omit this parameter. If you are using Splunk Enterprise, you can specify only one splunk_server argument. However, you can use a wildcard when you specify the server name to indicate multiple servers. For example, you can specify splunk_server=peer01 or splunk_server=peer*. Use local to refer to the search head. Default: All configured search peers return information splunk_server_group Syntax: splunk_server_group=<wc-string>... Description: Limits the results to one or more server groups. If you are using Splunk Cloud, omit this parameter. You can specify a wildcard character in the string to indicate multiple server groups. datatype-options Syntax: datatype=[metric|event] Description: Specifies whether to limit the search to the metrics index or the event index.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "metadata", "section_heading": "Syntax", "section_id": "id_1abddcb4_f26b_46b7_9894_222f3cdb7c28--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/metadata", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:01.791489+00:00", "version": "10.2"}}
{"id": "55da8f0ec4e2cb7b", "content": "The metadata command is a report-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. Although the metadata command fetches data from all peers, any command run after it runs only on the search head. The command shows the first, last, and most recent events that were seen for each value of the specified metadata type. For example, if you search for: Your results should look something like this: The firstTime field is the timestamp for the first time that the indexer saw an event from this host. The lastTime field is the timestamp for the last time that the indexer saw an event from this host. The recentTime field is the indextime for the most recent time that the index saw an event from this host. In other words, this is the time of the last update. The totalcount field is the total number of events seen from this host. The type field is the specified type of metadata to display. Because this search specifies type=hosts , there is also a host column. In most cases, when the data is streaming live, the lastTime and recentTime field values are equal. If the data is historical, however, the values might be different. In small testing environments, the data is complete. However, in environments with large numbers of values for each category, the data might not be complete. This is intentional and allows the metadata command to operate within reasonable time and memory usage. Real-time searches Running the metadata command in a real-time search that returns a large number of results will very quickly consume all the available memory on the Splunk server. Use caution when you use the metadata command in real-time searches. Time ranges Set the time range using the Time Range Picker. You cannot use the earliest or latest time range modifiers in the search string. Time range modifiers must be set before the first piped command and generating commands in general do not allow anything to be specified before the first pipe. If you specify a time range other than All Time for your search, the search results might not be precise. The metadata is stored as aggregate numbers for each bucket on the index. A bucket is either included or not included based on the time range you specify. For example, you run the following search specifying a time range of Last 7 days. The time range corresponds to January 1st to January 7th. There is a bucket on the index that contains events from both December 31st and January 1st. The metadata from that bucket is included in the information returned from search. Maximum results By default, a maximum of 10,000 results are returned. This maximum is controlled by the maxresultrows setting in the [metadata] stanza In the limits.conf file.", "code_examples": [{"language": "spl", "code": "| metadatatype=hosts"}, {"language": "spl", "code": "| metadatatype=sourcetypes index=ap"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "metadata", "section_heading": "Usage", "section_id": "id_80147486_1472_42d0_b3f4_944439f35fb2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/metadata", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:01.791494+00:00", "version": "10.2"}}
{"id": "a03eb066e3a7a96d", "content": "1. Search multiple indexes Return the metadata for indexes that represent different regions. 2. Search for sourcetypes Return the values of sourcetypes for events in the _internal index. This returns the following report. 3. Search for values of host Return the values of host for data points in the mymetrics index. 4. Format the results from the metadata command You can also use the fieldformat command to format the results of the firstTime, lastTime, and recentTime columns to be more readable. Click on the Count field label to sort the results and show the highest count first. Now, the results are more readable: 5. Return values of \"sourcetype\" for events in a specific index on a specific server or wildcarded server Return values of sourcetype for events in the _audit index on server peer01. To return values of sourcetype for events in the _audit index on any server name that begins with peer .", "code_examples": [{"language": "spl", "code": "| metadatatype=hosts index=cs* index=na* index=ap* index=eu*"}, {"language": "spl", "code": "| metadatatype=sourcetypes index=_internal"}, {"language": "spl", "code": "| metadatatype=hosts index=mymetrics datatype=metric"}, {"language": "spl", "code": "| metadatatype=sourcetypes index=_internal | rename totalCount as Count firstTime as\"First Event\"lastTime as\"Last Event\"recentTime as\"Last Update\"| fieldformat Count=tostring(Count,\"commas\") | fieldformat\"First Event\"=strftime('First Event',\"%c\") | fieldformat\"Last Event\"=strftime('Last Event',\"%c\") | fieldformat\"Last Update\"=strftime('Last Update',\"%c\")"}, {"language": "spl", "code": "| metadatatype=sourcetypes index=_audit splunk_server=peer01"}, {"language": "spl", "code": "| metadatatype=sourcetypes index=_audit splunk_server=peer*"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "metadata", "section_heading": "Examples", "section_id": "id_4220394d_0a3d_497c_b50b_78e1a4dd8379--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/metadata", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:01.791499+00:00", "version": "10.2"}}
{"id": "5cd6a8206c07a012", "content": "dbinspect tstats", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "metadata", "section_heading": "See also", "section_id": "id_7f4c2770_78f7_4366_8e10_d25258655eb3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/metadata", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:01.791503+00:00", "version": "10.2"}}
{"id": "06ae364f1de5cbcb", "content": "The sitop command is the summary indexing version of the top command, which returns the most frequent value of a field or combination of fields. The sitop command populates a summary index with the statistics necessary to generate a top report. After you populate the summary index, use the regular top command with the exact same search string as the sitop command search to report against it.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "sitop", "section_heading": "Description", "section_id": "f6b4558d_2846_4d2e_ae7b_a0d2e853f4d7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sitop", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:16.503191+00:00", "version": "10.2"}}
{"id": "59cd37c64e0c913c", "content": "sitop [<N>] [<top-options>...] <field-list> [<by-clause>] Note: This is the exact same syntax as that of the top command. Required arguments <field-list> Syntax: <field>, ... Description: Comma-delimited list of field names. Optional arguments <N> Syntax: <int> Description: The number of results to return. <top-options> Syntax: countfield=<string> | limit=<int> | otherstr=<string> | percentfield=<string> | showcount=<bool> | showperc=<bool> | useother=<bool> Description: Options for the sitop command. See Top options. <by-clause> Syntax: BY <field-list> Description: The name of one or more fields to group by. Top options countfield Syntax: countfield=<string> Description: The name of a new field that the value of count is written to. Default: count limit Syntax: limit=<int> Description: Specifies how many tuples to return, \"0\" returns all values. Default: \"10\" otherstr Syntax: otherstr=<string> Description: If useother is true, specify the value that is written into the row representing all other values. Default: \"OTHER\" percentfield Syntax: percentfield=<string> Description: Name of a new field to write the value of percentage. Default: \"percent\" showcount Syntax: showcount=<bool> Description: Specify whether to create a field called \"count\" (see \"countfield\" option) with the count of that tuple. Default: true showperc Syntax: showperc=<bool> Description: Specify whether to create a field called \"percent\" (see \"percentfield\" option) with the relative prevalence of that tuple. Default: true useother Syntax: useother=<bool> Description: Specify whether or not to add a row that represents all values not included due to the limit cutoff. Default: false", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "sitop", "section_heading": "Syntax", "section_id": "df91ee7b_d287_4911_af68_14d25797774f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sitop", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:16.503198+00:00", "version": "10.2"}}
{"id": "6fe694252a669e9b", "content": "Example 1: Compute the necessary information to later do 'top foo bar' on summary indexed results. Example 2: Populate a summary index with the top source IP addresses in a scheduled search that runs daily: Save the search as, \"Summary - firewall top src_ip\". Later, when you want to retrieve that information and report on it, run this search over the past year: Additionally, because this search specifies the search name, it filters out other data that have been placed in the summary index by other summary indexing searches.", "code_examples": [{"language": "spl", "code": "... | sitop foo bar"}, {"language": "spl", "code": "eventtype=firewall | sitop src_ip"}, {"language": "spl", "code": "index=summary search_name=\"summary - firewall top src_ip\"|top src_ip"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "sitop", "section_heading": "Examples", "section_id": "c2d8fe3f_5436_4f3d_9a2e_a803d9dfecee--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sitop", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:16.503204+00:00", "version": "10.2"}}
{"id": "b79709ed6f1da807", "content": "collect , overlap , sichart , sirare , sistats , sitimechart", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "sitop", "section_heading": "See also", "section_id": "cdfb415e_08cf_4a3a_8920_4c62e2ccecb4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sitop", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:16.503209+00:00", "version": "10.2"}}
{"id": "b3f251dd11c773bb", "content": "The map command is a looping operator that runs a search repeatedly for each input event or result. You can run the map command on a saved search or an ad hoc search. This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "map", "section_heading": "Description", "section_id": "id_8ae2eb25_b8fc_479e_a356_24655a0d55e1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/map", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:34.609917+00:00", "version": "10.2"}}
{"id": "730ccd0a1e845a44", "content": "The required syntax is in bold. map (<searchoption> | <savedsplunkoption>) [maxsearches=int] Required arguments You must specify either <savedsplunkoption> or <searchoption>. <savedsplunkoption> Syntax: <string> Description: The name of a saved search to run for each input result. Default: No default. <searchoption> Syntax: search=\"<string>\" Description: An ad hoc search to run for each input result. For example: ...| map search=\"search index=_internal earliest=$myearliest$ latest=$mylatest$\". Default: No default. Optional arguments maxsearches Syntax: maxsearches=<int> Description: The maximum number of searches to run. A message is generated if there are more search results than the maximum number that you specify. Zero ( 0 ) does not equate to unlimited searches. Default: 10", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "map", "section_heading": "Syntax", "section_id": "id_9669f136_d1e0_4778_bfe2_67183595eaff--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/map", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:34.609935+00:00", "version": "10.2"}}
{"id": "07903401383ba3d1", "content": "The map command is a dataset processing command. See Command types. A subsearch can be initiated through a search command such as the map command. See Initiating subsearches with search commands in the Splunk Cloud Platform Search Manual. Known limitations You cannot use the map command after an append or appendpipe command in your search pipeline. Variable for field names When using a saved search or a literal search, the map command supports the substitution of $variable$ strings that match field names in the input results. A search with a string like $count$, for example, will replace the variable with the value of the count field in the input search result. When using the map command in a dashboard <form> , use double dollar signs ($$) to specify a variable string. For example, $$count$$. See Dashboards and forms. Certain variables for field names might conflict with token names and produce unpredictable search results. For example, the following are some variables for field names that might conflict with token names: $alert.expires$ $alert.severity$ $cron_schedule$ $description$ $name$ $search$ $username$ Search ID field The map command also supports a search ID field, provided as $_serial_id$. The search ID field will have a number that increases incrementally each time that the search is run. In other words, the first run search will have the ID value 1, and the second 2, and so on.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "map", "section_heading": "Usage", "section_id": "id_7ea4fa8a_4868_47c3_815f_44f165e71185--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/map", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:34.609948+00:00", "version": "10.2"}}
{"id": "6f18bad68f9bc3df", "content": "1. Invoke the map command with a saved search 2. Map the start and end time values 3. Use the map command with a subsearch For complex ad hoc searches , use a subsearch for your map search. Alternatively, you can escape double quotation marks with backslashes ( \\\" ) in your ad hoc map search, as shown in example 4. You can use a subsearch with the map command like this: The search results look something like this: 4. Use the map command by escaping double quotation marks As an alternative to example 3, you can escape double quotation marks with backslashes ( \\\" ) in your map ad hoc searches like this: The search results look something like this:", "code_examples": [{"language": "spl", "code": "error | localize | map mytimebased_savedsearch"}, {"language": "spl", "code": "... | map search=\"search starttimeu::$start$ endtimeu::$end$\"maxsearches=10"}, {"language": "spl", "code": "| makeresults count=4 \n| streamstats count \n|evalfield=\"hello\".count\n| map \n    [ makeresults \n    |evalpony=1,field=\"$field$\",serial=\"$_serial_id$\",hello=\"buttercup\"]"}, {"language": "spl", "code": "| makeresults count=4 \n|evalfield=\"hello\"| map search=\"| makeresults | eval pony=1,field=\\\"$field$\\\",serial=\\\"$_serial_id$\\\",hello=\\\"buttercup\\\" \""}], "tables": [{"headers": ["_time", "field", "hello", "pony", "serial"], "rows": [["2024-01-04 17:23:42", "hello1", "buttercup", "1", "1"], ["2024-01-04 17:23:42", "hello2", "buttercup", "1", "2"], ["2024-01-04 17:23:42", "hello3", "buttercup", "1", "3"], ["2024-01-04 17:23:42", "hello4", "buttercup", "1", "4"]]}, {"headers": ["_time", "field", "hello", "pony", "serial"], "rows": [["2024-01-04 17:23:42", "hello", "buttercup", "1", "1"], ["2024-01-04 17:23:42", "hello", "buttercup", "1", "2"], ["2024-01-04 17:23:42", "hello", "buttercup", "1", "3"], ["2024-01-04 17:23:42", "hello", "buttercup", "1", "4"]]}], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "map", "section_heading": "Basic examples", "section_id": "c23ae54c_9dbd_45e3_b75a_a3dff3b3965a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/map", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:34.609978+00:00", "version": "10.2"}}
{"id": "d06f0251192708ae", "content": "1. Use a Sudo event to locate the user logins This example illustrates how to find a Sudo event and then use the map command to trace back to the computer and the time that users logged on before the Sudo event. Start with the following search for the Sudo event. This search returns a table of results. Pipe these results into the map command, substituting the username. It takes each of the three results from the previous search and searches in the ad_summary index for the logon event for the user. The results are returned as a table. (Thanks to Splunk user Alacercogitatus for this example.)", "code_examples": [{"language": "spl", "code": "sourcetype=syslog sudo | stats count by user host"}, {"language": "spl", "code": "sourcetype=syslog sudo | stats count by user host | map search=\"search index=ad_summary username=$user$ type_logon=ad_last_logon\""}], "tables": [{"headers": ["User", "Host", "Count"], "rows": [["userA", "serverA", "1"], ["userB", "serverA", "3"], ["userA", "serverB", "2"]]}, {"headers": ["_time", "computername", "computertime", "username", "usertime"], "rows": [["10/12/16 8:31:35.00 AM", "Workstation$", "10/12/2016 08:25:42", "userA", "10/12/2016 08:31:35 AM"]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "map", "section_heading": "Extended examples", "section_id": "id_5c2b2058_dfa0_4855_96ac_a0bff3d377fa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/map", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:34.610000+00:00", "version": "10.2"}}
{"id": "7afda21a2d56f8d3", "content": "Commands gentimes search", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "map", "section_heading": "See also", "section_id": "ca55c5a6_229c_49fb_8e7a_da3cc293d107--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/map", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:34.610012+00:00", "version": "10.2"}}
{"id": "47241166bea98485", "content": "Use the rangemap command to categorize the values in a numeric field. The command adds in a new field called range to each event and displays the category in the range field. The values in the range field are based on the numeric ranges that you specify. Set the range field to the names of any attribute_name that the value of the input field is within. If no range is matched, the range value is set to the default value. The ranges that you set can overlap. If you have overlapping values, the range field is created as a multivalue field containing all the values that apply. For example, if low=1-10, elevated=5-15, and the input field value is 10, range=low and code=elevated .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "rangemap", "section_heading": "Description", "section_id": "id_7e22947a_a0d4_48c4_acff_78006acd649e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rangemap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:49.438324+00:00", "version": "10.2"}}
{"id": "0cee4e5fbf0ee33a", "content": "The required syntax is in bold. rangemap field=<string> [<attribute_name>=<numeric_range>]... [default=<string>] Required arguments field Syntax: field=<string> Description: The name of the input field. This field must contain numeric values. Optional arguments attribute_name=numeric_range Syntax: <string>=<num>-<num> Description: The <attribute_name> is a string value that is output when the <numeric_range> matches the value in the <field>. The <attribute_name> is a output to the range field. The <numeric_range> is the starting and ending values for the range. The values can be integers or floating point numbers. The first value must be lower than the second. The <numeric_range> can include negative values. Example: Dislike=-5--1 DontCare=0-0 Like=1-5 default Syntax: default=<string> Description: If the input field does not match a range, use this to define a default value. Default: \"None\"", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "rangemap", "section_heading": "Syntax", "section_id": "id_90a20d4d_46cb_4c7d_b518_672cf7bb0487--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rangemap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:49.438335+00:00", "version": "10.2"}}
{"id": "4b6059667ebfc07a", "content": "The rangemap command is a distributable streaming command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "rangemap", "section_heading": "Usage", "section_id": "id_6daa92a0_3440_4361_8ee4_9bb4d0f22e31--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rangemap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:49.438341+00:00", "version": "10.2"}}
{"id": "478c0dba7c766dd2", "content": "Example 1: Set range to \"green\" if the date_second is between 1-30; \"blue\", if between 31-39; \"red\", if between 40-59; and \"gray\", if no range matches (for example, if date_second=0). Example 2: Sets the value of each event's range field to \"low\" if its count field is 0 (zero); \"elevated\", if between 1-100; \"severe\", otherwise.", "code_examples": [{"language": "spl", "code": "... | rangemap field=date_second green=1-30 blue=31-39 red=40-59 default=gray"}, {"language": "spl", "code": "... | rangemap field=count low=0-0 elevated=1-100 default=severe"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "rangemap", "section_heading": "Basic examples", "section_id": "id_3d4c0ada_7877_4afc_9077_d1dc5d4c5f1c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rangemap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:49.438347+00:00", "version": "10.2"}}
{"id": "6e329b0788a559cc", "content": "This search counts the number and magnitude of each earthquake that occurred in and around Alaska. Then a color is assigned to each magnitude using the rangemap command. The results look something like this: Summarize the results by range value The results look something like this: Arrange the results in a custom sort order By default the values in the search results are in descending order by the sum(count) field. You can apply a custom sort order to the results using the eval command with the case function. The results look something like this:", "code_examples": [{"language": "spl", "code": "source=all_month.csv place=*alaska* mag>=3.5 \n| stats count BY mag \n| rename mag AS magnitude \n| rangemap field=magnitude light=3.9-4.3 strong=4.4-4.9 severe=5.0-9.0 default=weak"}, {"language": "spl", "code": "source=all_month.csv place=*alaska* mag>=3.5 \n| stats count BY mag \n| rename mag AS magnitude \n| rangemap field=magnitude green=3.9-4.2 yellow=4.3-4.6 red=4.7-5.0 default=gray \n| stats sum(count) by range"}, {"language": "spl", "code": "source=all_month.csv place=*alaska* mag>=3.5 \n| stats count BY mag \n| rename mag AS magnitude \n| rangemap field=magnitude green=3.9-4.2 yellow=4.3-4.6 red=4.7-5.0 default=gray \n| stats sum(count) by range\n|evalsort_field=case(range=\"red\",1, range=\"yellow\",2, range=\"green\",3, range=\"gray\",4)\n| sort sort_field"}], "tables": [{"headers": [], "rows": [["This example uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand add it as an input. The following examples uses theAll Earthquakesunder thePast 30 dayslist."]]}, {"headers": ["magnitude", "count", "range"], "rows": [["3.7", "15", "weak"], ["3.8", "31", "weak"], ["3.9", "29", "light"], ["4", "22", "light"], ["4.1", "30", "light"], ["4.2", "15", "light"], ["4.3", "10", "light"], ["4.4", "22", "strong"], ["4.5", "3", "strong"], ["4.6", "8", "strong"], ["4.7", "9", "strong"], ["4.8", "6", "strong"], ["4.9", "6", "strong"], ["5", "2", "severe"], ["5.1", "2", "severe"], ["5.2", "5", "severe"]]}, {"headers": ["range", "sum(count)"], "rows": [["gray", "127"], ["green", "96"], ["red", "23"], ["yellow", "43"]]}, {"headers": ["range", "sum(count)", "sort_field"], "rows": [["red", "23", "1"], ["yellow", "43", "2"], ["green", "96", "3"], ["gray", "127", "4"]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "rangemap", "section_heading": "Extended example", "section_id": "bf09eb77_d12d_4824_9439_7f7461c4883f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rangemap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:49.438371+00:00", "version": "10.2"}}
{"id": "6a81cd79fcbdbc60", "content": "Commands eval Blogs Order Up! Custom Sort Orders", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "rangemap", "section_heading": "See also", "section_id": "id_8e8152ee_8900_4b14_b4ec_2457c4111206--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rangemap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:01:49.438376+00:00", "version": "10.2"}}
{"id": "1153b304612008e4", "content": "Concatenates string values from 2 or more fields. Combines together string values and literals into a new field. A destination field name is specified at the end of the strcat command.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "strcat", "section_heading": "Description", "section_id": "id_11b62a58_ac3d_411a_9196_4eeefe7efa07--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/strcat", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:06.330633+00:00", "version": "10.2"}}
{"id": "608864197455b973", "content": "strcat [allrequired=<bool>] <source-fields> <dest-field> Required arguments <dest-field> Syntax: <string> Description: A destination field to save the concatenated string values in, as defined by the <source-fields> argument. The destination field is always at the end of the series of source fields. <source-fields> Syntax: (<field> | <quoted-str>)... Description: Specify the field names and literal string values that you want to concatenate. Literal values must be enclosed in quotation marks. quoted-str Syntax: \"<string>\" Description: Quoted string literals. Examples: \"/\" or \":\" Optional arguments allrequired Syntax: allrequired=<bool> Description: Specifies whether or not all source fields need to exist in each event before values are written to the destination field. If allrequired=f , the destination field is always written and source fields that do not exist are treated as empty strings. If allrequired=t , the values are written to destination field only if all source fields exist. Default: false", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "strcat", "section_heading": "Syntax", "section_id": "fe84f0ea_9d4b_4e5b_81fe_4ee1552c48c9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/strcat", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:06.330642+00:00", "version": "10.2"}}
{"id": "42ace95bdaf27988", "content": "The strcat command is a distributable streaming command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "strcat", "section_heading": "Usage", "section_id": "b541348b_17ae_49ef_9694_cffe34dc5e52--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/strcat", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:06.330647+00:00", "version": "10.2"}}
{"id": "fc65b16ac6d678b4", "content": "Example 1: Add a field called comboIP, which combines the source and destination IP addresses. Separate the addresses with a forward slash character. Example 2: Add a field called comboIP, which combines the source and destination IP addresses. Separate the addresses with a forward slash character. Create a chart of the number of occurrences of the field values. Example 3: Add a field called address, which combines the host and port values into the format <host>::<port>.", "code_examples": [{"language": "spl", "code": "... | strcat sourceIP\"/\"destIP comboIP"}, {"language": "spl", "code": "host=\"mailserver\"| strcat sourceIP\"/\"destIP comboIP | chart count by comboIP"}, {"language": "spl", "code": "... | strcat host\"::\"port address"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "strcat", "section_heading": "Examples", "section_id": "id_19f438d1_d489_44ec_afad_3d30be8f0fee--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/strcat", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:06.330652+00:00", "version": "10.2"}}
{"id": "d2ad238cb1f54ec7", "content": "eval", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "strcat", "section_heading": "See also", "section_id": "id_9712f394_0fdf_49f9_b423_f5636bc440b6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/strcat", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:06.330656+00:00", "version": "10.2"}}
{"id": "a53fe5693bcc9757", "content": "Use the anomalies command to look for events or field values that are unusual or unexpected. The anomalies command assigns an unexpectedness score to each event and places that score in a new field named unexpectedness. Whether the event is considered anomalous or not depends on a threshold value. The threshold value is compared to the unexpectedness score. The event is considered unexpected or anomalous if the unexpectedness score is greater than the threshold value. After you use the anomalies command in a search, look at the Interesting Fields list in the Search & Reporting window. Select the unexpectedness field to see information about the values in your events. The unexpectedness score of an event is calculated based on the similarity of that event (X) to a set of previous events (P). The formula for unexpectedness is: In this formula, s( ) is a metric of how similar or uniform the data is. This formula provides a measure of how much adding X affects the similarity of the set of events. The formula also normalizes the results for the differing event sizes. Note: Use current Splunk machine learning (ML) tools to take advantage of the latest algorithms and get the most powerful results. See About the Splunk Machine Learning Toolkit in the Splunk Machine Learning Toolkit .", "code_examples": [{"language": "spl", "code": "unexpectedness =  [s(P and X) - s(P)] / [s(P) + s(X)]"}], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "anomalies", "section_heading": "Description", "section_id": "d80a47fa_cde9_43c1_b0b8_06ef5f2b241c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalies", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:23.567584+00:00", "version": "10.2"}}
{"id": "cd2785eeb2236661", "content": "The required syntax is in bold. anomalies [threshold=<num>] [labelonly=<bool>] [normalize=<bool>] [maxvalues=<num>] [field=<field>] [denylist=<filename>] [denylistthreshold=<num>] [by-clause] Optional arguments threshold Syntax: threshold=<num> Description: A number to represent the upper limit of expected or normal events. If unexpectedness calculated for an event is greater than this threshold limit, the event is considered unexpected or anomalous. Default: 0.01 labelonly Syntax: labelonly=<bool> Description: Specifies if you want the output result set to include all events or only the events that are above the threshold value. The unexpectedness field is appended to all events. If labelonly=true , no events are removed. If labelonly=false , events that have a unexpectedness score less than the threshold are removed from the output result set. Default: false normalize Syntax: normalize=<bool> Description: Specifies whether or not to normalize numeric text in the fields. All characters in the field from 0 to 9 are considered identical for purposes of the algorithm. The placement and quantity of the numbers remains significant. When a field contains numeric data that should not be normalized but treated as categories, set normalize=false. Default: true maxvalues Syntax: maxvalues=<num> Description: Specifies the size of the sliding set of previous events to include when determining the unexpectedness of a field value. By default the calculation uses the previous 100 events for the comparison. If the current event number is 1000, the calculation uses the values in events 900 to 999 in the calculation. If the current event number is 1500, the calculation uses the values in events 1400 to 1499 in the calculation. You can specify a number between 10 and 10000. Increasing the value of maxvalues increases the total CPU cost per event linearly. Large values have very long search runtimes. Default: 100 field Syntax: field=<field> Description: The field to analyze when determining the unexpectedness of an event. Default: _raw denylist Syntax: denylist=<filename> Description: The name of a CSV file that contains a list of events that are expected and should be ignored. Any incoming event that is similar to an event in the denylist is treated as not anomalous, or expected, and given an unexpectedness score of 0.0. The CSV file must be located in the $SPLUNK_HOME/var/run/splunk/csv directory on the search head. If you have Splunk Cloud Platform and want to configure a denylist file, file a Support ticket. denylistthreshold Syntax: denylistthreshold=<num> Description: Specifies a similarity score threshold for matching incoming events to denylisted events. If the incoming event has a similarity score above the denylistthreshold , the event is marked as unexpected. Default: 0.05 by-clause Syntax: by <fieldlist> Description: Use to specify a list of fields to segregate the results for anomaly detection. For each combination of values for the specified fields, the events with those values are treated entirely separately.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "anomalies", "section_heading": "Syntax", "section_id": "fcae7447_f2f7_4721_bbcd_204c1f02f352--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalies", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:23.567590+00:00", "version": "10.2"}}
{"id": "f3341d82b43232c2", "content": "1. Specify a denylist file of the events to ignore The following example shows the interesting events, ignoring any events in the denylist 'boringevents'. Sort the event list in descending order, with highest value in the unexpectedness field listed first. 2. Find anomalies in transactions This example uses transactions to find regions of time that look unusual. 3. Identify anomalies by source Look for anomalies in each source separately. A pattern in one source does not affect that it is anomalous in another source. 4. Specify a threshold when identifying anomalies This example shows how to tune a search for anomalies using the threshold value. Start with a search that uses the default threshold value. This search looks at events in the _internal index and calculates an unexpectedness score for sets of events that have the same group value. The sliding set of events that are used to calculate the unexpectedness score for each unique group value includes only the events that have the same group value. The search command is used to show events that only include the group field. The unexpectedness and group fields appear in the list of Interesting fields. Click on the field name and then click Yes to move the field to the Selected fields list. The fields are moved and also appear in the search results. Your results should look something like the following image. The key-value pairs in the first event include group=pipeline , name=indexerpipe , processor=indexer , cpu_seconds=0.022 , and so forth. With the default threshold , which is 0.01, you can see that some of these events might be very similar. The next search increases the threshold a little: With the higher threshold value, the timestamps and key-value pairs show more distinction between each of the events. Also, you might not want to hide the events that are not anomalous. Instead, you can add another field to your events that tells you whether or not the event is interesting to you. One way to do this is with the eval command: This search uses labelonly=true so that the boring events are still retained in the results list. The eval command is used to define a field named threshold and set it to the threshold value. This has to be done explicitly because the threshold attribute of the anomalies command is not a field. The second eval command is used to define another new field, score , that is either \"anomalous\" or \"boring\" based on how the unexpectedness compares to the threshold value. The following image shows a snapshot of the results.", "code_examples": [{"language": "spl", "code": "... | anomalies denylist=boringevents | sort -unexpectedness"}, {"language": "spl", "code": "... | transaction maxpause=2s | anomalies"}, {"language": "spl", "code": "... | anomalies bysource"}, {"language": "spl", "code": "index=_internal | anomalies BY group  | search group=*"}, {"language": "spl", "code": "index=_internal | anomalies threshold=0.03 by group | search group=*"}, {"language": "spl", "code": "index=_internal | anomalies threshold=0.03 labelonly=trueby group | search group=* |evalthreshold=0.03 |evalscore=if(unexpectedness>=threshold,\"anomalous\",\"boring\")"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "anomalies", "section_heading": "Examples", "section_id": "id_15fafe6b_54a8_4d9e_8f8a_f0de19eb32b6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalies", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:23.567595+00:00", "version": "10.2"}}
{"id": "0d9505be321440e4", "content": "anomalousvalue , cluster , kmeans , outlier", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "anomalies", "section_heading": "See also", "section_id": "id_01636e97_ebee_4649_bb5d_115dd228fe92--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/anomalies", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:23.567599+00:00", "version": "10.2"}}
{"id": "7733012a1773ff45", "content": "Computes the moving averages of fields: simple moving average (sma), exponential moving average (ema), and weighted moving average (wma) The output is written to a new field, which you can specify. SMA and WMA both compute a sum over the period of most recent values. WMA puts more weight on recent values rather than past values. EMA is calculated using the following formula. where alpha = 2/(period + 1) and field(t) is the current value of a field.", "code_examples": [{"language": "spl", "code": "EMA(t) = alpha * EMA(t-1) + (1 - alpha) * field(t)"}], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "trendline", "section_heading": "Description", "section_id": "b0c1bbb5_9f01_4e45_8fee_57687593df75--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/trendline", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:38.562825+00:00", "version": "10.2"}}
{"id": "406eebeaf5c8da42", "content": "trendline ( <trendtype><period>\"(\"<field>\")\" [AS <newfield>] )... Required arguments trendtype Syntax: sma | ema | wma Description: The type of trend to compute. Current supported trend types include simple moving average (sma), exponential moving average (ema), and weighted moving average (wma). period Syntax: <num> Description: The period over which to compute the trend, an integer between 2 and 10000. <field> Syntax: \"(\"<field>\")\" Description: The name of the field on which to calculate the trend. Optional arguments <newfield> Syntax: <field> Description: Specify a new field name to write the output to. Default: <trendtype><period>(<field>)", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "trendline", "section_heading": "Syntax", "section_id": "id_1d8b1ae1_f306_421f_94e2_6074bbd82a04--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/trendline", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:38.562833+00:00", "version": "10.2"}}
{"id": "b919a02511976e68", "content": "Example 1: Computes a five event simple moving average for field 'foo' and writes the result to new field called 'smoothed_foo.' Also, in the same line, computes ten event exponential moving average for field 'bar'. Because no AS clause is specified, writes the result to the field 'ema10(bar)'. Example 2: Overlay a trendline over a chart of events by month.", "code_examples": [{"language": "spl", "code": "... | trendline sma5(foo) AS smoothed_foo ema10(bar)"}, {"language": "spl", "code": "index=\"bar\"| stats count BY date_month | trendline sma2(count) AS trend | fields * trend"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "trendline", "section_heading": "Examples", "section_id": "cbe512a9_1036_41c7_95d8_072b4865de76--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/trendline", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:38.562838+00:00", "version": "10.2"}}
{"id": "0009849126a8ed72", "content": "accum , autoregress , delta , streamstats", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "trendline", "section_heading": "See also", "section_id": "fdf84dab_66d7_495f_a0b8_50c835bd13e4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/trendline", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:38.562841+00:00", "version": "10.2"}}
{"id": "540ba632bb757e5d", "content": "The localize command generates results that represent a list of time contiguous event regions. An event region is a period of time in which consecutive events are separated, at most, by the maxpause time value. The regions found can be expanded using the timeafter and timebefore arguments. The regions discovered by the localize command are meant to be fed into the map command. The map command uses a different region for each iteration.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "localize", "section_heading": "Description", "section_id": "id_86d609ad_8ed2_429d_819a_0ebb47cfdb87--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/localize", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:55.061128+00:00", "version": "10.2"}}
{"id": "8ad15f246c0a1306", "content": "localize [<maxpause>] [<timeafter>] [<timebefore>] Optional arguments maxpause Syntax: maxpause=<int>(s|m|h|d) Description: Specify the maximum (inclusive) time between two consecutive events in a contiguous time region. Default: 1m timeafter Syntax: timeafter=<int>(s|m|h|d) Description: Specify the amount of time to add to the output endtime field (expand the time region forward in time). Default: 30s timebefore Syntax: timebefore=<int>(s|m|h|d) Description: Specify the amount of time to subtract from the output starttime field (expand the time region backwards in time). Default: 30s", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "localize", "section_heading": "Syntax", "section_id": "id_411dd569_95c5_4eb4_a7ab_9fbd7b33e1d0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/localize", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:55.061138+00:00", "version": "10.2"}}
{"id": "c9a5aa22c483c2be", "content": "Expanding event ranges You can expand the event range after the last event or before the first event in the region. These expansions are done arbitrarily, possibly causing overlaps in the regions if the values are larger than maxpause. Event region order The regions are returned in search order, or descending time for historical searches and data-arrival order for realtime search. The time of each region is the initial pre-expanded start-time. Other information returned by the localize command The localize command also reports: The number of events in the range The range duration in seconds The region density defined as the number of events in range divided by <range duration - events per second .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "localize", "section_heading": "Usage", "section_id": "id_2643c3e1_ad7b_4b3c_ac72_a9322bce50b2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/localize", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:55.061144+00:00", "version": "10.2"}}
{"id": "5d3f7025318e3a2d", "content": "1. Search the time range of each previous result for the term \"failure\" 2: Finds suitable regions around where \"error\" occurs Searching for \"error\" and calling the localize command finds suitable regions around where error occurs and passes each on to the search inside of the map command. Each iteration works with a specific time range to find potential transactions.", "code_examples": [{"language": "spl", "code": "... | localize maxpause=5m | map search=\"search failure starttimeu=$starttime$ endtimeu=$endtime$\""}, {"language": "spl", "code": "error | localize | map search=\"search starttimeu::$starttime$ endtimeu::$endtime$ | transaction uid,qid maxspan=1h\""}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "localize", "section_heading": "Examples", "section_id": "id_2a8419d6_e70b_41b5_ad68_2bfd72e1affe--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/localize", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:55.061150+00:00", "version": "10.2"}}
{"id": "162d9605bceb2c14", "content": "map , transaction", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "localize", "section_heading": "See also", "section_id": "id_1724735b_e2fb_4987_a77e_7d0c00b1f147--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/localize", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:02:55.061155+00:00", "version": "10.2"}}
{"id": "be3de922e78987dd", "content": "You can use the join command to combine the results of a main search (left-side dataset) with the results of either another dataset or a subsearch (right-side dataset). You can also combine a search result set to itself using the selfjoin command. The left-side dataset is the set of results from a search that is piped into the join command and then merged on the right side with the either a dataset or the results from a subsearch. The left-side dataset is sometimes referred to as the source data. The following search example joins the source data from the search pipeline with a subsearch on the right side. Rows from each dataset are merged into a single row if the where predicate is satisfied. A maximum of 50,000 rows in the right-side dataset can be joined with the left-side dataset over a maximum runtime of 60 seconds. These maximum defaults are set to limit the impact of the join command on performance and resource consumption. If you are familiar with SQL but new to SPL, see Splunk SPL for SQL users .", "code_examples": [{"language": "spl", "code": "<left-dataset> \n| join left=L right=RwhereL.pid = R.pid [subsearch]"}], "tables": [], "chunk_index": 0, "total_chunks": 7, "metadata": {"title": "join", "section_heading": "Description", "section_id": "e495684e_1c20_4642_a9dd_d3a7dbe8e3db--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/join", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:14.207801+00:00", "version": "10.2"}}
{"id": "2486bef78ee58216", "content": "For flexibility and performance, consider using one of the following commands if you do not require join semantics. These commands provide event grouping and correlations using time and geographic location, transactions, subsearches, field lookups, and joins. For information about when to use a join, see the flowchart in About event grouping and correlation in the Search Manual .", "code_examples": [], "tables": [{"headers": ["Command", "Use"], "rows": [["append", "To append the results of a subsearch to the results of your current search. The events from both result sets are retained.Use only with historical data. Theappendcommand does not produce correct results if used in a real-time search.If you useappendto combine the events, use astatscommand to group the events in a meaningful way.  You cannot use atransactioncommand after you use anappendcommand."], ["appendcols", "Appends the fields of thesubsearchresults with the input search result fields. The first subsearch result is merged with the first main result, the second subsearch result is merged with the second main result, and so on."], ["lookup", "Use when one of the result sets or source files remains static or rarely changes. For example, a file from an external system such as a CSV file.The lookup cannot be a subsearch."], ["search", "In the most simple scenarios, you might need to search only for sources using the OR operator and then use astatsortransactioncommand to perform the grouping operation on the events."], ["stats", "To group events by a field and perform a statistical function on the events. For example to determine the average duration of events by host name.To usestats, the field must have a unique identifier.To view the raw event data, use thetransactioncommand instead."], ["transaction", "Usetransactionin the following situations.To group events by using theevalcommand with a conditional expression, such asif,case, ormatch.To group events by using a recycled field value, such as an ID or IP address.To group events by using a pattern, such as a start or end time for the event.To break up groups larger than a certain duration. For example, when a transaction does not explicitly end with a message and you want to specify a maximum span of time after the start of the transaction.To display the raw event data for the grouped events."]]}], "chunk_index": 1, "total_chunks": 7, "metadata": {"title": "join", "section_heading": "Alternative commands", "section_id": "cc5854ac_31f5_4920_812c_d7ee72505f61--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/join", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:14.207813+00:00", "version": "10.2"}}
{"id": "b3a6abfa67004395", "content": "The required syntax is in bold. join [<join-options>...] [<field-list>] | [left=<left-alias>] [right=<right-alias>] where <left-alias>.<field>=<right-alias>.<field> [<left-alias>.<field>=<right-alias>.<field>]... <dataset-type>:<dataset-name> | <subsearch> Required arguments dataset-type Syntax: datamodel | savedsearch | inputlookup Description: The type of dataset that you want to use to join with the source data. The dataset must be a dataset that you created or are authorized to use. You can specify datamodel , savedsearch , or inputlookup. The dataset type must precede the dataset name. For example, savedsearch:<dataset-name>. You can use either <dataset-type>:<dataset-name> or <subsearch> with the join command, but not both. dataset-name Syntax: <dataset-name> Description: The name of the dataset that you want to use to join with the source data. The dataset must be a dataset that you created or are authorized to use. The dataset name must follow the dataset type. For example, if the dataset name is january and the dataset type is datamodel, you specify datamodel:january. You can use either <dataset-type>:<dataset-name> or <subsearch> with the join command, but not both. subsearch Syntax: [<subsearch>] Description: A secondary search or dataset that specifies the source of the events that you want to join to. The subsearch must be enclosed in square brackets. The results of the subsearch should not exceed available memory. You can use either <dataset-type>:<dataset-name> or <subsearch> in a search, but not both. When [<subsearch>] is used in a search by itself with no join keys, the Splunk software autodetects common fields and combines the search results before the join command with the results of the subsearch. Optional arguments join-options Syntax: type=(inner | outer | left) | usetime=<bool> | earlier=<bool> | overwrite=<bool> | max=<int> Description: Arguments to the join command. Use either outer or left to specify a left outer join. See Descriptions for the join-options argument in this topic. field-list Syntax: <field> <field> ... Description: Specify the list of fields to use for the join. For example, to join fields ProductA , ProductB , and ProductC , you would specify | join ProductA ProductB ProductC... If <field-list> is specified, one or more of the fields must be common to each dataset. If no fields are specified, all of the fields that are common to both datasets are used. The values of the fields used in <field-list> are case sensitive. For example, a value that is all uppercase in the main search will not match the same value that is all lowercase in the subsearch. See the example later in this topic about performing a case-insensitive join. left alias Syntax: left=<left-alias> Description: The alias to use with the left-side dataset, the source data, to avoid naming collisions. Must be combined with the right alias and where clause, or the alias is ignored. The left alias must be used together with the right alias. right alias Syntax: right=<right-alias> Description: The alias to use with the right-side dataset to avoid naming collisions. Must be combined with the left alias and the where clause, or the alias is ignored. The right alias must be used together with the left alias. where clause Syntax: where <left-alias>.<field>=<right-alias>.<field>... Description: Identifies the names of the fields in the left-side dataset and the right-side dataset that you want to join on. You must specify the left and right aliases and the field name. Fields that are joined from the left and right datasets do not have to have the same names. For example: where L.host=R.user matches events in the host field from the left dataset with events in the user field from the right dataset. The where clause must be used with the right and left aliases and field name. You can specify the aliases and fields in a where clause on either side of the equal sign. For example: where <left-alias>.<left-field>=<right-alias>.<right-field> or where <right-alias>.<right-field>=<left-alias>.<left-field> Descriptions for the join-options argument type Syntax: type=inner | outer | left Description: Indicates the type of join to perform. The difference between an inner and a left (or outer ) join is how the events are treated in the main search that do not match any of the events in the subsearch. In both inner and left joins, events that match are joined. The results of an inner join do not include events from the main search that have no matches in the subsearch. The results of a left (or outer ) join includes all of the events in the main search and only those values in the subsearch have matching field values. Default: inner usetime Syntax: usetime=<bool> Description: A Boolean value that Indicates whether to use time to limit the matches in the subsearch results. Used with the earlier option to limit the subsearch results to matches that are earlier or later than the main search results. If you use the join command with usetime=true and type=left , the search results are similar to those of an inner join. This is because there might be non-matching results when using the left join that are the same as those produced by an inner join. Default: false earlier Syntax: earlier=<bool> Description: If usetime=true and earlier=true , the main search results are matched only against earlier results from the subsearch. If earlier=false , the main search results are matched only against later results from the subsearch. Results that occur at the same time (second) are not eliminated by either value. Default: true overwrite Syntax: overwrite=<bool> Description: If fields in the main search results and subsearch results have the same name, indicates whether fields from the subsearch results overwrite the fields from the main search results. Default: true max Syntax: max=<int> Description: Specifies the maximum number of subsearch results that each main search result can join with. If set to max=0 , there is no limit. Default: 1", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 7, "metadata": {"title": "join", "section_heading": "Syntax", "section_id": "id_62219ef0_0a0e_42d5_99a8_ab6e040a90cb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/join", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:14.207818+00:00", "version": "10.2"}}
{"id": "312ee31cc3e51d78", "content": "The join command is a centralized streaming command when there is a defined set of fields to join to. Otherwise the command is a dataset processing command. See Command types. A subsearch can be initiated through a search command such as the join command. See Initiating subsearches with search commands in the Splunk Cloud Platform Search Manual. Limitations on subsearches in joins Use the join command when the results of the subsearch are relatively small, for example 50,000 rows or less. To minimize the impact of this command on performance and resource consumption, Splunk software imposes some default limitations on the subsearch. Limitations on the subsearch for the join command are specified in the limits.conf file. The default limitations include a maximum of 50,000 rows in the subsearch to join against, and a maximum search time of 60 seconds for the subsearch. See Subsearches in the Search Manual. Splunk Cloud Platform To change the limits.conf settings subsearch_maxout or subsearch_maxtime , use one of the following methods: The Configure limits page in Splunk Web. For more information, see Configure limits using Splunk Web in the Splunk Cloud Platform Admin Manual. The Admin Config Service (ACS) API. For more information, see Manage limits.conf configurations in Splunk Cloud Platform in the Splunk Cloud Platform Admin Config Service Manual. The Admin Config Service (ACS) command line interface (CLI). For more information, see Administer Splunk Cloud Platform using the ACS CLI in the Splunk Cloud Platform Admin Config Service Manual. Splunk Enterprise To change the subsearch_maxout or subsearch_maxtime settings in your limits.conf file for join command subsearches, follow these steps. Prerequisites Only users with file system access, such as system administrators, can edit configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps Open or create a local limits.conf file at $SPLUNK_HOME/etc/system/local on the search head. Under the [join] stanza, add the line subsearch_maxout = <value> or subsearch_maxtime = <value>. One-to-many and many-to-many relationships To return matches for one-to-many, many-to-one, or many-to-many relationships, include the max argument in your join syntax and set the value to 0. By default max=1 , which means that the subsearch returns only the first result from the subsearch. Setting the value to a higher number or to 0, which is unlimited, returns multiple results from the subsearch.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 7, "metadata": {"title": "join", "section_heading": "Usage", "section_id": "b6554bc9_40ad_4552_b118_9846af526889--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/join", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:14.207823+00:00", "version": "10.2"}}
{"id": "7a8256b0e1de9350", "content": "1. A basic join Combine the results from a main search with the results from a subsearch search vendors. The result sets are joined on the product_id field, which is common to both sources. 2. Returning all subsearch rows By default, only the first row of the subsearch that matches a row of the main search is returned. To return all of the matching subsearch rows, include the max=<int> argument and set the value to 0. This argument joins each matching subsearch row with the corresponding main search row. 3. Join datasets on fields that have the same name Combine the results from a search with the vendors dataset. The data is joined on the product_id field, which is common to both datasets. 4. Join datasets on fields that have different names Combine the results from a search with the vendors dataset. The data is joined on a product ID field, which have different field names in each dataset. The field in the left-side dataset is product_id. The field in the right-side dataset is pid. 5. Use words instead of letters as aliases You can use words for the aliases to help identify the datasets involved in the join. This example uses products and vendors for the aliases. 6. Perform a case-insensitive join Say you want to join a field with values that have prefixes that use both upper and lower case letters. But, the <field-list> argument for the join command is case sensitive. To work around this limitation, you can make the case consistent before and after you perform the join by using the lower() or upper() evaluation function. In this example, the value for the myfield field is converted to lower case, which makes the case consistent for the join command. See Evaluation functions .", "code_examples": [{"language": "spl", "code": "... | join product_id [search vendors]"}, {"language": "spl", "code": "... | join product_id max=0 [search vendors]"}, {"language": "spl", "code": "... | join left=L right=RwhereL.product_id=R.product_id [search vendors]"}, {"language": "spl", "code": "... | join left=L right=RwhereL.product_id=R.pid [search vendors]"}, {"language": "spl", "code": "... | join left=products right=vendorswhereproducts.product_id=vendors.pid [search vendors]"}, {"language": "spl", "code": "... |evalmyfield=lower(myfield) | join myfield [... |evalmyfield=lower(myfield)]"}], "tables": [], "chunk_index": 4, "total_chunks": 7, "metadata": {"title": "join", "section_heading": "Basic examples", "section_id": "id_558ae708_a5ad_4517_a3f3_8b53b3dea635--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/join", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:14.207829+00:00", "version": "10.2"}}
{"id": "08fd49a3f3f6bcbc", "content": "1. Specifying dataset aliases with a saved search dataset This example joins each matching right-side dataset row with the corresponding source data row. This example uses products , which is a savedsearch type of dataset, for the right-side dataset. The field names in the left-side dataset and the right-side dataset are different. This search returns all of the matching rows in the left and right datasets by including max=0 in the search. 2. Use aliasing with commands following the join Commands following the join can take advantage of the aliasing provided through the join command. For example, you can use the aliasing in another command like stats as shown in the following example. 3. Using a join to display resource usage information The dashboards and alerts in the distributed management console shows you performance information about your Splunk deployment. The Resource Usage: Instance dashboard contains a table that shows the machine, number of cores, physical memory capacity, operating system, and CPU architecture. To display the information in the table, use the following search. This search includes the join command. The search uses the information in the dmc_assets table to look up the instance name and machine name. The search then uses the serverName field to join the information with information from the /services/server/info REST endpoint. The /services/server/info is the URI path to the Splunk REST API endpoint that provides hardware and operating system information for the machine. The $splunk_server$ part of the search is a dashboard token variable.", "code_examples": [{"language": "spl", "code": "... | join max=0 left=L right=RwhereL.vendor_id=R.vid  savedsearch:products"}, {"language": "spl", "code": "... | join left=L right=RwhereL.product_id=R.pid [search vendors] | stats count by L.product_id"}, {"language": "spl", "code": "| inputlookup dmc_assets \n| search serverName = $splunk_server$ \n| stats first(serverName) AS serverName, first(host) AS host, first(machine) AS machine\n| jointype=left serverName \n   [ | rest splunk_server=$splunk_server$ /services/server/info\n   | fields serverName, numberOfCores, physicalMemoryMB, os_name, cpu_arch]\n| fields machine numberOfCores physicalMemoryMB os_name cpu_arch \n| rename machine AS Machine, numberOfCores AS\"Number of Cores\", \n  physicalMemoryMB AS\"Physical Memory Capacity (MB)\", os_name AS\"Operating System\", \n  cpu_arch AS\"CPU Architecture\""}], "tables": [], "chunk_index": 5, "total_chunks": 7, "metadata": {"title": "join", "section_heading": "Extended examples", "section_id": "bcbd0fd4_7129_47c6_aaec_5142794fb183--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/join", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:14.207833+00:00", "version": "10.2"}}
{"id": "90885c1b0d26b321", "content": "selfjoin , append , set , appendcols", "code_examples": [], "tables": [], "chunk_index": 6, "total_chunks": 7, "metadata": {"title": "join", "section_heading": "See also", "section_id": "id_6372f78f_8fe3_4b79_b092_2e0eba685f96--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/join", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:14.207837+00:00", "version": "10.2"}}
{"id": "fe02e16edced388d", "content": "Converts a single valued field into a multivalue field by splitting the values on a string delimiter or by using a regular expression. The delimiter can be a multicharacter delimiter. Note: The makemv command does not apply to internal fields. See Use default fields in the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "makemv", "section_heading": "Description", "section_id": "id_73789da4_8b1d_4f21_89cd_6aa05152c562--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makemv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:30.450117+00:00", "version": "10.2"}}
{"id": "2e593d0ddb8a7f47", "content": "makemv [delim=<string> | tokenizer=<string>] [allowempty=<bool>] [setsv=<bool>] <field> Required arguments field Syntax: <field> Description: The name of a field to generate the multivalues from. Optional arguments delim Syntax: delim=<string> Description: A string value used as a delimiter. Splits the values in field on every occurrence of this delimiter. Default: A single space (\" \"). tokenizer Syntax: tokenizer=<string> Description: A regular expression with a capturing group that is repeat-matched against the values in the field. For each match, the first capturing group is used as a value in the newly created multivalue field. allowempty Syntax: allowempty=<bool> Description: Specifies whether to permit empty string values in the multivalue field. When using delim=true , repeats of the delimiter string produce empty string values in the multivalue field. For example if delim=\",\" and field=\"a,,b\" , by default does not produce any value for the empty string. When using the tokenizer argument, zero length matches produce empty string values. By default they produce no values. Default: false setsv Syntax: setsv=<bool> Description: If true, the makemv command combines the decided values of the field into a single value, which is set on the same field. (The simultaneous existence of a multivalue and a single value for the same field is a problematic aspect of this flag.) Default: false", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "makemv", "section_heading": "Syntax", "section_id": "id_1f91ac92_fa5c_4f29_b104_dd14e506a105--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makemv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:30.450126+00:00", "version": "10.2"}}
{"id": "28f2e02112153b6e", "content": "The makemv command is a distributable streaming command. See Command types. You can use evaluation functions and statistical functions on multivalue fields or to return multivalue fields.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "makemv", "section_heading": "Usage", "section_id": "id_4d14c48c_cd0e_4207_9782_7db4df92e0e8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makemv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:30.450131+00:00", "version": "10.2"}}
{"id": "f01b6cd55862f545", "content": "1. Use a comma to separate field values For sendmail search results, separate the values of \"senders\" into multiple values. Display the top values. 2. Use a colon delimiter and allow empty values Separate the value of \"product_info\" into multiple values. 3. Use a regular expression to separate values The following search creates a result and adds three values to the my_multival field. The makemv command is used to separate the values in the field by using a regular expression.", "code_examples": [{"language": "spl", "code": "eventtype=\"sendmail\"| makemv delim=\",\"senders | top senders"}, {"language": "spl", "code": "... | makemv delim=\":\"allowempty=trueproduct_info"}, {"language": "spl", "code": "| makeresults\n|evalmy_multival=\"one,two,three\"| makemv tokenizer=\"([^,]+),?\"my_multival"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "makemv", "section_heading": "Examples", "section_id": "cb78c0ea_fbae_4ce0_a12a_8873d7cc7e2e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makemv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:30.450137+00:00", "version": "10.2"}}
{"id": "b9f0f13edbfa8c80", "content": "Commands: mvcombine mvexpand nomv Functions: Multivalue eval functions Multivalue stats and chart functions split", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "makemv", "section_heading": "See also", "section_id": "a581d01b_1485_471b_82ba_8b50f24f047c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makemv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:30.450141+00:00", "version": "10.2"}}
{"id": "7d915cb85651ad57", "content": "The sirare command is the summary indexing version of the rare command, which returns the least common values of a field or combination of fields. The sirare command populates a summary index with the statistics necessary to generate a rare report. After you populate the summary index, use the regular rare command with the exact same search string as the rare command search to report against it.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "sirare", "section_heading": "Description", "section_id": "id_01302f60_a829_4cb0_a27f_c2590ee98c4a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sirare", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:49.462593+00:00", "version": "10.2"}}
{"id": "686ef98ebf15e4c2", "content": "sirare [<top-options>...] <field-list> [<by-clause>] Required arguments <field-list> Syntax: <string>,... Description: Comma-delimited list of field names. Optional arguments <by-clause> Syntax: BY <field-list> Description: The name of one or more fields to group by. <top-options> Syntax: countfield=<string> | limit=<int> | percentfield=<string> | showcount=<bool> | showperc=<bool> Description: Options that specify the type and number of values to display. These are the same <top-options> used by the rare and top commands. Top options countfield Syntax: countfield=<string> Description: Name of a new field to write the value of count. Default: \"count\" limit Syntax: limit=<int> Description: Specifies how many tuples to return, \"0\" returns all values. percentfield Syntax: percentfield=<string> Description: Name of a new field to write the value of percentage. Default: \"percent\" showcount Syntax: showcount=<bool> Description: Specify whether to create a field called \"count\" (see \"countfield\" option) with the count of that tuple. Default: true showpercent Syntax: showpercent=<bool> Description: Specify whether to create a field called \"percent\" (see \"percentfield\" option) with the relative prevalence of that tuple. Default: true", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "sirare", "section_heading": "Syntax", "section_id": "id_38fb3ce5_0e96_4a31_b575_993a4e3f9a87--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sirare", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:49.462602+00:00", "version": "10.2"}}
{"id": "67f0f50c90a73b81", "content": "Example 1: Compute the necessary information to later do 'rare foo bar' on summary indexed results.", "code_examples": [{"language": "spl", "code": "... | sirare foo bar"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "sirare", "section_heading": "Examples", "section_id": "id_3fea5801_ccc4_4873_8e61_fe45ed993149--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sirare", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:49.462608+00:00", "version": "10.2"}}
{"id": "acff8ec81e46e11b", "content": "collect , overlap , sichart , sistats , sitimechart , sitop", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "sirare", "section_heading": "See also", "section_id": "id_27348736_8f21_4644_9d81_a4b61e830b61--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sirare", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:03:49.462613+00:00", "version": "10.2"}}
{"id": "853ffb0e032c8d49", "content": "Returns the number of events in specified indexes.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "eventcount", "section_heading": "Description", "section_id": "dfc3ef92_7141_4049_a23b_432117bc7971--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eventcount", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:06.761703+00:00", "version": "10.2"}}
{"id": "10f0ce513974dcc0", "content": "The required syntax is in bold. | eventcount [index=<string>]... [summarize=<bool>] [report_size=<bool>] [list_federated_remote=<bool>] [list_vix=<bool>] Required arguments None. Optional arguments index Syntax: index=<string> Description: The name of an index to report on, or a wildcard matching a set of indexes to report on. You can specify this argument multiple times to specify multiple indexes or groups of indexes, like this: index=* index=_*. Default: If no index is specified, eventcount returns information about the default index. list_federated_remote Syntax: list_federated_remote=<bool> Description: Specify whether to return event counts from specified indexes on any federated providers to which your Splunk platform deployment is connected for the purpose of running federated searches over remote Splunk platform deployments. If list_federated_remote=false , eventcount returns event counts only from your local Splunk platform deployment. See Usage. Default: false list_vix Syntax: list_vix=<bool> Description: Specify whether to list virtual indexes. If list_vix=false , the command does not list virtual indexes. Default: true report_size Syntax: report_size=<bool> Description: Specify whether to report the index size. If report_size=true , the command returns the index size in bytes. Default: false summarize Syntax: summarize=<bool> Description: Specifies whether or not to summarize events across all indexes, providers, and search peers (servers). If summarize=false , the command splits the event counts by index, and additionally provides the provider and server values that correspond to each index. Default: true", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "eventcount", "section_heading": "Syntax", "section_id": "a3e058db_461e_4528_b35e_b54f50486580--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eventcount", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:06.761712+00:00", "version": "10.2"}}
{"id": "82f68989f37667a1", "content": "The eventcount command is a report-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. Specifying a time range has no effect on the results returned by the eventcount command. All of the events on the indexes you specify are counted. Specifying indexes You cannot specify indexes to exclude from the results. For example, index!=foo is not valid syntax. You can specify the index argument multiple times. For example: See event counts for indexes on remote Splunk platform deployments If you use Federated Search for Splunk, you can find the count of events in specified indexes on your federated providers by running eventcount with summarize=false and list_federated_remote=true. When you set summarize=false and list_federated_remote=true , eventcount can return event counts for specified remote indexes on federated providers to which your Splunk platform deployment is connected. The provider column identifies the federated providers that each specified remote index is associated with. Indexes that are present on your local Splunk platform deployment have a platform value of local. Your local Splunk platform deployment is the Splunk platform deployment from which you run searches. If you set summarize=false and do not set list_federated_remote or set list_federated_remote=false , eventcount returns event counts only for indexes on your local Splunk platform deployment. See About Federated Search for Splunk , in Federated Search. Running in clustered environments Do not use the eventcount command to count events for comparison in indexer clustered environments. When a search runs, the eventcount command checks all buckets , including replicated and primary buckets, across all indexers in a cluster. As a result, the search may return inaccurate event counts.", "code_examples": [{"language": "spl", "code": "|eventcount summarize=falseindex=_audit index=main"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "eventcount", "section_heading": "Usage", "section_id": "id_5df56722_52ab_4e9d_801f_d532b6b0c0d3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eventcount", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:06.761720+00:00", "version": "10.2"}}
{"id": "e0c6eef29f8b6e45", "content": "Example 1: Display a count of the events in the default indexes from all of the search peers. A single count is returned. Example 2: Return the number of events in only the internal default indexes. Display the corresponding providers and servers. Include the index size, in bytes, in the results. The results appear on the Statistics tab and will be similar to the results shown in the following table. When you specify summarize=false , the command returns four fields: count , index , provider , and server. When you specify report_size=true , the command returns the size_bytes field. The values in the size_bytes field are not the same as the index size on disk. Example 3: For each specified index, return an event count and its corresponding provider and server values. Filter internal indexes out of the result set. The results appear on the Statistics tab and will be similar to the results shown in the following table. To return the count all of the indexes including the internal indexes, you must specify the internal indexes separately from the external indexes: Example 4: Return event counts for the internal indexes in your local Splunk platform deployment and the internal indexes in the remote Splunk platform deployment that is connected to your Splunk deployment as a standard mode federated provider. Filter out indexes that are not internal. Because this search runs over a standard mode federated provider, you use the federated: syntax to specify the indexes on the federated provider. The results appear on the Statistics tab and will be similar to the results shown in the following table. The search returns event counts for two access_combined indexes and two access_combined_wcookie indexes, but they are not duplicates. Your local Splunk platform deployment has indexes that share names with indexes on its remote federated provider, which is expected. See Run federated searches over remote Splunk platform deployments , in Federated Search .", "code_examples": [{"language": "spl", "code": "| eventcount"}, {"language": "spl", "code": "| eventcount summarize=falseindex=_* report_size=true"}, {"language": "spl", "code": "| eventcount summarize=falseindex=*"}, {"language": "spl", "code": "| eventcount summarize=falseindex=* index=_*"}, {"language": "spl", "code": "|eventcount summarize=f list_federated_remote=t index=access_* index=federated:access_*"}], "tables": [{"headers": ["count", "index", "provider", "server", "size_bytes"], "rows": [["52550", "_audit", "local", "buttercup-mbpr15.sv.splunk.com", "7217152"], ["1423010", "_internal", "local", "buttercup-mbpr15.sv.splunk.com", "122138624"], ["22626", "_introspection", "local", "buttercup-mbpr15.sv.splunk.com", "98619392"], ["10", "_telemetry", "local", "buttercup-mbpr15.sv.splunk.com", "135168"], ["0", "_thefishbucket", "local", "buttercup-mbpr15.sv.splunk.com", "0"]]}, {"headers": ["count", "index", "provider", "server"], "rows": [["0", "history", "local", "sting-mba13.sv.splunk.com"], ["109864", "main", "local", "sting-mba13.sv.splunk.com"], ["0", "summary", "local", "sting-mba13.sv.splunk.com"], ["6906", "usgs_earthquake", "local", "sting-mba13.sv.splunk.com"]]}, {"headers": ["count", "index", "provider", "server"], "rows": [["5015002", "access_combined", "local", "sting-mba13.sv.splunk.com"], ["4994000", "access_combined", "remote01", "buttercup-mbpr15.sv.splunk.com"], ["4921285", "access_combined_wcookie", "local", "sting-mba13.sv.splunk.com"], ["4741874", "access_combined_wcookie", "remote01", "buttercup-mbpr15.sv.splunk.com"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "eventcount", "section_heading": "Examples", "section_id": "id_90198564_5db2_4d46_9f18_4dc311462aa5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eventcount", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:06.761742+00:00", "version": "10.2"}}
{"id": "eaf6c123950b8986", "content": "metadata , fieldsummary", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "eventcount", "section_heading": "See also", "section_id": "e72a9a90_271c_4652_bbec_4fe917888a46--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/eventcount", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:06.761749+00:00", "version": "10.2"}}
{"id": "8232d21f37d699c1", "content": "The bucket command is an alias for the bin command. See the bin command for syntax information and examples.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "d3e92ffcb1d3f44789fe40c429d32d4cc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/bucket", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:23.414622+00:00", "version": "10.2"}}
{"id": "8f4f371c6285d7bd", "content": "Use the gauge command to transform your search results into a format that can be used with the gauge charts. Gauge charts are a visualization of a single aggregated metric, such as a count or a sum. The output of the gauge command is a single numerical value stored in a field called x. You can specify a range to display in the gauge or use the default range of 0 to 100. For more information about using the gauge command with the gauge chart types, see Using gauges in the Gauges section in Dashboards and Visualizations .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "gauge", "section_heading": "Description", "section_id": "id_77df264b_0838_4662_80fa_209d9a3608fd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/gauge", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:39.211337+00:00", "version": "10.2"}}
{"id": "e6ddeff553046a93", "content": "gauge <value> [<range_val1> <range_val2> ...] Required arguments value Syntax: field_name | <num> Description: A numeric field or literal number to use as the current value of the gauge. If you specify a numeric field, the gauge command uses the first value in that field as the value for the gauge. Optional arguments range values Syntax: <range_val1> <range_val2> ... Description: A space-separated list of two or more numeric fields or numbers to use as the overall numeric range displayed in the gauge. Each range value can be a numeric field name or a literal number. If you specify a field name, the first value in that field is used as the range value. The total range of the gauge is from the first range_val to the last range_val. See Usage. Default range: 0 to 100", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "gauge", "section_heading": "Syntax", "section_id": "a43e2848_9515_4ee0_ac19_c795c0403acd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/gauge", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:39.211347+00:00", "version": "10.2"}}
{"id": "23428cf17720ab0a", "content": "You can create gauge charts without using the gauge command as long as your search results in a single value. The advantage of using the gauge command is that you can specify a set of range values instead of using the default range values of 0 to 100. Specifying ranges If you specify range values, you must specify at least two values. The gauge begins at the first value and ends at the last value that you specify. If you specify more than two range_val arguments, the intermediate range values are used to split the total range into subranges. Each subrange displays in different color, which creates a visual distinction. The range values are returned as a series of fields called y1 , y2 , and so on. If you do not specify range values, the range defaults to a low value of 0 and a high value of 100. If a single range value is specified, it is ignored. Gauge colors With a gauge chart, a single numerical value is mapped against a set of colors. These colors can have particular business meaning or business logic. As the value changes over time, the gauge marker changes position within this range. The color ranges in the gauge chart are based on the range values that you specify with the gauge command. When you specify range values, you define the overall numerical range represented by the gauge. You can define the size of the colored bands within that range. If you want to use the color bands, add four range values to the search string. These range values indicate the beginning and end of the range. These range values also indicate the relative sizes of the color bands within this range.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "gauge", "section_heading": "Usage", "section_id": "c5ed11db_4863_4532_9ba5_b6ca361d1426--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/gauge", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:39.211353+00:00", "version": "10.2"}}
{"id": "7bed6eb1140181bf", "content": "1. Create a gauge with multiple ranges Count the number of events and display the count on a gauge with four ranges, from 0-750, 750-1000, 1000-1250, and 1250-1500. Start by generating the results table using this search. Run the search using the Last 15 minutes time range. The results appear on the Statistics tab and look something like this: Click on the Visualizations tab. There are three types of gauges that you can choose from: radial, filler, and marker. The following image shows the radial gauge that is created based on the search results. For more information about using the gauge command with the gauge chart type, see the Gauges section in Dashboard and Visualizations .", "code_examples": [{"language": "spl", "code": "index=_internal | stats count as myCount | gauge myCount 750 1000 1250 1500"}], "tables": [{"headers": ["x", "y1", "y2", "y3", "y4"], "rows": [["3321", "750", "1000", "1250", "1500"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "gauge", "section_heading": "Examples", "section_id": "id_448a6c4c_87bf_44c3_82dc_095c25c53d1a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/gauge", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:39.211364+00:00", "version": "10.2"}}
{"id": "8df378a8d4f8031f", "content": "Commands eval stats", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "gauge", "section_heading": "See also", "section_id": "e33e2db3_447f_4ea7_9471_06b96ec1cd8a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/gauge", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:39.211369+00:00", "version": "10.2"}}
{"id": "742201ecf7c6112f", "content": "Finds the most common values for the fields in the field list. Calculates a count and a percentage of the frequency the values occur in the events. If the <by-clause> is included, the results are grouped by the field you specify in the <by-clause>.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "top", "section_heading": "Description", "section_id": "id_6de266a3_2120_4c8d_98cc_640b6957b9c9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/top", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:53.600731+00:00", "version": "10.2"}}
{"id": "1110159a6c9fcf43", "content": "top [<N>] [<top-options>...] <field-list> [<by-clause>] Required arguments <field-list> Syntax: <field>, <field>, ... Description: Comma-delimited list of field names. Optional arguments <N> Syntax: <int> Description: The number of results to return. Default: 10 <top-options> Syntax: countfield=<string> | limit=<int> | otherstr=<string> | percentfield=<string> | showcount=<bool> | showperc=<bool> | useother=<bool> Description: Options for the top command. See Top options. <by-clause> Syntax: BY <field-list> Description: The name of one or more fields to group by. Top options countfield Syntax: countfield=<string> Description: For each value returned by the top command, the results also return a count of the events that have that value. This argument specifies the name of the field that contains the count. The count is returned by default. If you do not want to return the count of events, specify showcount=false. Default: count limit Syntax: limit=<int> Description: Specifies how many results to return. To return all values, specify zero ( 0 ). Specifying top limit=<int> is the same as specifying top N. Default: 10 otherstr Syntax: otherstr=<string> Description: If useother=true , a row representing all other values is added to the results. Use otherstr=<string> to specify the name of the label for the row. Default: OTHER percentfield Syntax: percentfield=<string> Description: For each value returned by the top command, the results also return a percentage of the events that have that value. This argument specifies the name of the field that contains the percentage. The percentage is returned by default. If you do not want to return the percentage of events, specify showperc=false. Default: percent showcount Syntax: showcount=<bool> Description: Specify whether to create a field called \"count\" (see \"countfield\" option) with the count of that tuple. Default: true showperc Syntax: showperc=<bool> Description: Specify whether to create a field called \"percent\" (see \"percentfield\" option) with the relative prevalence of that tuple. Default: true useother Syntax: useother=<bool> Description: Specify whether or not to add a row that represents all values not included due to the limit cutoff. Default : false", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "top", "section_heading": "Syntax", "section_id": "id_222377e7_0f29_47c4_a6c7_57b6dedcfc51--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/top", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:53.600738+00:00", "version": "10.2"}}
{"id": "b84ec2a0d52cced8", "content": "The top command is a transforming command. See Command types. Default fields When you use the top command, two fields are added to the results: count and percent. Default maximum number of results By default the top command returns a maximum of 50,000 results. This maximum is controlled by the maxresultrows setting in the [top] stanza in the limits.conf file. Increasing this limit can result in more memory usage. Note: Only users with file system access, such as system administrators, can edit the configuration files. Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make the changes in the local directory. See How to edit a configuration file. If you have Splunk Cloud Platform, you need to file a Support ticket to change this limit. Lexicographic order of results In searches that use the limit option with multiple sets of field lists, only the last lexicographical value of the <field-list> is returned in the search results. For example, in the following search, Orlando is the only location field that is returned because it's the last value when sorted lexicographically. The search results look something like this.", "code_examples": [{"language": "spl", "code": "| makeresults\n|evallocation=\"Orlando Dallas Atlanta\"| makemv location\n| mvexpand location\n|evaluser=\"Alex Kai Morgan\"| makemv user\n| mvexpand user\n| toplimit=1 location by user"}], "tables": [{"headers": ["Field", "Description"], "rows": [["count", "The number of events in your search results that contain the field values that are returned by the top command. See thecountfieldandshowcountarguments."], ["percent", "The percentage of events in your search results that contain the field values that are returned by the top command. See thepercentfieldandshowpercarguments."]]}, {"headers": ["user", "location", "count", "percent"], "rows": [["Alex", "Orlando", "1", "33.333333"], ["Kai", "Orlando", "1", "33.333333"], ["Morgan", "Orlando", "1", "33.333333"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "top", "section_heading": "Usage", "section_id": "a5e7ef0c_b4c6_4f9a_8fbb_f97c2836771e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/top", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:53.600747+00:00", "version": "10.2"}}
{"id": "66bfa9c789a57b21", "content": "Example 1: Return the 20 most common values for a field This search returns the 20 most common values of the \"referer\" field. The results show the number of events (count) that have that a count of referer, and the percent that each referer is of the total number of events. Example 2: Return top values for one field organized by another field This search returns the top \"action\" values for each \"referer_domain\". Because a limit is not specified, this returns all the combinations of values for \"action\" and \"referer_domain\" as well as the counts and percentages: Example 3: Returns the top product purchased for each category This search returns the top product purchased for each category. Do not show the percent field. Rename the count field to \"total\".", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | toplimit=20 referer"}, {"language": "spl", "code": "sourcetype=access_* | top action by referer_domain"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | top 1 productName by categoryId showperc=f countfield=total"}], "tables": [{"headers": [], "rows": [["This example uses the sample dataset fromthe Search Tutorialand a field lookup to add more information to the event data.Download the data set fromAdd data tutorialand follow the instructions to load the tutorial data.Download the CSV file fromUse field lookups tutorialand follow the instructions to set up the lookup definition to add price and productName to the events.After you configure the field lookup, you can run this search using the time range,All time."]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "top", "section_heading": "Examples", "section_id": "id_76a6f216_2189_48c6_949a_a882f78e7daa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/top", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:53.600753+00:00", "version": "10.2"}}
{"id": "b88a5d7e6f4ab1d8", "content": "rare , sitop , stats", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "top", "section_heading": "See also", "section_id": "ad49e6f1_2dc9_4ca8_822e_197b16c65b9d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/top", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:04:53.600757+00:00", "version": "10.2"}}
{"id": "99c32761e4947294", "content": "Use the lookup command to invoke field value lookups. For information about the types of lookups you can define, see About lookups in the Knowledge Manager Manual. The lookup command supports IPv4 and IPv6 addresses and subnets that use CIDR notation.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 7, "metadata": {"title": "lookup", "section_heading": "Description", "section_id": "c8cd0727_2d18_4ceb_94eb_7a4330eb490c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/lookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:10.035307+00:00", "version": "10.2"}}
{"id": "46d8c59300178d62", "content": "The required syntax is in bold. lookup [local=<bool>] [update=<bool>] <lookup-table-name> ( <lookup-field> [AS <event-field>] )... [ OUTPUT | OUTPUTNEW (<lookup-destfield> [AS <event-destfield>] )... ] Note: The lookup command can accept multiple lookup and event fields and destfields. For example: Required arguments <lookup-table-name> Syntax: <string> Description: Can be either the name of a CSV file that you want to use as the lookup, or the name of a stanza in the transforms.conf file that specifies the location of the lookup table file. Optional arguments local Syntax: local=<bool> Description: If local=true , forces the lookup to run on the search head and not on any remote peers. Default: false update Syntax: update=<bool> Description: If the lookup table is modified on disk while the search is running, real-time searches do not automatically reflect the update. To do this, specify update=true. This does not apply to searches that are not real-time searches. This implies that local=true. Default: false <lookup-field> Syntax: <string> Description: Refers to a field in the lookup table to match against the events. You can specify multiple <lookup-field> values. <event-field> Syntax: <string> Description: Refers to a field in the events from which to acquire the value to match in the lookup table. You can specify multiple <event-field> values. Default: The value of the <lookup-field>. <lookup-destfield> Syntax: <string> Description: Refers to a field in the lookup table to be copied into the events. You can specify multiple <lookup-destfield> values. <event-destfield> Syntax: <string> Description: A field in the events. You can specify multiple <event-destfield> values. Default: The value of the <lookup-destfield> argument.", "code_examples": [{"language": "spl", "code": "...| lookup <lookup-table-name> <lookup-field1> AS <event-field1>, <lookup-field2> AS <event-field2> OUTPUTNEW <lookup-destfield1> AS <event-destfield1>, <lookup-destfield2> AS <event-destfield2>"}], "tables": [], "chunk_index": 1, "total_chunks": 7, "metadata": {"title": "lookup", "section_heading": "Syntax", "section_id": "a9f63386_2166_439d_ac08_97bb277079c4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/lookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:10.035317+00:00", "version": "10.2"}}
{"id": "54118a9082329f4b", "content": "The lookup command is a distributable streaming command when local=false , which is the default setting. See Command types. When using the lookup command, if an OUTPUT or OUTPUTNEW clause is not specified, all of the fields in the lookup table that are not the match fields are used as output fields. If the OUTPUT clause is specified, the output lookup fields overwrite existing fields. If the OUTPUTNEW clause is specified, the lookup is not performed for events in which the output fields already exist. Avoid lookup reference cycles When you set up the OUTPUT or OUTPUTNEW clause for your lookup, avoid accidentally creating lookup reference cycles, where you intentionally or accidentally reuse the same field names among the match fields and the output fields of a lookup search. For example, if you run a lookup search where type is both the match field and the output field, you are creating a lookup reference cycle. You can accidentally create a lookup reference cycle when you fail to specify an OUTPUT or OUTPUTNEW clause for lookup. For more information about lookup reference cycles see Define an automatic lookup in Splunk Web in the Knowledge Manager Manual. Optimizing your lookup search If you are using the lookup command in the same pipeline as a transforming command , and it is possible to retain the field you will lookup on after the transforming command, do the lookup after the transforming command. For example, run: and not: The lookup in the first search is faster because it only needs to match the results of the stats command and not all the Web access events.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | stats count by status | lookup status_desc status OUTPUT description"}, {"language": "spl", "code": "sourcetype=access_* | lookup status_desc status OUTPUT description | stats count by description"}], "tables": [], "chunk_index": 2, "total_chunks": 7, "metadata": {"title": "lookup", "section_heading": "Usage", "section_id": "id_0b4ef851_8dc3_4494_8dc4_54d6c335f028--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/lookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:10.035324+00:00", "version": "10.2"}}
{"id": "d9c4e9f6b79248a7", "content": "If you are running federated searches over standard mode Splunk platform federated providers , and you want to use the lookup command to enrich the results of a federated search, see Run federated searches over lookups in Federated Search. For an overview of federated search for Splunk, see About Federated Search for Splunk in Federated Search. Configure a lookup to run on the local federated search head In a standard mode federated search, you can force a lookup command to be processed locally on the federated search head by applying local=true to it. If you do not set local=true , Splunk software will optimize processing of the lookup command on the federated search head and the remote search head depending on the specific conditions of the search. If the lookup definition and lookup tables expected by the lookup are not present on the search heads on which it is processed, Splunk Web displays an error message when the search runs. See Manage knowledge objects for standard mode federated providers in Federated Search .", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 7, "metadata": {"title": "lookup", "section_heading": "Run lookup in federated searches", "section_id": "id_79bfc793_d23e_4077_b6ba_cb70ae14517d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/lookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:10.035328+00:00", "version": "10.2"}}
{"id": "8dcf6f2c5e82bdd3", "content": "1. Lookup users and return the corresponding group the user belongs to Suppose you have a lookup table specified in a stanza named usertogroup in the transforms.conf file. This lookup table contains (at least) two fields, user and group. Your events contain a field called local_user. For each event, the following search checks to see if the value in the field local_user has a corresponding value in the user field in the lookup table. For any entries that match, the value of the group field in the lookup table is written to the field user_group in the event.", "code_examples": [{"language": "spl", "code": "... | lookup usertogroup user as local_user OUTPUT group as user_group"}], "tables": [], "chunk_index": 4, "total_chunks": 7, "metadata": {"title": "lookup", "section_heading": "Basic example", "section_id": "df24d9fb_53e8_4ff2_8c10_41b380bc1bd3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/lookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:10.035333+00:00", "version": "10.2"}}
{"id": "25dbc4b88089fa7d", "content": "1. Lookup price and vendor information and return the count for each product sold by a vendor This example calculates the count of each product sold by each vendor. The prices.csv file contains the product names, price, and code. For example: The vendors.csv file contains vendor information, such as vendor name, city, and ID. For example: The search will query the vendor_sales.log file, which is part of the tutorialdata.zip file. The vendor_sales.log file contains the VendorID, Code, and AcctID fields. For example: The following search calculates the count of each product sold by each vendor and uses the time range All time. The stats command calculates the count by Code and VendorID. The lookup command uses the prices_lookup to match the Code field in each event and return the product names. The search results are displayed on displayed on the Statistics tab. You can extend the search to display more information about the vendor by using the vendors_lookup. Use the table command to return only the fields that you need. In this example you want the product_name , VendorID , and count fields. Use the vendors_lookup file to output all the fields in the vendors.csv file that match the VendorID in each event. The revised search results are displayed on the Statistics tab. To expand the search to display the results on a map, see the geostats command. 2. IPv6 CIDR match in Splunk Web In this example, CSV lookups are used to determine whether a specified IPv6 address is in a CIDR subnet. You can follow along with the example by performing these steps in Splunk Web. See Define a CSV lookup in Splunk Web. Prerequisites Your role must have the upload_lookup_files capability to upload lookup table files in Splunk Web. See Define roles with capabilities in Splunk Enterprise \"Securing the Splunk Platform\". A CSV lookup table file called ipv6test.csv that contains the following text. ip,expected 2001:0db8:ffff:ffff:ffff:ffff:ffff:ff00/120,true The ip field in the lookup table contains the subnet value, not the IP address. Steps You have to define a CSV lookup before you can match an IP address to a subnet. Select Settings > Lookups to go to the Lookups manager page. Click Add new next to Lookup table files. Select a Destination app from the drop-down list. Click Choose File to look for the ipv6test.csv file to upload. Enter ipv6test.csv as the destination filename. This is the name the lookup table file will have on the Splunk server. Click Save. In the Lookup table list, click Permissions in the Sharing column of the ipv6test lookup you want to share. In the Permissions dialog box, under Object should appear in , select All apps to share globally. If you want the lookup to be specific to this app only, select This app only. Click Save. Select Settings > Lookups. Click Add new next to Lookup definitions. Select a Destination app from the drop-down list. Give your lookup definition a unique Name , like ipv6test. Select File-based as the lookup Type. Select ipv6test.csv as the Lookup file from the drop-down list. Select the Advanced options check box. Enter a Match type of CIDR(ip). Click Save. In the Lookup definitions list, click Permissions in the Sharing column of the ipv6test lookup definition you want to share. In the Permissions dialog box, under Object should appear in , select All apps to share globally. If you want the lookup to be specific to this app only, select This app only. Note: Permissions for lookup table files must be at the same level or higher than those of the lookup definitions that use those files. Click Save. In the Search app, run the following search to match the IP address to the subnet. The IP address is in the subnet, so the search displays true in the expected field. The search results look something like this.", "code_examples": [{"language": "spl", "code": "sourcetype=vendor_* | stats count by Code VendorID | lookup prices_lookup Code OUTPUTNEW product_name"}, {"language": "spl", "code": "sourcetype=vendor_* | stats  count by Code VendorID | lookup prices_lookup Code OUTPUTNEW product_name | table product_name VendorID count | lookup vendors_lookup VendorID"}, {"language": "spl", "code": "| makeresults \n|evalip=\"2001:0db8:ffff:ffff:ffff:ffff:ffff:ff99\"| lookup ipv6test ip OUTPUT expected"}], "tables": [{"headers": [], "rows": [["This example uses the tutorialdata.zip file from the Search Tutorial. You can download this file and add it to your Splunk deployment. Seeupload the tutorial data. Additionally, this example uses theprices.csvand thevendors.csvfiles. To follow along with this example in your Splunk deployment, download these CSV files and complete the steps in theUse field lookupssection of the Search Tutorial for both theprices.csvand thevendors.csvfiles. When you create the lookup definition for thevendors.csvfile, name the lookupvendors_lookup. You can skip the step in the tutorial that makes the lookups automatic."]]}, {"headers": ["productId", "product_name", "price", "sale_price", "Code"], "rows": [["DB-SG-G01", "Mediocre Kingdoms", "24.99", "19.99", "A"], ["DC-SG-G02", "Dream Crusher", "39.99", "24.99", "B"], ["FS-SG-G03", "Final Sequel", "24.99", "16.99", "C"], ["WC-SH-G04", "World of Cheese", "24.99", "19.99", "D"]]}, {"headers": ["Vendor", "VendorCity", "VendorID", "VendorLatitude", "VendorLongitude", "Vendor StateProvince", "Vendor Country", "Weight"], "rows": [["Anchorage Gaming", "Anchorage", "1001", "61.17440033", "-149.9960022", "Alaska", "United States", "3"], ["Games of Salt Lake", "Salt Lake City", "1002", "40.78839874", "-111.9779968", "Utah", "United States", "3"], ["New Jack Games", "New York", "1003", "40.63980103", "-73.77890015", "New York", "United States", "4"], ["Seals Gaming", "San Francisco", "1004", "37.61899948", "-122.375", "California", "United States", "5"]]}, {"headers": ["Entries in the vendor_sales.log file"], "rows": [["[13/Mar/2018:18:24:02] VendorID=5036 Code=B AcctID=6024298300471575"], ["[13/Mar/2018:18:23:46] VendorID=7026 Code=C AcctID=8702194102896748"], ["[13/Mar/2018:18:23:31] VendorID=1043 Code=B AcctID=2063718909897951"], ["[13/Mar/2018:18:22:59] VendorID=1243 Code=F AcctID=8768831614147676"]]}, {"headers": ["time", "expected", "ip"], "rows": [["2020-11-19 16:43:31", "true", "2001:0db8:ffff:ffff:ffff:ffff:ffff:ff99"]]}], "chunk_index": 5, "total_chunks": 7, "metadata": {"title": "lookup", "section_heading": "Extended example", "section_id": "a78a65b4_4677_401c_820b_63450414c6d3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/lookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:10.035353+00:00", "version": "10.2"}}
{"id": "fbdb9fcd9dc220c3", "content": "Commands appendcols inputlookup outputlookup iplocation search Functions cidrmatch Related information About lookups in the Knowledge Manager Manual", "code_examples": [], "tables": [], "chunk_index": 6, "total_chunks": 7, "metadata": {"title": "lookup", "section_heading": "See also", "section_id": "e2d7ca5c_c889_486b_bd6c_5fcdffa85097--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/lookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:10.035358+00:00", "version": "10.2"}}
{"id": "00f014e37656872d", "content": "Prepares your events for calculating the autoregression, or the moving average , by copying one or more of the previous values for field into each event. The first few events will lack the augmentation of prior values, since the prior values do not exist.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "autoregress", "section_heading": "Description", "section_id": "d3271b27_0312_4e29_9c23_d8ee5fcd0234--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/autoregress", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:25.353113+00:00", "version": "10.2"}}
{"id": "2ad91f54006f71bf", "content": "autoregress <field> [AS <newfield>] [ p=<int> | p=<int>-<int> ] Required arguments field Syntax: <string> Description: The name of a field. Most usefully a field with numeric values. Optional arguments p Syntax: p=<int> | p=<int>-<int> Description: Specifies which prior events to copy values from. You can specify a single integer or a numeric range. For a single value, such as 3, the autoregress command copies field values from the third prior event into a new field. For a range, the autoregress command copies field values from the range of prior events. For example, if you specify a range such as p=2-4 , then the field values from the second, third, and fourth prior events are copied into new fields. Default: 1 newfield Syntax: <field> Description: If p is set to a single integer, the newfield argument specifies a field name to copy the single field value into. Invalid if p is set to a range. If the newfield argument is not specified, the single or multiple values are copied into fields with the names <field>_p<num>. For example, if p=2-4 and field=count , the field names are count_p2, count_p3, count_p4.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "autoregress", "section_heading": "Syntax", "section_id": "id_503c4987_bde2_4c53_b61d_af02f9b0ef1f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/autoregress", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:25.353121+00:00", "version": "10.2"}}
{"id": "970b75ee4117458b", "content": "The autoregress command is a centralized streaming command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "autoregress", "section_heading": "Usage", "section_id": "cbf5d255_57b2_47c1_af0b_b89aa2d28ae0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/autoregress", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:25.353127+00:00", "version": "10.2"}}
{"id": "d2abcf5ce8731af0", "content": "Example 1: For each event, copy the 3rd previous value of the 'ip' field into the field 'old_ip'. Example 2: For each event, copy the 2nd, 3rd, 4th, and 5th previous values of the 'count' field. Since the new field argument is not specified, the values are copied into the fields 'count_p2', 'count_p3', 'count_p4', and 'count_p5'. Example 3: Calculate a moving average of event size over the current event and the four prior events. This search omits the moving_average for the initial events, where the field would be wrong, because summing null fields is considered null.", "code_examples": [{"language": "spl", "code": "... | autoregress ip AS old_ip p=3"}, {"language": "spl", "code": "... | autoregress count p=2-5"}, {"language": "spl", "code": "... |evalrawlen=len(_raw) | autoregress rawlen p=1-4 |evalmoving_average=(rawlen + rawlen_p1 + rawlen_p2 + rawlen_p3 +rawlen_p4 ) /5"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "autoregress", "section_heading": "Examples", "section_id": "id_55bcf687_7ba8_4b47_ad15_712d1448494a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/autoregress", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:25.353132+00:00", "version": "10.2"}}
{"id": "c53c604c28e631d8", "content": "accum , delta , streamstats , trendline", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "autoregress", "section_heading": "See also", "section_id": "id_29887fab_8b8e_4321_a8ec_6209c9b4cde5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/autoregress", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:25.353136+00:00", "version": "10.2"}}
{"id": "89eb15567f6efb8f", "content": "Runs a saved search, or report, and returns the search results of a saved search. If the search contains replacement placeholder terms, such as $replace_me$ , the search processor replaces the placeholders with the strings you specify. For example:", "code_examples": [{"language": "spl", "code": "|savedsearch mysearch replace_me=\"value\""}], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "savedsearch", "section_heading": "Description", "section_id": "e3810edf_ca07_4161_ad4d_65008f569a8f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/savedsearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:43.632388+00:00", "version": "10.2"}}
{"id": "79d1e92c5d2266fb", "content": "| savedsearch <savedsearch_name> [<savedsearch-options>...] Required arguments savedsearch_name Syntax: <string> Description: Name of the saved search to run. Optional arguments savedsearch-options Syntax: <substitution-control> | <replacement> Description: Specify whether substitutions are allowed. If allowed, specify the key-value pair to use in the string substitution replacement. substitution-control Syntax: nosubstitution=<bool> Description: If true, no string substitution replacements are made. Default: false replacement Syntax: <field>=<string> Description: A key-value pair to use in string substitution replacement.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "savedsearch", "section_heading": "Syntax", "section_id": "id_921b19ad_390a_4997_9196_6bacc31496ef--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/savedsearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:43.632398+00:00", "version": "10.2"}}
{"id": "b5aa08bfdf7d1267", "content": "The savedsearch command is a generating command and must start with a leading pipe character. The savedsearch command always runs a new search. To reanimate the results of a previously run search, use the loadjob command. When the savedsearch command runs a saved search, the command always applies the permissions associated with the role of the person running the savedsearch command to the search. The savedsearch command never applies the permissions associated with the role of the person who created and owns the search to the search. This happens even when a saved search has been set up to run as the report owner. See Determine whether to run reports as the report owner or user in the Reporting Manual. Time ranges If you specify All Time in the time range picker, the savedsearch command uses the time range that was saved with the saved search. If you specify any other time in the time range picker, the time range that you specify overrides the time range that was saved with the saved search. In standard mode federated searches over remote Splunk platform deployments If you use Federated Search for Splunk to run searches over datasets on remote Splunk platform deployments, you can use the savedsearch command to run federated searches over saved search datasets on standard mode federated providers. See Run federated searches over remote Splunk platform deployments in Federated Search. If you use savedsearch to run a federated search over a remote saved search dataset, you can use the command's string substitution replacement syntax to replace certain strings in the remote saved search with strings of your design, if the remote saved search string contains replacement placeholder terms. You can also use the nosubstitution argument to block string replacements in the remote saved search. For example, say you have a federated index named remote1_ss_index_df_1. This federated index maps to a saved search dataset on your remote standard mode federated provider that is based on a saved search with a replacement placeholder term for the value of the sourcetype field. You can run the following federated search to insert a sourcetype value of universal_data_json into that remote saved search. Note: If you use savedsearch to reference a saved search dataset that requires a string substitution and you do not provide a replacement string, Splunk software will return an \"Error while replacing variable name\" error message.", "code_examples": [{"language": "spl", "code": "index=index_df_1 sourcetype=$replace_me$"}, {"language": "spl", "code": "| savedsearch federated:remote1_ss_index_df_1 replace_me=\"universal_data_json\""}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "savedsearch", "section_heading": "Usage", "section_id": "id_71706b9b_6d1b_47a8_8f29_39157fcd33c3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/savedsearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:43.632404+00:00", "version": "10.2"}}
{"id": "f16cb3003c831df9", "content": "Example 1 Run the saved search \"mysecurityquery\". Example2 Run the saved search \"mysearch\". Where the replacement placeholder term $replace_me$ appears in the saved search, use \"value\" instead.", "code_examples": [{"language": "spl", "code": "| savedsearch mysecurityquery"}, {"language": "spl", "code": "|savedsearch mysearch replace_me=\"value\"..."}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "savedsearch", "section_heading": "Examples", "section_id": "fd2578bb_b703_4451_b8e8_fc4dcb228e32--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/savedsearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:43.632409+00:00", "version": "10.2"}}
{"id": "11a19f4336541ba0", "content": "search , loadjob", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "savedsearch", "section_heading": "See also", "section_id": "id_9b2b7a8a_45de_4447_b92a_11860b9a8b67--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/savedsearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:05:43.632414+00:00", "version": "10.2"}}
{"id": "705bc6dda6905554", "content": "Replaces field values in your search results with the values that you specify. Does not replace values in fields generated by stats or eval functions. If you do not specify a field, the value is replaced in all non-generated fields.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "replace", "section_heading": "Description", "section_id": "c1c29f96_34eb_4f23_9490_546421416f0e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/replace", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:00.356277+00:00", "version": "10.2"}}
{"id": "c2f64fa93fa0eeba", "content": "replace (<wc-string> WITH <wc-string>)... [IN <field-list>] Required arguments wc-string Syntax: <string> Description: Specify one or more field values and their replacements. You can use wildcard characters to match one or multiple terms. Optional arguments field-list Syntax: <string> ... Description: Specify a comma or space delimited list of one or more field names for the field value replacements. To replace values on _internal fields, you must specify the field name with the IN <fieldname> clause.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "replace", "section_heading": "Syntax", "section_id": "d378869a_601c_4fe5_8058_df31d7c69f79--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/replace", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:00.356286+00:00", "version": "10.2"}}
{"id": "f532f87d6a44fae0", "content": "The replace command is a distributable streaming command. See Command types. Non-wildcard replacement values specified later take precedence over those replacements specified earlier. For a wildcard replacement, fuller matches take precedence over lesser matches. To assure precedence relationships, you are advised to split the replace into two separate invocations. When using wildcard replacements, the result must have the same number of wildcards, or none at all. Wildcards ( * ) can be used to specify many values to replace, or replace values with.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "replace", "section_heading": "Usage", "section_id": "id_865d5e34_c5ba_4084_a695_97ca7ecd3333--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/replace", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:00.356290+00:00", "version": "10.2"}}
{"id": "adfb5d032040a3e3", "content": "1. Replace a value in all fields Change any host value that ends with \"localhost\" to simply \"localhost\" in all fields. 2. Replace a value in a specific field Replace an IP address with a more descriptive name in the host field. 3. Change the value of two fields Replaces the values in the start_month and end_month fields. You can separate the names in the field list with spaces or commas. 4. Change the order of values in a field In the host field, change the order of string values that contain the word localhost so that the string \"localhost\" precedes the other strings. 5. Replace multiple values in a field Replace the values in a field with more descriptive names. Separate the value replacements with comma. 6. Replace empty strings Search for an error message and replace empty strings with a whitespace. Note: This example will not work unless you have values that are actually the empty string, which is not the same as not having a value. 7: Replace values in an internal field Replace values of the internal field _time .", "code_examples": [{"language": "spl", "code": "... | replace *localhost WITH localhost"}, {"language": "spl", "code": "... | replace 127.0.0.1 WITH localhost IN host"}, {"language": "spl", "code": "... | replace aug WITH August IN start_month end_month"}, {"language": "spl", "code": "... | replace\"* localhost\"WITH\"localhost *\"IN host"}, {"language": "spl", "code": "... | replace 0 WITH Critical, 1 WITH Error IN msg_level"}, {"language": "spl", "code": "\"Error exporting to XYZÂ :\"| rex\"Error exporting to XYZ:(?.*)\"| replace\"\"WITH\" \"IN errmsg"}, {"language": "spl", "code": "sourcetype=* | head 5 |eval_time=\"XYZ\"| stats count BY _time | replace *XYZ* WITH *ALL* IN _time"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "replace", "section_heading": "Examples", "section_id": "de91b6ed_0909_439a_89b3_d88521060c8d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/replace", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:00.356297+00:00", "version": "10.2"}}
{"id": "8d4ef064684d9e5f", "content": "Commands rename", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "replace", "section_heading": "See also", "section_id": "id_8f722623_db8d_4f4d_98bc_e03faebf1e8a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/replace", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:00.356301+00:00", "version": "10.2"}}
{"id": "05871627c8fc22b4", "content": "Appends the fields of the subsearch results with the input search results. All fields of the subsearch are combined into the current results, with the exception of internal fields. For example, the first subsearch result is merged with the first main result, the second subsearch result is merged with the second main result, and so on.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "appendcols", "section_heading": "Description", "section_id": "e702b222_226a_46af_bc18_3574255d297a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/appendcols", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:15.663519+00:00", "version": "10.2"}}
{"id": "9b213976137bc182", "content": "appendcols [override= <bool> | <subsearch-options>...] <subsearch> Required arguments subsearch Description: A secondary search added to the main search. See how subsearches work in the Search Manual. Optional arguments override Syntax: override=<bool> Description: If the override argument is false, and if a field is present in both a subsearch result and the main result, the main result is used. If override=true , the subsearch result value is used. Default: override=false subsearch-options Syntax: maxtime=<int> | maxout=<int> | timeout=<int> Description: These options control how the subsearch is executed. Subsearch options maxtime Syntax: maxtime=<int> Description: The maximum time, in units of seconds, to spend on the subsearch before automatically finalizing. Default : 60 maxout Syntax: maxout=<int> Description: The maximum number of result rows to output from the subsearch. Default: 50000 timeout Syntax: timeout=<int> Description: The maximum time, in units of seconds, to wait for subsearch to fully finish. Default: 60", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "appendcols", "section_heading": "Syntax", "section_id": "a128e2c1_fe20_4964_ac51_83cc766f586e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/appendcols", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:15.663526+00:00", "version": "10.2"}}
{"id": "f04a074299ba2613", "content": "The appendcols command must be placed in a search string after a transforming command such as stats , chart , or timechart. The appendcols command can't be used before a transforming command because it must append to an existing set of table-formatted results, such as those generated by a transforming command. See Command types. Note that the subsearch argument to the appendcols command doesn't have to contain a transforming command.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "appendcols", "section_heading": "Usage", "section_id": "id_2d7ae7ad_baab_408f_875b_abd5a859687f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/appendcols", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:15.663531+00:00", "version": "10.2"}}
{"id": "17abef7b67db0daa", "content": "Example 1. Search for \"404\" events and append the fields in each event to the previous search results. This is a valid search string because appendcols comes after the transforming command table and adds columns to an existing table of results. Example 2. This search uses appendcols to count the number of times a certain field occurs on a specific server and uses that value to calculate other fields. First, this search uses stats to count the number of individual users on a specific server and names that variable \"totalUsers\". Then, this search uses appendcols to search the server and count how many times a certain field occurs on that specific server. This count is renamed \"VariableA\". The eval command is used to define a \"variableB\". The result is a table with the fields totalUsers , variableA , and variableB .", "code_examples": [{"language": "spl", "code": "index=_internal \n| table host \n| appendcols \n    [ search 404]"}, {"language": "spl", "code": "specific.server \n| stats dc(userID) as totalUsers \n| appendcols \n    [ search specific.server AND\"text\"| stats count(<field>) as variableA ] \n|evalvariableB = exact(variableA/totalUsers)"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "appendcols", "section_heading": "Examples", "section_id": "cd2277a1_2e64_466c_bc3c_ba78aee9d597--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/appendcols", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:15.663536+00:00", "version": "10.2"}}
{"id": "13c7704ed06a16ac", "content": "append , appendpipe , join , set", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "appendcols", "section_heading": "See also", "section_id": "id_61143213_9416_4bc8_b06f_92e420f9f7bc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/appendcols", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:15.663540+00:00", "version": "10.2"}}
{"id": "e89531c85517a606", "content": "The gentimes command is useful in conjunction with the map command. Generates timestamp results starting with the exact time specified as start time. Each result describes an adjacent, non-overlapping time range as indicated by the increment value. This terminates when enough results are generated to pass the endtime value. The gentimes command generates events up to the end time, but not including the end time.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "gentimes", "section_heading": "Description", "section_id": "f970009e_ecda_4a20_b3e7_5483d01a9b2f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/gentimes", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:31.007700+00:00", "version": "10.2"}}
{"id": "51cdab391e9a2b58", "content": "| gentimes start=<timestamp> [end=<timestamp>] [increment=<increment>] Required arguments start Syntax: start=<timestamp> Description: Specify as start time. <timestamp> Syntax: MM/DD/YYYY[:HH:MM:SS] | <int> Description: Indicate the timeframe, using either a timestamp or an integer value. For example: 10/1/2020 for October 1, 2020, 4/1/2021:12:34:56 for April 1, 2021 at 12:34:56, or -5 for five days ago. Optional arguments end Syntax: end=<timestamp> Description: Specify an end time. Default: midnight, prior to the current time in local time increment Syntax: increment=<int>(s | m | h | d) Description: Specify a time period to increment from the start time to the end time. Supported increments are seconds, minutes, hours, and days. Default: 1d", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "gentimes", "section_heading": "Syntax", "section_id": "id_5432d5ca_059a_47ad_acef_0a3af4890dd6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/gentimes", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:31.007708+00:00", "version": "10.2"}}
{"id": "ea7ad79d3d68bc91", "content": "The gentimes command is an event-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. The gentimes command returns four fields. To specify future dates, you must include the end argument.", "code_examples": [], "tables": [{"headers": ["Field", "Description"], "rows": [["starttime", "The starting time range in UNIX time."], ["starthuman", "The human readable time range in the format DDD MMM DD HH:MM:SS YYYY. For example Sun Apr 4 00:00:00 2021."], ["endtime", "The ending time range in UNIX time."], ["endhuman", "The human readable time range in the format DDD MMM DD HH:MM:SS YYYY. For example Fri Apr 16 23:59:59 2021."]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "gentimes", "section_heading": "Usage", "section_id": "df4edb2c_047a_4067_81f4_d4c7d299dec1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/gentimes", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:31.007714+00:00", "version": "10.2"}}
{"id": "3e99e33b1a8dc949", "content": "1. Generate daily time ranges by specifying dates Generates daily time ranges from April 4 to April 7 in 2021. This search generates events up to the end time, but not including the end time. This search generates three intervals covering one day periods aligning with the calendar days April 4, 5, and 6, during 2021. The gentimes command generates events up to the end time, but not including the end time. The results look like this: 2. Generate daily time ranges by specifying relative times Generate daily time ranges from 30 days ago until 27 days ago. 3. Generate hourly time ranges Generate hourly time ranges from December 1 to December 5 in 2021. 4. Generate time ranges by only specifying a start date Generate daily time ranges from April 25 to today. 5. Generate weekly time ranges Although the week increment is not supported, you can generate a weekly increment by specifying increment=7d. This examples generates weekly time ranges from December 1, 2021 to April 30, 2022.", "code_examples": [{"language": "spl", "code": "| gentimes start=4/4/21 end=4/7/21"}, {"language": "spl", "code": "| gentimes start=-30 end=-27"}, {"language": "spl", "code": "| gentimes start=12/1/21 end=12/5/21 increment=1h"}, {"language": "spl", "code": "| gentimes start=4/25/22"}, {"language": "spl", "code": "| gentimes start=12/1/21 end=4/30/22 increment=7d"}], "tables": [{"headers": ["endhuman", "endtime", "starthuman", "starttime"], "rows": [["Sun Apr 4 23:59:59 2021", "1617605999", "Sun Apr 4 00:00:00 2021", "1617519600"], ["Mon Apr 5 23:59:59 2021", "1617692399", "Mon Apr 5 00:00:00 2021", "1617606000"], ["Tue Apr 6 23:59:59 2021", "1617778799", "Tue Apr 6 00:00:00 2021", "1617692400"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "gentimes", "section_heading": "Examples", "section_id": "id_8a1a43cc_2772_4f53_bcf5_fff520016b4b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/gentimes", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:31.007721+00:00", "version": "10.2"}}
{"id": "9ef69901a3c78ddd", "content": "Commands makeresults map", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "gentimes", "section_heading": "See also", "section_id": "bc65b4e3_e267_4a35_be3c_b040e79a60b6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/gentimes", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:31.007725+00:00", "version": "10.2"}}
{"id": "d5703e0de1c8e742", "content": "The sort command sorts all of the results by the specified fields. Results missing a given field are treated as having the smallest or largest possible value of that field if the order is descending or ascending, respectively. If the first argument to the sort command is a number, then at most that many results are returned, in order. If no number is specified, the default limit of 10000 is used. If the number 0 is specified, all of the results are returned. See the count argument for more information.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "sort", "section_heading": "Description", "section_id": "id_09fa0dd2_f484_4fc7_acab_2477556738d1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sort", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:45.240373+00:00", "version": "10.2"}}
{"id": "f57a0e1df040a496", "content": "The required syntax is in bold. sort [<count>] <sort-by-clause>... [desc] Required arguments <sort-by-clause> Syntax: [ - | + ] <sort-field>, ( - | + ) <sort-field> ... Description: List of fields to sort by and the sort order. Use a minus sign (-) for descending order and a plus sign (+) for ascending order. When specifying more than one field, separate the field names with commas. See Sort field options. Optional arguments <count> Syntax: <int> | limit=<int> Description: Specify the number of results to return from the sorted results. If no count is specified, the default limit of 10000 is used. If 0 is specified, all results are returned. You can specify the count using an integer or precede the count with a label, for example limit=10. CAUTION: Using sort 0 might have a negative impact performance, depending on how many results are returned. Default: 10000 desc Syntax: d | desc Description: Reverses the order of the results. If multiple fields are specified, reverses the order of the values in the fields in the order in which the fields are specified. For example, if there are three fields specified, the desc argument reverses the order of the values in the first field. For each set of duplicate values in the first field, reverses the order of the corresponding values in the second field. For each set of duplicate values in the second field, reverses the order of the corresponding values in the third field. Sort field options <sort-field> Syntax: <field> | auto(<field>) | str(<field>) | ip(<field>) | num(<field>) Description: Options you can specify with <sort-field>. <field> Syntax: <string> Description: The name of field to sort. auto Syntax: auto(<field>) Description: Determine automatically how to sort the values of the field. ip Syntax: ip(<field>) Description: Interpret the values of the field as IP addresses. num Syntax: num(<field>) Description: Interpret the values of the field as numbers. str Syntax: str(<field>) Description: Interpret the values of the field as strings and order the values alphabetically.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "sort", "section_heading": "Syntax", "section_id": "id_207a7667_6e9b_47df_a0c8_3335085bb7af--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sort", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:45.240381+00:00", "version": "10.2"}}
{"id": "c5217321102bc5f6", "content": "The sort command is a dataset processing command. See Command types. By default, sort tries to automatically determine what it is sorting. If the field contains numeric values, the collating sequence is numeric. If the field contains on IP address values, the collating sequence is for IP addresses. Otherwise, the collating sequence is in lexicographical order. Some specific examples are: Alphabetic strings are sorted lexicographically. Punctuation strings are sorted lexicographically. Numeric data is sorted as you would expect for numbers and the sort order is specified as ascending or descending. Alphanumeric strings are sorted based on the data type of the first character. If the string starts with a number, the string is sorted numerically based on that number alone. Otherwise, strings are sorted lexicographically. Strings that are a combination of alphanumeric and punctuation characters are sorted the same way as alphanumeric strings. The sort order is determined between each pair of values that are compared at any one time. This means that for some pairs of values, the order might be lexicographical, while for other pairs the order might be numerical. Lexicographical order Lexicographical order sorts items based on the values used to encode the items in computer memory. In Splunk software, this is almost always UTF-8 encoding, which is a superset of ASCII. Numbers are sorted before letters. Numbers are sorted based on the first digit. For example, the numbers 10, 9, 70, 100 are sorted lexicographically as 10, 100, 70, 9. Uppercase letters are sorted before lowercase letters. Symbols are not standard. Some symbols are sorted before numeric values. Other symbols are sorted before or after letters. Custom sort order You can specify a custom sort order that overrides the lexicographical order. See the blog Order Up! Custom Sort Orders .", "code_examples": [], "tables": [{"headers": ["Results in descending order", "Description"], "rows": [["10.19.1", "This set of values are sorted numerically because the values are all numeric."], ["9.1.a10.1.a", "This set of values are sorted lexicographically because the values are alphanumeric strings."]]}], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "sort", "section_heading": "Usage", "section_id": "id_47e45f66_f255_4a2a_a640_2f731413302d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sort", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:45.240388+00:00", "version": "10.2"}}
{"id": "f7b0d6b28d6bd0d5", "content": "1. Use the sort field options to specify field types Sort the results by the ipaddress field in ascending order and then sort by the url field in descending order. 2. Specifying the number of results to sort Sort first 100 results in descending order of the \"size\" field and then by the \"source\" value in ascending order. This example specifies the type of data in each of the fields. The \"size\" field contains numbers and the \"source\" field contains strings. 3. Specifying descending and ascending sort orders Sort results by the \"_time\" field in ascending order and then by the \"host\" value in descending order. 4. Changing the time format of events for sorting Change the format of the event's time and sort the results in descending order by the Time field that is created with the eval command. (Thanks to Splunk user Ayn for this example.) 5. Return the most recent event Return the most recent event: 6. Use a label with the <count> You can use a label to identify the number of results to return: Return the first 12 results, sorted by the \"host\" field in descending order.", "code_examples": [{"language": "spl", "code": "... | sort ip(ipaddress), -str(url)"}, {"language": "spl", "code": "... | sort 100 -num(size), +str(source)"}, {"language": "spl", "code": "... | sort _time, -host"}, {"language": "spl", "code": "... | bin _time span=60m |evalTime=strftime(_time,\"%m/%dÂ %H:%MÂ %Z\") | stats avg(time_taken) AS AverageResponseTime BY Time | sort - Time"}, {"language": "spl", "code": "... | sort 1 -_time"}, {"language": "spl", "code": "... | sortlimit=12 host"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "sort", "section_heading": "Basic examples", "section_id": "a39428ca_284b_4d2e_90a3_b85c4c653c47--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sort", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:45.240395+00:00", "version": "10.2"}}
{"id": "2df9f25c3a155db9", "content": "1. Specify a custom sort order Sort a table of results in a specific order, such as days of the week or months of the year, that is not lexicographical or numeric. For example, suppose you have a search that produces the following table: Sorting on the day field (Day) returns a table sorted alphabetically, which does not make much sense. Instead, you want to sort the table by the day of the week, Monday to Friday, with the Weekend at the end of the list. To create a custom sort order, you first need to create a field called sort_field that defines the order. Then you can sort on that field. This search uses the eval command to create the sort_field and the fields command to remove sort_field from the final results table. The results look something like this: (Thanks to Splunk users Ant1D and Ziegfried for this example.) For additional custom sort order examples, see the blog Order Up! Custom Sort Orders and the Extended example in the rangemap command.", "code_examples": [{"language": "spl", "code": "... |evalwd=lower(Day) |evalsort_field=case(wd==\"monday\",1, wd==\"tuesday\",2, wd==\"wednesday\",3, wd==\"thursday\",4, wd==\"friday\",5, wd==\"weekend\",6) \n| sort sort_field \n| fields - sort_field"}], "tables": [{"headers": ["Day", "Total"], "rows": [["Friday", "120"], ["Monday", "93"], ["Tuesday", "124"], ["Thursday", "356"], ["Weekend", "1022"], ["Wednesday", "248"]]}, {"headers": ["Day", "Total"], "rows": [["Monday", "93"], ["Tuesday", "124"], ["Wednesday", "248"], ["Thursday", "356"], ["Friday", "120"], ["Weekend", "1022"]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "sort", "section_heading": "Extended example", "section_id": "cbfdb6b5_9c88_4dc1_9f61_87460fb4ae2a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sort", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:45.240404+00:00", "version": "10.2"}}
{"id": "bdadda7edb55c6c7", "content": "reverse", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "sort", "section_heading": "See also", "section_id": "id_9b266b60_54c2_481d_bbb9_62c6917ea472--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sort", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:06:45.240409+00:00", "version": "10.2"}}
{"id": "f5257ab6e897b4a4", "content": "Use the erex command to extract data from a field when you do not know the regular expression to use. The command automatically extracts field values that are similar to the example values you specify. The values extracted from the fromfield argument are saved to the field. The search also returns a regular expression that you can then use with the rex command to extract the field.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "erex", "section_heading": "Description", "section_id": "b131a02f_60f0_4b64_b97b_471f64061e28--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/erex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:01.470037+00:00", "version": "10.2"}}
{"id": "de782adaea083cc1", "content": "The required syntax is in bold. erex [<field>] examples=<string> [counterexamples=<string>] [fromfield=<field>] [maxtrainers=<integer>] Required arguments examples Syntax: examples=<string>,<string>... Description: A comma-separated list of example values for the information to extract and save into a new field. Use quotation marks around the list if the list contains spaces. For example: \"port 3351, port 3768\". field Syntax: <string> Description: A name for a new field that will take the values extracted from the fromfield argument. The resulting regular expression is generated and placed as a message under the Jobs menu in Splunk Web. That regular expression can then be used with the rex command for more efficient extraction. Optional arguments counterexamples Syntax: counterexamples=<string>,<string>,... Description: A comma-separated list of example values that represent information not to be extracted. fromfield Syntax: fromfield=<field> Description: The name of the existing field to extract the information from and save into a new field. Default: _raw maxtrainers Syntax: maxtrainers=<int> Description: The maximum number values to learn from. Must be between 1 and 1000. Default: 100", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "erex", "section_heading": "Syntax", "section_id": "id_7e51a88c_7a2d_4073_b585_6a377d429a25--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/erex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:01.470045+00:00", "version": "10.2"}}
{"id": "c8de1e2dc8b98e93", "content": "The values specified in the examples and counterexample arguments must exist in the events that are piped into the erex command. If the values do not exist, the command fails. To make sure that the erex command works against your events, first run the search that returns the events you want without the erex command. Then copy the field values that you want to extract and use those for the example values with the Click the Job menu to see the generated regular expression based on your examples. After you run a search or open a report in Splunk Web, the erex command returns informational log messages that are displayed in the search jobs manager window. However, these messages aren't displayed if the infocsv_log_level setting is set to WARN or ERROR. If you do not see the informational log messages when you click Jobs from the Activity menu, make sure that infocsv_log_level is set to the default, which is INFO. Splunk Cloud Platform To change the infocsv_log_level setting, request help from Splunk Support. If you have a support contract, file a new case using the Splunk Support Portal at Support and Services. Otherwise, contact Splunk Customer Support. Splunk Enterprise To change the the infocsv_log_level setting in the limits.conf file, follow these steps. Prerequisites Only users with file system access, such as system administrators, can edit configuration files. Review the steps in How to edit a configuration file in the Splunk Enterprise Admin Manual. CAUTION: Never change or copy the configuration files in the default directory. The files in the default directory must remain intact and in their original location. Make changes to the files in the local directory. Steps Open or create a local limits.conf file at $SPLUNK_HOME/etc/system/local. Under the [search_info] stanza, change the value for the infocsv_log_level setting. View the regular expression You can see the regular expression that is generated based on the erex command by clicking the Job menu in Splunk Web. See Example 3. The output of the erex command is captured in the search.log file. You can see the output by searching for \"Successfully learned regex\". The search.log file is located in the $SPLUNK_HOME/var/run/splunk/dispatch/ directory. The search logs are not indexed by default. See Dispatch directory and search artifacts in the Search Manual .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "erex", "section_heading": "Usage", "section_id": "id_9c281e33_6c3d_484f_8417_1f0cf4711937--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/erex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:01.470048+00:00", "version": "10.2"}}
{"id": "90555f4b2df44094", "content": "1. Extract values based on an example The following search extracts out month and day values like 7/01 and puts the values into the monthday attribute. 2. Extract values based on examples and counter examples The following search extracts out month and day values like 7/01 and 7/02 , but not patterns like 99/2. The extracted values are put into the monthday attribute. 3. Extract values based on examples and return the most common values Determine which are the most common ports used by potential attackers. Run a search to find examples of the port values, where there was a failed login attempt. Then use the erex command to extract the port field. You must specify several examples with the erex command. Use the top command to return the most common port values. By default the top command returns the top 10 values. This search returns a table with the count of top ports that match the search. The results appear on the Statistics tab and look something like this: Click the Job menu to see the generated regular expression based on your examples. You can use the rex command with the regular expression instead of using the erex command. The regular expression for this search example is | rex (?i)^(?:[^\\.]*\\.){3}\\d+\\s+(?P<port>\\w+\\s+\\d+) for this search example. You can replace the erex command with the rex command and generated regular expression in your search. For example: Using the rex command with a regular expression is more cost effective than using the erex command.", "code_examples": [{"language": "spl", "code": "... | erex monthday examples=\"7/01\""}, {"language": "spl", "code": "... | erex monthday examples=\"7/01, 07/02\"counterexamples=\"99/2\""}, {"language": "spl", "code": "sourcetype=secure* port\"failed password\""}, {"language": "spl", "code": "sourcetype=secure* port\"failed password\"| erex port examples=\"port 3351, port 3768\"| top port"}, {"language": "spl", "code": "sourcetype=secure* port\"failed password\"| rex (?i)^(?:[^\\.]*\\.){3}\\d+\\s+(?P<port>\\w+\\s+\\d+) | top port"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["port", "count", "percent"], "rows": [["port 2444", "20", "0.060145"], ["port 3281", "19", "0.057138"], ["port 2842", "19", "0.057138"], ["port 2760", "19", "0.057138"], ["port 1174", "19", "0.057138"], ["port 4955", "18", "0.054130"], ["port 1613", "18", "0.054130"], ["port 1059", "18", "0.054130"], ["port 4542", "17", "0.051123"], ["port 4519", "17", "0.051123"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "erex", "section_heading": "Examples", "section_id": "e1aad9fb_6029_43f4_b39f_673efa1f7d1d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/erex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:01.470057+00:00", "version": "10.2"}}
{"id": "76702e874caad125", "content": "Commands extract kvform multikv regex rex xmlkv", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "erex", "section_heading": "See also", "section_id": "bb4a39ca_b1d2_46ee_b2ac_6da1503a47f4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/erex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:01.470061+00:00", "version": "10.2"}}
{"id": "7eb3d60d4338cc13", "content": "Computes the difference between nearby results using the value of a specific numeric field. For each event where <field> is a number, the delta command computes the difference, in search order, between the <field> value for the current event and the <field> value for the previous event. The delta command writes this difference into <newfield>.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "delta", "section_heading": "Description", "section_id": "bd81252f_a321_4bd7_976e_8c9445a98eaa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/delta", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:17.417444+00:00", "version": "10.2"}}
{"id": "1a94186fc77571d2", "content": "The required syntax is in bold. delta <field> [AS <newfield>] [p=int] Required arguments field Syntax: <field-name> Description: The name of a field to analyze. If <field> is not a numeric field, no output field is generated. Optional arguments newfield Syntax: <string> Description: The name of a new field to write the output to. Default: delta(<field>) p Syntax: p=<int> Description: Specifies how many results prior to the current result to use for the comparison to the value in field in the current result. The prior results are determined by the search order, which is not necessarily chronological order. If p=1 , compares the current result value against the value in the first result prior to the current result. If p=2 , compares the current result value against the value in the result that is two results prior to the current result, and so on. Default: 1", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "delta", "section_heading": "Syntax", "section_id": "id_80f753ba_d752_4ca6_a41f_61f8d912ea48--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/delta", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:17.417452+00:00", "version": "10.2"}}
{"id": "3f6640817affe5e8", "content": "The delta command works on the events in the order they are returned by search. By default, the events for historical searches are in reverse time order from new events to old events. Values ascending over time show negative deltas. For real-time search, the events are compared in the order they are received. The delta can be applied after any sequence of commands, so there is no input order guaranteed. For example, if you sort your results by an independent field and then use the delta command, the produced values are the deltas in that specific order.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "delta", "section_heading": "Usage", "section_id": "id_4d7211a2_1d79_42fe_b72a_f6574300c3a3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/delta", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:17.417458+00:00", "version": "10.2"}}
{"id": "9ecd7fdc6f02f586", "content": "1. Calculate the difference in activity With the logs from a cable TV provider, sourcetype=tv , you can analyze broadcasting ratings, customer preferences, and so on. Which channels do subscribers watch the most, activity=view , and how long do the subscribers stay on those channels? 2. Calculate the difference between that current value and the 3rd previous value Compute the difference between current value of count and the 3rd previous value of count and store the result in the default field, delta( fieldname ), which in this example is delta(count). 3. Calculate the difference between that current value and the previous value and rename the result field For each event where 'count' exists, compute the difference between count and its previous value and store the result in the field countdiff .", "code_examples": [{"language": "spl", "code": "sourcetype=tv activity=\"View\"| sort - _time | delta _time AS timeDeltaS |evaltimeDeltaS=abs(timeDeltaS) | stats sum(timeDeltaS) by ChannelName"}, {"language": "spl", "code": "... | delta count p=3"}, {"language": "spl", "code": "... | delta count AS countdiff"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "delta", "section_heading": "Basic examples", "section_id": "id_54afeadb_7139_4e0f_ad10_9ded48e5dadd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/delta", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:17.417464+00:00", "version": "10.2"}}
{"id": "cc4edc4e570e9c1a", "content": "1. Calculate the difference in the number of purchases between the top 10 buyers Find the top ten people who bought something yesterday, count how many purchases they made and the difference in the number of purchases between each buyer. The purchase events, action=purchase , are piped into the top command to find the top ten users, based on clientip , who bought something. These results, which include a count for each clientip are then piped into the delta command to calculate the difference between the count value of one event and the count value of the event preceding it, using the p=1 argument. By default, this difference is saved in a new field called delta(count). The first event does not have a delta(count) value. The results look something like this: 2. Calculate the difference in time between recent events Calculate the difference in time between each of the recent earthquakes in Alaska. Run the search using the time range All time. This example searches for earthquakes in Alaska. The delta command is used to calculate the difference in the timestamps, _time , between each earthquake and the one immediately before it. By default the difference is placed in a new field called delta(_time). The time is in seconds. The rename command is used to change the default field name to timeDeltaS. An eval command is used with the abs function to convert the time into the absolute value of the time. This conversion is necessary because the differences between one earthquake and the earthquake immediately before it result in negative values. Another eval command is used with the tostring function to convert the time, in seconds, into a string value. The duration argument is part of the tostring function that specifies to convert the value to a readable time format HH:MM:SS. The results look something like this: 3. Calculate the difference in time between consecutive transactions Calculate the difference in time between consecutive transactions. This example groups events into transactions if they have the same values of JSESSIONID and clientip. The beginning of a transaction is defined by an event that contains the string view. The end of a transaction is defined by an event that contains the string purchase. The keywords view and purchase correspond to the values of the action field. You might also notice other values for the action field, such as addtocart and remove. The transactions are then piped into the delta command, which uses the _time field to calculate the time between one transaction and the transaction immediately preceding it. Specifically the difference between the timestamp for the last event in the transaction and the timestamp in the last event in the previous transaction. The search renames the time change as timeDelta. An eval command is used with the abs function to convert the time into the absolute value of the time. This conversion is necessary because the differences between one transaction and the previous transaction it result in negative values. Another eval command is used with the tostring function to convert the time, in seconds, into a string value. The duration argument is part of the tostring function that specifies to convert the value to a readable time format HH:MM:SS.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | top clientip | delta count p=1"}, {"language": "spl", "code": "source=all_month.csv place=*alaska* | delta _time p=1  | rename delta(_time) AS timeDeltaS |evaltimeDeltaS=abs(timeDeltaS) |eval\"Time Between Quakes\"=tostring(timeDeltaS,\"duration\") | table place, _time,\"Time Between Quakes\""}, {"language": "spl", "code": "sourcetype=access_* | transaction JSESSIONID clientip startswith=\"view\"endswith=\"purchase\"| delta _time AS timeDelta p=1 |evaltimeDelta=abs(timeDelta) |evaltimeDelta=tostring(timeDelta,\"duration\")"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}, {"headers": ["clientip", "count", "percent", "delta(count)"], "rows": [["87.194.216.51", "134", "2.565084", ""], ["128.241.220.82", "95", "1.818530", "-39"], ["211.166.11.101", "91", "1.741960", "-4"], ["107.3.146.207", "72", "1.378254", "-19"], ["194.215.205.19", "60", "1.148545", "-12"], ["109.169.32.135", "60", "1.148545", "0"], ["188.138.40.166", "56", "1.071975", "-4"], ["74.53.23.135", "49", "0.937979", "-7"], ["187.231.45.62", "48", "0.918836", "-1"], ["91.208.184.24", "46", "0.880551", "-2"]]}, {"headers": [], "rows": [["This example uses recent earthquake data downloaded from theUSGS Earthquakes website. The data is a comma separated ASCII text file that contains magnitude (mag), coordinates (latitude, longitude), region (place), etc., for each earthquake recorded.You can download a current CSV file from theUSGS Earthquake Feedsand add it as an input."]]}, {"headers": ["place", "_time", "Time Between Quakes"], "rows": [["32km N of Anchor Point, Alaska", "2018-04-04 19:51:19.147", ""], ["6km NE of Healy, Alaska", "2018-04-04 16:26:14.741", "03:25:04.406"], ["34km NE of Valdez, Alaska", "2018-04-04 16:21:57.040", "00:04:17.701"], ["23km NE of Fairbanks, Alaska", "2018-04-04 16:10:05.595", "00:11:51.445"], ["53km SSE of Cantwell, Alaska", "2018-04-04 16:07:04.498", "00:03:01.097"], ["254km SE of Kodiak, Alaska", "2018-04-04 13:57:06.180", "02:09:58.318"], ["114km NNE of Arctic Village, Alaska", "2018-04-04 12:08:00.384", "01:49:05.796"], ["13km NNE of Larsen Bay, Alaska", "2018-04-04 11:49:21.816", "00:18:38.568"], ["109km W of Cantwell, Alaska", "2018-04-04 11:25:36.307", "00:23:45.509"], ["107km NW of Talkeetna, Alaska", "2018-04-04 10:26:21.610", "00:59:14.697"]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeYesterdaywhen you run the search."]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "delta", "section_heading": "Extended examples", "section_id": "id_3d39234f_7694_4136_8101_633b216bc07c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/delta", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:17.417483+00:00", "version": "10.2"}}
{"id": "bd8b8a2dcda76004", "content": "Commands accum autoregress streamstats trendline", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "delta", "section_heading": "See also", "section_id": "a7e266e7_823d_467f_be8c_f933283c8162--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/delta", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:17.417488+00:00", "version": "10.2"}}
{"id": "28a64f768a0a39a6", "content": "Partitions the events into k clusters, with each cluster defined by its mean value. Each event belongs to the cluster with the nearest mean value. Performs k-means clustering on the list of fields that you specify. If no fields are specified, performs the clustering on all numeric fields. Events in the same cluster are moved next to each other. You have the option to display the cluster number for each event.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "kmeans", "section_heading": "Description", "section_id": "f0cca773_d5c5_42b1_86a0_040ecf14d3d9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/kmeans", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:32.828901+00:00", "version": "10.2"}}
{"id": "3e1b65f435bb7a6e", "content": "kmeans [ kmeans-options ...] [ field-list ] Required arguments None. Optional arguments field-list Syntax: <field> ... Description: Specify a space separated list of the exact fields to use for the join. Default: If no fields are specified, uses all numerical fields that are common to both result sets. Skips events with non-numerical fields. kmeans-options Syntax: <reps> | <iters> | <t> | <k> | <cnumfield> | <distype> | <showcentroid> Description: Options for the kmeans command. kmeans options reps Syntax: reps=<int> Description: Specify the number of times to repeat kmeans using random starting clusters. Default: 10 iters Syntax: maxiters=<int> Description: Specify the maximum number of iterations allowed before failing to converge. Default: 10000 t Syntax: t=<num> Description: Specify the algorithm convergence tolerance. Default: 0 k Syntax: k=<int> | <int>-<int> Description: Specify as a scalar integer value or a range of integers. When provided as single number, selects the number of clusters to use. This produces events annotated by the cluster label. When expressed as a range, clustering is done for each of the cluster counts in the range and a summary of the results is produced. These results express the size of the clusters, and a 'distortion' field which represents how well the data fits those selected clusters. Values must be greater than 1 and less than maxkvalue (see Limits section). Default: k=2 cnumfield Syntax: cfield=<field> Description: Names the field to annotate the results with the cluster number for each event. Default: CLUSTERNUM distype Syntax: dt= ( l1 | l1norm | cityblock | cb ) | ( l2 | l2norm | sq | sqeuclidean ) | ( cos | cosine ) Description: Specify the distance metric to use. The l1 , l1norm , and cb distance metrics are synonyms for cityblock. The l2 , l2norm , and sq distance metrics are synonyms for sqeuclidean or sqEuclidean. The cos distance metric is a synonym for cosine. Default: sqeucildean showcentroid Syntax: showcentroid= true | false Description: Specify whether to expose the centroid centers in the search results (showcentroid=true) or not. Default: true", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "kmeans", "section_heading": "Syntax", "section_id": "id_88f45a6a_af6e_4591_8ef7_2c94ff897c2a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/kmeans", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:32.828909+00:00", "version": "10.2"}}
{"id": "72273394e6cae08d", "content": "Limits The number of clusters to collect the values into -- k -- is not permitted to exceed maxkvalue. The maxkvalue is specified in the limits.conf file, in the [kmeans] stanza. The maxkvalue default is 1000. When a range is given for the k option, the total distance between the beginning and ending cluster counts is not permitted to exceed maxkrange. The maxkrange is specified in the limits.conf file, in the [kmeans] stanza. The maxkrange default is 100. The above limits are designed to avoid the computation work becoming unreasonably expensive. The total number of values which are clustered by the algorithm (typically the number of input results) is limited by the maxdatapoints parameter in the [kmeans] stanza of limits.conf. If this limit is exceeded at runtime, a warning message displays in Splunk Web. This defaults to 100000000 or 100 million. This maxdatapoints limit is designed to avoid exhausting memory.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "kmeans", "section_heading": "Usage", "section_id": "aacf46bb_0ace_4786_bb30_57697de95508--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/kmeans", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:32.828915+00:00", "version": "10.2"}}
{"id": "9ae6a2771e3dc546", "content": "Example 1: Group search results into 4 clusters based on the values of the \"date_hour\" and \"date_minute\" fields. Example 2: Group results into 2 clusters based on the values of all numerical fields.", "code_examples": [{"language": "spl", "code": "... | kmeans k=4 date_hour date_minute"}, {"language": "spl", "code": "... | kmeans"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "kmeans", "section_heading": "Examples", "section_id": "d84756c5_2dbb_4ad5_ab43_1a9f54a625e8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/kmeans", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:32.828920+00:00", "version": "10.2"}}
{"id": "a115909107a64821", "content": "anomalies , anomalousvalue , cluster , outlier ,", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "kmeans", "section_heading": "See also", "section_id": "bd7bd10d_07e4_4f0b_83c3_aa99d707d96b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/kmeans", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:32.828930+00:00", "version": "10.2"}}
{"id": "caa04829ba576a99", "content": "The multisearch command is a generating command that runs multiple streaming searches at the same time. This command requires at least two subsearches and allows only streaming operations in each subsearch. Examples of streaming searches include searches with the following commands: search , eval , where , fields , and rex. For more information, see Types of commands in the Search Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "multisearch", "section_heading": "Description", "section_id": "fc6291ef_f294_489f_b7b4_767ab28a0776--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/multisearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:50.684216+00:00", "version": "10.2"}}
{"id": "d11ac254829ab6a3", "content": "| multisearch <subsearch1> <subsearch2> <subsearch3> ... Required arguments <subsearch> Syntax: \"[\"search <logical-expression>\"]\" Description: At least two streaming searches must be specified. See the search command for detailed information about the valid arguments for <logical-expression>. To learn more, see About subsearches in the Search Manual .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "multisearch", "section_heading": "Syntax", "section_id": "id_5fa1f464_d6db_44c8_a441_8a1d4b2284dd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/multisearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:50.684225+00:00", "version": "10.2"}}
{"id": "276c5925e396cb22", "content": "The multisearch command is an event-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. The multisearch command doesn't support peer selection You can't exclude search peers from multisearch searches because the multisearch command connects to all peers by default. For example, the following multisearch search connects to the indexer called myServer even though it is excluded using NOT : Instead of using the multisearch command to exclude search peers from your search, you can use other commands such as append with search optimization turned off. If you don't turn off search optimization, Splunk software might internally convert the append command to the multisearch command in order to optimize the search and might not exclude the search peers. You can turn off search optimization for a specific search by including the following command at the end of your search: For example, the following workaround uses the append command to exclude myServer: See Optimization settings in the Search Manual. Subsearch processing and limitations With the multisearch command, the events from each subsearch are interleaved. Therefore the multisearch command is not restricted by the subsearch limitations. Unlike the append command, the multisearch command does not run the subsearch to completion first. The following subsearch example with the append command is not the same as using the multisearch command.", "code_examples": [{"language": "spl", "code": "| multisearch\n[ search index=_audit NOT splunk_server=myServer]"}, {"language": "spl", "code": "|noop search_optimization=false"}, {"language": "spl", "code": "index=_internal splunk_server=myServer \n| append[| search index=_audit] \n| noop search_optimization=false"}, {"language": "spl", "code": "index=a |evaltype=\"foo\"| append [search index=b |evalmytype =\"bar\"]"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "multisearch", "section_heading": "Usage", "section_id": "b34b6a22_ada2_4c36_a791_6683e9373a71--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/multisearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:50.684232+00:00", "version": "10.2"}}
{"id": "632654f58b41fc94", "content": "Example 1: Search for events from both index a and b. Use the eval command to add different fields to each set of results.", "code_examples": [{"language": "spl", "code": "| multisearch [search index=a |evaltype=\"foo\"] [search index=b |evalmytype =\"bar\"]"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "multisearch", "section_heading": "Examples", "section_id": "id_41301030_d8c9_4aba_bbec_37bb34560162--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/multisearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:50.684237+00:00", "version": "10.2"}}
{"id": "72c88bd163dd7dfa", "content": "append , join", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "multisearch", "section_heading": "See also", "section_id": "id_405e38e9_68cf_46e2_8af1_6373854a9a7e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/multisearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:07:50.684241+00:00", "version": "10.2"}}
{"id": "e410b8a24f086a16", "content": "Efficiently returns transaction events that match a transaction type and contain specific text. Note: For Splunk Cloud Platform, you must create a private app that contains your transaction type definitions. If you are a Splunk Cloud administrator with experience creating private apps, see Manage private apps in your Splunk Cloud Platform deployment in the Splunk Cloud Admin Manual. If you have not created private apps, contact your Splunk account representative for help with this customization.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "searchtxn", "section_heading": "Description", "section_id": "id_3ad64393_d1b8_4dfa_aa24_10a25a7a012a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/searchtxn", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:07.583907+00:00", "version": "10.2"}}
{"id": "952f44e3f982f796", "content": "| searchtxn <transaction-name> [max_terms=<int>] [use_disjunct=<bool>] [eventsonly=<bool>] <search-string> Required arguments <transaction-name> Syntax: <transactiontype> Description: The name of the transaction type stanza that is defined in transactiontypes.conf. <search-string> Syntax: <string> Description: Terms to search for within the transaction events. Optional arguments eventsonly Syntax: eventsonly=<bool> Description: If true, retrieves only the relevant events but does not run \"| transaction\" command. Default: false max_terms Syntax: maxterms=<int> Description: Integer between 1-1000 which determines how many unique field values all fields can use. Using smaller values speeds up search, favoring more recent values. Default: 1000 use_disjunct Syntax: use_disjunct=<bool> Description: Specifies if each term in <search-string> should be processed as if separated by an OR operator on the initial search. Default : true", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "searchtxn", "section_heading": "Syntax", "section_id": "id_96e95ec1_2ce0_4b9f_9338_a5739495730b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/searchtxn", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:07.583914+00:00", "version": "10.2"}}
{"id": "6598cb81174ce51b", "content": "The searchtxn command is an event-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. Transactions The command works only for transactions bound together by particular field values, not by ordering or time constraints. Suppose you have a <transactiontype> stanza in the transactiontypes.conf.in file called \"email\". The stanza contains the following settings. fields=qid, pid search=sourcetype=sendmail_syslog to=root The searchtxn command finds all of the events that match sourcetype=\"sendmail_syslog\" to=root. From those results, all fields that contain a qid or pid located are used to further search for relevant transaction events. When no additional qid or pid values are found, the resulting search is run: sourcetype=\"sendmail_syslog\" ((qid=val1 pid=val1) OR (qid=valn pid=valm) | transaction name=email | search to=root", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "searchtxn", "section_heading": "Usage", "section_id": "c9e14f8e_2df1_483a_b649_aa9ee944aa32--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/searchtxn", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:07.583919+00:00", "version": "10.2"}}
{"id": "97b23764a1415a0e", "content": "Example 1: Find all email transactions to root from David Smith.", "code_examples": [{"language": "spl", "code": "| searchtxn email to=root from=\"David Smith\""}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "searchtxn", "section_heading": "Examples", "section_id": "id_7d0b19af_1204_4ca5_8c77_95b358f1d027--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/searchtxn", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:07.583924+00:00", "version": "10.2"}}
{"id": "91e0b63d7d5d6b51", "content": "transaction", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "searchtxn", "section_heading": "See also", "section_id": "id_3d99f2ee_8359_46fc_b633_857f77512631--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/searchtxn", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:07.583928+00:00", "version": "10.2"}}
{"id": "63496919092fa06f", "content": "Converts values of the specified multivalue field into one single value. Separates the values using a new line \"\\n delimiter. Overrides the configurations for the multivalue field that are set in the fields.conf file.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "nomv", "section_heading": "Description", "section_id": "id_0fa8dfc5_d6f1_4cf8_95e6_0cd1a8874def--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/nomv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:24.830386+00:00", "version": "10.2"}}
{"id": "11e9aa805c0d13b5", "content": "nomv <field> Required arguments field Syntax: <field> Description: The name of a multivalue field.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "nomv", "section_heading": "Syntax", "section_id": "id_6734a608_ed07_4aea_8dda_03e23da63a3b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/nomv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:24.830396+00:00", "version": "10.2"}}
{"id": "bc718974bf62a56c", "content": "The nomv command is a distributable streaming command. See Command types. You can use evaluation functions and statistical functions on multivalue fields or to return multivalue fields.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "nomv", "section_heading": "Usage", "section_id": "d2f64121_8e93_4e3e_adf8_d3a601a5f537--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/nomv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:24.830401+00:00", "version": "10.2"}}
{"id": "dd03899df1e96af6", "content": "Example 1: For sendmail events, combine the values of the senders field into a single value. Display the top 10 values.", "code_examples": [{"language": "spl", "code": "eventtype=\"sendmail\"| nomv senders | top senders"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "nomv", "section_heading": "Examples", "section_id": "id_4deac4ba_ac1b_4cda_a08a_a31216621fd4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/nomv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:24.830408+00:00", "version": "10.2"}}
{"id": "2401d3c2938f3f94", "content": "Commands: makemv mvcombine mvexpand convert Functions: Multivalue eval functions Multivalue stats and chart functions split", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "nomv", "section_heading": "See also", "section_id": "id_63ce17e2_3a0e_43e2_ac3a_21c01a71e1d1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/nomv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:24.830413+00:00", "version": "10.2"}}
{"id": "4fcb8742c7be32e3", "content": "Generates the specified number of search results in temporary memory. If you do not specify any of the optional arguments, this command runs on the local machine and generates one result with only the _time field.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "makeresults", "section_heading": "Description", "section_id": "a9024609_448a_4659_919b_d4013b232fa3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makeresults", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:43.039242+00:00", "version": "10.2"}}
{"id": "dc6f303b1cf5dbca", "content": "The required syntax is in bold. | makeresults [count=<num>] [annotate=<bool>] [splunk_server=<string>] [splunk_server_group=<string>...] [<format>=<format_type>] [data=<string>] Required arguments None. Optional arguments count Syntax: count=<num> Description: The number of results to generate. If you do not specify the annotate argument, the results have only the _time field. Default: 1 annotate Syntax: annotate=<bool> Description: If annotate=true , generates results with the fields shown in the table below. If annotate=false , generates results with only the _time field. Default: false Fields generated with annotate=true You can use these fields to compute aggregate statistics. splunk_server Syntax: splunk_server=<string> Description: Use to generate results on one specific server. Use 'local' to refer to the search head. Default: local. See the Usage section. If you use Federated Search for Splunk in transparent mode, you must use either splunk_server or splunk_server_group to identify the local or remote search head, search head cluster, indexer, or indexer cluster to use for your makeresults search. See the Usage section for more details. splunk_server_group Syntax: (splunk_server_group=<string>)... Description: Use to generate results on a specific server group or groups. You can specify more than one <splunk_server_group>. Default: none. See the Usage section. If you use Federated Search for Splunk in transparent mode, you must use either splunk_server or splunk_server_group to identify the local or remote search head, search head cluster, indexer, or indexer cluster to use for your makeresults search. See the Usage section for more details. You can use the format and data arguments to convert CSV- or JSON-formatted data into Splunk events. If you specify these arguments, makeresults ignores other arguments such as count or annotate. <format>=<format_type> Syntax: format = csv | json Description: Specifies the format of the inline data supplied by the data argument. If you provide a format argument, makeresults expects a corresponding data argument with inline data that fits the specified format. See the Usage section for examples. data Syntax: data=<string> Description: A collection of inline data that makeresults converts into events. If you provide a data argument, makeresults expects this data to follow the format specified by a corresponding format argument. See the Usage section for examples.", "code_examples": [], "tables": [{"headers": ["Field", "Value"], "rows": [["_raw", "None."], ["_time", "Date and time that you run themakeresultscommand."], ["host", "None."], ["source", "None."], ["sourcetype", "None."], ["splunk_server", "The name of the server that themakeresultscommand is run on."], ["splunk_server_group", "None."]]}], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "makeresults", "section_heading": "Syntax", "section_id": "id_7a873167_d401_411b_a369_dbd3089f363a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makeresults", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:43.039261+00:00", "version": "10.2"}}
{"id": "63330f312e728cf6", "content": "The makeresults command is a report-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. The search results created by the makeresults command are created in temporary memory and are not saved to disk or indexed. You can use this command with the eval command to generate an empty result for the eval command to operate on. See the Examples section. Note: Order-sensitive processors might fail if the internal _time field is absent. Specifying server and server groups If you use Splunk Cloud Platform, omit any server or server group argument. If you are using Splunk Enterprise, by default results are generated only on the originating search head, which is equivalent to specifying splunk_server=local. If you provide a specific splunk_server or splunk_server_group , then the number of results you specify with the count argument are generated on the all servers or server groups that you specify. If you specify a server, the results are generated for that server, regardless of the server group that the server is associated with. If you specify a count of 5 and you target 3 servers, then you will generate 15 total results. If annotate=true , the names for each server appear in the splunk_server column. This column will show that each server produced 5 results. Specifying servers for transparent mode federated searches If you run Federated Search for Splunk in transparent mode, to run a makeresults search, you must use either the splunk_server or the splunk_server_group argument to identify the local or remote search head, search head cluster, indexer, or indexer cluster over which you want to run your makeresults search. Note: If you do not identify the transparent mode server or servers that you want to run the search over, Splunk software blocks the makeresults search. For example, if you want to run your search over a search head on your transparent mode federated provider, and that search head is named sh1.kualalumpur.blue, you must add splunk_server=sh1.kualalumpur.blue to your makeresults search. For more information, see Run federated searches over remote Splunk platform deployments in Federated Search. Generating results from inline CSV- or JSON-formatted data Use the format and data arguments in conjunction to generate events from CSV- or JSON-formatted data. Inline JSON data must be provided as a series of JSON objects, all within a single JSON array. The makeresults command generates a separate event for each JSON object. The keys of that object become fields, and the object values become field values. Each key must be bracketed in escape quotes. The entire JSON array must be placed within double quotation marks ( \" ). Here is an example of JSON formatted data: Inline data in CSV format consists of a set of lines. The first line contains the schema, or headers, for the CSV table. This first line consists of a comma-separated list of strings, and each string corresponds to a field name. The schema ends when a newline character is reached. Each line following the schema line contains comma-separated field values, and each of these subsequent lines is translated by makeresults into an individual event. Use newlines to indicate the end of one event and the beginning of another. Here is an example of CSV-formatted data: Inline datasets cannot exceed a threshold of 29,999 characters. If makeresults cannot parse the data for the specified format, it returns an error.", "code_examples": [{"language": "spl", "code": "| makeresults format=json data=\"[{\\\"name\\\":\\\"John\\\", \\\"age\\\":35}, {\\\"name\\\":\\\"Sarah\\\", \\\"age\\\":39}]\""}, {"language": "spl", "code": "| makeresults format=csv data=\"name, age\nJohn,35\nSarah,39\""}], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "makeresults", "section_heading": "Usage", "section_id": "id_2888f8c3_eb65_42f0_a562_940c792588e0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makeresults", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:43.039270+00:00", "version": "10.2"}}
{"id": "76b3242bdcbdbc42", "content": "1. Create a result as an input into the eval command Sometimes you want to use the eval command as the first command in a search. However, the eval command expects events as inputs. You can create a placeholder event at the beginning of a search by using the makeresults command. You can then use the eval command in your search. The results look something like this: 2. Determine if the modified time of an event is greater than the relative time For events that contain the field scheduled_time in UNIX time, determine if the scheduled time is greater than the relative time. The relative time is 1 minute before now. This search uses a subsearch that starts with the makeresults command.", "code_examples": [{"language": "spl", "code": "| makeresults |evalnewfield=\"some value\""}, {"language": "spl", "code": "index=_internal sourcetype=scheduler ( scheduled_time > [ makeresults |evalit=relative_time(now(),\"-m\") |return$it] )"}], "tables": [{"headers": ["_time", "newfield"], "rows": [["2020-01-09 14:35:58", "some value"]]}], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "makeresults", "section_heading": "Basic examples", "section_id": "id_2a538943_9661_417c_8a01_be42746b28c9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makeresults", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:43.039279+00:00", "version": "10.2"}}
{"id": "6b6d79db67010dea", "content": "1. Create daily results for testing You can use the makeresults command to create a series of results to test your search syntax. For example, the following search creates a set of five results: The results look something like this: Each result has the same timestamp which, by itself, is not very useful. But with a few additions, you can create a set of unique dates. Start by adding the streamstats command to count your results: The results look something like this: You can now use that count to create different dates in the _time field, using the eval command. The calculation multiplies the value in the count field by the number of seconds in a day. The result is subtracted from the original _time field to get new dates equivalent to 24 hours ago, 48 hours ago, and so forth. The seconds in the date are different because _time is calculated the moment you run the search. The results look something like this: The dates start from the day before the original date, 2020-01-09, and go back five days. Need more than five results? Simply change the count value in the makeresults command. 2. Create hourly results for testing You can create a series of hours instead of a series of days for testing. Use 3600, the number of seconds in an hour, instead of 86400 in the eval command. The results look something like this: Notice that the hours in the timestamp are 1 hour apart. 3. Add a field with string values You can specify a list of values for a field. But to have the values appear in separate results, you need to make the list a multivalue field and then expand that multivalued list into separate results. Use this search, substituting your strings for buttercup and her friends: The results look something like this: 4. Create a set of events with multiple fields Let's start by creating a set of four events. One of the events contains a null value in the age field. The streamstats command is used to create the count field. The streamstats command calculates a cumulative count for each event, at the time the event is processed. The eval command is used to create two new fields, age and city. The eval command uses the value in the count field. The case function takes pairs of arguments, such as count=1, 25. The first argument is a Boolean expression. When that expression is TRUE, the corresponding second argument is returned. The results of the search look like this: In this example, the eventstats command generates the average age for each city. The generated averages are placed into a new field called avg(age). The following search is the same as the previous search, with the eventstats command added at the end: For San Francisco , the average age is 28 = (25 + 31) / 2. For Seattle , there is only one event with a value. The average is 39 = 39 / 1. The eventstats command places that average in every event for Seattle, including events that did not contain a value for age. The results of the search look like this: 5. Add a field with a set of random numbers If you need to test something with a set of numbers, you have two options: You can add a field with a set of numbers that you specify. This is similar to adding a field with a set of string values, which is shown in the previous example. You can add a field with a set of randomly generated numbers by using the random function, as shown below: The results look something like this: Use the round function to round the numbers up. For example, this search rounds the numbers up to four digits to the right of the decimal: The results look something like this: 6. Generate a table of results from JSON-formatted data This makeresults search provides a JSON array of objects with the names and ages of a set of individuals. makeresults transforms this JSON object array into a result table where the keys have been turned into fields and the values have been transformed into field values. The results look something like this: 7. Generate a table of results from CSV-formatted data This makeresults search provides an inline collection of CSV-formatted data. It is a table containing the names and ages of a set of individuals. You can add the fields command to reorder the fields so they do not appear in alphabetical order. The results look something like this:", "code_examples": [{"language": "spl", "code": "| makeresults count=5"}, {"language": "spl", "code": "| makeresults count=5 \n | streamstats count"}, {"language": "spl", "code": "| makeresults count=5 \n | streamstats count\n |eval_time=_time-(count*86400)"}, {"language": "spl", "code": "| makeresults count=5 \n | streamstats count\n |eval_time=_time-(count*3600)"}, {"language": "spl", "code": "| makeresults\n |evaltest=\"buttercup rarity tenderhoof dash mcintosh fleetfoot mistmane\"| makemv delim=\" \"test| mvexpandtest"}, {"language": "spl", "code": "| makeresults count=4 \n| streamstats count \n|evalage =case(count=1, 25, count=2, 39, count=3, 31, count=4, null())\n|evalcity =case(count=1 OR count=3,\"San Francisco\", count=2 OR count=4,\"Seattle\")"}, {"language": "spl", "code": "| makeresults count=4 \n| streamstats count \n|evalage =case(count=1, 25, count=2, 39, count=3, 31, count=4, null())\n|evalcity =case(count=1 OR count=3,\"San Francisco\", count=2 OR count=4,\"Seattle\")\n| eventstats avg(age) BY city"}, {"language": "spl", "code": "| makeresults count=5 \n | streamstats count\n |evaltest=random()/random()"}, {"language": "spl", "code": "...|evaltest=round(random()/random(),4)"}, {"language": "spl", "code": "| makeresults format=json data=\"[{\\\"name\\\":\\\"Larson\\\",\\\"age\\\":32}, {\\\"name\\\":\\\"Nyeti\\\",\\\"age\\\":44}, {\\\"name\\\":\\\"Vero\\\",\\\"age\\\":22}]\""}, {"language": "spl", "code": "| makeresults format=csv data=\"name, age\nSujata,61\nLinus,29\nKarina,33\"| fields name, age"}], "tables": [{"headers": ["_time"], "rows": [["2020-01-09 14:35:58"], ["2020-01-09 14:35:58"], ["2020-01-09 14:35:58"], ["2020-01-09 14:35:58"], ["2020-01-09 14:35:58"]]}, {"headers": ["_time", "count"], "rows": [["2020-01-09 14:35:58", "1"], ["2020-01-09 14:35:58", "2"], ["2020-01-09 14:35:58", "3"], ["2020-01-09 14:35:58", "4"], ["2020-01-09 14:35:58", "5"]]}, {"headers": ["_time", "count"], "rows": [["2020-01-08 14:45:24", "1"], ["2020-01-07 14:45:24", "2"], ["2020-01-06 14:45:24", "3"], ["2020-01-05 14:45:24", "4"], ["2020-01-04 14:45:24", "5"]]}, {"headers": ["_time", "count"], "rows": [["2020-01-09 15:35:14", "1"], ["2020-01-09 14:35:14", "2"], ["2020-01-09 13:35:14", "3"], ["2020-01-09 12:35:14", "4"], ["2020-01-09 11:35:14", "5"]]}, {"headers": ["_time", "test"], "rows": [["2020-01-09 16:35:14", "buttercup"], ["2020-01-09 16:35:14", "rarity"], ["2020-01-09 16:35:14", "tenderhoof"], ["2020-01-09 16:35:14", "dash"], ["2020-01-09 16:35:14", "mcintosh"], ["2020-01-09 16:35:14", "fleetfoot"], ["2020-01-09 16:35:14", "mistmane"]]}, {"headers": ["_time", "age", "city", "count"], "rows": [["2020-02-05 18:32:07", "25", "San Francisco", "1"], ["2020-02-05 18:32:07", "39", "Seattle", "2"], ["2020-02-05 18:32:07", "31", "San Francisco", "3"], ["2020-02-05 18:32:07", "", "Seattle", "4"]]}, {"headers": ["_time", "age", "avg(age)", "city", "count"], "rows": [["2020-02-05 18:32:07", "25", "28", "San Francisco", "1"], ["2020-02-05 18:32:07", "39", "39", "Seattle", "2"], ["2020-02-05 18:32:07", "31", "28", "San Francisco", "3"], ["2020-02-05 18:32:07", "", "39", "Seattle", "4"]]}, {"headers": ["_time", "count", "test"], "rows": [["2020-01-08 14:45:24", "1", "5.371091109260495"], ["2020-01-07 14:45:24", "2", "0.4563314783228324"], ["2020-01-06 14:45:24", "3", "0.804991002129475"], ["2020-01-05 14:45:24", "4", "1.4946919835236068"], ["2020-01-04 14:45:24", "5", "24.193952675772845"]]}, {"headers": ["_time", "count", "test"], "rows": [["2020-01-08 14:45:24", "1", "5.3711"], ["2020-01-07 14:45:24", "2", "0.4563"], ["2020-01-06 14:45:24", "3", "0.8050"], ["2020-01-05 14:45:24", "4", "1.4947"], ["2020-01-04 14:45:24", "5", "24.1940"]]}, {"headers": ["_raw", "_time", "age", "name"], "rows": [["{\"name\":\"Larson\",\"age\":32}", "2021-09-13 22:27:41", "32", "Larson"], ["{\"name\":\"Nyeti\",\"age\":44}", "2021-09-13 22:27:41", "44", "Nyeti"], ["{\"name\":\"Vero\",\"age\":22}", "2021-09-13 22:27:41", "22", "Vero"]]}, {"headers": ["name", "age"], "rows": [["Sujata", "61"], ["Linus", "29"], ["Karina", "33"]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "makeresults", "section_heading": "Extended examples", "section_id": "id_45f97b16_300d_47a9_a29e_8010f0be157f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makeresults", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:43.039334+00:00", "version": "10.2"}}
{"id": "c7757222aafcd5d7", "content": "Commands gentimes", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "makeresults", "section_heading": "See also", "section_id": "id_1354a01f_47fc_4c48_8021_1151a61756bf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/makeresults", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:08:43.039340+00:00", "version": "10.2"}}
{"id": "e2c7ad207230e7b0", "content": "Writes search results to a static lookup table, or KV store collection, that you specify. CAUTION: This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "outputlookup", "section_heading": "Description", "section_id": "c7e2a23d_dc82_473e_b6c8_3df171eefa1b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputlookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:00.449902+00:00", "version": "10.2"}}
{"id": "f6f379f9ecd3d9e5", "content": "The required syntax is in bold. | outputlookup [append=<bool>] [create_empty=<bool>] [override_if_empty=<bool>] [max=<int>] [key_field=<field>] [allow_updates=<bool>] [createinapp=<bool>] [create_context=<string>] [output_format=<string>] <filename> | <tablename> Required arguments You must specify one of the following required arguments, either filename or tablename. filename Syntax: <string> Description: The name of the lookup file. The file must end with .csv or .csv.gz. tablename Syntax: <string> Description: The name of the lookup table as specified by a stanza name in transforms.conf , which corresponds to the lookup definition. The lookup table can be configured for any lookup type (CSV, external, or KV store). If your lookup file and the lookup definition that it is associated with have the same name, you can provide a tablename that is the same value as the corresponding filename without the .csv extension. For example, say you have a lookup file named staff.csv. If you associate that file with a lookup called staff , you can use either staff.csv or staff as the tablename with the outputlookup command. See Create a CSV lookup definition in the Splunk Enterprise Knowledge Manager Manual. Optional arguments allow_updates Syntax: allow_updates=<bool> Description: The allow_updates argument is set to true by default if either the append argument is set to true or if the key_field argument is set to a valid field name. If allow_updates is set to true , the outputlookup command updates existing records and inserts new records. If allow_updates is set to false , the outputlookup only inserts records. append Syntax: append=<bool> Description: The default setting, append=false , writes the search results to the .csv file or KV store collection. Fields that are not in the current search results are removed from the file. If append=true , the outputlookup command attempts to append search results to an existing .csv file or KV store collection. Otherwise, it creates a file. If there is an existing .csv file, the outputlookup command writes only the fields that are present in the previously existing .csv file. An outputlookup search that is run with append=true might result in a situation where the lookup table or collection is only partially updated. This means that a subsequent lookup or inputlookup search on that lookup table or collection might return stale data along with new data. The outputlookup command cannot append to .gz files. Default: false create_context Syntax: create_context= app | user | system Description: Specifies where the lookup table file is created. Ignored in favor of the createinapp argument if both arguments are used in the search. See Usage for details. Default: app create_empty Syntax: create_empty=<bool> Description: If set to true and there are no results, a zero-length file is created. When set to false and there are no results, no file is created. If the file previously existed, the file is deleted. For example, suppose there is a system-level lookup called \"test\" with the lookup defined in \"test.csv\". There is also an app-level lookup with the same name. If an app overrides that \"test.csv\" in it's own app directory with an empty file create_empty=true , the app-level lookup behaves as if the lookup is empty. However, if there's no file at all create_empty=false at the app level, then the lookup file in the system-level is used. Default: false createinapp Syntax: createinapp=<bool> Description: Specifies whether the lookup table file is created in the system directory or the lookups directory for the current app context. Overrides the create_context argument if both arguments are used in the search. See Usage for details. Default: true key_field Syntax: key_field=<field> Description: For KV store-based lookups, uses the specified field name as the key to a value and replaces that value. An outputlookup search using the key_field argument might result in a situation where the lookup table or collection is only partially updated. A subsequent lookup or inputlookup search on that collection might return stale data along with new data. A partial update only occurs with concurrent searches, one with the outputlookup command and a search with the inputlookup command. It is possible that the inputlookup occurs when the outputlookup is still updating some of the records. When key_field is used in an outputlookup search, by default, append is set to true , which appends search results to an existing KV store collection. You can override this default behavior by directly setting key_field with append set to false. max Syntax: max=<int> Description: Specifies whether there is a limit to the number of rows to output to a CSV file or a KV store collection. For example, to write 50,000 rows to a CSV file or KV store collection, set max=50000 in your outputlookup search. Default: no limit output_format Syntax: output_format=splunk_sv_csv | splunk_mv_csv Description: Controls the output data format of the lookup. Use output_format=splunk_mv_csv when you want to output multivalued fields to a lookup table file, and then read the fields back into Splunk using the inputlookup command. The default, splunk_sv_csv outputs a CSV file which excludes the _mv_<fieldname> fields. Default: splunk_sv_csv override_if_empty Syntax: override_if_empty=<bool> Description: If override_if_empty=true and no results are passed to the output file, the existing output file is deleted, If override_if_empty=false and no results are passed to the output file, the command does not delete the existing output file. Default: true", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "outputlookup", "section_heading": "Syntax", "section_id": "id_0545aa65_591a_49c1_b593_62ac22855304--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputlookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:00.449909+00:00", "version": "10.2"}}
{"id": "c3c206013d9acdc1", "content": "The lookup table must be a CSV or GZ file, or a table name specified with a lookup table configuration in transforms.conf. The lookup table can refer to a KV store collection or a CSV lookup. The outputlookup command cannot be used with external lookups. If you specify a lookup table file name with the .gz extension, the file that's created is compressed. Determine where the lookup table file is created For CSV lookups, outputlookup creates a lookup table file for the results of the search. There are three locations where outputlookup can put the file it creates: The system lookups directory: $SPLUNK_HOME/etc/system/local/lookups The lookups directory for the current app context: $SPLUNK_HOME/etc/apps/<app>/lookups The app-based lookups directory for the user running the search: etc/users/<user>/<app>/lookups You can use the createinapp or create_context arguments to determine where outputlookup creates the lookup table for a given search. If you try to use both of these arguments in the same search, createinapp argument overrides the create_context argument. If you do not use either argument in your search, the create_context setting in limits.conf determines where outputlookup creates the lookup table file. This setting defaults to app if there is an app context when you run the search, or to system , if there is not an app context when you run the search. To have outputlookup create the lookup table file in the system lookups directory, set createinapp=false or set create_context=system. Alternatively, if you do not have an app context when you run the search, leave both arguments out of the search and rely on the limits.conf version of create_context to put the lookup table file in the system directory. This last approach only works if the create_context setting in limits.conf has not been set to user. To have outputlookup create the lookup table file in the lookups directory for the current app context, set createinapp=true or set create_context=app. Alternatively, if you do have an app context when you run the search, leave both arguments out of the search and rely on the limits.conf version of create_context to put the lookup table file in the app directory. This last approach only works if the create_context setting in limits.conf has not been set to user. To have outputlookup create the lookup table file in the lookups directory for the user running the search, set create_context=user. Alternatively, if you want all outputlookup searches to create lookup table files in user lookup directories by default, you can set create_context=user in limits.conf. The createinapp and create_context arguments can override this setting if they are used in the search. Note: If the lookup table file already exists in the location to which it is written, the existing version of the file is overwritten with the results of the outputlookup search. Restrict write access to lookup table files with check_permission For permissions in CSV lookups in Splunk Enterprise deployments, use the check_permission field in transforms.conf and outputlookup_check_permission in limits.conf to restrict write access to users with the appropriate permissions when using the outputlookup command. Both check_permission and outputlookup_check_permission default to false, which means that, by default, Splunk software doesn't perform any permissions checks. As a result, all users can write to lookup table files, regardless of their capabilities. To ensure that only users who have the admin or power role can write to a shared CSV lookup file by default, instruct Splunk software to verify permission settings for lookups for users by setting check_permission and outputlookup_check_permission to true. Note that the permission checks are performed only after the lookup stanza has been created. You can change lookup table file permissions in the .meta file for each lookup file, or Settings > Lookups > Lookup table files. For more information about creating lookups, see About lookups in the Knowledge Manager Manual. For more information about App Key Value Store collections, see About KV store in the Admin Manual. Append results Suppose you have an existing CSV file that contains fields A, D, and J. The results of your search are fields A, C, and J. If you run a search with outputlookup append=false , then fields A, C, and J are written to the CSV file. Field D is not retained. If you run a search with outputlookup append=true , then only the fields that are currently in the file are preserved. In this example, fields A and J are written to the CSV file. Field C is lost because it does not already exist in the CSV file. Field D is retained. You can work around this issue by using the eval command to add a field to your CSV file before you run the search. For example, if your CSV file is named users , you would do something like this: Then run your search and pipe the results to the fields command for the fields in the file that you want to preserve. Multivalued fields When you output to a static lookup table, the outputlookup command merges values in a multivalued field into single space-delimited value. This does not apply to a KV store collection.", "code_examples": [{"language": "spl", "code": "| inputlookup users |evalc=null | outputlookup users append=false...."}, {"language": "spl", "code": "... | fields A C J | outputlookup append=trueusers"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "outputlookup", "section_heading": "Usage", "section_id": "id_8dfbccc7_556e_457b_8b53_6e5928787e53--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputlookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:00.449914+00:00", "version": "10.2"}}
{"id": "860472f740118dbb", "content": "1. Write to a lookup table using settings in the transforms.conf file Write to usertogroup lookup table as specified in the transforms.conf file. 2. Write to a lookup file in a specific system or app directory Write to users.csv lookup file under $SPLUNK_HOME/etc/system/lookups or $SPLUNK_HOME/etc/apps/*/lookups. 3. Specify not to override the lookup file if no results are returned Write to users.csv lookup file, if results are returned, under $SPLUNK_HOME/etc/system/lookups or $SPLUNK_HOME/etc/apps/*/lookups. Do not delete the users.csv file if no results are returned. 4. Write to a KV store collection Write food inspection events for Shalimar Restaurant to a KV store collection called kvstorecoll. This collection is referenced in a lookup table called kvstorecoll_lookup. 5. Overwrite KV store collections By default, append is set to true when the key_field is used with the outputlookup command. If you don't want to append search results to an existing KV store collection, you can override the default behavior by directly setting key_field with append=false. For example, in the following outputlookup search, the KV store called accounts is appended. This is because key_field sets append=true by default. However, in the following outputlookup search, the KV store called accounts is overwritten because append=false. In this case, the append subsearch runs before the main search, which empties the entire KV store before the fields are written to accounts. Alternatively, if you want your entire lookup to reflect your search results and you don't mind using the default system-generated keys, eliminate key_field=key from your outputlookup search, like this. 6. Write from a CSV file to a KV store collection Write the contents of a CSV file to the KV store collection kvstorecoll using the lookup table kvstorecoll_lookup. This requires usage of both inputlookup and outputlookup commands. 7. Update field values for a single KV store collection record Update field values for a single KV store collection record. This requires you to use the inputlookup , outputlookup , and eval commands. The record is indicated by the value of its internal key ID (the _key field) and is updated with a new customer name and customer city. The record belongs to the KV store collection kvstorecoll , which is accessed through the lookup table kvstorecoll_lookup. To learn how to obtain the internal key ID values of the records in a KV store collection, see Example 5 for the inputlookup command.", "code_examples": [{"language": "spl", "code": "| outputlookup usertogroup"}, {"language": "spl", "code": "| outputlookup users.csv"}, {"language": "spl", "code": "| outputlookup users.csv override_if_empty=false"}, {"language": "spl", "code": "index=sf_food_health sourcetype=sf_food_inspections name=\"SHALIMAR RESTAURANT\"| outputlookup kvstorecoll_lookup"}, {"language": "spl", "code": "| makeresults \n|evalkey=1 \n| outputlookup key_field=key accounts"}, {"language": "spl", "code": "| makeresults \n|evalkey=1 \n| outputlookup append=falsekey_field=key accounts"}, {"language": "spl", "code": "| makeresults \n|evalkey=1 \n| outputlookup accounts"}, {"language": "spl", "code": "| inputlookup customers.csv | outputlookup kvstorecoll_lookup"}, {"language": "spl", "code": "| inputlookup kvstorecoll_lookup | search _key=544948df3ec32d7a4c1d9755 |evalCustName=\"Vanya Patel\"|evalCustCity=\"Springfield\"| outputlookup kvstorecoll_lookup append=True key_field=_key"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "outputlookup", "section_heading": "Examples", "section_id": "id_331d710c_1591_49a4_80b8_3e6bc162905f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputlookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:00.449919+00:00", "version": "10.2"}}
{"id": "dd1b50cb30da2622", "content": "Commands collect inputlookup lookup inputcsv mcollect meventcollect outputcsv outputtext", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "outputlookup", "section_heading": "See also", "section_id": "d6c36aae_9a8a_41cf_8934_816ac1474b9b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputlookup", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:00.449923+00:00", "version": "10.2"}}
{"id": "125880fc642fb9fb", "content": "Extracts key-value pairs from events based on a form template that describes how to extract the values. Note: For Splunk Cloud Platform, you must create a private app to extract key-value pairs from events. If you are a Splunk Cloud administrator with experience creating private apps, see Manage private apps in your Splunk Cloud Platform deployment in the Splunk Cloud Admin Manual. If you have not created private apps, contact your Splunk account representative for help with this customization.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "kvform", "section_heading": "Description", "section_id": "id_657ffb1e_c78b_4cab_9d67_bdb7efa43057--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/kvform", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:17.312294+00:00", "version": "10.2"}}
{"id": "e0d6654f06d79419", "content": "kvform [form=<string>] [field=<field>] Optional arguments form Syntax: form=<string> Description: Specify a .form file located in a $SPLUNK_HOME/etc/apps/*/forms/ directory. field Syntax: field=<field_name> Description: Uses the field name to look for .form files that correspond to the field values for that field name. For example, your Splunk deployment uses the splunkd and mongod sourcetypes. If you specify field=sourcetype , the kvform command looks for the splunkd.form and mongod.form in the $SPLUNK_HOME/etc/apps/*/forms/ directory. Default: sourcetype", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "kvform", "section_heading": "Syntax", "section_id": "cf89dd46_cb3d_49d7_b99a_9ac86c72ce8f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/kvform", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:17.312302+00:00", "version": "10.2"}}
{"id": "bee341a0a5ad7407", "content": "Before you can use the kvform command, you must: Create the forms directory in the appropriate application path. For example $SPLUNK_HOME/etc/apps/<app_name>/forms. Create the .form files and add the files to the forms directory. Format for the .form files A .form file is essentially a text file of all static parts of a form. It might be interspersed with named references to regular expressions of the type found in the transforms.conf file. An example .form file might look like this: Specifying a form If the form argument is specified, the kvform command uses the <form_name>.form file found in the Splunk configuration forms directory. For example, if form=sales_order , the kvform command looks for a sales_order.form file in the $SPLUNK_HOME/etc/apps/<app_name>/forms directory for all apps. All the events processed are matched against the form, trying to extract values. Specifying a field If you specify the field argument, the the kvform command looks for forms in the forms directory that correspond to the values for that field. For example, if you specify field=error_code , and an event has the field value error_code=404 , the command looks for a form called 404.form in the $SPLUNK_HOME/etc/apps/<app_name>/forms directory. Default value If no form or field argument is specified, the kvform command uses the default value for the field argument, which is sourcetype. The kvform command looks for <sourcetype_value>.form files to extract values.", "code_examples": [{"language": "spl", "code": "Students Name: [[string:student_name]]\nAge: [[int:age]] Zip: [[int:zip]]"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "kvform", "section_heading": "Usage", "section_id": "id_2e29a92e_1bc6_45ea_ac4d_0e9cbb3d8393--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/kvform", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:17.312307+00:00", "version": "10.2"}}
{"id": "1df8726473ed416c", "content": "1. Extract values using a specific form Use a specific form to extract values from. 2. Extract values using a field name Specify field=sourcetype to extract values from forms such as splunkd.form and mongod.form. If there is a form for a source type, values are extracted from that form. If one of the source types is access_combined but there is no access_combined.form file, that source type is ignored. 3. Extract values using the eventtype field", "code_examples": [{"language": "spl", "code": "... | kvform form=sales_order"}, {"language": "spl", "code": "... | kvform field=sourcetype"}, {"language": "spl", "code": "... | kvform field=eventtype"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "kvform", "section_heading": "Examples", "section_id": "abb4c73a_d4f2_47cb_8d93_57f2230103e9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/kvform", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:17.312312+00:00", "version": "10.2"}}
{"id": "bf9161706ac1892b", "content": "Commands extract multikv rex xmlkv", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "kvform", "section_heading": "See also", "section_id": "id_5983dca1_4244_46b7_b863_ae0d78d2cfe8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/kvform", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:17.312316+00:00", "version": "10.2"}}
{"id": "e529cf4f751508a4", "content": "The from command retrieves data from a dataset, such as a data model dataset, a CSV lookup, a KV Store lookup, a saved search, or a table dataset. Design a search that uses the from command to reference a dataset. Optionally add additional SPL such as lookups, eval expressions, and transforming commands to the search. Save the result as a report, alert, or dashboard panel. If you use Splunk Cloud Platform, or use Splunk Enterprise and have installed the Splunk Datasets Add-on, you can also save the search as a table dataset. See the Usage section.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "from", "section_heading": "Description", "section_id": "id_526aa3b7_6809_4e7e_8f91_990eeac3b460--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/from", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:34.393348+00:00", "version": "10.2"}}
{"id": "521ed3fa83a7b34f", "content": "The required syntax is in bold. | from <dataset_type>:<dataset_name> | <dataset_type> <dataset_name> You can specify a colon ( : ) or a space between <dataset_type> and <dataset_name>. Required arguments <dataset_type> Syntax: <dataset_type> Description: The type of dataset. Valid values are: datamodel , lookup , and savedsearch. The datamodel dataset type can be either a data model dataset or a table dataset. You create data model datasets with the Data Model Editor. You can create table datasets with the Table Editor if you use Splunk Cloud Platform, or use Splunk Enterprise and have installed the Splunk Datasets Add-on. The lookup dataset type can be either a CSV lookup or a KV Store lookup. The savedsearch dataset type is a saved search. You can use from to reference any saved search as a dataset. See About datasets in the Knowledge Manager Manual. <dataset_name> Syntax: <dataset_name> Description: The name of the dataset that you want to retrieve data from. If the dataset_type is a data model, the syntax is <datamodel_name>.<dataset_name>. If the name of the dataset contains spaces, enclose the dataset name in quotation marks. Example: If the data model name is internal_server , and the dataset name is splunkdaccess , specify internal_server.splunkdaccess for the dataset_name. Note: In older versions of the Splunk software, the term \"data model object\" was used. That term has been replaced with \"data model dataset\". Optional arguments None.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "from", "section_heading": "Syntax", "section_id": "id_874f3f99_39d8_4621_b179_9bc80e7c93da--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/from", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:34.393356+00:00", "version": "10.2"}}
{"id": "be86be04eed26921", "content": "The from command is a generating command. It can be either report-generating or event-generating depending on the search or knowledge object that is referenced by the command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. However, you can use the from command inside the append command. When you use the from command, you must reference an existing dataset. You can reference any dataset listed in the Datasets listing page, such as data model datasets, CSV lookup files, CSV lookup definitions, and table datasets. You can also reference saved searches and KV Store lookup definitions. See View and manage datasets in the Knowledge Manager Manual. Knowledge object dependencies When you create a knowledge object such as a report, alert, dashboard panel, or table dataset, that knowledge object has a dependency on the referenced dataset. This is referred to as a dataset extension. When you make a change to the original dataset, such as removing or adding fields, that change propagates down to the reports, alerts, dashboard panels, and tables that have been extended from that original dataset. See Dataset extension in the Knowledge Manager Manual. When field filtering is disabled for a data model When you search the contents of a data model using the from command, by default the search returns a strictly-filtered set of fields. It returns only default fields and fields that are explicitly identified in the constraint search that defines the data model. If you have edit access to your local datamodel.conf file, you can disable field filtering for specific data models by adding the strict_fields=false setting to their stanzas. When you do this, | from searches of data models with that setting return all fields related to the data model, including fields inherited from parent data models, fields extracted at search time, calculated fields, and fields derived from lookups.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "from", "section_heading": "Usage", "section_id": "a62d4eaa_9f47_4597_b1c3_7830c0438626--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/from", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:34.393360+00:00", "version": "10.2"}}
{"id": "10f68b1ba9d30b9d", "content": "1. Search a data model Search a data model that contains internal server log events for REST API calls. In this example, internal_server is the data model name and splunkdaccess is the dataset inside the internal_server data model. 2. Search a lookup file Search a lookup file that contains geographic attributes for each country, such as continent, two-letter ISO code, and subregion. 3. Retrieve data by using a lookup file Search the contents of the KV store collection kvstorecoll that have a CustID value greater than 500 and a CustName value that begins with the letter P. The collection is referenced in a lookup table called kvstorecoll_lookup. Using the stats command, provide a count of the events received from the table. 4. Retrieve data using a saved search This search retrieves the timestamp and client IP from the saved search called mysecurityquery. The search results look something like this. Even if the saved search is scheduled, this search is rerun, which can be expensive and lead to concurrency issues if more searches are run at the same time than the system can support. Alternatively, you can use the loadjob command instead of the from command in conjunction with a scheduled search if you are concerned about the number and frequency of searches that your users run. 5. Specify a dataset name that contains spaces When the name of a dataset includes spaces, enclose the dataset name in quotation marks.", "code_examples": [{"language": "spl", "code": "| from datamodel:internal_server.splunkdaccess"}, {"language": "spl", "code": "| from lookup geo_attr_countries.csv"}, {"language": "spl", "code": "| from lookup:kvstorecoll_lookup |where(CustID>500) AND (CustName=\"P*\") | stats count"}, {"language": "spl", "code": "| from savedsearch:mysecurityquery | fields _time clientip ..."}, {"language": "spl", "code": "| from savedsearch\"Top five sourcetypes\""}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "from", "section_heading": "Examples", "section_id": "id_283ba285_351b_4d43_a8e4_7113d74bac10--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/from", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:34.393364+00:00", "version": "10.2"}}
{"id": "59cfe9d59d4d7a05", "content": "Commands datamodel inputlookup inputcsv lookup loadjob", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "from", "section_heading": "See also", "section_id": "id_1b7dc9d9_3de6_449d_9641_443935f5156b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/from", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:34.393368+00:00", "version": "10.2"}}
{"id": "abcc729e57261662", "content": "The x11 command removes the seasonal pattern in your time-based data series so that you can see the real trend in your data. This command has a similar purpose to the trendline command , but it uses the more sophisticated and industry popular X11 method. The seasonal component of your time series data can be either additive or multiplicative, defined as the two types of seasonality that you can calculate with x11: add() for additive and mult() for multiplicative. See About time-series forecasting in the Search Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "x11", "section_heading": "Description", "section_id": "c3f42e89_ec1a_4943_8187_5182df0b7f92--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/x11", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:51.972564+00:00", "version": "10.2"}}
{"id": "edb85007c53e6054", "content": "x11 [<type>] [<period>] (<fieldname>) [AS <newfield>] Required arguments <fieldname> Syntax: <field> Description: The name of the field to calculate the seasonal trend. Optional arguments <type> Syntax: add() | mult() Description: Specify the type of x11 to compute, additive or multiplicative. Default: mult() <period> Syntax: <int> Description: The period of the data relative to the number of data points, expressed as an integer between 5 and 1000. If the period is 7, the command expects the data to be periodic every 7 data points. If you omit this parameter, Splunk software calculates the period automatically. The algorithm does not work if the period is less than 5 and will be too slow if the period is greater than 1000. <newfield> Syntax: <string> Description: Specify a field name for the output of the x11 command. Default: None", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "x11", "section_heading": "Syntax", "section_id": "e4a40f9a_1678_4b26_b603_09bcedbac0bf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/x11", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:51.972572+00:00", "version": "10.2"}}
{"id": "0cea7d28f08d8c91", "content": "Example 1: In this example, the type is the default mult and the period is 15. The field name specified is count. Note: Because span=1d, every data point accounts for 1 day. As a result, the period in this example is 15 days. You can change the syntax in this example to ... | x11 15(count) because the mult type is the default type. Example 2: In this example, the type is add and the period is 20. The field name specified is count .", "code_examples": [{"language": "spl", "code": "index=download | timechart span=1d count(file) as count | x11 mult15(count)"}, {"language": "spl", "code": "index=download | timechart span=1d count(file) as count | x11 add20(count)"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "x11", "section_heading": "Examples", "section_id": "id_649e7530_2130_4828_8e79_9a8f5a73aaf0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/x11", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:51.972577+00:00", "version": "10.2"}}
{"id": "52e101c5e4410a21", "content": "predict , trendline", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "x11", "section_heading": "See also", "section_id": "id_6c734f1a_9afa_4d0b_b3e5_c59bcdf69e48--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/x11", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:09:51.972580+00:00", "version": "10.2"}}
{"id": "755f5fc45428c59a", "content": "Converts results into a tabular format that is suitable for graphing. This command is the inverse of the untable command.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "xyseries", "section_heading": "Description", "section_id": "id_35fc1323_3fc3_4a09_b745_dbadee4ff490--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xyseries", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:06.471601+00:00", "version": "10.2"}}
{"id": "a591f065b596f290", "content": "xyseries [grouped=<bool>] <x-field> <y-name-field> <y-data-field>... [sep=<string>] [format=<string>] Required arguments <x-field> Syntax: <field> Description: The name of the field to use for the x-axis label. The values of this field appear as labels for the data series plotted on the x-axis. <y-name-field> Syntax: <field> Description: The field that contains the values to use as labels for the data series. <y-data-field> Syntax: <field> [,<field>] ... Description: One or more fields that contain the data to chart. When specifying multiple fields, separate the field names with commas. Optional arguments format Syntax: format=<string> Description: Used to construct output field names when multiple data series are used in conjunction with a split-by-field and separate the <y-name-field> and the <y-data-field>. format takes precedence over sep and lets you specify a parameterized expression with the stats aggregator and function ($AGG$) and the value of the split-by-field ($VAL$). grouped Syntax: grouped= true | false Description: If true, indicates that the input is sorted by the value of the <x-field> and multifile input is allowed. Default: false sep Syntax: sep=<string> Description: Used to construct output field names when multiple data series are used in conjunctions with a split-by field. This is equivalent to setting format to $AGG$<sep>$VAL$ .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "xyseries", "section_heading": "Syntax", "section_id": "id_0ce8c09c_b09b_48f8_8ee5_7d1e97ba88f3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xyseries", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:06.471609+00:00", "version": "10.2"}}
{"id": "e48be32d52ea6e6c", "content": "The xyseries command is a distributable streaming command , unless grouped=true is specified and then the xyseries command is a transforming command. See Command types. Alias The alias for the xyseries command is maketable. Results with duplicate field values When you use the xyseries command to converts results into a tabular format, results that contain duplicate values are removed. You can use the streamstats command create unique record numbers and use those numbers to retain all results. For an example, see the Extended example for the untable command .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "xyseries", "section_heading": "Usage", "section_id": "id_7f14b108_3c1d_497d_b0fe_a52ab469c5be--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xyseries", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:06.471614+00:00", "version": "10.2"}}
{"id": "5d4e18365711812e", "content": "Let's walk through an example to learn how to reformat search results with the xyseries command. Write a search Run this search in the search and reporting app: The top command automatically adds the count and percent fields to the results. For each categoryId, there are two values, the count and the percent. The search results look like this: Identify your fields in the xyseries command syntax In this example: <x-field> = categoryId <y-name-field> = count <y-data-field> = percent Reformat search results with xyseries When you apply the xyseries command, the categoryId serves as the <x-field> in your search results. The results of the calculation count become the columns, <y-name-field>, in your search results. The <y-data-field>, percent , corresponds to the values in your search results. Run this search in the search and reporting app: The search results look like this:", "code_examples": [{"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | top categoryId"}, {"language": "spl", "code": "sourcetype=access_* status=200 action=purchase | top categoryId | xyseries categoryId count percent"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["categoryId", "count", "percent"], "rows": [["STRATEGY", "806", "30.495649"], ["ARCADE", "493", "18.653046"], ["TEE", "367", "13.885736"], ["ACCESSORIES", "348", "13.166856"], ["SIMULATION", "246", "9.307605"], ["SHOOTER", "245", "9.269769"], ["SPORTS", "138", "5.221339"]]}, {"headers": ["categoryId", "138", "245", "246", "348", "367", "493", "806"], "rows": [["SPORTS", "5.221339", "", "", "", "", "", ""], ["ACCESSORIES", "", "", "", "13.166856", "", "", ""], ["ARCADE", "", "", "", "", "", "18.653046", ""], ["SHOOTER", "", "9.269769", "", "", "", "", ""], ["SIMULATION", "", "", "9.307605", "", "", "", ""], ["STRATEGY", "", "", "", "", "", "", "30.495649"], ["TEE", "", "", "", "", "", "13.885736", ""]]}], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "xyseries", "section_heading": "Example", "section_id": "id_1a3f5423_e8b8_4fc6_8813_7a35c18bd5e6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xyseries", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:06.471630+00:00", "version": "10.2"}}
{"id": "ce15641fcafbc574", "content": "Let's walk through an example to learn how to add optional arguments to the xyseries command. Write a search To add the optional arguments of the xyseries command, you need to write a search that includes a split-by-field command for multiple aggregates. Use the sep and format arguments to modify the output field names in your search results. Run this search in the search and reporting app: This search sorts referrer domain, count(host) and count(productId) by clientIp. Run this search in the search and reporting app: In this example: <x-field> = clientip <y-name-field> = referrer domain <y-data-field> = host, productId The xyseries command needs two aggregates, in this example they are: count(host) count(productId). The first few search results look like this: Add optional argument: sep Add a string to the sep argument to change the default character that separates the <y-name-field> host,and the <y-data-field> productId. The format argument adds the <y-name-field> and separates the field name and field value by the default \":\" Run this search in the search and reporting app: The first few search results look like this: Add optional argument: format The format argument adds the <y-name-field> and separates the field name and field value by the default \":\" For example, the default for this example looks like count(host):referrer_domain When you specify a string to separate the <y-name-field> and <y-data-field> with the format argument, it overrides any assignment from the sep argument. In the following example, the sep argument assigns the \"-\" character to separate the <y-name-field> and <y-data-field> fields. The format argument assigns a \"+\" and this assignment takes precedence over sep. In this case $VAL$ and $AGG$ represent both the <y-name-field> and <y-data-field>. As seen in the search results, the <y-name-field>, host, and <y-data-field>, productId can correspond to either $VAL$ or $AGG$. Run this search in the search and reporting app: The first few search results look like this: Add optional argument: grouped The grouped argument determines whether the xyseries command runs as a distributable streaming command , or a transforming command. The default state grouped=FALSE for the xyseries command runs as a streaming command.", "code_examples": [{"language": "spl", "code": "sourcetype=access_combined_wcookie | stats count(host) count(productId) by clientip, referer_domain"}, {"language": "spl", "code": "sourcetype=access_combined_wcookie | stats count(host) count(productId) by clientip, referer_domain  | xyseries clientip referer_domain count(host), count(productId)"}, {"language": "spl", "code": "sourcetype=access_combined_wcookie | stats count(host) count(productId) by clientip, referer_domain  | xyseries clientip referer_domain count(host), count(productId) sep=\"-\""}, {"language": "spl", "code": "sourcetype=access_combined_wcookie | stats count(host) count(productId) by clientip, referer_domain  | xyseries clientip referer_domain count(host), count(productId) sep=\"-\"format=\"$AGG$  + $VAL$ TEST\""}], "tables": [], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "xyseries", "section_heading": "Extended example", "section_id": "id_7b2bcee4_023e_4eee_a64d_cf0d375fa2ea--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xyseries", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:06.471636+00:00", "version": "10.2"}}
{"id": "6e83a0c1e27d7702", "content": "Commands untable", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "xyseries", "section_heading": "See also", "section_id": "id_4c03700c_c126_486c_b795_46211ef4479f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xyseries", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:06.471641+00:00", "version": "10.2"}}
{"id": "0132750c5801c3b9", "content": "Returns the last N number of specified results. The events are returned in reverse order, starting at the end of the result set. The last 10 events are returned if no integer is specified", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "tail", "section_heading": "Description", "section_id": "id_91b9f153_5a74_4ef6_a5d4_43c97c026a12--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tail", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:22.889550+00:00", "version": "10.2"}}
{"id": "84b9c535f023c86f", "content": "tail [<N>] Required arguments None. Optional arguments <N> Syntax: <int> Description: The number of results to return. Default: 10", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "tail", "section_heading": "Syntax", "section_id": "id_0c23847e_1fd6_4d64_850f_75790b685733--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tail", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:22.889559+00:00", "version": "10.2"}}
{"id": "2f7d4ce372cead95", "content": "The tail command is a dataset processing command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "tail", "section_heading": "Usage", "section_id": "df997f15_13bf_495b_aea0_75e474f6d731--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tail", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:22.889563+00:00", "version": "10.2"}}
{"id": "701b42309fd143c2", "content": "Example 1: Return the last 20 results in reverse order.", "code_examples": [{"language": "spl", "code": "... | tail 20"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "tail", "section_heading": "Examples", "section_id": "id_5553ab14_0590_4856_b9b7_76a799485c57--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tail", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:22.889568+00:00", "version": "10.2"}}
{"id": "6c82fbad7602d538", "content": "head , reverse", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "tail", "section_heading": "See also", "section_id": "id_0432ac92_56b0_43c8_91ed_51e2ec321667--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tail", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:22.889571+00:00", "version": "10.2"}}
{"id": "b3403b040b83c77e", "content": "Outputs the contents of the _raw field to the _xml field. The outputtext command was created as an internal mechanism to render event texts for output.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "outputtext", "section_heading": "Description", "section_id": "id_0083e11f_cae5_49c6_8bdb_7d10e8d5b45e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputtext", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:39.900477+00:00", "version": "10.2"}}
{"id": "6de3d6e0c197231f", "content": "outputtext [usexml=<bool>] Optional arguments usexml Syntax: usexml=<bool> Description: If set to true, the copy of the _raw field in the _xml is escaped XML. If usexml is set to false, the _xml field is an exact copy of _raw. Default: true", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "outputtext", "section_heading": "Syntax", "section_id": "id_447dc27e_d735_48c4_ae92_34f3c915e4de--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputtext", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:39.900487+00:00", "version": "10.2"}}
{"id": "ac85c159961263fa", "content": "The outputtext command is a reporting command. The outputtext command writes all search results to the search head. In Splunk Web, the results appear in the Statistics tab.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "outputtext", "section_heading": "Usage", "section_id": "id_24effd99_cd36_4884_87fe_e8a8e271b8a8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputtext", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:39.900493+00:00", "version": "10.2"}}
{"id": "8f18baced5add5b8", "content": "1. Output the _raw field into escaped XML Output the \"_raw\" field of your current search into \"_xml\".", "code_examples": [{"language": "spl", "code": "... | outputtext"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "outputtext", "section_heading": "Examples", "section_id": "cd36603a_b67d_4a9b_bd0e_0735bde0af85--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputtext", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:39.900498+00:00", "version": "10.2"}}
{"id": "dfcd7175924ec5e5", "content": "outputcsv", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "outputtext", "section_heading": "See also", "section_id": "e070f276_f701_4b3d_8281_3269e22279e6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outputtext", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:39.900503+00:00", "version": "10.2"}}
{"id": "8a0253b7da300d35", "content": "Use this command to run a subsearch that includes a template to iterate over the following elements: Each field in a wildcard field list Each value in a single multivalue field A single field representing a JSON array Read use cases from Splunk platform experts about improving Splunk platform searches with the foreach command in the Splunk Lantern Customer Success Center.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "foreach", "section_heading": "Description", "section_id": "id_3089c1c6_a516_4a5a_9b51_fcd7d3cf0a13--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/foreach", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:54.858590+00:00", "version": "10.2"}}
{"id": "60f069266cf73282", "content": "The required syntax is in bold. foreach mode=(auto_collections | multivalue | json_array | multifield) <wildcard-field-list> | <field> [<mode-options>] <subsearch> [<subsearch-options>] Required arguments wildcard-field-list Syntax: <string> ... Description: A space-delimited wildcard field name or list of wildcard field names that is used to iterate over one or more fields in a search. You can use the asterisk ( * ) as a wildcard to specify a list of fields with similar names. For example, if you want to specify all fields that start with \"value\", you can use a wildcard field such as value*. You can also specify a list of wildcard fields, such as hostA* hostB* hostC*. You can use this argument only with the multifield mode. field Syntax: <string> Description: A field name that is used when iterating over elements in a multivalue field or JSON array, using the multivalue mode or json_array mode in a search. subsearch Syntax: [ subsearch ] Description: A subsearch that includes a template for replacing the values of the specified fields, which depends on which mode you are using: auto_collections , multivalue , json_array or multifield. For each field that is matched, the templated subsearch replaces values as follows: Optional arguments mode Syntax: mode=<mode-name> Description: Tells the foreach command to iterate over multiple fields, a multivalue field, or a JSON array. If a mode is not specified, the foreach command defaults to the mode for multiple fields, which is the multifield mode. You can specify one of the following modes for the foreach command: mode options The mode options depend on the mode you use. Use the following options to iterate over a field or list of fields using the multifield mode. These options are available only with the multifield mode. Use the following options to iterate over multivalue fields or JSON arrays using the multivalue mode, the json_array mode, or the auto_collections mode. subsearch options Syntax: <option-name> Description: The subsearch options depend on the mode you use. You can use the following optional template values in a subsearch when iterating over one or more fields. These template values are available only with the multifield mode. You can use the following optional template values in a subsearch to iterate over elements or numbers in a multivalue field or JSON array. These template values are available with all modes except the multifield mode.", "code_examples": [{"language": "spl", "code": "| foreach mode=multivalue itemstr=<<NUMBER>> numbers \n    [ eval result = result + <<NUMBER>>]"}, {"language": "spl", "code": "| foreach mode=multivalue numbers \n    [evalresult = result + <<ITER>>]"}, {"language": "spl", "code": "| foreach mode=multivalue iterstr=<<NUMBER>> numbers \n    [ eval result = result + <<NUMBER>>]"}], "tables": [{"headers": ["Option", "Default template value", "Replacement", "Mode"], "rows": [["fieldstr", "<<FIELD>>", "The whole field name.", "multifield"], ["matchstr", "<<MATCHSTR>>", "The part of the field name that matches the wildcard values in the wildcard field.", "multifield"], ["matchseg1", "<<MATCHSEG1>>", "The part of the field name that matches the first wildcard.", "multifield"], ["matchseg2", "<<MATCHSEG2>>", "The part of the field name that matches the second wildcard.", "multifield"], ["matchseg3", "<<MATCHSEG3>>", "The part of the field name that matches the third wildcard.", "multifield"], ["itemstr", "<<ITEM>>", "Matches each element in a multivalue field or JSON array.", "auto_collections, multivalue, json_array"], ["iterstr", "<<ITER>>", "Matches each number in a multivalue field or JSON array. Use as a placeholder for a zero-based iterator in the multivalue field or JSON array.", "auto_collections, multivalue, json_array"]]}, {"headers": ["Argument", "Syntax", "Description"], "rows": [["auto_collections", "mode=auto_collections", "Dynamically iterates over a JSON array or multivalue field depending on which element is present in the search. When you use this mode, you don't need to know whether your functions return JSON arrays or multivalue fields.Theauto_collectionsmode can only be used with theevalcommand."], ["multivalue", "mode=multivalue", "Iterates over a single supplied multivalue field. Use this mode if your search returns multivalue fields. Add<<ITEM>>to your subsearch as a reference to the iterable object, or add<<ITER>>as a reference to the iterator.Themultivaluemode can only be used with theevalcommand."], ["json_array", "mode=json_array", "Iterates over a single supplied JSON array. Use this mode withJSON functions. Add<<ITEM>>to your subsearch as a reference to the iterable object, or add<<ITER>>as a reference to the iterator.Thejson_arraymode can only be used with theevalcommand."], ["multifield", "mode=multifield", "Iterates over a single supplied field name or a list of multiple field names, which can include wildcard characters.This is the default if themodeis not specified.Themultifieldmode can be used with any streaming command."]]}, {"headers": ["Option", "Syntax", "Description"], "rows": [["fieldstr", "fieldstr=<string>", "A customizable string that replaces the<<FIELD>>template value with the name that you specify. The value of thefieldstroption must match the<<FIELD>>template value. For example, if you change the default<<FIELD>>template value toMYFIELD, then you must also change the value offieldstroption toMYFIELD."], ["matchstr", "matchstr=<string>", "A customizable string that replaces the<<MATCHSTR>>template value with the segment of the field name that matches the wildcard(s) in each field in the list. The value of thematchstroption must match the<<MATCHSTR>>template value. For example, if you change the default<<MATCHSTR>>template value toID, then you must also change the value ofmatchstrtoID.To avoid unpredictable results in searches, do not use thematchstroption with anymatchseg*options."], ["matchseg1", "matchseg1=<string>", "A customizable string that replaces the<<MATCHSEG1>>template value with the segment of the field name that matches the first wildcard in each field in the list. The value of  thematchseg1option must match the<<MATCHSEG1>>template value. For example, if you change the default<<MATCHSEG1>>template value toPHONE, then you must also change the value ofmatchseg1toPHONE.To avoid unpredictable results in searches, do not use thematchseg1option with thematchstroption."], ["matchseg2", "matchseg2=<string>", "A customizable string that replaces the<<MATCHSEG2>>template value with the segment of the field name that matches the second wildcard in each field in the list. The value of thematchseg2option must match the<<MATCHSEG2>>template value. For example, if you change the default<<MATCHSEG2>>template value toPRICE, then you must also change the value ofmatchseg2toPRICE.To avoid unpredictable results in searches, do not use thematchseg2option together with thematchstroption."], ["matchseg3", "matchseg3=<string>", "A customizable string that replaces the<<MATCHSEG3>>template value with the segment of the field name that matches the third wildcard in each field in the list. The value of thematchseg3option must match the<<MATCHSEG3>>template value. For example, if you change the default<<MATCHSEG3>>template value toADDRESS, then you must also change the value ofmatchseg3toADDRESS.To avoid unpredictable results in searches, do not use thematchseg3option together with thematchstroption."]]}, {"headers": ["Option", "Syntax", "Description"], "rows": [["itemstr", "itemstr=<string>", "Replaces the<<ITEM>>template value with each element in a multivalue field or JSON array. Theitemstroption lets you redefine the placeholder for the item.The value of theitemstroption must match the<<ITEM>>template value. For example, if you change the default<<ITEM>>template value toNUMBER, then you must also change the value ofitemstrtoNUMBER, like this:| foreach mode=multivalue itemstr=<<NUMBER>> numbers \n    [ eval result = result + <<NUMBER>>]"], ["iterstr", "iterstr=<string>", "Replaces the<<<ITER>>template value with the index value in a multivalue field or JSON array. Theiterstroption lets you redefine the placeholder for the iterator, which is the count or index.The value of theiterstroption must match the<<ITER>>template value. For example, if you change the default<<ITER>>template value toNUMBER, then you must also change the value ofiterstrtoNUMBER.For example, consider the following search:| foreach mode=multivalue numbers \n    [evalresult = result + <<ITER>>]That search is equivalent to this search:| foreach mode=multivalue iterstr=<<NUMBER>> numbers \n    [ eval result = result + <<NUMBER>>]"]]}, {"headers": ["Template value", "Description"], "rows": [["<<FIELD>>", "A customizable string replacement for each field name in the field list. Each time you run a subsearch, this value is used to replace the whole field name in thefieldstroption for each field you specify. For example, if you change the value of<<FIELD>>toMYFIELD, then the value of thefieldstroption is alsoMYFIELD. Your search might look like this:...|foreach test* fieldstr=MYFIELD  [eval total=total + MYFIELD]."], ["<<MATCHSTR>>", "A customizable string replacement that represents wildcards in each matching field name in the list. For example, if the wildcard field that is being matched istest*and the field name istest8, then the value of<<MATCHSTR>>is 8.To avoid unpredictable results in searches, do not use the<<MATCHSTR>>template value together with any<<MATCHSEG*>>template values."], ["<<MATCHSEG1>>", "A customizable string replacement for thesegmentof the field name that matches the first segment before the first wildcard in each matching field name in the list.To avoid unpredictable results in searches, do not use the<<MATCHSEG1>>template value with the<<MATCHSTR>>template value."], ["<<MATCHSEG2>>", "A customizable string replacement for the segment of the field name that matches the second segment before the second wildcard in each matching field name in the list.To avoid unpredictable results in searches, do not use the<<MATCHSEG2>>template value with the <<<MATCHSTR>>template value."], ["<<MATCHSEG3>>", "A customizable string replacement for the segment of the field name that matches the third segment before the third wildcard in each matching field name in the list.To avoid unpredictable results in searches, do not use the<<MATCHSEG3>>template value with the<<MATCHSTR>>template value."]]}, {"headers": ["Template value", "Description"], "rows": [["<<ITEM>>", "A customizable string that replaces the <<ITEM>> template value with some other string that is substituted with the contents of the multivalue field or JSON array being iterated over. Only a singleevalstatement is permitted in the search pipeline when using this template value with themultivaluemode orjson_arraymode."], ["<<ITER>>", "A customizable string that replaces the <<ITER>> template value with some other string that is substituted with the contents of the multivalue field or JSON array being iterated over. Only a singleevalstatement is permitted in the search pipeline when using this template value with themultivaluemode orjson_arraymode."]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "foreach", "section_heading": "Syntax", "section_id": "id_8397b23a_4351_44ba_a3f1_8d3613a5a1db--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/foreach", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:54.858627+00:00", "version": "10.2"}}
{"id": "145b0cedaa315064", "content": "The foreach command is a streaming command. You can use the foreach command in the following ways: To obtain results across multiple fields in each result row. This is useful, for example, when you need to calculate the average or sum of a value across multiple columns in each row. If you want to iterate over one or more matching fields, use the multifield mode. To iterate over multiple values within a single row's field in multivalue fields or JSON arrays. This is useful, for example, when you need to concatenate strings or calculate the average or sum of a set of numbers in a single field across multiple columns in each row in a multivalue field or JSON array. If you want to iterate over a multivalue field, use the multivalue mode. If you want to iterate over a JSON array node, use the json_array mode. The default <<ITEM>> template value should be used only when the mode is multivalue or json_array. Supported commands The foreach command with mode=multifield supports searches with any streaming command. The foreach command with all other modes ( mode=auto_collections , mode=multivalue , or mode=json_array ) only supports searches with the eval command. Iterating over multiple matching fields containing nonalphanumeric characters If the field names contain characters other than alphanumeric characters, such as dashes, underscores, or periods, enclose the <<FIELD>> template value in single quotation marks in the right side of the eval command portion of the search to avoid unpredictable results. For example, the following search uses the default foreach multifield mode and adds the values from all of the fields that match myfield_*. The search results look something like this: The <<FIELD>> template value in the foreach subsearch is just a string replacement of the field named myfield_*. The eval expression does not recognize field names with nonalphanumeric characters unless the field names are surrounded by single quotation marks. For the eval expression to work, the <<FIELD>> template value must be surrounded by single quotation marks. Support for multiple eval statements If you need to include multiple eval statements with the foreach command, use the default multifield mode. Multiple eval statements are not supported in foreach searches that use multivalue mode or JSON array mode. As a result, your searches on multivalue fields or JSON arrays must contain only a single eval statement in the pipeline. However, your eval statement can include as many assignments as you want. For example, the following multivalue search with multiple eval assignments completes successfully because there is only one eval statement, which means there aren't any piped commands following the eval command. The search results look something like this. Wildcards are not supported in multivalue fields or JSON arrays Unlike multifield mode, the modes for multivalue fields and JSON arrays don't support wildcards in search expressions. Instead, these modes treat a wildcard as part of the field name. For example, the following search includes a field called mv* , which looks like a wildcarded field. However, multivalue and JSON array modes don't recognize the wildcard and add up all of the fields containing mv. As a result, the search results for multivalue mode look something like this: Elements of the same type in multivalue fields or JSON arrays Elements in a subsearch and the eval expression must be of the same type as either strings or numbers. For example, the following search correctly adds up the three numbers in the JSON array because all of the elements are numbers. However, the following search results in an error because adding a number to a string isn't allowed.", "code_examples": [{"language": "spl", "code": "| makeresults\n|evalmyfield_1 = 5, myfield_2 = 10\n| foreach myfield_* \n       [eval<<FIELD>> = '<<FIELD>>' + <<MATCHSTR>>]"}, {"language": "spl", "code": "| makeresults\n|evalmv=mvappend(\"5\",\"15\"), total = 0, count = 0\n| foreach mode=multivalue mv\n     [evaltotal = total + <<ITEM>>, count = count + 1]"}, {"language": "spl", "code": "| makeresults \n|evalmv1=mvappend(\"1\",\"2\"), mv2=mvappend(\"3\",\"4\"), mv*=mvappend(\"100\",\"300\"), total = 0\n| foreach mode=multivalue mv* \n     [evaltotal = total + <<ITEM>>]"}, {"language": "spl", "code": "| foreach json_array(1, 2, 3) \n     [evaltotal = total + <<ITEM>>]"}, {"language": "spl", "code": "| foreach json_array(1, 2,\"hello\") \n     [evaltotal = total + <<ITEM>>]"}], "tables": [{"headers": ["_time", "myfield_1", "myfield_2"], "rows": [["2023-3-14 15:55:50", "6", "12"]]}, {"headers": ["_time", "count", "mv", "total"], "rows": [["2022-03-29 19:52:38", "2", "515", "20"]]}, {"headers": ["_time", "mv*", "mv1", "mv2", "total"], "rows": [["2022-4-20 15:55:50", "100300", "12", "34", "400"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "foreach", "section_heading": "Usage", "section_id": "id_6e26743b_1a2e_4e51_9356_5faa128643c9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/foreach", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:54.858640+00:00", "version": "10.2"}}
{"id": "bb33526af34f0fe9", "content": "1. Generate a total for each row in search results Suppose you have events that contain the following data: Use the foreach command with the default multifield mode to iterate over each field that starts with www and generate a total for each row in the search results. The results look like this: 2. Add the values from all fields that start with similar names The following search adds the values from all of the fields that start with similar names and match the wildcard field test*. It uses the foreach command with the default multifield mode The results of the search look something like this. This search creates one result using the makeresults command. The search then uses the eval command to create the fields total , test1 , test2 , and test3 with corresponding values. The foreach command is used to perform the subsearch for every field that starts with \"test\". Each time the subsearch is run, the previous total is added to the value of the test field to calculate the new total. The final total after all of the test fields are processed is 6. The following table shows how the subsearch iterates over each test field. The table shows the beginning value of the total field each time the subsearch is run and the calculated total based on the value for the test field. 3. Iterate over fields using the eval and foreach commands The eval command and foreach command can be used in similar ways. For example, this search uses the eval command: It is equivalent to this search that uses the foreach command with the default multifield mode: The results of both searches look something like this: 4. Monitor license usage Use the foreach command to monitor license usage. First run the following search on the license manager to return the daily license usage per source type in bytes: The search results for one user across several days looks something like this: You can also use the foreach command with the default multifield mode to calculate the daily license usage in gigabytes for each field: This time the search results look something like this: 5. Use the MATCHSTR template value In this example, the <<FIELD>> template value is a placeholder for test , and the <<MATCHSTR>> template value represents the wildcarded value that follows test in each field name in the eval expression. This example uses the default multifield mode. The results look something like this: The value of each field is added to the value that replaces the wildcard in the field name. For example, for the test1 field, 5 + 1 = 6. 6. Use the MATCHSEG1 and MATCHSEG2 template values This example uses the default multifield mode. The matchseg1 and matchseg2 options are used to add each field value to the two values represented by the wildcard in the corresponding <<MATCHSEG1>> and <<MATCHSEG2>> template values. Let's take a closer look at the syntax for the test1ab2=5 eval expression: The wildcard field is test*ab*. The <<FIELD>> template value called MYFIELD is test1ab2=5. The field value is 5. The <<MATCHSEG1>> template value called SEG1 is 1. The <<MATCHSEG2>> template value called SEG2 is 2. The results of the search look something like this: The value of the test1ab2 field in the search results is 8 because 5 + 1 + 2 = 8. 7. Add values in a multivalue field using the auto_collections mode In this example using the auto_collections mode, <<ITEM>> is a placeholder for each number in the multivalue field, which is added to the total. The results of the search look something like this. The previous search produces similar results as the following eval search, which also displays the total, but without listing each of the values that make up the total. The search results look like this: 8. Calculate grade averages using multivalue fields To find the average of a set of student grades using the multivalue mode, you could run this search: The search results look something like this: 9. Add values in a JSON array If you want to do something simple like add up each element in a JSON array, you could run a search like this: The search results look like this: 10. Categorize employees by manager using multivalue fields You can create lists of employee names and organize them by manager using the eval command or the foreach command. This is an example of a search on employees and their manager using the eval command: The results of the eval search look something like this. To create multivalue fields of employee names and organize them by manager, you can run a similar search using multivalue fields with the foreach command: The search results this time look like this: 11. Add values to a JSON array Now let's take the names of the employees in a multivalue field and append them to a JSON array. In this search, the employees_array is empty. The search results look like this: To copy all the values from the multivalue field into json_array() , use foreach to iterate over the employees values and append each of the employee names to the array, like this search: Now the search results look like this: The foreach command just copied over each of the employees' names to the JSON array. 12. Extracting values from a JSON array What if you want to extract values for given key names from a JSON array and do something with them? For example, the following search extracts a list of employee IDs from a JSON array of employees and puts them in a new field called ID_array that you can use for other operations. The results of this search look something like this: 13. Multiplying elements in a JSON array You can use the foreach command to multiply numbers and append to a JSON array in searches like this: The results look something like this: This search doubles each value in the array in price and then adds the values to a new array called double_price. 14. Calculating weights To find the weights of values in a JSON array called grades , you could run a search like this: The search results look something like this: 15. Iterate over multivalue fields and concatenate the values The following example iterates over multivalue fields and concatenates the values, so that each letter is added to the index value in the search results. The search results look like this:", "code_examples": [{"language": "spl", "code": "...|evaltotal=0\n| foreach www* \n     [evaltotal=total + <<FIELD>>]"}, {"language": "spl", "code": "| makeresults\n|evaltotal=0, test1=1, test2=2, test3=3 \n| foreachtest* \n     [evaltotal=total + <<FIELD>>]"}, {"language": "spl", "code": "| makeresults\n|evalname=\"name\"|evalprice=\"price\"|evalcategory=\"category\""}, {"language": "spl", "code": "| makeresults \n| foreach name price category \n     [eval<<FIELD>> = \"<<FIELD>>\"]"}, {"language": "spl", "code": "index=_internalsource=*license_usage.logtype!=\"*Summary\"earliest=-30d\n| timechart span=1d sum(b) AS daily_bytes by st"}, {"language": "spl", "code": "index=_internalsource=*license_usage.logtype!=\"*Summary\"earliest=-30d\n| timechart span=1d sum(b) AS daily_bytes by st\n| foreach * [eval<<FIELD>>='<<FIELD>>'/1024/1024/1024]"}, {"language": "spl", "code": "| makeresults\n|evaltest1 = 5, test2 = 10\n| foreachtest* [eval<<FIELD>> = <<FIELD>> + <<MATCHSTR>>]"}, {"language": "spl", "code": "| makeresults\n|evaltest1ab2=5, test2ab3=10\n| foreachtest*ab* fieldstr=MYFIELD matchseg1=SEG1 matchseg2=SEG2 \n     [evalMYFIELD = MYFIELD + SEG1 + SEG2]"}, {"language": "spl", "code": "| makeresults \n|evalmvfield=mvappend(\"1\",\"2\",\"3\"), total=0 \n| foreach mode=auto_collections\n     [evaltotal = total + <<ITEM>>] \n| table mvfield, total"}, {"language": "spl", "code": "| makeresults\n|evaltotal = 0\n|evaltotal = total + 1 \n|evaltotal = total + 2 \n|evaltotal = total + 3"}, {"language": "spl", "code": "| makeresults \n|evalteacher=\"James\", student_grades=mvappend(\"50\",\"100\",\"30\"), sum = 0, count = 0\n| foreach mode=multivalue student_grades\n    [evalsum = sum + <<ITEM>>, count = count + 1]\n| eval average = sum / count"}, {"language": "spl", "code": "| makeresults \n|evaljsonfield=json_array(1, 2, 3), total=0 \n| foreach mode=json_array jsonfield \n     [evaltotal = total + <<ITEM>>] \n| table jsonfield, total"}, {"language": "spl", "code": "| makeresults \n|evalmanager=\"Rutherford\", employees=mvappend(\"Alex\",\"Claudia\",\"David\")\n| fields - _time"}, {"language": "spl", "code": "| makeresults \n|evalmanager=\"Rutherford\", employees=mvappend(\"Alex\",\"Claudia\",\"David\"), employees_array=json_array()\n| foreach mode=multivalue employees \n     [evalemployees_array=json_append(employees_array,\"\", <<ITEM>>)]\n| fields - _time"}, {"language": "spl", "code": "| makeresults \n|evalmanager=\"Rutherford\", employees=mvappend(\"Alex\",\"Claudia\",\"David\"), employees_array=json_array()\n| fields - _time"}, {"language": "spl", "code": "| makeresults \n|evalmanager=\"Rutherford\", employees=mvappend(\"Alex\",\"Claudia\",\"David\"), employees_array=json_array()\n| foreach mode=multivalue employees \n     [evalemployees_array=json_append(employees_array,\"\", <<ITEM>>)]\n| fields - _time"}, {"language": "spl", "code": "| makeresults \n|evalmanager=\"Rutherford\", employees=mvappend(\"Alex\",\"Claudia\",\"David\"), ID_array=json_array(), IDs=json_object(\"Alex\", 4125,\"Claudia\", 2538,\"David\", 3957)\n| foreach mode=multivalue employees \n     [evalID_array=json_append(ID_array,\"\", json_extract(IDs, <<ITEM>>))]\n| fields - _time"}, {"language": "spl", "code": "| makeresults \n|evalprice=json_array(1,2,3,4), double_price=json_array()\n| foreach mode=json_array price \n    [evaldouble_price = json_append(double_price,\"\",  <<ITEM>> * 2)]"}, {"language": "spl", "code": "| makeresults \n|evalgrades=json_array(1,2,3,4), weight=json_array()\n|evalsum = 0\n| foreach mode=json_array grades \n    [evalsum = sum + <<ITEM>>]\n| foreach mode=json_array grades\n    [eval weight = json_append(weight, \"\", <<ITEM>> / sum)]"}, {"language": "spl", "code": "|makeresults\n|fields - _time\n|evalword=split(\"ABCDE\",\"\"), nums=split(\"01234\",\"\")\n|foreach word mode=multivalue [evalword_and_num=mvappend(word_and_num, <<ITEM>>.mvindex(nums, <<ITER>>))]"}], "tables": [{"headers": ["categoryId", "www1", "www2"], "rows": [["ACCESSORIES", "1000", "500"], ["SIMULATION", "3000", "750"], ["ARCADE", "800", ""], ["STRATEGY", "400", "200"]]}, {"headers": ["categoryId", "www1", "www2", "total"], "rows": [["ACCESSORIES", "1000", "500", "1500"], ["SIMULATION", "3000", "750", "3750"], ["ARCADE", "800", "", "800"], ["STRATEGY", "400", "200", "600"]]}, {"headers": ["_time", "test1", "test2", "test3", "total"], "rows": [["2022-4-20 15:55:50", "1", "2", "3", "6"]]}, {"headers": ["Subsearch iteration", "test field", "total field start value", "test field value", "calculation of total field"], "rows": [["1", "test1", "0", "1", "0+1=1"], ["2", "test2", "1", "2", "1+2=3"], ["3", "test3", "3", "3", "3+3=6"]]}, {"headers": ["_time", "category", "name", "price"], "rows": [["2022-4-20 15:55:50", "category", "name", "price"]]}, {"headers": ["_time", "csv", "universal_data_json"], "rows": [["2022-04-03", "8308923", "36069628"], ["2022-04-04", "7290647", "48851560"], ["2022-04-05", "7676935", "12542231"], ["2022-04-06", "3016517", "17521059"]]}, {"headers": ["_time", "csv", "universal_data_json"], "rows": [["2022-04-03", "0.2335968237849277853", "0.6309081468478106"], ["2022-04-04", "0.9703636813411461080", "0.4818287762321547"], ["2022-04-05", "0.3825212419915378210", "0.9126501722671725"], ["2022-04-06", "0.0028093503788113594", "0.0163177577778697"]]}, {"headers": ["_time", "test1", "test2"], "rows": [["2022-03-28 15:43:39", "6", "12"]]}, {"headers": ["_time", "test1ab2", "test2ab3"], "rows": [["2022-03-28 17:10:56", "8", "15"]]}, {"headers": ["_time", "mvfield", "total"], "rows": [["2024-4-20 15:55:50", "123", "6"]]}, {"headers": ["_time", "total"], "rows": [["2024-4-20 15:55:50", "6"]]}, {"headers": ["_time", "average", "count", "student_grades", "sum", "teacher"], "rows": [["2022-03-21 16:02:30", "60", "3", "5010030", "180", "James"]]}, {"headers": ["jsonfield", "total"], "rows": [["[1, 2, 3]", "6"]]}, {"headers": ["employees", "manager"], "rows": [["AlexClaudiaDavid", "Rutherford"]]}, {"headers": ["employees", "employees_array", "manager"], "rows": [["AlexClaudiaDavid", "[\"Alex\", \"Claudia\", \"David\"]", "Rutherford"]]}, {"headers": ["employees", "employees_array", "manager"], "rows": [["AlexClaudiaDavid", "[ ]", "Rutherford"]]}, {"headers": ["employees", "employees_array", "manager"], "rows": [["AlexClaudiaDavid", "[\"Alex\", \"Claudia\", \"David\"]", "Rutherford"]]}, {"headers": ["ID_array", "IDs", "employees", "manager"], "rows": [["[4125,2538,3957]", "{\"Alex\":4125,\"Claudia\":2538,\"David\":3957}", "AlexClaudiaDavid", "Rutherford"]]}, {"headers": ["_time", "double_price", "price"], "rows": [["2022-03-21 16:24:49", "[2,4,6,8]", "[1,2,3,4]"]]}, {"headers": ["_time", "grades", "sum", "weight"], "rows": [["2022-03-31 12:58:16", "[1,2,3,4]", "10", "[0.1,0.2,0.3,0.4]"]]}, {"headers": ["nums", "word", "word_and_num"], "rows": [["0", "A", "A0"], ["1", "B", "B1"], ["2", "C", "C2"], ["3", "D", "D3"], ["4", "E", "E4"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "foreach", "section_heading": "Examples", "section_id": "dfd8e103_39e7_4716_9d5a_26e2e37593cc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/foreach", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:54.858696+00:00", "version": "10.2"}}
{"id": "21f05e17c41777eb", "content": "Commands eval , map Related information Evaluate and manipulate fields with multiple values JSON functions", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "foreach", "section_heading": "See also", "section_id": "id_87cf0ec5_2343_4b9b_8f1a_61b9e0524dcf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/foreach", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:10:54.858701+00:00", "version": "10.2"}}
{"id": "36a0b3b9116d88cf", "content": "Use the sendemail command to generate email notifications. You can email search results to specified email addresses. You must have a Simple Mail Transfer Protocol (SMTP) server available to send email. An SMTP server is not included with the Splunk instance. CAUTION: This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "sendemail", "section_heading": "Description", "section_id": "abfc4a1e_1bbf_414c_a2e7_5e463d1163f2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sendemail", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:13.135153+00:00", "version": "10.2"}}
{"id": "614d4b3469a94b7f", "content": "The required syntax is in bold : sendemail to=<email_list> [from=<email_list>] [cc=<email_list>] [bcc=<email_list>] [subject=<string>] [format=csv | table | raw] [inline= <bool>] [sendresults=<bool>] [sendpdf=<bool>] [priority=highest | high | normal | low | lowest] [server=<string>] [width_sort_columns=<bool>] [graceful=<bool>] [content_type=html | plain] [message=<string>] [sendcsv=<bool>] [use_ssl=<bool>] [use_tls=<bool>] [pdfview=<string>] [papersize=letter | legal | ledger | a2 | a3 | a4 | a5] [paperorientation=portrait | landscape] [maxinputs=<int>] [maxtime=<int> m | s | h | d] [footer=<string>] Required arguments to Syntax: to=<email_list> Description: List of email addresses to send search results to. Specify email addresses in a comma-separated and quoted list. For example: \"alex@email.com, maria@email.com, wei@email.com\" Note: The set of domains to which you can send emails can be restricted by the Allowed Domains setting on the Email Settings page. For example, that setting could restrict you to sending emails only to addresses in your organization's email domain. For more information, see Email notification action in the Alerting Manual. Optional arguments bcc Syntax: bcc=<email_list> Description: Blind courtesy copy line. Specify email addresses in a comma-separated and quoted list. cc Syntax: cc=<email_list> Description: Courtesy copy line. Specify email addresses in a comma-separated and quoted list. content_type Syntax: content_type=html | plain Description: The format type of the email. Default: The default value for the content_type argument is set in the [email] stanza of the alert_actions.conf file. The default value for a new or upgraded Splunk installation is html. format Syntax: format=csv | raw | table Description: Specifies how to format inline results. Default: The default value for the format argument is set in the [email] stanza of the alert_actions.conf file. The default value for a new or upgraded Splunk installation is table. footer Syntax: footer=<string> Description: Specify an alternate email footer. Default: The default footer is: If you believe you've received this email in error, please see your Splunk administrator. splunk > the engine for machine data. Note: To force a new line in the footer, use Shift+Enter. from Syntax: from=<email_list> Description: Email address from line. Default: \"splunk@<hostname>\" inline Syntax: inline=<boolean> Description: Specifies whether to send the results in the message body or as an attachment. By default, an attachment is provided as a CSV file. See the Usage section. Default: The default value for the inline argument is set in the [email] stanza of the alert_actions.conf file. The default value for a new or upgraded Splunk installation is false. graceful Syntax: graceful=<boolean> Description: If set to true, no error is returned if sending the email fails for whatever reason. The remainder of the search continues as if the the sendemail command was not part of the search. If graceful=false and sending the email fails, the search returns an error. Default: false maxinputs Syntax: maxinputs=<integer> Description: Sets the maximum number of search results sent via alerts per invocation of the command. The sendemail command is invoked repeatedly in increments according to the maxinputs argument until the search is complete and all of the results have been displayed. Do not change the value of maxinputs unless you know what you are doing. Default: 50000 maxtime Syntax: maxtime=<integer>m | s | h | d Description: The maximum amount of time that the execution of an action is allowed to take before the action is aborted. Example: 2m Default: no limit message Syntax: message=<string> Description: Specifies the message sent in the email. Default: The default message depends on which other arguments are specified with the sendemail command. If sendresults=false the message defaults to \"Search complete.\" If sendresults=true, inline=true, and either sendpdf=false or sendcsv=false, message defaults to \"Search results.\" If sendpdf=true or sendcsv=true, message defaults to \"Search results attached.\" paperorientation Syntax: paperorientation=portrait | landscape Description: The orientation of the paper. Default: portrait papersize Syntax: papersize=letter | legal | ledger | a2 | a3 | a4 | a5 Description: Default paper size for PDFs. Acceptable values: letter, legal, ledger, a2, a3, a4, a5. Default: letter pdfview Syntax: pdfview=<string> Description: Name of a view.xml file to send as a PDF. For example, mydashboard.xml , search.xml , or foo.xml. Generally this is the name of a dashboard, but it could also be the name of a single page application or some other object. Specify the name only. Do not specify the filename extension. The view.xml files are located in <<SPLUNK_HOME>/etc/apps/<app_name>/default/data/ui/views. priority Syntax: priority=highest | high | normal | low | lowest Description: Set the priority of the email as it appears in the email client. Lowest or 5, low or 4, high or 2, highest or 1. Default: normal or 3 sendcsv Syntax: sendcsv=<boolean> Description: Specify whether to send the results with the email as an attached CSV file or not. Default: The default value for the sendcsv argument is set in the [email] stanza of the alert_actions.conf file. The default value for a new or upgraded Splunk installation is false. sendpdf Syntax: sendpdf=<boolean> Description: Specify whether to send the results with the email as an attached PDF or not. For more information about generating PDFs, see \"Generate PDFs of your reports and dashboards\" in the Reporting Manual. Default: The default value for the sendpdf argument is set in the [email] stanza of the alert_actions.conf file. The default value for a new or upgraded Splunk installation is false. sendpng Syntax : sendpng=<boolean> Description : Specify whether to send the results with the email as an attached PNG or not. sendpng is only available for usage with Dashboard Studio. For more details, see the Splunk Dashboard Studio manual. Default : The default value for the sendpng argument is set in the [email] stanza of the alert_actions.conf file. The default value for a new or upgraded Splunk installation is false. sendresults Syntax: sendresults=<boolean> Description: Determines whether the results should be included with the email. See the Usage section. Default: The default value for the sendresults argument is set in the [email] stanza of the alert_actions.conf file. The default value for a new or upgraded Splunk installation is false. server Syntax: server=<host>[:<port>] Description: If the SMTP server is not local, use this argument to specify the SMTP mail server to use when sending emails. The <host> can be either the hostname or the IP address. You have the option to specify the SMTP <port> that the Splunk instance should connect to. If you set use_ssl=true , you must specify both <host> and <port> in the server argument. This setting takes precedence over the mailserver setting in the alert_actions.conf file. The default setting for mailserver is localhost:25. Note: If an alert action is configured to send an email notification when an alert triggers, the sendemail command might not be able to use the server you specify in the server argument. The values in the Email domains setting on the Email Settings page might restrict the server you can use. The sendemail command uses the Mail host that is set on the Email Settings page. For more information, see Email notification action in the Alerting Manual. Default: localhost subject Syntax: subject=<string> Description: Specifies the subject line. Default: \"Splunk Results\" use_ssl Syntax: use_ssl=<boolean> Description: Specifies whether to use SSL when communicating with the SMTP server. When set to true , you must also specify both the <host> and <port> in the server argument. Default: false use_tls Syntax: use_tls=<boolean> Description: Specify whether to use TLS (transport layer security) when communicating with the SMTP server (starttls). Default: false width_sort_columns Syntax: width_sort_columns=<boolean> Description: This is only valid for plain text emails. Specifies whether the columns should be sorted by their width. Default: true", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "sendemail", "section_heading": "Syntax", "section_id": "fb7398e0_b323_47a9_a01f_397341435865--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sendemail", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:13.135160+00:00", "version": "10.2"}}
{"id": "22fadb3ffd3e849f", "content": "If you set sendresults=true and inline=false and do not specify format , a CSV file is attached to the email. Note: If you use fields as tokens in your sendemail messages, use the rename command to remove curly brace characters such as { and } from them before they are processed by the sendemail command. The sendemail command cannot interpret curly brace characters when they appear in tokens such as $results$. Capability requirements To use sendemail , your role must have the schedule_search and list_settings capabilities.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "sendemail", "section_heading": "Usage", "section_id": "d27e73b6_42f6_4ea6_9189_a2ee7b2c2cd4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sendemail", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:13.135165+00:00", "version": "10.2"}}
{"id": "f6fb36f708e3ff6f", "content": "1: Send search results to the specified email Send search results to the specified email. By default, the results are formatted as a table. 2: Send search results in raw format Send search results in a raw format with the subject \"myresults\". 3. Include a PDF attachment, a message, and raw inline results Send an email notification with a PDF attachment, a message, and raw inline results. 4: Use email notification tokens with the sendemail command You can use the eval command in conjunction with email notification tokens to customize your search results emails. The search in the following example sends an email to sample@splunk.com with a custom message that says sample sendemail message body. See Use tokens in email notifications in the Splunk Cloud Platform Alerting Manual .", "code_examples": [{"language": "spl", "code": "... | sendemail to=\"elvis@splunk.com\"sendresults=true"}, {"language": "spl", "code": "... | sendemail to=\"elvis@splunk.com,john@splunk.com\"format=raw subject=myresults server=mail.splunk.com sendresults=true"}, {"language": "spl", "code": "index=_internal | head 5 | sendemail to=example@splunk.com server=mail.example.com subject=\"Here is an email from Splunk\"message=\"This is an example message\"sendresults=trueinline=trueformat=raw sendpdf=true"}, {"language": "spl", "code": "|makeresults\n|evalcustommessage=\"sample sendemail message body\"|evaldest=\"sample@splunk.com\"|sendemail to=\"$result.dest$\"message=\"$result.custommessage$\""}], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "sendemail", "section_heading": "Examples", "section_id": "id_7b0ac612_4f27_434f_811d_0e9f9b483db6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sendemail", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:13.135170+00:00", "version": "10.2"}}
{"id": "af8b709ba43b7f93", "content": "Find events in a summary index that overlap in time, or find gaps in time during which a scheduled saved search might have missed events. If you find a gap, run the search over the period of the gap and summary index the results using \"| collect\". If you find overlapping events, manually delete the overlaps from the summary index by using the search language. The overlap command invokes an external python script $SPLUNK_HOME/etc/apps/search/bin/ sumindexoverlap.py. The script expects input events from the summary index and finds any time overlaps and gaps between events with the same 'info_search_name' but different 'info_search_id'. Important: Input events are expected to have the following fields: 'info_min_time', 'info_max_time' (inclusive and exclusive, respectively) , 'info_search_id' and 'info_search_name' fields. If the index contains raw events (_raw), the overlap command does not work. Instead, the index should contain events such as chart , stats , and timechart results.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "overlap", "section_heading": "Description", "section_id": "id_762c3c33_e978_49e0_ad61_a73319249145--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/overlap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:30.573105+00:00", "version": "10.2"}}
{"id": "355b0fb172059557", "content": "overlap", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "overlap", "section_heading": "Syntax", "section_id": "id_92fbce57_335f_4f3f_af83_412d8cff9221--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/overlap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:30.573112+00:00", "version": "10.2"}}
{"id": "0e71cf49b9cad3c4", "content": "Example 1: Find overlapping events in the \"summary\" index.", "code_examples": [{"language": "spl", "code": "index=summary | overlap"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "overlap", "section_heading": "Examples", "section_id": "id_9d54b716_7fed_49cc_975d_7b3c4198925c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/overlap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:30.573117+00:00", "version": "10.2"}}
{"id": "d77813eea7157bd2", "content": "collect , sistats , sitop , sirare , sichart , sitimechart", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "overlap", "section_heading": "See also", "section_id": "dcf032d6_c74e_4cf0_a6ae_6d33c4004500--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/overlap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:30.573121+00:00", "version": "10.2"}}
{"id": "8ef4a2060f45802f", "content": "Examine and search data model datasets. Use the datamodel command to return the JSON for all or a specified data model and its datasets. You can also search against the specified data model or a dataset within that datamodel. A data model is a hierarchically-structured search-time mapping of semantic knowledge about one or more datasets. A data model encodes the domain knowledge necessary to build a variety of specialized searches of those datasets. For more information, see About data models and Design data models in the Knowledge Manager Manual. The datamodel search command lets you search existing data models and their datasets from the search interface. The datamodel command is a generating command and should be the first command in the search. Generating commands use a leading pipe character.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "datamodel", "section_heading": "Description", "section_id": "id_769f0a10_71f6_42a1_9dd4_97c048ffdbcd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/datamodel", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:45.847025+00:00", "version": "10.2"}}
{"id": "835a1b180ba968c8", "content": "| datamodel [<data model name>] [<dataset name>] [<data model search mode>] [strict_fields=<bool>] [allow_old_summaries=<bool>] [summariesonly=<bool>] Required arguments None Optional arguments data model name Syntax: <string> Description: The name of the data model to search. When only the data model is specified, the search returns the JSON for the single data model. dataset name Syntax: <string> Description: The name of a data model dataset to search. Must be specified after the data model name. The search returns the JSON for the single dataset. data model search mode Syntax: <data model search result mode> | <data model search string mode> Description: You can use datamodel to run a search against a data model or a data model dataset that returns either results or a search string. If you want to do this, you must provide a <data model search mode>. There are two <data model search mode> subcategories: modes that return results and modes that return search strings. See Data model search mode options. allow_old_summaries Syntax: allow_old_summaries=<bool> Description: This argument applies only to accelerated data models. When you change the constraints that define a data model but the Splunk software has not fully updated the summaries to reflect that change, the summaries may have some data that matches the old definition and some data that matches the new definition. By default, allow_old_summaries = false , which means that the search head does not use summary directories that are older than the new summary definition. This ensures that the datamodel search results always reflect your current configuration. When you set allow_old_summaries = true , datamodel uses both current summary data and summary data that was generated prior to the definition change. You can set allow_old_summaries=true in your search if you feel that the old summary data is close enough to the new summary data that its results are reliable. Default: false summariesonly Syntax: summariesonly=<bool> Description: This argument applies only to accelerated data models. When set to false, the datamodel search returns both summarized and unsummarized data for the selected data model. When set to true, the search returns results only from the data that has been summarized in TSIDX format for the selected data model. You can use this argument to identify what data is currently summarized for a given data model, or to ensure that a particular data model search runs efficiently. Default: false strict_fields Syntax: strict_fields=<bool> Description: Determines the scope of the datamodel search in terms of fields returned. When strict_fields=true , the search returns only default fields and fields that are included in the constraints of the specified data model dataset. When strict_fields=false , the search returns all fields defined in the data model, including fields inherited from parent data model datasets, extracted fields, calculated fields, and fields derived from lookups. You can also arrange for strict_fields to default to false for a specific data model. See Design data models in the Knowledge Manager Manual. Default: true Data model search mode options data model search result mode Syntax: search | flat | acceleration_search Description: The modes for running searches on a data model or data model dataset that return results. data model search string mode Syntax: search_string | flat_string | acceleration_search_string Description: These modes return the strings for the searches that the Splunk software is actually running against the data model when it runs your SPL through the corresponding <data model search result mode>. For example, if you choose acceleration_search_string , the Splunk software returns the search string it would actually use against the data model when you run your SPL through acceleration_search mode.", "code_examples": [], "tables": [{"headers": ["Mode", "Description"], "rows": [["search", "Returns the search results exactly how they are defined."], ["flat", "Returns the same results as thesearch, except that it strips the hierarchical information from the field names. For example, wheresearchmode might return a field nameddmdataset.server, theflatmode returns a field namedserver."], ["acceleration_search", "Runs the search that the search head uses to accelerate the data model. This mode works only on root event datasets and root search datasets that only use streaming commands."]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "datamodel", "section_heading": "Syntax", "section_id": "id_404dc00f_b41e_4110_b453_8d4b930c18b4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/datamodel", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:45.847036+00:00", "version": "10.2"}}
{"id": "0ac7ec30174e1281", "content": "The datamodel command is a report-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "datamodel", "section_heading": "Usage", "section_id": "id_9be7b762_6d3a_4e1a_b61d_f943e3dd3e53--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/datamodel", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:45.847042+00:00", "version": "10.2"}}
{"id": "5de7b37af6d8e06f", "content": "1. Return the JSON for all data models Return JSON for all data models available in the current app context. 2. Return the JSON for a specific datamodel Return JSON for the Splunk's Internal Audit Logs - SAMPLE data model, which has the model ID internal_audit_logs. 3. Return the JSON for a specific dataset Return JSON for Buttercup Games's Client_errors dataset. 4. Run a search on a specific dataset Run the search for Buttercup Games's Client_errors. 5. Run a search on a dataset for specific criteria Search Buttercup Games's Client_errors dataset for 404 errors and count the number of events. 6. For an accelerated data model, reveal what data has been summarized over a selected time range After the Tutorial data model is accelerated, this search uses the summariesonly argument in conjunction with timechart to reveal what data has been summarized for the Client_errors dataset over a selected time range.", "code_examples": [{"language": "spl", "code": "| datamodel"}, {"language": "spl", "code": "| datamodel internal_audit_logs"}, {"language": "spl", "code": "| datamodel Tutorial Client_errors"}, {"language": "spl", "code": "| datamodel Tutorial Client_errors search"}, {"language": "spl", "code": "| datamodel Tutorial Client_errors search | search Tutorial.status=404  | stats count"}, {"language": "spl", "code": "| datamodel Tutorial summariesonly=truesearch | timechart span=1h count"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "datamodel", "section_heading": "Examples", "section_id": "a14738b8_e8e3_4b4f_aee9_5ab7a814508a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/datamodel", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:45.847048+00:00", "version": "10.2"}}
{"id": "ebf17913ad626b2b", "content": "pivot", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "datamodel", "section_heading": "See also", "section_id": "bb48d664_2ddb_4e06_8ac7_44b6008e705b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/datamodel", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:11:45.847052+00:00", "version": "10.2"}}
{"id": "6d2e39c6b944b42a", "content": "Replaces null values with the last non-null value for a field or set of fields. If no list of fields is given, the filldown command will be applied to all fields. If there are not any previous values for a field, it is left blank (NULL).", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "filldown", "section_heading": "Description", "section_id": "b14cb268_2957_4145_9964_12ff4edc2a7d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/filldown", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:02.525269+00:00", "version": "10.2"}}
{"id": "0877f4d1ffc04b6b", "content": "filldown <wc-field-list> Required arguments <wc-field-list> Syntax: <field> ... Description: A space-delimited list of field names. You can use the asterisk ( * ) as a wildcard to specify a list of fields with similar names. For example, if you want to specify all fields that start with \"value\", you can use a wildcard such as value* .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "filldown", "section_heading": "Syntax", "section_id": "e0d63884_0826_41ae_aeb3_ba7de9efd5e7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/filldown", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:02.525277+00:00", "version": "10.2"}}
{"id": "aa54dd5527b13546", "content": "Example 1: Filldown null values for all fields. Example 2: Filldown null values for the count field only. Example 3: Filldown null values for the count field and any field that starts with 'score'.", "code_examples": [{"language": "spl", "code": "... | filldown"}, {"language": "spl", "code": "... | filldown count"}, {"language": "spl", "code": "... | filldown count score*"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "filldown", "section_heading": "Examples", "section_id": "id_432f38db_1663_4394_9a37_51ee0cd31739--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/filldown", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:02.525283+00:00", "version": "10.2"}}
{"id": "f2593505a0571ddc", "content": "fillnull", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "filldown", "section_heading": "See also", "section_id": "id_53a1175c_af93_4b55_9aac_2d3301b02659--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/filldown", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:02.525287+00:00", "version": "10.2"}}
{"id": "2708f284a0b66bf2", "content": "Replaces null values with a specified value. Null values are field values that are missing in a particular result but present in another result. Use the fillnull command to replace null field values with a string. You can replace the null values in one or more fields. You can specify a string to fill the null field values or use the default, field value which is zero ( 0 ). â€‹", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "fillnull", "section_heading": "Description", "section_id": "id_718e6092_e8ae_4bf3_a005_87dec447fed3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fillnull", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:21.451569+00:00", "version": "10.2"}}
{"id": "07dc534639c7fe50", "content": "The required syntax is in bold. â€‹ fillnull [value=<string>] [<field-list>] Required arguments None. â€‹ Optional arguments field-list Syntax: <field>... Description: A space-delimited list of one or more fields. If you specify a field list, all of the fields in that list are filled in with the value you specify. If you specify a field that didn't previously exist, the field is created. If you do not specify a field list, the value is applied to all fields. value Syntax: value=<string> Description: Specify a string value to replace null values. If you do not specify a value, the default value is applied to the <field-list>. Default : 0", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "fillnull", "section_heading": "Syntax", "section_id": "e15e761d_a51d_4a78_b915_aa5814a4852a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fillnull", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:21.451577+00:00", "version": "10.2"}}
{"id": "2d841f7cac39b3d6", "content": "The fillnull command is a distributable streaming command when a field-list is specified. When no field-list is specified, the fillnull command fits into the dataset processing type. See Command types. Fields in the event set should have at least one non-null value Due to the unique behavior of the fillnull command, Splunk software isn't able to distinguish between a null field value and a null field that doesn't exist in the Splunk schema. In order for a field to exist in the schema, it must have at least one non-null value in the event set. To ensure downstream processing of fields by the fillnull command, ensure that there is at least one non-null value for the fields in the event set. For example, consider the following search: The results look something like this: Notice that the test2 field doesn't show up in the results, even though the eval command created it. The reason the test2 field isn't in the results is that there isn't at least one non-null value for the field in the event set. If a field doesn't have at least one non-null value in the event set, it's considered a nonexistent field, so downstream commands like the fillnull command can't process it. For example, consider the following search: The results look something like this: The search results display the test2 field, but not the intended NULL value. This is because the upstream eval command initially set test2 to null , so the field doesn't exist in the schema. Now consider the following search: The results look something like this: This search generates at least one non-null value for each field and shows the expected behavior by setting the null value of the test2 field to the NULL string. Now all the values display as expected because the test2 field has at least one non-null value.", "code_examples": [{"language": "spl", "code": "| makeresults\n|evaltest=\"123123\", test2=null()\n| fillnull value=NULL"}, {"language": "spl", "code": "| makeresults \n|evaltest=\"123123\"|evaltest2=null() \n| tabletesttest2 \n| fillnull value=NULL"}, {"language": "spl", "code": "| makeresults \n|evaltest1=split(\"123,456\",\",\") \n| mvexpand test1 \n|evaltest2=if(test1=\"123\", null(),\"abc\") \n| fillnull value=NULL"}], "tables": [{"headers": ["_time", "test"], "rows": [["2023-06-07 17:49:45", "123123"]]}, {"headers": ["test", "test2"], "rows": [["123123", ""]]}, {"headers": ["_time", "test1", "test2"], "rows": [["2023-06-07 18:22:24", "123", "NULL"], ["2023-06-07 18:22:24", "456", "abc"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "fillnull", "section_heading": "Usage", "section_id": "id_605be964_cd4d_4cd2_8523_c43449fb1602--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fillnull", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:21.451587+00:00", "version": "10.2"}}
{"id": "905a54f42b27259d", "content": "1. Fill all empty field values with the default value â€‹Your search has produced the following search results: â€‹ â€‹ You can fill all of empty field values with the zero by adding the fillnull command to your search. â€‹ â€‹ The search results will look like this: â€‹ 2. Fill all empty fields with the string \"NULL\" For the current search results, fill all empty field values with the string \"NULL\". 3. Fill the specified fields with the string \"unknown\" â€‹Suppose that your search has produced the following search results: â€‹ â€‹ You can fill all empty field values in the \"host\" and \"kbps\" fields with the string \"unknown\" by adding the fillnull command to your search. â€‹ â€‹ â€‹The results look like this: â€‹ â€‹If you specify a field that does not exist the field is created and the value you specify is added to the new field. â€‹ For example if you specify bytes in the field list, the bytes field is created and filled with the string \"unknown\". â€‹ â€‹ â€‹The results look like this: 4. Use the fillnull command with the timechart command Build a time series chart of web events by host and fill all empty fields with the string \"NULL\". â€‹", "code_examples": [{"language": "spl", "code": "... | fillnull"}, {"language": "spl", "code": "... | fillnull value=NULL"}, {"language": "spl", "code": "... | fillnull value=unknown host kbps"}, {"language": "spl", "code": "... | fillnull value=unknown host kbps bytes"}, {"language": "spl", "code": "sourcetype=\"web\"| timechart count by host | fillnull value=NULL"}], "tables": [{"headers": ["_time", "ACCESSORIES", "ARCADE", "SHOOTER", "SIMULATION", "SPORTS", "STRATEGY", "TEE"], "rows": [["2021-03-17", "5", "17", "6", "3", "5", "32", ""], ["2021-03-16", "", "63", "39", "30", "22", "127", "56"], ["2021-03-15", "65", "94", "38", "42", "", "128", "60"]]}, {"headers": ["_time", "ACCESSORIES", "ARCADE", "SHOOTER", "SIMULATION", "SPORTS", "STRATEGY", "TEE"], "rows": [["2021-03-17", "5", "17", "6", "3", "5", "32", "0"], ["2021-03-16", "0", "63", "39", "30", "22", "127", "56"], ["2021-03-15", "65", "94", "38", "42", "0", "128", "60"]]}, {"headers": ["_time", "host", "average_kbps", "instanenous_kbps", "kbps"], "rows": [["2021/02/14 12:00", "danube.sample.com", "", "1.865", "3.420"], ["2021/02/14 11:53", "mekong.buttercupgames.com", "0.710", "0.164", "1.256"], ["2021/02/14 11:47", "danube.sample.com", "1.325", "", "2.230"], ["2021/02/14 11:42", "yangtze.buttercupgames.com", "2.249", "0.000", "2.249"], ["2021/02/14 11:39", "", "2.874", "3.841", "1.906"], ["2021/02/14 11:33", "nile.example.net", "2.023", "0.915", ""]]}, {"headers": ["_time", "host", "average_kbps", "instanenous_kbps", "kbps"], "rows": [["2021/02/14 12:00", "danube.sample.com", "", "1.865", "3.420"], ["2021/02/14 11:53", "mekong.buttercupgames.com", "0.710", "0.164", "1.256"], ["2021/02/14 11:47", "danube.sample.com", "1.325", "", "2.230"], ["2021/02/14 11:42", "yangtze.buttercupgames.com", "2.249", "0.000", "2.249"], ["2021/02/14 11:39", "unknown", "2.874", "3.841", "1.906"], ["2021/02/14 11:33", "nile.example.net", "2.023", "0.915", "unknown"]]}, {"headers": ["_time", "host", "average_kbps", "instanenous_kbps", "kbps", "bytes"], "rows": [["2021/02/14 12:00", "danube.sample.com", "", "1.865", "3.420", "unknown"], ["2021/02/14 11:53", "mekong.buttercupgames.com", "0.710", "0.164", "1.256", "unknown"], ["2021/02/14 11:47", "danube.sample.com", "1.325", "", "2.230", "unknown"], ["2021/02/14 11:42", "yangtze.buttercupgames.com", "2.249", "0.000", "2.249", "unknown"], ["2021/02/14 11:39", "unknown", "2.874", "3.841", "1.906", "unknown"], ["2021/02/14 11:33", "nile.example.net", "2.023", "0.915", "unknown", "unknown"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "fillnull", "section_heading": "Examples", "section_id": "fb8d6bea_66a2_4c2a_944c_37912845996a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fillnull", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:21.451606+00:00", "version": "10.2"}}
{"id": "4b328f4d3c080fcd", "content": "Related commands filldown streamstats", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "fillnull", "section_heading": "See also", "section_id": "b903a1a4_4010_4c25_9b12_fc956f444af0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fillnull", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:21.451611+00:00", "version": "10.2"}}
{"id": "5b1442236fee3542", "content": "Keeps or removes fields from search results based on the field list criteria.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "fields", "section_heading": "Description", "section_id": "id_76b7777d_07f3_4c11_8f70_8a0785139527--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:39.683995+00:00", "version": "10.2"}}
{"id": "2ecd4f930c1f1df3", "content": "fields [+|-] <wc-field-list> Required arguments <wc-field-list> Syntax: <field>, <field>, ... Description: Comma-delimited list of fields to keep or remove. You can use the asterisk ( * ) as a wildcard to specify a list of fields with similar names. For example, if you want to specify all fields that start with \"value\", you can use a wildcard such as value*. To specify the wildcard pattern for internal fields, use _*. Optional arguments + | - Syntax: + | - Description: If the plus ( + ) symbol is specified, only the fields in the wc-field-list are kept in the results. If the negative ( - ) symbol is specified, the fields in the wc-field-list are removed from the results. Default: +", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "fields", "section_heading": "Syntax", "section_id": "f8ed8b8e_a656_46dc_9ab0_d74458cd995b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:39.684002+00:00", "version": "10.2"}}
{"id": "df842fa4ada72e51", "content": "The fields command is a distributable streaming command. See Command types. Internal fields and Splunk Web The leading underscore is reserved for names of internal fields such as _raw and _time. By default, the internal fields _raw and _time are included in the search results in Splunk Web. The fields command does not remove these internal fields unless you explicitly specify that the fields should not appear in the output in Splunk Web. For example, to remove all internal fields, you specify: ... | fields - _* To exclude a specific field, such as _raw , you specify: ... | fields - _raw Note: Be cautious removing the _time field. Statistical commands, such as timechart and chart , cannot display date or time information without the _time field. Displaying internal fields in Splunk Web Other than the _raw and _time fields, internal fields do not display in Splunk Web, even if you explicitly specify the fields in the search. For example, the following search does not show the _bkt field in the results. To display an internal field in the results, the field must be copied or renamed to a field name that does not include the leading underscore character. For example: Internal fields and the outputcsv command You can include additional internal fields in your search results by using the <codeph>outputcsv</codeph> command. When the outputcsv command is used in the search, there are additional internal fields that are automatically added to the CSV file. The most common internal fields that are added are: _raw _time _indextime To exclude internal fields from the output, specify each field that you want to exclude. For example: You cannot match wildcard characters in searches that use the fields command You can use the asterisk ( * ) in your searches as a wildcard character, but you can't use a backslash ( \\ ) to escape an asterisk in search strings. A backslash \\ and an asterisk * match the characters \\* in searches, not an escaped wildcard * character. Because Splunk platform doesn't support escaping wildcards, asterisk ( * ) characters in field names can't be matched in searches that keep or remove fields from search results. Support for backslash characters ( \\ ) in the fields command To match a backslash character ( \\ ) in a field name when using the fields command, use 2 backslashes for each backslash. For example, to display fields that contain http:\\\\ , use the following command in your search: See Backslashes in the Search Manual .", "code_examples": [{"language": "spl", "code": "index=_internal | head 5 | fields + _bkt | table _bkt"}, {"language": "spl", "code": "index=_internal | head 5 | fields + _bkt |evalbkt=_bkt | table bkt"}, {"language": "spl", "code": "... | fields - _raw _indextime _sourcetype _serial | outputcsv MyTestCsvFile"}, {"language": "spl", "code": "... | fields http:\\\\\\\\*"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "fields", "section_heading": "Usage", "section_id": "id_9605826a_6122_40d3_876e_d61971dad345--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:39.684008+00:00", "version": "10.2"}}
{"id": "67c434dcce05981f", "content": "Example 1: Remove the host and ip fields from the results Example 2: Keep only the host and ip fields. Remove all of the internal fields. The internal fields begin with an underscore character, for example _time. Example 3: Remove unwanted internal fields from the output CSV file. The fields to exclude are _raw _indextime , _sourcetype , _subsecond , and _serial. Example 4: Keep only the fields source , sourcetype , host , and all fields beginning with error .", "code_examples": [{"language": "spl", "code": "... | fields - host, ip"}, {"language": "spl", "code": "... | fields host, ip | fields - _*"}, {"language": "spl", "code": "index=_internal sourcetype=\"splunkd\"| head 5 | fields - _raw, _indextime, _sourcetype, _subsecond, _serial | outputcsv MyTestCsvfile"}, {"language": "spl", "code": "... | fieldssource, sourcetype, host, error*"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "fields", "section_heading": "Examples", "section_id": "id_6154b048_c52f_4197_bf1e_0b7e256465fb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:39.684012+00:00", "version": "10.2"}}
{"id": "b3d70411c0125451", "content": "rename , table", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "fields", "section_heading": "See also", "section_id": "a1741d47_2309_4d81_87b7_cd08e023a70a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:39.684016+00:00", "version": "10.2"}}
{"id": "28c2407fdc07fa50", "content": "This command is used to remove outliers, not detect them. It removes or truncates outlying numeric values in selected fields. If no fields are specified, then the outlier command attempts to process all fields. To identify outliers and create alerts for outliers, see finding and removing outliers in the Search Manual. Note: Use current Splunk machine learning (ML) tools to take advantage of the latest algorithms and get the most powerful results. See About the Splunk Machine Learning Toolkit in the Splunk Machine Learning Toolkit .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "outlier", "section_heading": "Description", "section_id": "id_05a07707_815c_4bc7_b238_32381e5230f4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outlier", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:54.964466+00:00", "version": "10.2"}}
{"id": "480b56d956172383", "content": "outlier <outlier-options>... [<field-list>] Optional arguments <outlier-options> Syntax: <action> | <mark> | <param> | <uselower> Description: Outlier options. <field-list> Syntax: <field> ... Description: A space-delimited list of field names. Outlier options <action> Syntax: action=remove | transform Description: Specifies what to do with the outliers. The remove option removes events that containing the outlying numerical values. The transform option truncates the outlying values to the threshold for outliers. If action=transform and mark=true , prefixes the values with \"000\". Abbreviations: The remove action can be shorted to rm. The transform action can be shorted to tf. Default: transform <mark> Syntax: mark=<bool> Description: If action=transform and mark=true , prefixes the outlying values with \"000\". If action=remove , the mark argument has no effect. Default: false <param> Syntax: param=<num> Description: Parameter controlling the threshold of outlier detection. An outlier is defined as a numerical value that is outside of param multiplied by the inter-quartile range (IQR). Default: 2.5 <uselower> Syntax: uselower=<bool> Description: Controls whether to look for outliers for values below the median in addition to above. Default: false", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "outlier", "section_heading": "Syntax", "section_id": "id_57c79c5d_071e_45b9_b869_2e711dd0c4cd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outlier", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:54.964473+00:00", "version": "10.2"}}
{"id": "e9d0062f9671d3b9", "content": "The outlier command is a dataset processing command. See Command types. Filtering is based on the inter-quartile range (IQR), which is computed from the difference between the 25th percentile and 75th percentile values of the numeric fields. If the value of a field in an event is less than (25th percentile) - param*IQR or greater than (75th percentile) + param*IQR , that field is transformed or that event is removed based on the action parameter.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "outlier", "section_heading": "Usage", "section_id": "id_3312e6a0_4559_4191_b3b4_b164aa4a3888--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outlier", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:54.964477+00:00", "version": "10.2"}}
{"id": "e4d8de529198cf01", "content": "Example 1: For a timechart of webserver events, transform the outlying average CPU values. Example 2: Remove all outlying numerical values.", "code_examples": [{"language": "spl", "code": "404 host=\"webserver\"| timechart avg(cpu_seconds) by host | outlier action=tf"}, {"language": "spl", "code": "... | outlier"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "outlier", "section_heading": "Examples", "section_id": "id_76632cd3_70d0_4789_a7a1_efc646ac7135--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outlier", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:54.964485+00:00", "version": "10.2"}}
{"id": "12a29a4c242ae057", "content": "anomalies , anomalousvalue , cluster , kmeans Finding and removing outliers", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "outlier", "section_heading": "See also", "section_id": "f7937c57_dd34_4c62_99fd_b3c84aab5458--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/outlier", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:12:54.964491+00:00", "version": "10.2"}}
{"id": "fa6eb209c0b2bed8", "content": "For Splunk Enterprise deployments, loads search results from the specified .csv file, which is not modified. The filename must refer to a relative path in $SPLUNK_HOME/var/run/splunk/csv. If dispatch=true , the path must be in $SPLUNK_HOME/var/run/splunk/dispatch/<job id>. If the specified file does not exist and the filename does not have an extension, then the Splunk software assumes it has a filename with a .csv extension. Note: If you run into an issue with the inputcsv command resulting in an error, ensure that your CSV file ends with a BLANK LINE.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "inputcsv", "section_heading": "Description", "section_id": "aceb2650_5fac_4e59_ac1f_4e673bf69633--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/inputcsv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:12.136285+00:00", "version": "10.2"}}
{"id": "bdbb28c6aec87b68", "content": "The required syntax is in bold. | inputcsv [dispatch=<bool>] [append=<bool>] [strict=<bool>] [start=<int>] [max=<int>] [events=<bool>] <filename> [WHERE <search-query>] Required arguments filename Syntax: <filename> Description: Specify the name of the .csv file, located in $SPLUNK_HOME/var/run/splunk/csv. Optional arguments dispatch Syntax: dispatch=<bool> Description: When set to true, this argument indicates that the filename is a .csv file in the dispatch directory. The relative path is $SPLUNK_HOME/var/run/splunk/dispatch/<job id>/. Default: false append Syntax: append=<bool> Description: Specifies whether the data from the .csv file is appended to the current set of results (true) or replaces the current set of results (false). Default: false strict Syntax: strict=<bool> Description: When set to true this argument forces the search to fail completely if inputcsv raises an error. This happens even when the errors apply to a subsearch. When set to false , many inputcsv error conditions return warning messages but do not otherwise cause the search to fail. Certain error conditions cause the search to fail even when strict=false. Default: false events Syntax: events=<bool> Description: Specifies whether the data in the CSV file are treated as events or as a table of search results. By default events=false returns the data in a table with field names as column headings. The table appears on the Statistics tab. If you set events=true , the imported CSV data must have the _time and _raw fields. The data is treated as events, which appear on the Events tab. Default: false max Syntax: max=<int> Description: Controls the maximum number of events to be read from the file. If max is not specified, there is no limit to the number of events that can be read. Default: 1000000000 (1 billion) start Syntax: start=<int> Description: Controls the 0-based offset of the first event to be read. Default: 0 WHERE Syntax: WHERE <search-criteria> Description: Use this clause to improve search performance by prefiltering data returned from the CSV file. Supports a limited set of search query operators: =, !=, <, >, <=, >=, AND, OR, NOT. Any combination of these operators is permitted. Also supports wildcard string searches.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "inputcsv", "section_heading": "Syntax", "section_id": "id_77f95355_27dc_45d5_8ac1_88e6470246bd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/inputcsv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:12.136300+00:00", "version": "10.2"}}
{"id": "2e0235d019fb439d", "content": "The inputcsv command is an event-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. Appending or replacing results If the append argument is set to true , you can use the inputcsv command to append the data from the CSV file to the current set of search results. With append=true , you use the inputcsv command later in your search, after the search has returned a set of results. See Examples. The append argument is set to false by default. If the append argument is not specified or is set to false , the inputcsv command must be the first command in the search. Data is loaded from the specified CSV file into the search. Working with large CSV files The WHERE clause allows you to narrow the scope of the search of the inputcsv file. It restricts the inputcsv to a smaller number of rows, which can improve search efficiency when you are working with significantly large CSV files. Distributed deployments The inputcsv command is not compatible with search head pooling and search head clustering. The command saves the *.csv file on the local search head in the $SPLUNK_HOME/var/run/splunk/ directory. The *.csv files are not replicated on the other search heads. Strict error handling Use the strict argument to make inputcsv searches fail whenever they encounter an error condition. You can set this at the system level for all inputcsv and inputlookup searches by changing input_errors_fatal in limits.conf Note: If you use Splunk Cloud Platform, file a Support ticket to change the input_errors_fatal setting. Use the strict argument to override the input_errors_fatal setting for an inputcsv search.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "inputcsv", "section_heading": "Usage", "section_id": "id_35ca9537_0b5f_4d15_ab24_b847f660ea12--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/inputcsv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:12.136306+00:00", "version": "10.2"}}
{"id": "28a0ff9bb04cb9cf", "content": "1. Load results that contain a specific string This example loads search results from the $SPLUNK_HOME/var/run/splunk/csv/all.csv file. Those that contain the string error are saved to the $SPLUNK_HOME/var/run/splunk/csv/error.csv file. 2. Load a specific range of results This example loads results 101 to 600 from either the bar file, if exists, or from the bar.csv file. 3. Specifying which results to load with operators and expressions You can use comparison operators and Boolean expression to specify which results to load. This example loads all of the events from the CSV file $SPLUNK_HOME/var/run/splunk/csv/students.csv and then filters out the events that do not match the WHERE clause, where the values in the age field are greater than 13, less than 19, but not 16. The search returns a count of the remaining search results. 4. Append data from a CSV file to search results You can use the append argument to append data from a CSV file to a set of search results. In this example the combined data is then output back to the same CSV file. 5. Appending multiple CSV files You can also append the search results of one CSV file to another CSV file by using the append command and a subsearch. This example uses the eval command to add a field to each set of data to denote which CSV file the data originated from.", "code_examples": [{"language": "spl", "code": "| inputcsv all.csv | search error | outputcsv errors.csv"}, {"language": "spl", "code": "| inputcsv start=100 max=500 bar"}, {"language": "spl", "code": "| inputcsv students.csv WHERE (age>=13 age<=19) AND NOT age=16 | stats count"}, {"language": "spl", "code": "error earliest=-d@d | inputcsv append=trueall_errors.csv | outputcsv all_errors.csv"}, {"language": "spl", "code": "| inputcsv file1.csv |evalsource=\"file1\"| append [inputcsv file2.csv |evalsource=\"file2\"]"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "inputcsv", "section_heading": "Examples", "section_id": "id_652bd6ec_a93a_4f9c_bbf6_0d01000c54eb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/inputcsv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:12.136311+00:00", "version": "10.2"}}
{"id": "5bfeb16b737b286e", "content": "outputcsv", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "inputcsv", "section_heading": "See also", "section_id": "id_4dd64560_a28d_44d8_87c5_100632fa7e62--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/inputcsv", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:12.136317+00:00", "version": "10.2"}}
{"id": "1c2de9c41dafa720", "content": "Reverses the order of the results. The reverse command does not affect which results are returned by the search, only the order in which the results are displayed. For the CLI, this includes any default or explicit maxout setting. Note: On very large result sets, which means sets with millions of results or more, reverse command requires large amounts of temporary storage, I/O, and time.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "reverse", "section_heading": "Description", "section_id": "id_812c620a_7ef8_40ad_9ca1_06d0cd233ae1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/reverse", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:27.730591+00:00", "version": "10.2"}}
{"id": "7348b7f4dbc94a56", "content": "reverse", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "reverse", "section_heading": "Syntax", "section_id": "d6d05bb5_0c5b_4683_9a97_3c90ae6e1534--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/reverse", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:27.730598+00:00", "version": "10.2"}}
{"id": "56c781e26cf1f155", "content": "The reverse command is a dataset processing command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "reverse", "section_heading": "Usage", "section_id": "id_6a051271_ad86_4a23_92aa_a9fdb0d273a6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/reverse", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:27.730602+00:00", "version": "10.2"}}
{"id": "4e60239782c1dc6e", "content": "Example 1: Reverse the order of a result set.", "code_examples": [{"language": "spl", "code": "... | reverse"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "reverse", "section_heading": "Examples", "section_id": "e44b7691_7084_4756_ae11_f26eaef5cc65--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/reverse", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:27.730607+00:00", "version": "10.2"}}
{"id": "260d276be97ed957", "content": "Commands head sort tail", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "reverse", "section_heading": "See also", "section_id": "id_2d8e35d4_eaf7_4271_84e0_b843aec1b0a8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/reverse", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:27.730610+00:00", "version": "10.2"}}
{"id": "b404ec8ca92ce7e5", "content": "Generates suggested event types by taking the results of a search and producing a list of potential event types. At most, 5000 events are analyzed for discovering event types.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "findtypes", "section_heading": "Description", "section_id": "f89b237e_f797_4315_abb8_616022db47a0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/findtypes", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:44.963172+00:00", "version": "10.2"}}
{"id": "5374028529cc1468", "content": "findtypes max=<int> [notcovered] [useraw] Required arguments max Datatype: <int> Description: The maximum number of events to return. Default : 10 Optional arguments notcovered Description: If this keyword is used, the findtypes command returns only event types that are not already covered. useraw Description: If this keyword is used, the findtypes command uses phrases in the _raw text of events to generate event types.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "findtypes", "section_heading": "Syntax", "section_id": "id_639f55a2_e32c_4958_9d6d_baa014d68641--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/findtypes", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:44.963179+00:00", "version": "10.2"}}
{"id": "794e0526e4886a99", "content": "Example 1: Discover 10 common event types. Example 2: Discover 50 common event types and add support for looking at text phrases.", "code_examples": [{"language": "spl", "code": "... | findtypes"}, {"language": "spl", "code": "... | findtypes max=50 useraw"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "findtypes", "section_heading": "Examples", "section_id": "id_53efa91e_91c0_42f8_a2af_d245b48a666f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/findtypes", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:44.963184+00:00", "version": "10.2"}}
{"id": "18b0bd2aa35664f2", "content": "typer", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "findtypes", "section_heading": "See also", "section_id": "f1d06bab_3067_4f47_84b8_5b5531ffc3cb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/findtypes", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:44.963187+00:00", "version": "10.2"}}
{"id": "06c0fa40ae23da77", "content": "Puts continuous numerical values into discrete sets, or bins, by adjusting the value of <field> so that all of the items in a particular set have the same value. Note: The bin command is automatically called by the chart and the timechart commands. Use the bin command for only statistical operations that the chart and the timechart commands cannot process. Do not use the bin command if you plan to export all events to CSV or JSON file formats.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "bin", "section_heading": "Description", "section_id": "id_301584e3_ace4_400a_afd5_fef5e296df74--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/bin", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:59.418720+00:00", "version": "10.2"}}
{"id": "528765f11eb4a26b", "content": "The required syntax is in bold. bin [<bin-options>...] <field> [AS <newfield>] Required arguments field Syntax: < field > Description: Specify a field name. Optional arguments bin-options Syntax: bins | minspan | span | <start-end> | aligntime Description: Discretization options. See the Bins options section in this topic for the syntax and description for each of these options. newfield Syntax: <string> Description: A new name for the field. Bin options bins Syntax: bins=<int> Description: Sets the maximum number of bins to discretize into. The default is set in the [discretize] stanza in the limits.conf file. Default: 100 minspan Syntax: minspan=<span-length> Description: Specifies the smallest span granularity to use automatically inferring span from the data time range. span Syntax: span = <log-span> | <span-length> Description: Sets the size of each bin, using a span length based on a logarithm-based span or based on time. Note: When a <span-length> of a day or more is used, the span is aligned to midnight in the timezone of the user. <start-end> Syntax: start=<num> | end=<num> Description: Sets the minimum and maximum extents for numerical bins. The data in the field is analyzed and the beginning and ending values are determined. The start and end arguments are used when a span value is not specified. You can use the start or end arguments only to expand the range, not to shorten the range. For example, if the field represents seconds the values are from 0-59. If you specify a span of 10, then the bins are calculated in increments of 10. The bins are 0-9, 10-19, 20-29, and so forth. If you do not specify a span, but specify end=1000, the bins are calculated based on the actual beginning value and 1000 as the end value. If you set end=10 and the values are >10, the end argument has no effect. aligntime Syntax: aligntime=(earliest | latest | <time-specifier>) Description: Align the bin times to something other than base UTC time (epoch 0). The aligntime option is valid only when doing a time-based discretization. Ignored if span is in days, months, or years. Span options log-span Syntax: [<num>]log[<num>] Description: Sets to log-based span. The first number is a coefficient. The second number is the base. If the first number is supplied, it must be a real number >= 1.0 and < base. Base, if supplied, must be real number > 1.0 (strictly greater than 1). Example: span=2log10 span-length Syntax: <int>[<timescale>] Description: A span of each bin. If discretizing based on the _time field or used with a timescale, this is treated as a time range. If not, this is an absolute bin length. <timescale> Syntax: <sec> | <min> | <hr> | <day> | <month> | <subseconds> Description: Time scale units. If discretizing based on the _time field. Default: sec", "code_examples": [], "tables": [{"headers": ["Time scale", "Syntax", "Description"], "rows": [["<sec>", "s | sec | secs | second | seconds", "Time scale in seconds."], ["<min>", "m | min |  mins |  minute |  minutes", "Time scale in minutes."], ["<hr>", "h | hr |  hrs |  hour | hours", "Time scale in hours."], ["<day>", "d |  day | days", "Time scale in days."], ["<month>", "mon | month |  months", "Time scale in months."], ["<subseconds>", "us | ms |  cs |  ds", "Time scale in microseconds (us), milliseconds (ms), centiseconds (cs), or deciseconds (ds)"]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "bin", "section_heading": "Syntax", "section_id": "id_3ddfeb33_f289_4c46_a166_67273511622f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/bin", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:59.418733+00:00", "version": "10.2"}}
{"id": "7ee5d998be5f22cd", "content": "The bucket command is an alias for the bin command. The bin command is usually a dataset processing command. If the span argument is specified with the command, the bin command is a streaming command. See Command types. Subsecond bin time spans Subsecond span timescales, which are time spans that are made up of deciseconds (ds), centiseconds (cs), milliseconds (ms), or microseconds (us), should be numbers that divide evenly into a second. For example, 1s = 1000ms. This means that valid millisecond span values are 1, 2, 4, 5, 8, 10, 20, 25, 40, 50, 100, 125, 200, 250, or 500ms. In addition, span = 1000ms is not allowed. Use span = 1s instead.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "bin", "section_heading": "Usage", "section_id": "bb1df051_a06c_4338_8116_ede23ee1947f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/bin", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:59.418738+00:00", "version": "10.2"}}
{"id": "8b24da3709598aa2", "content": "1. Specify a time span Return the average \"thruput\" of each \"host\" for each 5 minute time span. 2. Specify the number of bins Bin search results into 10 bins, and return the count of raw events for each bin. 3. Specify an end value Create bins with an end value larger than you need to ensure that all possible values are included. 4. Specify a relative time to align the bins to Align the time bins to 3am (local time). Set the span to 12h. The bins will represent 3am - 3pm, then 3pm - 3am (the next day), and so on. 5. Specify a UTC time to align the bins to Align the bins to the specific UTC time of 1500567890.", "code_examples": [{"language": "spl", "code": "... | bin _time span=5m | stats avg(thruput) by _time host"}, {"language": "spl", "code": "... | bin size bins=10 | stats count(_raw) by size"}, {"language": "spl", "code": "... | bin amount end=1000"}, {"language": "spl", "code": "...| bin _time span=12h aligntime=@d+3h"}, {"language": "spl", "code": "...| bin _time aligntime=1500567890"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "bin", "section_heading": "Examples", "section_id": "a96eb15f_5101_49bf_a3d5_84e7913f730b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/bin", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:59.418743+00:00", "version": "10.2"}}
{"id": "d54fdc7c4ebf405f", "content": "chart , timechart", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "bin", "section_heading": "See also", "section_id": "id_24a6dcd6_1f81_4702_893b_c715ccb4e9a6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/bin", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:13:59.418747+00:00", "version": "10.2"}}
{"id": "ed365606deea67a2", "content": "Use this command to determine how many times a value in <field1> and a value in <field2> occur together. For example, if you have a field that contains user IDs and another field that contains items names, this command finds how common each pair of user and item occur. This command implements one step in a collaborative filtering analysis for making recommendations.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "cofilter", "section_heading": "Description", "section_id": "fcfa490d_1810_4cab_b527_286a4ab59270--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/cofilter", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:16.349414+00:00", "version": "10.2"}}
{"id": "ec0febe38e58ebf9", "content": "cofilter <field1> <field2> Required arguments field1 Syntax: <field> Description: The name of field. field2 Syntax: <field> Description: The name of a field.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "cofilter", "section_heading": "Syntax", "section_id": "id_60c244d2_f95e_463a_b612_31d9b5059f95--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/cofilter", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:16.349422+00:00", "version": "10.2"}}
{"id": "3ac7b7eb0d437652", "content": "The cofilter command is a transforming command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "cofilter", "section_heading": "Usage", "section_id": "f14f0f5d_c8b3_49f4_aa5d_e8b0447932b3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/cofilter", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:16.349426+00:00", "version": "10.2"}}
{"id": "06bc2111455f694d", "content": "Example 1 Find the cofilter for user and item. The user field must be specified first and followed by the item field. The output is an event for each pair of items with: the first item and its popularity, the second item and its popularity, and the popularity of that pair of items. Let's start with a simple search to create a few results: The results appear on the Statistics tab and look something like this: The eval command with the modulus ( % ) operator is used to create the item field: The results look like this: Add the cofilter command to the search to determine how many user values occurred with each item value, The results look something like this:", "code_examples": [{"language": "spl", "code": "| makeresults \n|evaluser=\"a b c a b c a b c\"| makemv user\n| mvexpand user\n| streamstats count"}, {"language": "spl", "code": "| makeresults \n|evaluser=\"a b c a b c a b c\"| makemv user\n| mvexpand user\n| streamstats count\n|evalitem = countÂ % 5"}, {"language": "spl", "code": "| makeresults \n|evaluser=\"a b c a b c a b c\"| makemv user\n| mvexpand user\n| streamstats count\n|evalitem = countÂ % 5\n| cofilter user item"}], "tables": [{"headers": ["_time", "count", "user"], "rows": [["2020-02-19 21:17:54", "1", "a"], ["2020-02-19 21:17:54", "2", "b"], ["2020-02-19 21:17:54", "3", "c"], ["2020-02-19 21:17:54", "4", "a"], ["2020-02-19 21:17:54", "5", "b"], ["2020-02-19 21:17:54", "6", "c"], ["2020-02-19 21:17:54", "7", "a"], ["2020-02-19 21:17:54", "8", "b"], ["2020-02-19 21:17:54", "9", "c"]]}, {"headers": ["_time", "count", "item", "user"], "rows": [["2020-02-19 21:17:54", "1", "1", "a"], ["2020-02-19 21:17:54", "2", "2", "b"], ["2020-02-19 21:17:54", "3", "3", "c"], ["2020-02-19 21:17:54", "4", "4", "a"], ["2020-02-19 21:17:54", "5", "0", "b"], ["2020-02-19 21:17:54", "6", "1", "c"], ["2020-02-19 21:17:54", "7", "2", "a"], ["2020-02-19 21:17:54", "8", "3", "b"], ["2020-02-19 21:17:54", "9", "4", "c"]]}, {"headers": ["Item 1", "Item 1 user count", "Item 2", "Item 2 user count", "Pair count"], "rows": [["1", "2", "2", "2", "1"], ["1", "2", "3", "2", "1"], ["1", "2", "4", "2", "2"], ["2", "2", "3", "2", "1"], ["2", "2", "4", "2", "1"], ["2", "2", "0", "1", "1"], ["3", "2", "4", "2", "1"], ["3", "2", "0", "1", "1"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "cofilter", "section_heading": "Examples", "section_id": "id_795c9f93_773b_45dd_9220_387f30ca004b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/cofilter", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:16.349440+00:00", "version": "10.2"}}
{"id": "cd20dfaa7a8f5069", "content": "associate , correlate", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "cofilter", "section_heading": "See also", "section_id": "cdd5bbc5_ecfb_43cf_bde4_151a8b3646d9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/cofilter", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:16.349445+00:00", "version": "10.2"}}
{"id": "e03ef73757a73194", "content": "The addcoltotals command appends a new result to the end of the search result set. The result contains the sum of each numeric field or you can specify which fields to summarize. Results are displayed on the Statistics tab. If the labelfield argument is specified, a column is added to the statistical results table with the name specified.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "addcoltotals", "section_heading": "Description", "section_id": "a85bccde_92ca_43e2_9f0e_fe75f3f8f692--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addcoltotals", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:31.449864+00:00", "version": "10.2"}}
{"id": "85f4447a5c1c5c5c", "content": "addcoltotals [labelfield=<field>] [label=<string>] [<wc-field-list>] Optional arguments <wc-field-list> Syntax: <field> ... Description: A space delimited list of valid field names. The addcoltotals command calculates the sum only for the fields in the list you specify. You can use the asterisk ( * ) as a wildcard to specify a list of fields with similar names. For example, if you want to specify all fields that start with \"value\", you can use a wildcard such as value*. Default: Calculates the sum for all of the fields. labelfield Syntax: labelfield=<fieldname> Description: Specify a field name to add to the result set. Default: none label Syntax: label=<string> Description: Used with the labelfield argument to add a label in the summary event. If the labelfield argument is absent, the label argument has no effect. Default: Total", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "addcoltotals", "section_heading": "Syntax", "section_id": "id_26095b58_4e47_4129_aa50_ed37bc76e6f6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addcoltotals", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:31.449871+00:00", "version": "10.2"}}
{"id": "c85905eda89ea8a0", "content": "1. Compute the sums of all the fields Compute the sums of all the fields, and put the sums in a summary event called \"change_name\". 2. Add a column total for two specific fields Add a column total for two specific fields in a table. 3. Create the totals for a field that match a field name pattern Filter fields for two name-patterns, and get totals for one of them. 4. Specify a field name for the column totals Augment a chart with a total of the values present.", "code_examples": [{"language": "spl", "code": "... | addcoltotals labelfield=change_name label=ALL"}, {"language": "spl", "code": "sourcetype=access_* | table userId bytes avgTime duration | addcoltotals bytes duration"}, {"language": "spl", "code": "...  | fields user*, *size | addcoltotals *size"}, {"language": "spl", "code": "index=_internalsource=\"metrics.log\"group=pipeline | stats avg(cpu_seconds) by processor | addcoltotals labelfield=processor"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "addcoltotals", "section_heading": "Basic examples", "section_id": "id_0a5ded09_c859_4b64_a8ac_dedba78e1fd2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addcoltotals", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:31.449876+00:00", "version": "10.2"}}
{"id": "05a4d18177f33338", "content": "1. Generate a total for a column The following search looks for events from web access log files that were successful views of strategy games. A count of the events by each product ID is returned. The results appear on the Statistics tab and look like this: You can use the addcoltotals command to generate a total of the views and display the total at the bottom of the column. The results appear on the Statistics tab and look something like this: You can use add a field to the results that labels the total. The results appear on the Statistics tab and look something like this:", "code_examples": [{"language": "spl", "code": "sourcetype=access_* status=200 categoryId=STRATEGY | chart count AS views by productId"}, {"language": "spl", "code": "sourcetype=access_* status=200 categoryId=STRATEGY | chart count AS views by productId | addcoltotals"}, {"language": "spl", "code": "sourcetype=access_* status=200 categoryId=STRATEGY | chart count AS views by productId | addcoltotals labelfield=\"Total views\""}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["productId", "views"], "rows": [["DB-SG-G01", "1796"], ["DC-SG-G02", "1642"], ["FS-SG-G03", "1482"], ["PZ-SG-G05", "1300"]]}, {"headers": ["productId", "views"], "rows": [["DB-SG-G01", "1796"], ["DC-SG-G02", "1642"], ["FS-SG-G03", "1482"], ["PZ-SG-G05", "1300"], ["", "6220"]]}, {"headers": ["productId", "views", "Total views"], "rows": [["DB-SG-G01", "1796", ""], ["DC-SG-G02", "1642", ""], ["FS-SG-G03", "1482", ""], ["PZ-SG-G05", "1300", ""], ["", "6220", "Total"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "addcoltotals", "section_heading": "Extended example", "section_id": "id_33378c89_857f_4a3c_bcf8_1dd8890bc4dc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addcoltotals", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:31.449886+00:00", "version": "10.2"}}
{"id": "2f7fea96d37a6ae7", "content": "Commands addtotals stats", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "addcoltotals", "section_heading": "See also", "section_id": "id_276354a3_22bf_4d73_98dc_4223835b6adf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/addcoltotals", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:31.449890+00:00", "version": "10.2"}}
{"id": "5c3e0b863dde1055", "content": "Generates a list of terms or indexed fields from each bucket of event indexes. Watch this Splunk How-To video, Using the Walklex Command , to see a demonstration about how to use this command. Note: Due to the variable nature of merged_lexicon.lex and .tsidx files, the walklex command does not always return consistent results. The walklex command doesn't work on hot buckets. This command only works on warm or cold buckets, after the buckets have a merged lexicon file or single time-series index (tsidx) file. If neither of these files exist, a message is returned, as expected. This message doesn't indicate that there is a problem with the health of your environment.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "walklex", "section_heading": "Description", "section_id": "id_55ee76da_e9ca_4c60_9606_7fede4998f00--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/walklex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:50.288631+00:00", "version": "10.2"}}
{"id": "ad6cf5d3220ca27d", "content": "The required syntax is in bold. | walklex [ type=<walklex-type> ] [ prefix=<string> | pattern=<wc-string> ] <index-list> [ splunk_server=<wc-string> ] [ splunk_server_group=<wc-string> ]... Required arguments <index-list> Syntax: index=<index-name> index=<index-name> ... Description: Limits the search to one or more indexes. For example, index=_internal. Optional arguments prefix | pattern Syntax: prefix=<string> | pattern=<wc-string> Description: Limits results to terms that match a specific pattern or prefix. Either prefix or pattern can be specified but not both. Includes only buckets with a merged_lexicon file or a single tsidx file. This means that hot buckets are generally not included. Default: pattern=* splunk_server Syntax: splunk_server=<wc-string> Description: Specifies the distributed search peers from which to return results. If you are using Splunk Cloud Platform, omit this parameter. If you are using Splunk Enterprise, you can specify only one splunk_server argument. However, you can use a wildcard when you specify the server name to indicate multiple servers. For example, you can specify splunk_server=peer01 or splunk_server=peer*. Use local to refer to the search head. Default: All configured search peers return information splunk_server_group Syntax: splunk_server_group=<wc-string> Description: Limits the results to one or more server groups. You can specify a wildcard character in the string to indicate multiple server groups with similar names. If you are using Splunk Cloud Platform, omit this parameter. Default: None type Syntax: type = ( all | field | fieldvalue | term ) Description: Specifies which type of terms to return in the lexicon. See Usage for more information about using the type argument options. Use field to return only the unique field names in each index bucket. Use fieldvalue to include only indexed field terms. Use term to exclude all indexed field terms of the form <field>::<value>. Default: all", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "walklex", "section_heading": "Syntax", "section_id": "id_9c782627_ff2c_4d1e_a6ae_f30ea24d08b9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/walklex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:50.288639+00:00", "version": "10.2"}}
{"id": "fda24a5b2b38c109", "content": "The walklex command is a generating command , which use a leading pipe character. The walklex command must be the first command in a search. See Command types. When the Splunk software indexes event data, it segments each event into raw tokens using rules specified in segmenters.conf file. You might end up with raw tokens that are actually key-value pairs separated by an arbitrary delimiter such as an equal ( = ) symbol. The following search uses the walklex and where commands to find the raw tokens in your index. It uses the stats command to count the raw tokens. Return only indexed field names Specify the type=field argument to have walklex return only the field names from indexed fields. The indexed fields returned by walklex can include default fields such as host , source , sourcetype , the date_* fields, punct , and so on. It can also include additional indexed fields configured as such in props.conf and transforms.conf and created with the INDEXED_EXTRACTIONS setting or other WRITE_META methods. The discovery of this last set of additional indexed fields is likely to help you with accelerating your searches. Return the set of terms that are indexed fields with indexed values Specify type=fieldvalue argument to have walklex return the set of terms from the index that are indexed fields with indexed values. The type=fieldvalue argument returns the list terms from the index that are indexed fields with indexed values. Unlike the type=field argument, where the values returned are only the field names themselves, the type=fieldvalue argument returns indexed field names that have any field value. For example, if the indexed field term is runtime::0.04 , the value returned by the type=fieldvalue argument is runtime::0.04. The value returned by the type=field argument is runtime. Return all TSIDX keywords that are not part of an indexed field structure Specify type=term to have walklex return the keywords from the TSIDX files that are not part of any indexed field structure. In other words, it excludes all indexed field terms of the form <field>::<value>. Return terms of all three types When you do not specify a type, or when you specify type=all , walklex uses the default type=all argument. This causes walklex to return the terms in the index of all three types: field , fieldvalue , and term. Note: When you use type=all , the indexed fields are not called out as explicitly as the fields are with the type=field argument. You need to split the term field on :: to obtain the field values from the indexed term. Support for hot buckets Because the walklex command doesn't work on hot buckets, recently loaded data displays in search results only after buckets have rolled over from hot to warm. You can either wait for buckets of an index to roll over from hot to warm on their own, or you can restart Splunk platform or manually roll the buckets over to warm. See Rolling buckets manually from hot to warm. Restrictions The walklex command applies only to event indexes. It cannot be used with metrics indexes. People who have search filters applied to one or more of their roles cannot use walklex unless they also have a role with either the run_walklex capability or the admin_all_objects capability. For more information about role-based search filters, see Create and manage roles with Splunk Web in Securing the Splunk Platform. For more information about role-based capabilities, see Define roles on the Splunk platform with capabilities , in Securing the Splunk Platform .", "code_examples": [{"language": "spl", "code": "| walklex index=<target-index> |whereNOT like(term,\"%::%\") | stats sum(count) by term"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "walklex", "section_heading": "Usage", "section_id": "id_514c3431_0af3_45aa_a1ac_e52a0542f286--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/walklex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:50.288644+00:00", "version": "10.2"}}
{"id": "c1b41e6ad7db2ec4", "content": "1. Return the total count for each term in a specific bucket The following example returns all of the terms in each bucket of the _internal index and finds the total count for each term. 2. Specifying multiple indexes The following example returns all of the terms that start with foo in each bucket of the _internal and _audit indexes. 3. Use a pattern to locate indexed field terms The following example returns all of the indexed field terms for each bucket that end with bar in the _internal index. 4. Return all field names of indexed fields The following example returns all of the field names of indexed fields in each bucket of the _audit index.", "code_examples": [{"language": "spl", "code": "| walklex index=_internal | stats sum(count) BY term"}, {"language": "spl", "code": "| walklex prefix=foo index=_internal index=_audit"}, {"language": "spl", "code": "| walklex pattern=*bartype=fieldvalue index=_internal"}, {"language": "spl", "code": "| walklextype=field index=_audit"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "walklex", "section_heading": "Basic examples", "section_id": "id_3a6e9608_61cd_466d_b2bd_cbb2435267fd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/walklex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:50.288649+00:00", "version": "10.2"}}
{"id": "dc8283401fb9da93", "content": "Commands metadata tstats", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "walklex", "section_heading": "See also", "section_id": "efa56143_84ae_497b_bc12_73fa6f46916c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/walklex", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:14:50.288653+00:00", "version": "10.2"}}
{"id": "f72fdc97fe5c233c", "content": "Replaces a field value with higher-level grouping, such as replacing filenames with directories. Returns the maxcount events, by taking the incoming events and rolling up multiple sources into directories, by preferring directories that have many files but few events. The field with the path is PATHFIELD (e.g., source), and strings are broken up by a separator character. The default pathfield=source; sizefield=totalCount; maxcount=20; countfield=totalCount; sep=\"/\" or \"\\\\\", depending on the operation system.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "bucketdir", "section_heading": "Description", "section_id": "id_4a75a928_af4d_403c_b9af_64b6fb5580dc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/bucketdir", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:06.216570+00:00", "version": "10.2"}}
{"id": "92c7adc93b32fb53", "content": "bucketdir pathfield=<field> sizefield=<field> [maxcount=<int>] [countfield=<field>] [sep=<char>] Required arguments pathfield Syntax: pathfield=<field> Description: Specify a field name that has a path value. sizefield Syntax: sizefield=<field> Description: Specify a numeric field that defines the size of bucket. Optional arguments countfield Syntax: countfield=<field> Description: Specify a numeric field that describes the count of events. maxcount Syntax: maxcount=<int> Description: Specify the total number of events to bucket. sep Syntax: <char> Description: The separating character. Specify either a forward slash \"/\" or double back slashes \"\\\\\", depending on the operating system.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "bucketdir", "section_heading": "Syntax", "section_id": "f073eb41_027f_4564_96e5_8df454257baa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/bucketdir", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:06.216578+00:00", "version": "10.2"}}
{"id": "5155ebda15cadd19", "content": "The bucketdir command is a streaming command. It is distributable streaming by default, but centralized streaming if the local setting specified for the command in the commands.conf file is set to true. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "bucketdir", "section_heading": "Usage", "section_id": "id_7f99a78a_f9e1_48d4_aad2_6e5540a42752--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/bucketdir", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:06.216582+00:00", "version": "10.2"}}
{"id": "ed66b45585289ae3", "content": "Example 1: Return 10 best sources and directories.", "code_examples": [{"language": "spl", "code": "... | topsource| bucketdir pathfield=sourcesizefield=count maxcount=10"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "bucketdir", "section_heading": "Examples", "section_id": "fd060733_ded7_4f57_9b53_a082c50dc398--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/bucketdir", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:06.216586+00:00", "version": "10.2"}}
{"id": "46d231891e2766d2", "content": "cluster , dedup", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "bucketdir", "section_heading": "See also", "section_id": "id_1d7a5925_955b_4c58_88c3_dd0b29dba015--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/bucketdir", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:06.216590+00:00", "version": "10.2"}}
{"id": "4002273bf36c0d51", "content": "Converts events into JSON objects. You can specify which fields get converted by identifying them through exact match or through wildcard expressions. You can also apply specific JSON datatypes to field values using datatype functions. The tojson command converts multivalue fields into JSON arrays. When fields are specifically named in a tojson search, the command generates JSON objects that are limited to the values of just those named fields. If no fields are specified for tojson , tojson generates JSON objects for all fields that would otherwise be returned by the search.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "tojson", "section_heading": "Description", "section_id": "id_6cfd136a_2980_4e02_b87a_a1ca7928f500--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tojson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:23.506602+00:00", "version": "10.2"}}
{"id": "cec048b545190aaa", "content": "Required syntax is in bold. | tojson [<tojson-function>]... [default_type=<datatype>] [fill_null=<boolean>] [include_internal=<boolean>] [output_field=<string>] Optional arguments tojson-function Syntax: [auto | bool | json | none | num | str](<wc-field>)... Description: Applies JSON datatype functions to values of named fields. See Usage for details about how tojson interprets these datatype functions, and how tojson applies datatypes to field values when it converts events into JSON objects. If you provide no fields, the tojson processor creates JSON objects for each event that include all available fields. In other words, it applies none(*) to the search. Default: none(*) default_type Syntax: default_type=<datatype> Description: Specifies the datatype that the tojson processor should apply to fields that aren't specifically associated with a datatype function. Default: none fill_null Syntax: fill_null=<boolean> Description: When set to true, tojson outputs a literal null value when tojson skips a value. For example, normally, when tojson tries to apply the json datatype to a field that does not have proper JSON formatting, tojson skips the field. However, if fill_null=true , the tojson processor outputs a null value Default: false include_internal Syntax: include_internal=<boolean> Description: When set to true, tojson includes internal fields such as _time , _indextime , or _raw in its JSON object output. Default: false output_field Syntax: output_field=<string> Description: Specifies the name of the field to which the tojson search processor writes the output JSON objects. Default: _raw", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "tojson", "section_heading": "Syntax", "section_id": "id_251721bd_7fef_4afa_a54e_e5e484321701--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tojson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:23.506614+00:00", "version": "10.2"}}
{"id": "aa45e462d53abfbe", "content": "The tojson command is a streaming command , which means it operates on each event as it is returned by the search. See Types of commands. Apply JSON datatypes to field values The tojson command applies JSON datatypes to field values according to logic encoded in its datatype functions. You can assign specific datatype functions to fields when you write a tojson search. Alternatively, you can name a set of fields without associating them with datatype functions, and then identify a default_type that tojson can apply to those unaffiliated fields. If you do not specify any fields for the tojson command, the tojson returns JSON objects for each field that can possibly be returned by the search at that point, and applies the none datatype function to the values of those fields. The none datatype function applies the numeric datatype to field values that are purely numeric, and applies the string datatype to all other field values. The following table explains the logic that the various datatype functions use to apply datatypes to the values of the fields with which they are associated. When a field includes multivalues, tojson outputs a JSON array and applies the datatype function logic to each element of the array.", "code_examples": [], "tables": [{"headers": ["Datatype function", "Conversion logic"], "rows": [["auto", "Converts all values of the specified field into JSON-formatted output. Automatically determines the field datatypes.If the value is numeric, the JSON output has a numeric output and includes a literal numeric.If the value is the stringtrueorfalsethe JSON output has a Boolean type.If the value is a literalnull,, the JSON output has a null type and includes a null value.If the value is a string other than the previously mentioned strings,tojsonexamines the string. If it is proper JSON,tojsonoutputs a nested JSON object. If it is not proper JSON,tojsonincludes the string in the output."], ["bool", "Converts valid values of the specified field to the Boolean datatype, and skips invalid values, using string validation.If the value is a number,tojsonoutputsfalseonly if that value is0. Otherwisetojsonoutputsfalse.If the value is a string,tojsonoutputsfalseonly if the value isfalse,f, orno.Thetojsonprocessor outputstrueonly if the value is codetrue,t, oryes. If the value does not fit into those two sets of strings, it is skipped.The validation for thebooldatatype function is case insensitive. This means that it also interpretsFALSE,False,F, andNOasfalse."], ["json", "Converts all values of the specified field to the JSON type, using string validation. Skips values with invalid JSON.If the value is a number,tojsonoutputs that number.If the value is a string,tojsonoutputs the string as a JSON block.If the value is invalid JSON,tojsonskips it."], ["none", "Outputs all values for the specified field in the JSON type. Does not apply string validation.If the value is a number,tojsonoutputs a numeric datatype in the JSON block.If the value is a string,tojsonoutputs a string datatype."], ["num", "Converts all values of the specified field to the numeric type, using string validation.If the value is a number,tojsonoutputs that value and gives it the numeric datatype.If the value is a string,tojsonattempts to parse the string as a number. If it cannot, it skips the value."], ["str", "Converts all values of the specified field into the string datatype, using string validation.Thetojsonprocessor applies the string type to all values of the specified field, even if they are numbers, Boolean values, and so on."]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "tojson", "section_heading": "Usage", "section_id": "id_2ca5b6a9_64c4_45fb_8f87_272335dbb3e8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tojson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:23.506636+00:00", "version": "10.2"}}
{"id": "cc0db2782a2b97d6", "content": "1. Convert all events returned by a search into JSON objects This search of index=_internal converts all events it returns for its time range into JSON-formatted data. Because the search string does not assign datatype functions to specific fields, by default tojson applies the none datatype function to all fields returned by the search. This means all of their values get either the numeric or string datatypes. For example, say you start with events that look like this: After being processed by tojson , such events have JSON formatting like this: 2. Specify different datatypes for 'date' fields The following search of the _internal index converts results into JSON objects that have only the date_* fields from each event. The numeric datatype is applied to all date_hour field values. The string datatype is applied to all other date field values. This search produces JSON objects like this: Note that all fields that do not start with date_ have been stripped from the output. 3. Limit JSON object output and apply datatypes to the field values This search returns JSON objects only for the name , age , and isRegistered fields. It uses the auto datatype function to have tojson automatically apply appropriate JSON datatypes to the values of those fields. 4. Convert all events into JSON objects and apply appropriate datatypes to all field values This search converts all of the fields in each event returned by the search into JSON objects. It uses the auto datatype function in conjunction with a wildcard to apply appropriate datatypes to the values of all fields returned by the search. Notice that this search references the auto datatype function, which ensures that Boolean, JSON, and null field values are appropriately typed alongside numeric and string values. Alternatively, you can use default_type to apply the auto datatype function to all fields returned by a search: 5. Apply the Boolean datatype to a specific field This example generates JSON objects containing values of the isInternal field. It uses the bool datatype function to apply the Boolean datatype to those field values. 6. Include internal fields and assign a 'null' value to skipped fields This example demonstrates usage of the include_internal and fill_null arguments. 7. Designate a default datatype for a set of fields and write the JSON objects to another field This search generates JSON objects based on the values of four fields. It uses the default_type argument to convert the first three fields to the num datatype. It applies the string datatype to a fourth field. Finally, it writes the finished JSON objects to the field my_JSON_field .", "code_examples": [{"language": "spl", "code": "index=_internal | tojson"}, {"language": "spl", "code": "12-18-2020 18:19:25.601 +0000 INFO  Metrics - group=thruput, name=thruput, instantaneous_kbps=5.821, instantaneous_eps=27.194, average_kbps=5.652, total_k_processed=444500.000, kb=180.443, ev=843, load_average=19.780"}, {"language": "spl", "code": "{ [-]\n   component: Metrics\n   date_hour: 18\n   date_mday: 18\n   date_minute: 22\n   date_month: december\n   date_second: 9\n   date_wday: friday\n   date_year: 2020\n   date_zone: 0\n   event_message: group=thruput, name=thruput, instantaneous_kbps=2.914, instantaneous_eps=13.903, average_kbps=5.062, total_k_processed=398412.000, kb=90.338, ev=431, load_average=14.690\n   group: thruput\n   host: sh1\n   index: _internal\n   linecount: 1\n   log_level: INFO\n   name: thruput\n   punct: --_::._+____-_=,_=,_=.,_=.,_=.,_=.,_=.,_=,_=.source: /opt/splunk/var/log/splunk/metrics.log\n   sourcetype: splunkd\n   splunk_server: idx2\n   timeendpos: 29\n   timestartpos: 0\n}"}, {"language": "spl", "code": "index=_internal | tojson num(date_hour) str(date_*)"}, {"language": "spl", "code": "{ [-]\n   date_hour: 18\n   date_mday: 18\n   date_minute: 28\n   date_month: december\n   date_second: 45\n   date_wday: friday\n   date_year: 2020\n   date_zone: 0\n}"}, {"language": "spl", "code": "... | tojson auto(name) auto(age) auto(isRegistered)"}, {"language": "spl", "code": "... | tojson auto(*)"}, {"language": "spl", "code": "... | tojson default_type=auto"}, {"language": "spl", "code": "... | tojson bool(isInternal)"}, {"language": "spl", "code": "... | tojson include_internal=truefill_null=true"}, {"language": "spl", "code": "... | tojson age height weight str(name) default_type=num output_field=my_JSON_field"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "tojson", "section_heading": "Examples", "section_id": "id_16bd2d56_600a_460c_95de_8caf414cca07--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tojson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:23.506643+00:00", "version": "10.2"}}
{"id": "4c3a158209a18272", "content": "Commands fromjson Evaluation functions JSON functions", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "tojson", "section_heading": "See also", "section_id": "e209d295_fc32_475e_9525_7e7231bfc199--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tojson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:23.506647+00:00", "version": "10.2"}}
{"id": "767bcfc72d87c004", "content": "Creates a higher-level grouping, such as replacing filenames with directories. Replaces the attr attribute value with a more generic value, which is the result of grouping the attr value with other values from other results, where grouping occurs by tokenizing the attr value on the sep separator value. For example, the folderize command can group search results, such as those used on the Splunk Web home page, to list hierarchical buckets (e.g. directories or categories). Rather than listing 200 sources, the folderize command breaks the source strings by a separator (e.g. / ) and determines if looking only at directories results in the number of results requested.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "folderize", "section_heading": "Description", "section_id": "id_1df5e90e_f0df_4498_ac06_14cd7082d9e8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/folderize", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:38.540715+00:00", "version": "10.2"}}
{"id": "577c226243d2a98f", "content": "folderize attr=<string> [sep=<string>] [size=<string>] [minfolders=<int>] [maxfolders=<int>] Arguments attr Syntax: attr=<string> Description: Replaces the attr attribute value with a more generic value, which is the result of grouping it with other values from other results, where grouping occurs by tokenizing the attribute (attr) value on the separator (sep) value. sep Syntax: sep=<string> Description: Specify a separator character used to construct output field names when multiple data series are used in conjunction with a split-by field. Default : :: size Syntax: size=<string> Description: Supply a name to be used for the size of the folder. Default : totalCount minfolders Syntax: minfolders=<int> Description: Set the minimum number of folders to group. Default: 2 maxfolders Syntax: maxfolders=<int> Description: Set the maximum number of folders to group. Default: 20", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "folderize", "section_heading": "Syntax", "section_id": "id_1faa682f_396a_4868_bd9f_a766b4ef2b72--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/folderize", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:38.540724+00:00", "version": "10.2"}}
{"id": "88b0cc9d2b618a9c", "content": "1. Group results into folders based on URI Consider this search. The following image shows the results of the search run using the All Time time range. Many of the results start with /en-US/account. Because some of the URIs are very long, the image does not show the second column on the far right. That column is the count(uri) column created by the stats command. Using the folderize command, you can summarize the URI values into more manageable groupings. The following image shows the URIs grouped in the result set. In this example, the count(uri) column is the count of the unique URIs that were returned from the stats command. The memberCount column shows the count of the URIs in each group. For example, the /en-US/ URI was found 22 times in the events, as shown in the count(uri) column. When the folderize command arranges the URI into groups, there is only 1 member in the /en-US/ group. Whereas the URIs that start with /services/ occurred 10088 times in the events, but there are only 1648 unique members in the /services/* group.", "code_examples": [{"language": "spl", "code": "index=_internal | stats count(uri) by uri"}, {"language": "spl", "code": "index=_internal | stats count(uri) by uri | folderize size=count(uri) attr=uri sep=\"/\""}], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "folderize", "section_heading": "Examples", "section_id": "f455573a_84e3_461a_8070_0bf7495a9d62--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/folderize", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:38.540729+00:00", "version": "10.2"}}
{"id": "8133c5c0efa0185f", "content": "Calls an external python program that can modify or generate search results. Splunk Cloud Platform You must create a private app that contains your custom script. If you are a Splunk Cloud administrator with experience creating private apps, see Manage private apps in your Splunk Cloud Platform deployment in the Splunk Cloud Admin Manual. If you have not created private apps, contact your Splunk account representative for help with this customization. Splunk Enterprise Scripts must be declared in the commands.conf file and be located in the $SPLUNK_HOME/etc/apps/<app_name>/bin/ directory. The script is executed using $SPLUNK_HOME/bin/python. CAUTION: This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "script", "section_heading": "Description", "section_id": "d8b3fc49_175a_4be3_9093_3f73f0fb4ff7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/script", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:56.200624+00:00", "version": "10.2"}}
{"id": "2f1ea56c03c3e71f", "content": "script <script-name> [<script-arg>...] [maxinputs=<int>] Required arguments script-name Syntax: <string> Description: The name of the scripted search command to run, as defined in the commands.conf file. Optional arguments maxinputs Syntax: maxinputs=<int> Description: Specifies how many of the input results are passed to the script per invocation of the command. The script command is invoked repeatedly in increments according to the maxinputs argument until the search is complete and all of the results have been displayed. Do not change the value of maxinputs unless you know what you are doing. Default: 50000 script-arg Syntax: <string> ... Description: One or more arguments to pass to the script. If you are passing multiple arguments, delimit each argument with a space.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "script", "section_heading": "Syntax", "section_id": "id_77b563cf_39f1_4fa8_9673_a48164a8ccd3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/script", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:56.200631+00:00", "version": "10.2"}}
{"id": "26a8418985ea589b", "content": "The script command is effectively an alternative way to invoke custom search commands. See Create custom search commands for apps in Splunk Cloud Platform or Splunk Enterprise in the Developer Guide on the Developer Portal. The following search: is the same as this search: Note: Some functions of the script command have been removed over time. The explicit choice of Perl or Python as an argument is no longer functional and such an argument is ignored. If you need to write Perl search commands, you must declare them as Perl in the commands.conf file. This is not recommended, as you need to determine a number of underdocumented things about the input and output formats. Additionally, support for the etc/searchscripts directory has been removed. Search commands must be located in the bin directory of an app in your Splunk deployment. For more information about creating custom search commands for apps in Splunk Cloud Platform or Splunk Enterprise, see the Developer Guide for Splunk Cloud Platform and Splunk Enterprise .", "code_examples": [{"language": "spl", "code": "| script commandname"}, {"language": "spl", "code": "| commandname"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "script", "section_heading": "Usage", "section_id": "fe0f6e35_045e_49bf_91f3_ea1d94448ecb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/script", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:56.200636+00:00", "version": "10.2"}}
{"id": "f15f7bf973752102", "content": "Example 1: Run the Python script \"myscript\" with arguments, myarg1 and myarg2; then, email the results.", "code_examples": [{"language": "spl", "code": "... | script myscript myarg1 myarg2 | sendemail to=david@splunk.com"}], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "script", "section_heading": "Examples", "section_id": "id_57c6cb4b_b5d6_4834_b2bb_6b0068492c9a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/script", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:15:56.200639+00:00", "version": "10.2"}}
{"id": "ddf87a1c95d33ce1", "content": "The fieldsummary command calculates summary statistics for all fields or a subset of the fields in your events. The summary information is displayed as a results table.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "fieldsummary", "section_heading": "Description", "section_id": "ea446fd5_eb9c_44d7_b291_6e2080cf090d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fieldsummary", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:13.739425+00:00", "version": "10.2"}}
{"id": "62bac15127452465", "content": "fieldsummary [maxvals=<unsigned_int>] [<wc-field-list>] Optional arguments maxvals Syntax: maxvals=<unsigned_int> Description: Specifies the maximum distinct values to return for each field. Cannot be negative. Set maxvals = 0 to return all available distinct values for each field. Default : 100 wc-field-list Syntax: <field> ... Description: A single field name or a space-delimited list of field names. You can use the asterisk ( * ) as a wildcard to specify a list of fields with similar names. For example, if you want to specify all fields that start with \"value\", you can use a wildcard such as value* .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "fieldsummary", "section_heading": "Syntax", "section_id": "id_8fa759a7_82c5_4477_9740_f808ecee3773--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fieldsummary", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:13.739433+00:00", "version": "10.2"}}
{"id": "34dfee30436e863d", "content": "The fieldsummary command is a dataset processing command. See Command types. The fieldsummary command displays the summary information in a results table. The following information appears in the results table:", "code_examples": [], "tables": [{"headers": ["Summary field name", "Description"], "rows": [["field", "The field name in the event."], ["count", "The number of events/results with that field."], ["distinct_count", "The number of unique values in the field."], ["is_exact", "Whether or not the field is exact. This is related to the distinct count of the field values.  If the number of values of the field exceedsmaxvals, thenfieldsummarywill stop retaining all the values and compute an approximate distinct count instead of an exact one. 1 means it is exact, 0 means it is not."], ["max", "If the field is numeric, the maximum of its value."], ["mean", "If the field is numeric, the mean of its values."], ["min", "If the field is numeric, the minimum of its values."], ["numeric_count", "The count of numeric values in the field. This would not include NULL values."], ["stdev", "If the field is numeric, the standard deviation of its values."], ["values", "The distinct values of the field and count of each value. The values are sorted first by highest count and then by distinct value, in ascending order."]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "fieldsummary", "section_heading": "Usage", "section_id": "id_924d01ed_bee6_4b82_8e96_5434a9675f85--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fieldsummary", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:13.739443+00:00", "version": "10.2"}}
{"id": "16657af06014be73", "content": "1. Return summaries for all fields This example returns summaries for all fields in the _internal index from the last 15 minutes. In this example, the results in the max , min , and stdev fields are formatted to display up to 4 decimal points. 2. Return summaries for specific fields This example returns summaries for fields in the _internal index with names that contain \"size\" and \"count\". The search returns only the top 10 values for each field from the last 15 minutes.", "code_examples": [{"language": "spl", "code": "index=_internal earliest=-15m latest=now | fieldsummary"}, {"language": "spl", "code": "index=_internal earliest=-15m latest=now | fieldsummary maxvals=10 *size* *count*"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "fieldsummary", "section_heading": "Examples", "section_id": "d291c1b8_a33a_48c7_b056_7b848039e976--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fieldsummary", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:13.739448+00:00", "version": "10.2"}}
{"id": "2758c613cd568981", "content": "analyzefields , anomalies , anomalousvalue , stats", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "fieldsummary", "section_heading": "See also", "section_id": "fb19789d_7ee5_4965_9170_1b3f0c562749--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fieldsummary", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:13.739453+00:00", "version": "10.2"}}
{"id": "5809c6074d695b7b", "content": "Retrieves event metadata from indexes based on terms in the <logical-expression>.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "metasearch", "section_heading": "Description", "section_id": "id_9e5a6cbe_a56c_4b70_8adb_ae9c0613e285--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/metasearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:29.228896+00:00", "version": "10.2"}}
{"id": "27966ea86a6f119b", "content": "metasearch [<logical-expression>] Optional arguments <logical-expression> Syntax: <time-opts> | <search-modifier> | [NOT] <logical-expression> | <index-expression> | <comparison-expression> | <logical-expression> [OR <logical-expression>] Description: Includes time and search modifiers, comparison and index expressions. Logical expression <comparison-expression> Syntax: <field><cmp><value> Description: Compare a field to a literal value or values of another field. <index-expression> Syntax: \"<string>\" | <term> | <search-modifier> <time-opts> Syntax: [<timeformat>] [<time-modifier>]... Comparison expression <cmp> Syntax: = | != | < | <= | > | >= Description: Comparison operators. <field> Syntax: <string> Description: The name of one of the fields returned by the metasearch command. See Usage. <lit-value> Syntax: <string> | <num> Description: An exact, or literal, value of a field that is used in a comparison expression. <value> Syntax: <lit-value> | <field> Description: In comparison-expressions, the literal value of a field or another field name. The <lit-value> must be a number or a string. Index expression <search-modifier> Syntax: <field-specifier> | <savedsplunk-specifier> | <tag-specifier> Time options The search allows many flexible options for searching based on time. For a list of time modifiers, see the topic Time modifiers for search in the Search Manual. <timeformat> Syntax: timeformat=<string> Description: Set the time format for starttime and endtime terms. By default, timestamp is formatted: timeformat=%m/%d/%Y:%H:%M:%S. <time-modifier> Syntax: earliest=<time_modifier> | latest=<time_modifier> Description: Specify start and end times using relative or absolute time. For more about the time modifier index, see Specify time modifiers in your search in the Search Manual .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "metasearch", "section_heading": "Syntax", "section_id": "id_36fda100_6294_4855_ac7f_1fe9576aab51--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/metasearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:29.228904+00:00", "version": "10.2"}}
{"id": "8c67a1e1f78be9fc", "content": "The metasearch command is an event-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search. The metasearch command returns these fields:", "code_examples": [], "tables": [{"headers": ["Field", "Description"], "rows": [["host", "A default field that contains the host name or IP address of the network device that generated an event."], ["index", "The repository for data. When the Splunk platform indexes raw data, it transforms the data into searchable events."], ["source", "A default field that identifies the source of an event, that is, where the event originated."], ["sourcetype", "A default field that identifies the data structure of an event."], ["splunk_server", "The name of the instance where Splunk Enterprise is installed."], ["_time", "The _time field contains an event's timestamp expressed in UNIX time."]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "metasearch", "section_heading": "Usage", "section_id": "id_63dde957_6a15_4fc6_a001_345f2d5e52be--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/metasearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:29.228911+00:00", "version": "10.2"}}
{"id": "2e49c180fa29da58", "content": "Example 1: Return metadata on the default index for events with \"404\" and from host \"webserver1\".", "code_examples": [{"language": "spl", "code": "| metasearch 404 host=\"webserver1\""}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "metasearch", "section_heading": "Examples", "section_id": "a41e3155_02d0_4a84_b342_d6fef6513ede--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/metasearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:29.228915+00:00", "version": "10.2"}}
{"id": "cc705a01d60db715", "content": "Commands metadata search", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "metasearch", "section_heading": "See also", "section_id": "id_72ffc8f3_8e7c_43f9_a444_678abff341fc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/metasearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:29.228919+00:00", "version": "10.2"}}
{"id": "813479d0161c945d", "content": "Extracts the xpath value from field and sets the outfield attribute. Note: Due to limitations with XML extraction, the xpath command returns empty results when input XML strings have prologue headers, such as xml version or DOCTYPE. As a result, use the spath command instead of the xpath command when extracting XML content. Syntax xpath [outfield=<field>] <xpath-string> [field=<field>] [default=<string>] Required arguments xpath-string Syntax: <string> Description: Specifies the XPath reference. Optional arguments field Syntax: field=<field> Description: The field to find and extract the referenced xpath value from. Default: _raw outfield Syntax: outfield=<field> Description: The field to write, or output, the xpath value to. Default: xpath default Syntax: default=<string> Description: If the attribute referenced in xpath doesn't exist, this specifies what to write to the outfield. If this isn't defined, there is no default value.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "xpath", "section_heading": "Description", "section_id": "id_36698bc2_2f1b_4354_b6fa_3748646fc799--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xpath", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:46.435737+00:00", "version": "10.2"}}
{"id": "732f91e56dda7e8e", "content": "The xpath command is a distributable streaming command. See Command types. The xpath command supports the syntax described in the Python Standard Library 19.7.2.2. Supported XPath syntax .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "xpath", "section_heading": "Usage", "section_id": "id_6171e6ca_dddb_4070_9c0c_c74b13254d28--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xpath", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:46.435745+00:00", "version": "10.2"}}
{"id": "84b461f14ff23553", "content": "1. Extract values from a single element in _raw XML events You want to extract values from a single element in _raw XML events and write those values to a specific field. The _raw XML events look like this: Extract the nickname values from _raw XML events. Output those values to the name field. 2. Extract multiple values from _raw XML events Extract multiple values from _raw XML events The _raw XML events look like this: Extract the values from the identity_id element from the _raw XML events: This search returns two results: identity_id=3017669 and identity_id=1037669. To extract a combination of two elements, sname with a specific value and instrument_id , use this search: Because you specify sname='BARC' , this search returns one result: instrument_id=912383KM1. 3. Testing extractions from XML events You can use the makeresults command to test xpath extractions. You must add field=xml to the end of your search. For example:", "code_examples": [{"language": "spl", "code": "<foo>\n      <bar nickname=\"spock\">\n      </bar>\n   </foo>\n   <foo>\n      <bar nickname=\"scotty\">\n      </bar>\n   </foo>\n   <foo>\n      <bar nickname=\"bones\">\n      </bar>\n   </foo>"}, {"language": "spl", "code": "sourcetype=\"xml\"| xpath outfield=name\"//bar/@nickname\""}, {"language": "spl", "code": "<DataSet xmlns=\"\">\n        <identity_id>3017669</identity_id>\n        <instrument_id>912383KM1</instrument_id>\n        <transaction_code>SEL</transaction_code>\n        <sname>BARC</sname>\n        <currency_code>USA</currency_code>\n   </DataSet> \n\n   <DataSet xmlns=\"\">\n        <identity_id>1037669</identity_id>\n        <instrument_id>219383KM1</instrument_id>\n        <transaction_code>SEL</transaction_code>\n        <sname>TARC</sname>\n        <currency_code>USA</currency_code>\n   </DataSet>"}, {"language": "spl", "code": "... | xpath outfield=identity_id\"//DataSet/identity_id\""}, {"language": "spl", "code": "... | xpath outfield=instrument_id\"//DataSet[sname='BARC']/instrument_id\""}, {"language": "spl", "code": "| makeresults\n|evalxml=\"<DataSet xmlns=\\\"\\\">\n        <identity_id>1037669</identity_id>\n        <instrument_id>219383KM1</instrument_id>\n        <transaction_code>SEL</transaction_code>\n        <sname>TARC</sname>\n        <currency_code>USA</currency_code>\n   </DataSet>\"| xpath outfield=identity_id\"//DataSet/identity_id\"field=xml"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "xpath", "section_heading": "Examples", "section_id": "a634b55a_323a_4775_8c64_32e45141c08f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xpath", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:46.435751+00:00", "version": "10.2"}}
{"id": "4c9346be4dcac28f", "content": "extract , kvform , multikv , rex , spath , xmlkv", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "xpath", "section_heading": "See also", "section_id": "a268641e_54ae_4313_bf29_99040dc2ef2c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xpath", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:16:46.435756+00:00", "version": "10.2"}}
{"id": "5a34b08f8b729f0b", "content": "The rest command reads a Splunk REST API endpoint and returns the resource data as a search result. Splunk Cloud Platform For information about Splunk REST API endpoints, see the Splunk platform REST API Reference Manual. Splunk Enterprise For information about the REST API, see the Splunk platform REST API User Manual. For information about Splunk REST API endpoints, see the Splunk platform REST API Reference Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "rest", "section_heading": "Description", "section_id": "id_10c86634_4146_4071_b4d8_0e6782345d1c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rest", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:01.152047+00:00", "version": "10.2"}}
{"id": "733f2ba0d5c35311", "content": "The required syntax is in bold. | rest <rest-uri> [count=<int>] [strict=<bool>] [splunk_server=<wc-string>] [splunk_server_group=<wc-string>]... [timeout=<int>] [<get-arg-name>=<get-arg-value>]... Required arguments rest-uri Syntax: <uri> Description: URI path to the Splunk REST API endpoint. Optional arguments count Syntax: count=<int> Description: Limits the number of results returned from each REST call. For example, you have four indexers and one search head. You set the limit to count=25000. This results in a total limit of 125000, which is 25000 x 5. When count=0, there is no limit. Default: 0 get-arg-name Syntax: <string> Description: REST argument name for the REST endpoint. For the specific set of arguments supported by a specific endpoint, see the Splunk platform REST API Reference Manual. get-arg-value Syntax: <string> Description: REST argument value for the REST endpoint. For the specific set of arguments supported by a specific endpoint, see the Splunk platform REST API Reference Manual. splunk_server Syntax: splunk_server=<wc-string> Description: Specifies the distributed search peer from which to return results. You can specify only one splunk_server argument, However, you can use a wildcard character when you specify the server name to indicate multiple servers. For example, you can specify splunk_server=peer01 or splunk_server=peer*. Use local to refer to the search head. Default: All configured search peers return information splunk_server_group Syntax: splunk_server_group=<wc-string>... Description: Limits the results to one or more server groups. You can specify a wildcard character in the string to indicate multiple server groups. strict Syntax: strict=<bool> Description: When set to true this argument forces the search to fail completely if rest raises an error. This happens even when the errors apply to a subsearch. When set to false , many rest error conditions return warning messages but do not otherwise cause the search to fail. Certain error conditions cause the search to fail even when strict=false. Default: false timeout Syntax: timeout=<int> Description: Specify the timeout, in seconds, to wait for the REST endpoint to respond. Specify timeout=0 to indicate no limit on the time to wait for the REST endpoint to respond. Default: 60", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "rest", "section_heading": "Syntax", "section_id": "ecc79cdd_9f1a_4ede_9acb_151e11f16878--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rest", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:01.152054+00:00", "version": "10.2"}}
{"id": "ee8f97dc455d94af", "content": "The rest command authenticates using the ID of the person that runs the command. Strict error handling Use the strict argument to make rest searches fail whenever they encounter an error condition. You can set this at the system level for all rest searches by changing restprocessor_errors_fatal in limits.conf. Note: If you use Splunk Cloud Platform, file a Support ticket to change the restprocessor_errors_fatal setting. Use the strict argument to override the restprocessor_errors_fatal setting for a rest search.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "rest", "section_heading": "Usage", "section_id": "c54d901b_eaa2_4aa5_a7fa_1aa5b3643f38--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rest", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:01.152058+00:00", "version": "10.2"}}
{"id": "97c75527d4ba1bbf", "content": "1. Access saved search jobs 2. Find all saved searches with searches that include a specific sourcetype Find all saved searches with search strings that include the speccsv sourcetype. 3. Showing events only associated with the current user To create reports that only show events associated with the logged in user, you can add the current search user to all events. 4. Use the GET method pagination and filtering arguments Most GET methods support a set of pagination and filtering arguments. To determine if an endpoint supports these arguments, find the endpoint in the Splunk platform REST API Reference Manual. Click Expand on the GET method and look for a link to the Pagination and filtering arguments topic. For more information about the Pagination and filtering arguments, see the Request and response details in the Splunk Cloud Platform REST API Reference manual. The following example uses the search argument for the saved/searches endpoint to identify if a search is scheduled and deactivated. The search looks for scheduled searches on Splunk servers that match the Monitoring Console role of \"search heads\". Here is an explanation for each part of this search: 5. Return a table of results with custom endpoints When you create a custom endpoint, you can format the response to return a table of results. The following example shows a custom endpoint: Here's an example of the response you can use to return a table of results:", "code_examples": [{"language": "spl", "code": "| rest /services/search/jobscount=0 splunk_server=local| search isSaved=1"}, {"language": "spl", "code": "| rest /services/saved/searches splunk_server=local| rename search AS saved_search | fields author, title, saved_search | search saved_search=*speccsv*"}, {"language": "spl", "code": "* | head 10 | join [ | rest splunk_server=local/services/authentication/current-context | rename username as auth_user_id | fields auth_user_id ]"}, {"language": "spl", "code": "| rest /servicesNS/-/-/saved/searches splunk_server_group=dmc_group_search_head timeout=0 search=\"is_scheduled=1\"search=\"disabled=0\""}, {"language": "spl", "code": "|rest /servicesNS/-/-/saved/searches"}, {"language": "spl", "code": "splunk_server_group=dmc_group_search_head"}, {"language": "spl", "code": "timeout=0"}, {"language": "spl", "code": "search=\"is_scheduled=1\""}, {"language": "spl", "code": "search=\"disabled=0\""}, {"language": "spl", "code": "| rest /servicesNS/-/myapp/myapp/endpoint"}, {"language": "spl", "code": "{\"links\": {},\"entry\": [\n                {\"content\": {\"name\":\"world\",\"fish\":\"salmon\"}},\n                {\"content\": {\"name\":\"muu\",\"fish\":\"whale\"}}\n            ]\n    }"}], "tables": [{"headers": ["Description", "Part of the search"], "rows": [["The name of the REST call.", "|rest /servicesNS/-/-/saved/searches"], ["Look only at Splunk servers that match the Monitoring Console role of \"search heads\".", "splunk_server_group=dmc_group_search_head"], ["Don't time out waiting for the REST call to finish.", "timeout=0"], ["Look only for scheduled searches.", "search=\"is_scheduled=1\""], ["Look only for active searches (not deactivated).", "search=\"disabled=0\""]]}], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "rest", "section_heading": "Examples", "section_id": "id_7d451809_f03a_4053_b410_61af58aa84ba--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rest", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:01.152065+00:00", "version": "10.2"}}
{"id": "1cba23d6b7baa971", "content": "Returns the specified number of rows (search results) as columns (list of field values), such that each search row becomes a column.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "transpose", "section_heading": "Description", "section_id": "id_662546e9_99b6_4fa7_aad9_70158eb9a2ce--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transpose", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:17.141241+00:00", "version": "10.2"}}
{"id": "190e2b04df455aaa", "content": "The required syntax is in bold. transpose [int] [column_name=<string>] [header_field=<field>] [include_empty=<bool>] Required arguments None. Optional arguments column_name Syntax: column_name=<string> Description: The name of the first column that you want to use for the transposed rows. This column contains the names of the fields. Default: column header_field Syntax: header_field=<field> Description: The field in your results to use for the names of the columns (other than the first column) in the transposed data. Default: row 1, row 2, row 3, and so on. include_empty Syntax: include_empty=<bool> Description: Specify whether to include (true) or not include (false) fields that contain empty values. Default: true int Syntax: <int> Description: Limit the number of rows to transpose. To transpose all rows, specify | transpose 0 , which indicates that the number of rows to transpose is unlimited. Default: 5", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "transpose", "section_heading": "Syntax", "section_id": "dffcd73b_2c38_4ff7_8ff6_ea5667d745f4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transpose", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:17.141250+00:00", "version": "10.2"}}
{"id": "9507c92f2d749645", "content": "When you use the transpose command the field names used in the output are based on the arguments that you use with the command. By default the field names are: column , row 1 , row 2 , and so forth.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "transpose", "section_heading": "Usage", "section_id": "c5519e0c_510c_4f9e_a530_d7580beacefd--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transpose", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:17.141255+00:00", "version": "10.2"}}
{"id": "46777deb853108ad", "content": "1. Transpose the results of a chart command Use the default settings for the transpose command to transpose the results of a chart command. Suppose you run a search like this: The search produces the following search results: When you add the transpose command to the end of the search, the results look something like this: 2. Specifying a header field In the previous example, the default settings for the transpose command are used in the search: The results look like this: Instead of using the default field names like row 1, row 2, and so forth, you can use the values in a field for the field names by specifying the header_field argument. The results look like this: 3. Count the number of events by sourcetype and transpose the results to display the 3 highest counts Count the number of events by sourcetype and display the sourcetypes with the highest count first. Use the transpose command to convert the rows to columns and show the source types with the 3 highest counts. 4. Transpose a set of data into a series to produce a chart Search all successful events and count the number of views, the number of times items were added to the cart, and the number of purchases. This search produces a single row of data. Note: The value for count AS views is the total number of the events that match the criteria sourcetype=access_* status=200 , or the total count for all actions. The values for addtocart and purchases show the number of events for those specific actions. When you switch to the Visualization tab, the data displays a chart with the \"34282 views\" as the X axis label and two columns, one for \"addtocart \"and one for \"purchases\". Because the information about the views is placed on the X axis, this chart is confusing. If you change to a pie chart, you see only the \"views\". Use the transpose command to convert the columns of the single row into multiple rows. Now these rows can be displayed in a column or pie chart where you can compare the values. Note: In this particular example, using a pie chart is misleading. The views is a total count of all the actions, not just the addtocart and purchases actions. Using a pie chart implies that views is an action like addtocart and purchases. The pie chart implies that the value for views is 1 part of the total, when in fact views is the total. There are a few ways to fix this issue: Use a column chart You can remove the count AS views criteria from your search You can add the table command before the transpose command in the search, for example:", "code_examples": [{"language": "spl", "code": "sourcetype=access_* status=200 | chart count BY host"}, {"language": "spl", "code": "sourcetype=access_* status=200 | chart count BY host | transpose"}, {"language": "spl", "code": "sourcetype=access_* status=200 | chart count BY host | transpose header_field=host"}, {"language": "spl", "code": "index=_internal | stats count by sourcetype | sort -count"}, {"language": "spl", "code": "index=_internal | stats count by sourcetype | sort -count | transpose 3"}, {"language": "spl", "code": "sourcetype=access_* status=200 | stats count AS views count(eval(action=\"addtocart\")) AS addtocart count(eval(action=\"purchase\")) AS purchases"}, {"language": "spl", "code": "sourcetype=access_* status=200 | stats count AS views count(eval(action=\"addtocart\")) AS addtocart count(eval(action=\"purchase\")) AS purchases | transpose"}, {"language": "spl", "code": "sourcetype=access_* status=200 | stats count AS views count(eval(action=\"addtocart\")) AS addtocart count(eval(action=\"purchase\")) AS purchases | table addtocart purchases | transpose"}], "tables": [{"headers": ["host", "count"], "rows": [["www1", "11835"], ["www2", "11186"], ["www3", "11261"]]}, {"headers": ["column", "row 1", "row 2", "row 3"], "rows": [["host", "www1", "www2", "www3"], ["count", "11835", "11186", "11261"]]}, {"headers": ["column", "row 1", "row 2", "row 3"], "rows": [["host", "www1", "www2", "www3"], ["count", "11835", "11186", "11261"]]}, {"headers": ["column", "www1", "www2", "www3"], "rows": [["count", "11835", "11186", "11261"]]}, {"headers": [], "rows": [["This example uses the sample dataset fromthe Search Tutorial.Download the data set fromAdd data tutorialand follow the instructions to get the tutorial data into your Splunk deployment."]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "transpose", "section_heading": "Examples", "section_id": "id_194313d6_e8e6_4089_ac9d_701f54256fc1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transpose", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:17.141268+00:00", "version": "10.2"}}
{"id": "b76cd017ee7ef29b", "content": "Commands fields stats untable xyseries", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "transpose", "section_heading": "See also", "section_id": "c9728531_90ec_4d79_89a9_23b6335c468a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/transpose", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:17.141274+00:00", "version": "10.2"}}
{"id": "ce2196eeb94a4d6d", "content": "Sets the field values for all results to a common value. Sets the value of the given fields to the specified values for each event in the result set. Delimit multiple definitions with commas. Missing fields are added, present fields are overwritten. Whenever you need to change or define field values, you can use the more general purpose eval command. See usage of an eval expression to set the value of a field in Example 1.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "setfields", "section_heading": "Description", "section_id": "da87e988_6dc4_495c_b35e_cb91f101e5ae--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/setfields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:34.678772+00:00", "version": "10.2"}}
{"id": "276b298929ff82a8", "content": "setfields <setfields-arg>, ... Required arguments <setfields-arg> Syntax: string=\"<string>\", ... Description: A key-value pair, with the value quoted. If you specify multiple key-value pairs, separate each pair with a comma. Standard key cleaning is performed. This means all non-alphanumeric characters are replaced with '_' and leading '_' are removed.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "setfields", "section_heading": "Syntax", "section_id": "cc24f9af_69cc_4748_81b4_fcd2000fdbac--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/setfields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:34.678781+00:00", "version": "10.2"}}
{"id": "b9cea6a71eef99bc", "content": "Example 1: Specify a value for the ip and foo fields. To do this with the eval command:", "code_examples": [{"language": "spl", "code": "... | setfields ip=\"10.10.10.10\", foo=\"foo bar\""}, {"language": "spl", "code": "... |evalip=\"10.10.10.10\"|evalfoo=\"foo bar\""}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "setfields", "section_heading": "Examples", "section_id": "id_91d753fb_cbf9_496f_ad25_9f4905730ea0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/setfields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:34.678787+00:00", "version": "10.2"}}
{"id": "9054a6e687fc308c", "content": "eval , fillnull , rename", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "setfields", "section_heading": "See also", "section_id": "e043e569_6ab3_48d0_9a54_09fb6f2ea10e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/setfields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:34.678791+00:00", "version": "10.2"}}
{"id": "82936af1b3512a5f", "content": "Using <field> as a discrete random variable, this command analyzes all numerical fields to determine the ability for each of those fields to predict the value of the classfield. It determines the stability of the relationship between values in the target classfield and numeric values in other fields. As a reporting command, analyzefields consumes all input results and generates one row for each numeric field in the output results. The values in that row indicate the performance of the analyzefields command at predicting the value of a classfield. For each event, if the conditional distribution of the numeric field with the highest z-probability based on matches the actual class, the event is counted as accurate. The highest z-probablility is based on the classfield .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "analyzefields", "section_heading": "Description", "section_id": "d62a9ffb_afaf_4e53_8e34_7a1d1e477898--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/analyzefields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:54.091644+00:00", "version": "10.2"}}
{"id": "ac1427c48aa542a6", "content": "analyzefields classfield=<field> You can use the abbreviation af for the analyzefields command. The analyzefields command returns a table with five columns. Required arguments classfield Syntax: classfield=<field> Description: For best results, classfield should have two distinct values, although multiclass analysis is possible.", "code_examples": [], "tables": [{"headers": ["Field", "Description"], "rows": [["field", "The name of a numeric field from the input search results."], ["count", "The number of occurrences of the field in the search results."], ["cocur", "The co-occurrence of the field.  In the results whereclassfieldis present, this is the ratio of results in whichfieldis also present.  Thecocuris 1 if thefieldexists in every event that has aclassfield."], ["acc", "The accuracy in predicting the value of theclassfield, using the value of the field. This the ratio of the number of accurate predictions to the total number of events with thatfield.  This argument is valid only for numerical fields."], ["balacc", "The balanced accuracy is the non-weighted average of the accuracies in predicted each value of theclassfield. This is only valid for numerical fields."]]}], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "analyzefields", "section_heading": "Syntax", "section_id": "c178aec9_ed02_4dd2_9c59_0f0d746de152--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/analyzefields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:54.091655+00:00", "version": "10.2"}}
{"id": "3417026cb9775039", "content": "Example 1: Analyze the numerical fields to predict the value of \"is_activated\".", "code_examples": [{"language": "spl", "code": "... | analyzefields classfield=is_activated"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "analyzefields", "section_heading": "Examples", "section_id": "cd71a60f_37e7_40f3_8768_deb2bda31360--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/analyzefields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:54.091660+00:00", "version": "10.2"}}
{"id": "b4bc2d066f2f0783", "content": "anomalousvalue", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "analyzefields", "section_heading": "See also", "section_id": "id_228135b4_9ad5_4033_9ace_a12d88b0ed15--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/analyzefields", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:17:54.091665+00:00", "version": "10.2"}}
{"id": "fa63c88478323922", "content": "Concurrency measures the number of events which have spans that overlap with the start of each event. Alternatively, this measurement represents the total number of events in progress at the time that each particular event started, including the event itself. This command does not measure the total number of events that a particular event overlapped with during its total span.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "concurrency", "section_heading": "Description", "section_id": "id_540452f7_de21_40e8_b80e_217bd9a496da--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/concurrency", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:13.297140+00:00", "version": "10.2"}}
{"id": "0e224b7592c34444", "content": "concurrency duration=<field> [start=<field>] [output=<field>] Required arguments duration Syntax: duration=<field> Description: A field that represents a span of time. This field must be a numeric with the same units as the start field. For example, the duration field generated by the transaction command is in seconds (see Example 1), which can be used with the default of _time which is also in units of seconds. Optional arguments start Syntax: start=<field> Description: A field that represents the start time. Default: _time output Syntax: output=<field> Description: A field to write the resulting number of concurrent events. Default: \"concurrency\"", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "concurrency", "section_heading": "Syntax", "section_id": "id_90de2f8b_167c_40c8_87db_a2d9433f9359--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/concurrency", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:13.297147+00:00", "version": "10.2"}}
{"id": "42111991a7b78acd", "content": "The concurrency command is a dataset processing command. See Command types. An event X is concurrent with event Y if X.start is between Y.start and (Y.start + Y.duration) If your events have a time that represents event completion and a span that represents the time before the completion, you need to subtract duration from the start time before the concurrency command: Limits There is a limitation on quantity of overlapping items. If the maximum tracked concurrency exceeds max_count, from the [concurrency] stanza in limits.conf, a warning will be produced in the UI / search output, and the values will be clamped, making them potentially inaccurate. This limit defaults to 10000000 or ten million.", "code_examples": [{"language": "spl", "code": "... |evalnew_start = start - duration | concurrency start=new_start duration=duration"}], "tables": [], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "concurrency", "section_heading": "Usage", "section_id": "id_4fbd12df_26f1_432d_b57e_28c292535d83--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/concurrency", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:13.297152+00:00", "version": "10.2"}}
{"id": "a0e33eb2f38b00f3", "content": "1. Determine the number of overlapping HTTP requests Determine the number of overlapping HTTP requests outstanding from browsers accessing splunkd at the time that each http request begins. This relies on the fact that the timestamp of the logged message is the time that the request came in, and the 'spent' field is the number of milliseconds spent handling the request. As always, you must be an 'admin' user, or have altered your roles scheme in order to access the _internal index. 2. Calculate the number of concurrent events Calculate the number of concurrent events for each event and emit as field 'foo': 3. Use existing fields to specify the start time and duration Calculate the number of concurrent events using the 'et' field as the start time and 'length' as the duration:", "code_examples": [{"language": "spl", "code": "index=_internal sourcetype=splunkd_ui_access |evalspent_in_seconds = spent / 1000 | concurrency duration=spent_in_seconds"}, {"language": "spl", "code": "... | concurrency duration=total_time output=foo"}, {"language": "spl", "code": "... | concurrency duration=length start=et"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "concurrency", "section_heading": "Basic examples", "section_id": "id_74585352_549b_4c96_87b1_a1d81a36129c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/concurrency", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:13.297158+00:00", "version": "10.2"}}
{"id": "bed67694c7d3ab57", "content": "1. Count the transactions that occurred at the same time Use the duration or span of a transaction to count the number of other transactions that occurred at the same time. This search groups events into transactions if they have the same values of JSESSIONID and clientip. An event is the beginning of the transaction if the event contains the string \"view\". An event is the last event of the transaction if the event contains the string \"purchase\". The transaction command returns a field called duration. The transactions are then piped into the concurrency command, which counts the number of events that occurred at the same time based on the timestamp and duration of the transaction. The search also uses the eval command and the tostring() function to reformat the values of the duration field to a more readable format, HH:MM:SS. To see the values in each transaction for the JSESSIONID, clientip, concurrency, and duration fields: In the list of Interesting Fields, click the field name. In the information box, for Selected , click Yes. Select the next field in the list of Interesting Fields. The information box automatically refreshes. For Selected , click Yes. Repeat these steps for every field you want to appear in the result list. The results should appear similar to the following image: 2. Count the purchases that occurred at the same time Use the time between each purchase to count the number of different purchases that occurred at the same time. This search uses the delta command and the _time field to calculate the time between one purchase event ( action=purchase ) and the purchase event immediately preceding it. The search renames this change in time as timeDelta. Some of the values of timeDelta are negative. Because the concurrency command does not work with negative values, the eval command is used to redefine timeDelta as its absolute value ( abs(timeDelta) ). The timeDelta is then used as the duration for calculating concurrent events. 3. Calculate the transactions using the time between consecutive transactions Use the time between each consecutive transaction to calculate the number of transactions that occurred at the same time. This search groups events into transactions if they have the same values of JSESSIONID and clientip. An event is the beginning of the transaction if the event contains the string \"view\". An event is the last event of the transaction if the event contains the string \"purchase\". The transaction command returns a field called duration. The transactions are then piped into the delta command, which uses the _time field to calculate the time between one transaction and the transaction immediately preceding it. The search renames this change in time as timeDelta. Some of the values of timeDelta are negative. Because the concurrency command does not work with negative values, the eval command is used to redefine timeDelta as its absolute value ( abs(timeDelta) ). This timeDelta is then used as the duration for calculating concurrent transactions.", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | transaction JSESSIONID clientip startswith=\"view\"endswith=\"purchase\"| concurrency duration=duration |evalduration=tostring(duration,\"duration\")"}, {"language": "spl", "code": "sourcetype=access_* action=purchase | delta _time AS timeDelta p=1 |evaltimeDelta=abs(timeDelta) | concurrency duration=timeDelta"}, {"language": "spl", "code": "sourcetype=access_* | transaction JSESSIONID clientip startswith=\"view\"endswith=\"purchase\"| delta _time AS timeDelta p=1 |evaltimeDelta=abs(timeDelta) | concurrency duration=timeDelta |evaltimeDelta=tostring(timeDelta,\"duration\")"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": [], "rows": [["This example uses the sample data from the Search Tutorial. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "concurrency", "section_heading": "Extended examples", "section_id": "e6e942a0_8a74_45e2_b237_efeaf043f4ed--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/concurrency", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:13.297165+00:00", "version": "10.2"}}
{"id": "54efd67ebe9f7a8b", "content": "timechart", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "concurrency", "section_heading": "See also", "section_id": "id_1c4a90c1_0ec0_481f_802d_2ec0754f9615--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/concurrency", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:13.297169+00:00", "version": "10.2"}}
{"id": "836c5b0bfa1012c3", "content": "Use this command to view your search history in the current application. This search history is presented as a set of events or as a table.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "history", "section_heading": "Description", "section_id": "id_9a82513e_0ad8_4254_9f5c_37ff4c618054--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/history", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:30.311944+00:00", "version": "10.2"}}
{"id": "55a07db6778463fe", "content": "| history [events=<bool>] Required arguments None. Optional arguments events Syntax: events=<bool> Description: When you specify events=true , the search history is returned as events. This invokes the event-oriented UI which allows for convenient highlighting, or field-inspection. When you specify events=false , the search history is returned in a table format for more convenient aggregate viewing. Default: false Fields returned when events=false .", "code_examples": [], "tables": [{"headers": ["Output field", "Description"], "rows": [["_time", "The time that the search was started."], ["api_et", "The earliest time of the API call, which is the earliest time for which events were requested."], ["api_lt", "The latest time of the API call, which is the latest time for which events were requested."], ["event_count", "If the search retrieved or generated events, the count of events returned with the search."], ["exec_time", "The execution time of the search in integer quantity of seconds into the Unix epoch."], ["is_realtime", "Indicates whether the search was real-time (1) or historical (0)."], ["result_count", "If the search is a transforming search, the count of results for the search."], ["scan_count", "The number of events retrieved from a Splunk index at a low level."], ["search", "The search string."], ["search_et", "The earliest time set for the search to run."], ["search_lt", "The latest time set for the search to run."], ["sid", "The search job ID."], ["splunk_server", "The host name of the machine where the search was run."], ["status", "The status of the search."], ["total_run_time", "The total time it took to run the search in seconds."]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "history", "section_heading": "Syntax", "section_id": "id_5963d442_a21d_4bb3_a939_a85fcfb178a7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/history", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:30.311960+00:00", "version": "10.2"}}
{"id": "e25510a327c6db4f", "content": "The history command is a generating command and should be the first command in the search. Generating commands use a leading pipe character. The history command returns your search history only from the application where you run the command.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "history", "section_heading": "Usage", "section_id": "bd54e875_05c7_4668_a243_e4798a8da67f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/history", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:30.311964+00:00", "version": "10.2"}}
{"id": "c471e3558c52b942", "content": "Return search history in a table Return a table of the search history. You do not have to specify events=false , since that this the default setting. Return search history as events Return the search history as a set of events.", "code_examples": [{"language": "spl", "code": "|history"}, {"language": "spl", "code": "|historyevents=true"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "history", "section_heading": "Examples", "section_id": "id_23707bde_dcb6_4bc3_8d18_f601d41266d2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/history", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:30.311969+00:00", "version": "10.2"}}
{"id": "4a5e15fddb37287e", "content": "Commands search", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "history", "section_heading": "See also", "section_id": "fb2b3206_37d3_4a30_bff2_bab3db0ff7c2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/history", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:30.311973+00:00", "version": "10.2"}}
{"id": "e9e8c5c4379dc014", "content": "Buffers events from real-time search to emit them in ascending time order when possible. The rtorder command creates a streaming event buffer that takes input events, stores them in the buffer in ascending time order, and emits them in that order from the buffer. This is only done after the current time reaches at least the span of time given by buffer_span, after the timestamp of the event. Events are also emitted from the buffer if the maximum size of the buffer is exceeded. If an event is received as input that is earlier than an event that has already been emitted previously, the out of order event is emitted immediately unless the discard option is set to true. When discard is set to true, out of order events are always discarded to assure that the output is strictly in time ascending order.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "rtorder", "section_heading": "Description", "section_id": "id_5af92c26_edbd_4de7_96ca_caa1904b59a8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rtorder", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:47.426709+00:00", "version": "10.2"}}
{"id": "a95e811c963bd573", "content": "rtorder [discard=<bool>] [buffer_span=<span-length>] [max_buffer_size=<int>] Optional arguments buffer_span Syntax: buffer_span=<span-length> Description: Specify the length of the buffer. Default: 10 seconds discard Syntax: discard=<bool> Description: Specifies whether or not to always discard out-of-order events. Default: false max_buffer_size Syntax: max_buffer_size=<int> Description: Specifies the maximum size of the buffer. Default: 50000, or the max_result_rows setting of the [search] stanza in limits.conf.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "rtorder", "section_heading": "Syntax", "section_id": "id_68d675ac_7121_4577_8814_d269371e24b1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rtorder", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:47.426717+00:00", "version": "10.2"}}
{"id": "6022ce9b39693ddf", "content": "Example 1: Keep a buffer of the last 5 minutes of events, emitting events in ascending time order once they are more than 5 minutes old. Newly received events that are older than 5 minutes are discarded if an event after that time has already been emitted.", "code_examples": [{"language": "spl", "code": "... | rtorder discard=t buffer_span=5m"}], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "rtorder", "section_heading": "Examples", "section_id": "id_23530851_79a4_4691_bab7_0b43a1190ff1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rtorder", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:47.426722+00:00", "version": "10.2"}}
{"id": "a1448fc3435f0ebc", "content": "sort", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "rtorder", "section_heading": "See also", "section_id": "b633e3a3_4aa1_424d_8aa8_4df511a0ab6a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/rtorder", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:18:47.426726+00:00", "version": "10.2"}}
{"id": "f941a66bc1cad260", "content": "Causes Splunk Web to display an icon for each different value in the list of fields that you specify. The iconify command adds a field named _icon to each event. This field is the hash value for the event. Within Splunk Web, a different icon for each unique value in the field is displayed in the events list. If multiple fields are listed, the UI displays a different icon for each unique combination of the field values.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "iconify", "section_heading": "Description", "section_id": "id_3fad2468_ea12_4384_96ae_6c2017a3c650--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/iconify", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:04.514698+00:00", "version": "10.2"}}
{"id": "f38eec84cbd75444", "content": "iconify <field-list> Required arguments field-list Syntax: <field>... Description: Comma or space-delimited list of fields. You cannot specify a wildcard character in the field list.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "iconify", "section_heading": "Syntax", "section_id": "id_421cf007_5058_4580_96bc_5a3b153198bb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/iconify", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:04.514706+00:00", "version": "10.2"}}
{"id": "1ba26378f9caaa46", "content": "The iconify command is a distributable streaming command. See Command types .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "iconify", "section_heading": "Usage", "section_id": "e5e4c33a_6fa6_47de_bb8b_67f7bd2c7152--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/iconify", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:04.514711+00:00", "version": "10.2"}}
{"id": "9c157e190e82f33a", "content": "1. Display a different icon for each eventtype 2. Display a different icon for unique pairs of field values Display a different icon for unique pair of clientip and method values. Here is how Splunk Web displays the results in your Events List :", "code_examples": [{"language": "spl", "code": "... | iconify eventtype"}, {"language": "spl", "code": "... | iconify clientip method"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "iconify", "section_heading": "Examples", "section_id": "id_78201061_4798_4fad_85e1_f70a90e2c78d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/iconify", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:04.514716+00:00", "version": "10.2"}}
{"id": "cec778e7e9147c6c", "content": "highlight", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "iconify", "section_heading": "See also", "section_id": "d0d18b6c_92c6_4098_9484_3dba10117771--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/iconify", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:04.514720+00:00", "version": "10.2"}}
{"id": "58a6177b272cdfc4", "content": "The pivot command makes simple pivot operations fairly straightforward, but can be pretty complex for more sophisticated pivot operations. Fundamentally this command is a wrapper around the stats and xyseries commands. The pivot command does not add new behavior, but it might be easier to use if you are already familiar with how Pivot works. See the Pivot Manual. Also, read how to open non-transforming searches in Pivot. Run pivot searches against a particular data model object. This requires a large number of inputs: the data model, the data model object, and pivot elements.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "pivot", "section_heading": "Description", "section_id": "id_6f921a6f_8a18_4e69_ad93_3748d0e8581a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/pivot", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:21.581486+00:00", "version": "10.2"}}
{"id": "fe9bde87cc8a9142", "content": "| pivot <datamodel-name> <object-name> <pivot-element> Required arguments datamodel-name Syntax: <string> Description: The name of the data model to search. objectname Syntax: <string> Description: The name of a data model object to search. pivot element Syntax: (<cellvalue>)* (SPLITROW <rowvalue>)* (SPLITCOL colvalue [options])* (FILTER <filter expression>)* (LIMIT <limit expression>)* (ROWSUMMARY <true | false>)* (COLSUMMARY <true | false>)* (SHOWOTHER <true | false>)* (NUMCOLS <num>)* (rowsort [options])* Description: Use pivot elements to define your pivot table or chart. Pivot elements include cell values, split rows, split columns, filters, limits, row and column formatting, and row sort options. Cell values always come first. They are followed by split rows and split columns, which can be interleaved, for example: avg(val), SPLITCOL foo, SPLITROW bar, SPLITCOL baz. Cell value <cellvalue> Syntax: <function>(fieldname) [AS <label>] Description: Define the values of a cell and optionally rename it. Here, label is the name of the cell in the report. The set of allowed functions depend on the data type of the fieldname : Strings: list, values, first, last, count, and distinct_count (dc) Numbers: sum, count, avg, max, min, stdev, list, and values Timestamps: duration, earliest, latest, list, and values Object or child counts: count Descriptions for row split-by elements SPLITROW <rowvalue> Syntax: SPLITROW <field> [AS <label>] [RANGE start=<value> end=<value> max=<value> size=<value>] [PERIOD (auto | year | month | day | hour | minute | second)] [TRUELABEL <label>] [FALSELABEL <label>] Description: You can specify one or more of these options on each SPLITROW. The options can appear in any order. You can rename the <field> using \"AS <label>\", where \"label\" is the name of the row in the report. Other options depend on the data type of the <field> specified: RANGE applies only for numbers. You do not need to specify all of the options (start, end, max, and size). PERIOD applies only for timestamps. Use it to specify the period to bucket by. TRUELABEL applies only for booleans. Use it to specify the label for true values. FALSELABEL applies only for booleans. Use it to specify the label for false values. Descriptions for column split-by elements SPLITCOL colvalue <options> Syntax: fieldname [ RANGE start=<value> end=<value> max=<value> size=<value>] [PERIOD (auto | year | month| day | hour | minute | second)] [TRUELABEL <label>] [FALSELABEL <label>] Description: You can have none, some, or all of these options on each SPLITCOL. They may appear in any order. Other options depend on the data type of the field specified (fieldname): RANGE applies only for numbers. The options (start, end, max, and size) do not all have to be specified. PERIOD applies only for timestamps. Use it to specify the period to bucket by. TRUELABEL applies only for booleans. Use it to specify the label for true values. FALSELABEL applies only for booleans. Use it to specify the label for false values. Descriptions for filter elements Filter <filter expression> Syntax: <fieldname> <comparison-operator> <value> Description: The expression used to identify values in a field. The comparison operator that you use depends on the type of field value. Strings: is, contains, in, isNot, doesNotContain, startsWith, endsWith, isNull, isNotNull For example: ... filter fieldname in ( value1 , value2 , ...) ipv4: is, contains, isNot, doesNotContain, startsWith, isNull, isNotNull Numbers: =, !=, <, <=, >, >=, isNull, isNotNull Booleans: is, isNull, isNotNull Descriptions for limit elements Limit <limit expression> Syntax: LIMIT <fieldname> BY <limittype> <number> <stats-function>(<fieldname>) Description: Use to limit the number of elements in the pivot. The limittype argument specifies where to place the limit. The valid values are top or bottom. The number argument must be a positive integer. You can use any stats function, such as min , max , avg , and sum. Example: LIMIT foo BY TOP 10 avg(bar)", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "pivot", "section_heading": "Syntax", "section_id": "eb96ea22_ba78_42e4_9bd6_e69cd8525d39--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/pivot", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:21.581494+00:00", "version": "10.2"}}
{"id": "c3f00fa55a33409f", "content": "The pivot command is a report-generating command. See Command types. Generating commands use a leading pipe character and should be the first command in a search.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "pivot", "section_heading": "Usage", "section_id": "id_3df905d4_6dbe_4b81_9025_b85a7978d282--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/pivot", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:21.581499+00:00", "version": "10.2"}}
{"id": "1cf9720f7d45c8a3", "content": "Example 1: This command counts the number of events in the \"HTTP Requests\" object in the \"Tutorial\" data model. This can be formatted as a single value report in the dashboard panel: Example 2: Using the Tutorial data model, create a pivot table for the count of \"HTTP Requests\" per host.", "code_examples": [{"language": "spl", "code": "| pivot Tutorial HTTP_requests count(HTTP_requests) AS\"Count of HTTP requests\""}, {"language": "spl", "code": "| pivot Tutorial HTTP_requests count(HTTP_requests) AS\"Count\"SPLITROW host AS\"Server\"SORT 100 host"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "pivot", "section_heading": "Examples", "section_id": "bf2c2ffa_106c_4813_b931_1cc20f9aeb8d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/pivot", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:21.581504+00:00", "version": "10.2"}}
{"id": "6889dceb55b2f5eb", "content": "datamodel , stats , xyseries", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "pivot", "section_heading": "See also", "section_id": "e01fccf3_c690_4fce_a203_abbe4007a7f2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/pivot", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:21.581508+00:00", "version": "10.2"}}
{"id": "f7f24bf608da7ef3", "content": "The tscollect command uses indexed fields to create time series index (tsidx) files in a namespace that you define. The result tables in these files are a subset of the data that you have already indexed. This then enables you to use the tstats command to search and report on these tsidx files instead of searching raw data. Because you are searching on a subset of the full index, the search should complete faster than it would otherwise. The tscollect command creates multiple tsidx files in the same namespace. The command will begin a new tsidx file when it determines that the tsidx file it is currently creating has gotten big enough. Only users with the indexes_edit capability can run this command. See Usage. CAUTION: This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "tscollect", "section_heading": "Description", "section_id": "id_893ff913_54a7_43ff_b51a_214a1600e6a8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tscollect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:36.175701+00:00", "version": "10.2"}}
{"id": "8fe532a8c11e7c43", "content": "... | tscollect [namespace=<string>] [squashcase=<bool>] [keepresults=<bool>] Optional arguments keepresults Syntax: keepresults = true | false Description: If true, tscollect outputs the same results it received as input. If false, tscollect returns the count of results processed (this is more efficient since it does not need to store as many results). Default: false namespace Syntax: namespace=<string> Description: Define a location for the tsidx file(s). If namespace is provided, the tsidx files are written to a directory of that name under the main tsidxstats directory (that is, within $SPLUNK_DB/tsidxstats ). These namespaces can be written to multiple times to add new data. Default: If namespace is not provided, the files are written to a directory within the job directory of that search, and will live as long as the job does. If you have Splunk Enterprise, you can configure the namespace location by editing indexes.conf and setting the attribute tsidxStatsHomePath. squashcase Syntax: squashcase = true | false Description: Specify whether or not the case for the entire field::value tokens are case sensitive when it is put into the lexicon. To create indexed field tsidx files that are similar to those created by Splunk Enterprise, set squashcase=true for results to be converted to all lowercase. Default: false", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "tscollect", "section_heading": "Syntax", "section_id": "f020e846_7a7f_4a35_9beb_b57b31fbb729--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tscollect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:36.175710+00:00", "version": "10.2"}}
{"id": "4faf2d9214aa4f21", "content": "You must have the indexes_edit capability to run the tscollect command. By default, the admin role has this capability and the user and power roles do not have this capability.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "tscollect", "section_heading": "Usage", "section_id": "efbee9e1_d135_452a_9851_e7a31330bc09--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tscollect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:36.175714+00:00", "version": "10.2"}}
{"id": "6b79fce35a8e7b04", "content": "Example 1: Write the results table to tsidx files in namespace foo. Example 2: Retrieve events from the main index and write the values of field foo to tsidx files in the job directory.", "code_examples": [{"language": "spl", "code": "... | tscollect namespace=foo"}, {"language": "spl", "code": "index=main | fields foo | tscollect"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "tscollect", "section_heading": "Examples", "section_id": "a88e1d3e_dc6e_4070_8773_8743afa101be--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tscollect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:36.175719+00:00", "version": "10.2"}}
{"id": "eec7bb9339adbf7f", "content": "collect , stats , tstats", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "tscollect", "section_heading": "See also", "section_id": "e031030b_deca_417e_ba15_28190a3b3ab1--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tscollect", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:36.175722+00:00", "version": "10.2"}}
{"id": "a161f38484830c87", "content": "Annotates specified fields in your search results with tags. If there are fields specified, only annotates tags for those fields. Otherwise, this command looks for tags for all fields. See About tags and aliases in the Knowledge Manager Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "tags", "section_heading": "Description", "section_id": "id_7623772f_9560_4972_afde_11cbd75d3979--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tags", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:53.489842+00:00", "version": "10.2"}}
{"id": "8b5c8e243c8541b4", "content": "The required syntax is in bold. tags [outputfield=<field>] [inclname=<bool>] [inclvalue=<bool>] [allowed_tags=<string>] <field-list> Required arguments None. Optional arguments allowed_tags Syntax: allowed_tags=<string> | allowed_tags=\"<string-list>\" Description: If specified, returns only the tag names in the allowed_tags argument. You can specify multiple tags using a comma-separated, double-quoted string. For example: allowed_tags=\"host, sourcetype\". Default : None <field-list> Syntax: <field> <field> ... Description: Specify the fields that you want to output the tags from. The tag names are written to the outputfield. Default : All fields inclname Syntax: inclname=true | false Description: If outputfield is specified, this controls whether or not the event field name is added to the output field, along with the tag names. Specify true to include the field name. Default : false inclvalue Syntax: inclvalue=true | false Description: If outputfield is specified, controls whether or not the event field value is added to the output field, along with the tag names. Specify true to include the event field value. Default : false outputfield Syntax: outputfield=<field> Description: If specified, the tag names for all of the fields are written to this one new field. If not specified, a new field is created for each field that contains tags. The tag names are written to these new fields using the naming convention tag_name::<field>. In addition, a new field is created called tags that lists all of the tag names in all of the fields. Default : New fields are created and the tag names are written to the new fields.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "tags", "section_heading": "Syntax", "section_id": "fd329842_6854_44a0_a956_3eaff28128d7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tags", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:53.489849+00:00", "version": "10.2"}}
{"id": "51b28579f19db438", "content": "The tags command is a distributable streaming command. See Command types. Viewing tag information To view the tags in a table format, use a command before the tags command such as the stats command. Otherwise, the fields output from the tags command appear in the list of Interesting fields. See Examples. Using the <outputfield> argument If outputfield is specified, the tag names for the fields are written to this field. By default, the tag names are written in the format <field>::<tag_name>. For example, sourcetype::apache. If outputfield is specified, the inclname and inclvalue arguments control whether or not the field name and field values are added to the outputfield. If both inclname and inclvalue are set to true , then the format is <field>::<value>::<tag_name>. For example, sourcetype::access_combined_wcookie::apache .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "tags", "section_heading": "Usage", "section_id": "f654575b_f857_434b_bec8_d9a4ae068b36--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tags", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:53.489854+00:00", "version": "10.2"}}
{"id": "4ab53cb44ac2c801", "content": "1. Results using the default settings This search looks for web access events and counts those events by host. The results look something like this: When you use the tags command without any arguments, two new fields are added to the results tag and tag::host. The results look something like this: There are no tags for host=www3. Add the sourcetype field to the stats command BY clause. The results look something like this: The tag field list all of the tags used in the events that contain the combination of host and sourcetype. The tag::host field list all of the tags used in the events that contain that host value. The tag::sourcetype field list all of the tags used in the events that contain that sourcetype value. 2. Specifying a list of fields Return the tags for the host and eventtype fields. 3. Specifying an output field Write the tags for all fields to the new field test. The results look like this: 4. Including the field names in the search results Write the tags for the host and sourcetype fields into the test field. New fields are returned in the output using the format host::<tag> or sourcetype::<tag>. Include the field name in the output. The results look like this: 5. Identifying a specific a list of tags to return Write the \"error\" and \"group\" tags for the host field into the test field. New fields are returned in the output using the format host::<tag>. Include the field name in the output. If you don't have a command before the tags command that organizes the results in a table format, you will see the output of the tags command in the Interesting fields list, as shown in the following image: Notice that the tag field in the list of Interesting fields shows that there are 3 tag values. Because the search specified that only the error and group tags should be returned to the test output field, those are the only tag values that appear in the image.", "code_examples": [{"language": "spl", "code": "sourcetype=access_*  | stats count by host"}, {"language": "spl", "code": "sourcetype=access_*  | stats count by host | tags"}, {"language": "spl", "code": "sourcetype=access_*  | stats count by host sourcetype | tags"}, {"language": "spl", "code": "... | tags host eventtype"}, {"language": "spl", "code": "...  | stats count by host sourcetype | tags outputfield=test"}, {"language": "spl", "code": "...  | stats count by host sourcetype | tags outputfield=testinclname=t"}, {"language": "spl", "code": "index=main | tags outputfield=testinclname=t allowed_tags=\"error, group\"host"}], "tables": [{"headers": [], "rows": [["This example uses the sample data from the Search Tutorial but should work with any format of Apache web access log. To try this example on your own Splunk instance, you must download the sample data and follow the instructions toget the tutorial data into Splunk. Use the time rangeAll timewhen you run the search."]]}, {"headers": ["host", "count"], "rows": [["www1", "13628"], ["www2", "12912"], ["www3", "12992"]]}, {"headers": ["host", "count", "tag", "tag::host"], "rows": [["www1", "13628", "tag2", "tag2"], ["www2", "12912", "tag1", "tag1"], ["www3", "12992", "", ""]]}, {"headers": ["host", "sourcetype", "count", "tag", "tag:host", "tag::sourcetype"], "rows": [["www1", "access_combined_wcookie", "13628", "apachetag2", "tag2", "apache"], ["www2", "access_combined_wcookie", "12912", "apachetag1", "tag1", "apache"], ["www3", "access_combined_wcookie", "12992", "apache", "", "apache"]]}, {"headers": ["host", "sourcetype", "count", "test"], "rows": [["www1", "access_combined_wcookie", "13628", "apachetag2"], ["www2", "access_combined_wcookie", "12912", "apachetag1"], ["www3", "access_combined_wcookie", "12992", "apache"]]}, {"headers": ["host", "sourcetype", "count", "test"], "rows": [["www1", "access_combined_wcookie", "13628", "sourcetype::apachehost::tag2"], ["www2", "access_combined_wcookie", "12912", "sourcetype::apachehost::tag1"], ["www3", "access_combined_wcookie", "12992", "sourcetype::apache"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "tags", "section_heading": "Examples", "section_id": "f8a7798a_b203_4015_9ca7_0724a1697a09--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tags", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:53.489870+00:00", "version": "10.2"}}
{"id": "6eb5bd69962f9df0", "content": "Related information About tags and aliases in the Knowledge Manager Manual Tag field-value pairs in Search in the Knowledge Manager Manual Define and manage tags in Settings in the Knowledge Manager Manual Commands eval", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "tags", "section_heading": "See also", "section_id": "a6cac6e3_340e_490d_b5cf_f483ddb7fc4b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/tags", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:19:53.489874+00:00", "version": "10.2"}}
{"id": "3d7528c5bbfdfd43", "content": "Causes a search to fail if the queries and commands that precede it in the search string do not return any events or results.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "require", "section_heading": "Description", "section_id": "b0bac76f_1fa8_4c13_bb61_cdbb54448d7b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/require", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:11.152624+00:00", "version": "10.2"}}
{"id": "4d68f0b924199b77", "content": "The required syntax is in bold. | require", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "require", "section_heading": "Syntax", "section_id": "id_97eaf44e_9651_407c_8ca6_dcafc1d9e0b6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/require", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:11.152633+00:00", "version": "10.2"}}
{"id": "d8269d28f791ae30", "content": "When require is used in a search string, it causes the search to fail if the queries and commands that precede it in the search string return zero events or results. When you use it in a subsearch, it causes the parent search to fail when the subsearch fails to return results. Use this command to prevent the Splunk platform from running zero-result searches when this might have certain negative side effects, such as generating false positives, running custom search commands that make costly API calls, or creating empty search filters via a subsearch. The require command cannot be used in real-time searches. Require and subsequent commands Do not expect the require command to mitigate all possible negative consequences of a search. When the require command causes a search to fail, it prevents subsequent commands in the search from receiving the results, but it does not prevent the Splunk software from invoking those commands before the search is finalized. This means that those subsequent search command processors may receive empty \"chunks\" before the search is finalized. If you are implementing a custom search command, make sure it interoperates well with the require command. Ensure that it avoids exhibiting side effects in response to partial input. See Create custom search commands for apps in Splunk Cloud Platform or Splunk Enterprise in the Developer Guide on the Developer Portal.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "require", "section_heading": "Usage", "section_id": "id_8b248911_77d2_4af2_86e7_02fe139151c3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/require", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:11.152638+00:00", "version": "10.2"}}
{"id": "4feec1e00312b860", "content": "1. Cause a search to fail if it doesn't return any results or events If a search doesn't return any results, the require command causes the search to fail. 2. Raise an exception if the subsearch returns zero events or results, and stop the parent search.", "code_examples": [{"language": "spl", "code": "... | require"}, {"language": "spl", "code": "... [ search index=other_index NOSUCHVALUE | require ]"}], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "require", "section_heading": "Examples", "section_id": "cb13a666_624c_45ef_a3ed_db0d9e8b261c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/require", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:11.152643+00:00", "version": "10.2"}}
{"id": "f16cf8e525ea29e6", "content": "Merges the results from two or more datasets into one dataset. One of the datasets can be a result set that is then piped into the union command and merged with a second dataset. The union command appends or merges event from the specified datasets, depending on whether the dataset is streaming or non-streaming and where the command is run. The union command runs on indexers in parallel where possible, and automatically interleaves results on the _time when processing events. See Usage. If you are familiar with SQL but new to SPL, see Splunk SPL for SQL users .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "union", "section_heading": "Description", "section_id": "id_0cbd9808_3dbc_48c1_bcd5_ebc2b1ffa3f8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/union", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:25.969169+00:00", "version": "10.2"}}
{"id": "05e12c78dcdff9c9", "content": "The required syntax is in bold. union [<subsearch-options>] <dataset> [<dataset>...] Required arguments dataset Syntax: <dataset-type>:<dataset-name> | <subsearch> Description: The dataset that you want to perform the union on. The dataset can be either a named or unnamed dataset. A named dataset is comprised of <dataset-type>:<dataset-name>. For <dataset-type> you can specify a data model , a saved search , or an inputlookup. For example datamodel:\"internal_server.splunkdaccess\". A subsearch is an unnamed dataset. When specifying more than one dataset, use a space or a comma separator between the dataset names. Optional arguments subsearch-options Syntax: maxtime=<int> maxout=<int> timeout=<int> Description: You can specify one set of subsearch-options that apply to all of the subsearches. You can specify one or more of the subsearch-options. These options apply only when the subsearch is treated as a non-streaming search. The maxtime argument specifies the maximum number of seconds to run the subsearch before finalizing. The default is 60 seconds. The maxout argument specifies the maximum number of results to return from the subsearch. The default is 50000 results. This value is the maxresultrows setting is in the [searchresults] stanza in the limits.conf file. The timeout argument specifies the maximum amount of time, in seconds, to cache the subsearch results. The default is 300 seconds.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "union", "section_heading": "Syntax", "section_id": "f144f777_4f06_432b_9c3a_85fe023b8d9f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/union", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:25.969177+00:00", "version": "10.2"}}
{"id": "d0606d8b53751a1b", "content": "The union command is a dataset processing command. See Command types. How the union command processes datasets depends on whether the dataset is a streaming or non-streaming dataset. The type of dataset is determined by the commands that are used to create the dataset. See Types of commands. Note: There are two types of streaming commands, distributable streaming and centralized streaming. For this discussion about the union command, streaming datasets refers to distributable streaming. A subsearch can be initiated through a search command such as the union command. See Initiating subsearches with search commands in the Splunk Cloud Platform Search Manual. Where the command is run Whether the datasets are streaming or non-streaming determines if the union command is run on the indexers or the search head. The following table specifies where the command is run. How the command is processed The type of dataset also determines how the union command is processed. Optimized syntax for streaming datasets With streaming datasets, instead of this syntax: <streaming_dataset1> | union <streaming_dataset2> Your search is more efficient with this syntax: ... | union <streaming_dataset1>, <streaming_dataset2> Why unioned results might be truncated Consider the following search, which uses the union command to merge the events from three indexes. Each index contains 60,000 events, for a total of 180,000 events. This search produces the following union results: In this example, all of the subsearches are distributable streaming, so they are unioned by using same processing as the multisearch command. All 60,000 results for each index are unioned for a total of 180,000 merged events. However, if you specify a centralized streaming command, such as the head command, in one of the subsearches the results change. This search produces the following union results for a total of 160,000 merged events. Because the head command is a centralized streaming command rather than distributable streaming command, any subsearches that follow the head command are processed using the append command. In other words, when a command forces the processing to the search head, all subsequent commands must also be processed on the search head. Internally, the search is converted to this: When the union command is used with commands that are non-streaming commands, the default for the maxout argument is enforced. The default for the maxout argument is 50,000 events. In this example, the default for the maxout argument is enforced starting with the subsearch that used the non-streaming command. The default is enforced for any subsequent subsearches. If the non-streaming command is on the last subsearch, the first two subsearches are processed as streaming. These subsearches are unioned using the multisearch command processing. The final subsearch includes a non-streaming command, the head command. That subsearch gets unioned using the append command processing. Internally this search is converted to this: In this example, the default for the maxout argument applies only to the last subsearch. That subsearch returns only 50,000 events instead of the entire set of 60,000 events. The total number events merged is 170,000. 60,000 events for the first and second subsearches and 50,000 events from the last subsearch. Interleaving results When two datasets are retrieved from disk in descending time order, which is the default sort order, the union command interleaves the results. The interleave is based on the _time field. For example, you have the following datasets: dataset_A dataset_B Both datasets are descending order by _time. When | union dataset_A, dataset_B is run, the following dataset is the result.", "code_examples": [{"language": "spl", "code": "| union maxout=10000000\n   [ search index=union_1 ]\n   [ search index=union_2 ]\n   [ search index=union_3 ]\n| stats count by index"}, {"language": "spl", "code": "| union maxout=10000000\n   [ search index=union_1  | head 60000]\n   [ search index=union_2 ]\n   [ search index=union_3 ]\n| stats count by index"}, {"language": "spl", "code": "| search index=union_1\n| head 60000\n| append\n  [ search index=union_2 ]\n| append\n  [ search index=union_3 ]\n| stats count by index"}, {"language": "spl", "code": "| multisearch \n  [ search index=union_1 ]\n  [ search index=union_2 ]| \n| append\n  [ search index=union_3 | head 60000 ]\n | stats count by index"}], "tables": [{"headers": ["Dataset type", "Dataset 1 is streaming", "Dataset 1 is non-streaming"], "rows": [["Dataset 2 is streaming", "Indexers", "Search head"], ["Dataset 2 is non-streaming", "Search head", "Search head"]]}, {"headers": ["Dataset type", "Impact on processing"], "rows": [["Centralized streaming or non-streaming", "Processed as anappendcommand."], ["Distributable streaming", "Processed as amultisearchcommand.Placing<streaming_dataset1>after theunioncommand is more efficient."]]}, {"headers": ["index", "count"], "rows": [["union_1", "60000"], ["union_2", "60000"], ["union_3", "60000"]]}, {"headers": ["index", "count"], "rows": [["union_1", "60000"], ["union_2", "50000"], ["union_3", "50000"]]}, {"headers": ["_time", "host", "bytes"], "rows": [["4", "mailsrv1", "2412"], ["1", "dns15", "231"]]}, {"headers": ["_time", "host", "bytes"], "rows": [["3", "router1", "23"], ["2", "dns12", "22o"]]}, {"headers": ["_time", "host", "bytes"], "rows": [["4", "mailsrv1", "2412"], ["3", "router1", "23"], ["2", "dns12", "22o"], ["1", "dns15", "231"]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "union", "section_heading": "Usage", "section_id": "b419a455_4470_4059_8bd1_549536c836c4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/union", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:25.969203+00:00", "version": "10.2"}}
{"id": "20203b6df0000a3d", "content": "1. Union events from two subsearches The following example merges events from index a and index b. New fields type and mytype are added in each subsearch using the eval command. 2. Union the results of a subsearch to the results of the main search The following example appends the current results of the main search with the tabular results of errors from the subsearch. 3. Union events from a data model and events from an index The following example unions a built-in data model that is an internal server log for REST API calls and the events from index a. 4. Specify the subsearch options The following example sets a maximum of 20,000 results to return from the subsearch. The example specifies to limit the duration of the subsearch to 120 seconds. The example also sets a maximum time of 600 seconds (5 minutes) to cache the subsearch results.", "code_examples": [{"language": "spl", "code": "| union [search index=a |evaltype=\"foo\"] [search index=b |evalmytype =\"bar\"]"}, {"language": "spl", "code": "... | chart count by category1 | union [search error | chart count by category2]"}, {"language": "spl", "code": "... | union datamodel:\"internal_server.splunkdaccess\"[search index=a]"}, {"language": "spl", "code": "... | chart count by category1 | union maxout=20000 maxtime=120 timeout=600 [search error | chart count by category2]"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "union", "section_heading": "Examples", "section_id": "eac1b3f9_02af_499a_b388_438f7c4f7802--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/union", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:25.969211+00:00", "version": "10.2"}}
{"id": "1ff1314ca9313558", "content": "Related information About subsearches in the Search Manual About data models in the Knowledge Manager Manual Commands search inputlookup", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "union", "section_heading": "See also", "section_id": "f2d27d61_98ec_456b_a735_84a66414e8a7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/union", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:25.969216+00:00", "version": "10.2"}}
{"id": "41c663d5f474d342", "content": "Displays, or wraps, the output of the timechart command so that every period of time is a different series. You can use the timewrap command to compare data over specific time period, such as day-over-day or month-over-month. You can also use the timewrap command to compare multiple time periods, such as a two week period over another two week period. See Timescale options .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "timewrap", "section_heading": "Description", "section_id": "id_153c0a16_21bc_402f_bc34_34b843610c43--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timewrap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:42.974267+00:00", "version": "10.2"}}
{"id": "89585361784748a2", "content": "The required syntax is in bold. timewrap <timewrap-span> [align=now | end] [series=relative | exact | short] [time_format=<str>] Required arguments timewrap-span Syntax: [<int>]<timescale> Description: A span of each bin, based on time. The timescale is required. The int is not required. If <int> is not specified, 1 is assumed. For example if day is specified for the timescale, 1day is assumed. See Timescale options. Optional arguments align Syntax: align=now | end Description: Specifies if the wrapping should be aligned to the current time or the end time of the search. Default: end series Syntax: series=relative | exact | short Description: Specifies how the data series is named. If series=relative and timewrap-span is set to week, the field names are latest_week , 1week_before , 2weeks_before , and so forth. If series=exact , use the time_format argument to specify a custom format for the series names. If series=short , the field names are an abbreviated version of the field names used with series=relative. With series=short , the field names are abbreviated to \"s\" followed by a number representing the period of time. For example, if timewrap-span is set to week, the field names are s0, s1, s2 and so forth. The field s0 represents the latest week. The field s1 represents 1 week before the latest week. Default: relative time_format Syntax: time_format=<str> Description: Use with series=exact to specify a custom name for the series. The time_format is designed to be used with the time format variables. For example, if you specify time_format=\"week of %d/%m/%y\" , this format appears as week of 13/2/17 and week of 20/2/17. If you specify time_format=week of %b %d , this format appears as week of Feb 13 and week of Feb 20. See the Usage section. Default: None Timescale options <timescale> Syntax: <sec> | <min> | <hr> | <day> | <week> | <month> | <quarter> | <year> Description: Time scale units. Note: The timewrap command uses the abbreviation m to refer to months. Other commands , such as timechart and bin use the abbreviation m to refer to minutes.", "code_examples": [], "tables": [{"headers": ["Time scale", "Syntax", "Description"], "rows": [["<sec>", "s | sec | secs | second | seconds", "Time scale in seconds."], ["<min>", "min |  mins |  minute |  minutes", "Time scale in minutes."], ["<hr>", "h | hr |  hrs |  hour | hours", "Time scale in hours."], ["<day>", "d |  day | days", "Time scale in days."], ["<week>", "w |  week | weeks", "Time scale in weeks."], ["<month>", "m | mon | month |  months", "Time scale in months."], ["<quarter>", "qtr | quarter |  quarters", "Time scale in quarters"], ["<year>", "y | yr |  year |  years", "Time scale in years."]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "timewrap", "section_heading": "Syntax", "section_id": "id_21669831_54d2_42bf_b5d5_24a9e3c81d52--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timewrap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:42.974283+00:00", "version": "10.2"}}
{"id": "25da01a7569110bd", "content": "The timewrap command is a reporting command. You must use the timechart command in the search before you use the timewrap command. The wrapping is based on the end time of the search. If you specify the time range of All time , the wrapping is based on today's date. You see this in the timestamps for the _time field and in the data series names. Field names with a timechart BY clause If you use a BY clause in the timechart command part of your search, the field names generated by the timewrap command are appended to the field names generated with the BY clause. For example, suppose you have a search that includes BY categoryId in the timechart command and the results look something like this: When you add the timewrap command, such as | timewrap w series=short , the series field names are appended to the category ID names from the timechart BY clause. The output looks something like this: Using the time_format argument If you do not include any time specifiers with the time_format argument, all of the data series display the same name and are compressed into each other.", "code_examples": [], "tables": [{"headers": ["_time", "ACCESSORIES", "SPORTS", "STRATEGY"], "rows": [["2020-05-21", "5", "17", "32"], ["2020-05-22", "62", "22", "127"], ["2020-05-23", "65", "34", "128"], ["2020-05-24", "5", "17", "32"], ["2020-05-25", "62", "22", "127"], ["2020-05-26", "65", "34", "128"]]}, {"headers": ["_time", "ACCESSORIES_s1", "SPORTS_s1", "STRATEGY_s1", "ACCESSORIES_s0", "SPORTS_s0", "STRATEGY_s0"], "rows": [["2020-05-21", "", "", "", "5", "17", "32"], ["2020-05-22", "", "", "", "62", "22", "127"], ["2020-05-23", "", "", "", "65", "34", "128"], ["2020-05-24", "", "", "", "5", "17", "32"], ["2020-05-25", "62", "22", "127", "17", "54", "39"], ["2020-05-26", "65", "34", "128", "", "", ""]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "timewrap", "section_heading": "Usage", "section_id": "id_80156539_b226_4d21_b4fa_bd1ae8ccff64--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timewrap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:42.974301+00:00", "version": "10.2"}}
{"id": "cd5ee8e3c1d97331", "content": "1. Compare week over week Display a timechart that has a span of 1 day for each count in a week over week comparison table. Each table column, which is the series, is 1 week of time. 2. Compare today, yesterday, and average for the week To compare a few days with the weekly average, you need to calculate the daily totals, calculate the weekly average, and remove the days you don't want to use. For example: Use the timewrap command to generate results over the last 7 days. By using the series=short argument, field names are generated in the output which start with \"s\", making it easy to create totals using the addtotals command. Use the addtotals and eval commands to calculate the average over those 7 days. The table command is used to cut out days 3-7 so that only today, yesterday, and the weekly average are returned. The rename command is used to rename the fields. The output looks something like this: 3. Compare a day of the week to the same day of the previous weeks You can compare a day of the week to the same day of the weeks by specifying a filter at the end of the search. For example, to compare Wednesdays your search would be like this: The output looks something like this: If you change the timechart span to 1d instead of 1h, your output will look like this:", "code_examples": [{"language": "spl", "code": "... | timechart count span=1d | timewrap 1week"}, {"language": "spl", "code": "...| timechart count span=1h\n| timewrap  d series=short\n| addtotals s*\n|eval7dayavg=Total/7.0\n| table _time, _span, s0, s1, 7dayavg\n| rename s0 as now, s1 as yesterday"}, {"language": "spl", "code": "...| timechart count span=1h \n| timewrap w \n|wherestrftime(_time,\"%A\") ==\"Wednesday\""}], "tables": [{"headers": ["_time", "now", "yesterday", "7dayavg"], "rows": [["2020-02-20 15:00", "0", "0", "0.0"], ["2020-02-20 16:00", "0", "0", "0.29"], ["2020-02-20 17:00", "0", "0", "0.0"], ["2020-02-20 18:00", "0", "0", "0.0"], ["2020-02-20 19:00", "0", "0", "0.57"], ["2020-02-20 20:00", "0", "0", "0.0"], ["2020-02-20 21:00", "0", "0", "0.29"], ["2020-02-20 22:00", "0", "0", "1.1"]]}, {"headers": ["_time", "4weeks_before", "3weeks_before", "2weeks_before", "1week_before", "latest_week"], "rows": [["2020-02-19 00:00", "0", "1", "4", "0", "1"], ["2020-02-19 01:00", "2", "0", "0", "0", "1"], ["2020-02-19 02:00", "3", "5", "7", "2", "0"], ["2020-02-19 03:00", "6", "4", "0", "1", "2"], ["2020-02-19 04:00", "9", "0", "4", "0", "0"], ["2020-02-19 05:00", "2", "8", "7", "3", "1"], ["2020-02-19 06:00", "4", "2", "7", "0", "1"], ["2020-02-19 07:00", "6", "9", "2", "2", "0"]]}, {"headers": ["_time", "4weeks_before", "3weeks_before", "2weeks_before", "1week_before", "latest_week"], "rows": [["2020-02-19", "32", "29", "31", "8", "6"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "timewrap", "section_heading": "Examples", "section_id": "fbbdcc49_9220_441e_a9d3_de85493eb45c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timewrap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:42.974319+00:00", "version": "10.2"}}
{"id": "a7ac6e3c0aef21cc", "content": "timechart", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "timewrap", "section_heading": "See also", "section_id": "id_17092362_bf34_4ac7_bc4c_ae21cd67c311--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/timewrap", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:20:42.974323+00:00", "version": "10.2"}}
{"id": "1c39e24e71850f59", "content": "Anonymizes the search results by replacing identifying data - usernames, ip addresses, domain names, and so forth - with fictional values that maintain the same word length. For example, it might turn the string user=carol@adalberto.com into user=aname@mycompany.com. This lets Splunk users share log data without revealing confidential or personal information. See the Usage section for more information.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "scrub", "section_heading": "Description", "section_id": "a58f8e7b_e9be_4ece_9348_f63b4b4e350a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/scrub", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:00.518778+00:00", "version": "10.2"}}
{"id": "8bd4a01fe14f64e4", "content": "scrub [public-terms=<filename>] [private-terms=<filename>] [name-terms=<filename>] [dictionary=<filename>] [timeconfig=<filename>] [namespace=<string>] Required arguments None Optional arguments public-terms Syntax: public-terms=<filename> Description: Specify a filename that includes the public terms NOT to anonymize. private-terms Syntax: private-terms=<filename> Description: Specify a filename that includes the private terms to anonymize. name-terms Syntax: name-terms=<filename> Description: Specify a filename that includes the names to anonymize. dictionary Syntax: dictionary=<filename> Description: Specify a filename that includes a dictionary of terms NOT to anonymize, unless those terms are in the private-terms file. timeconfig Syntax: timeconfig=<filename> Description: Specify a filename that includes the time configurations to anonymize. namespace Syntax: namespace=<string> Description: Specify an application that contains the alternative files to use for anonymizing, instead of using the built-in anonymizing files.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "scrub", "section_heading": "Syntax", "section_id": "id_0acc666d_ebde_4036_9462_4bc42a639597--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/scrub", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:00.518786+00:00", "version": "10.2"}}
{"id": "17b34179c6f66859", "content": "By default, the scrub command uses the dictionary and configuration files that are located in the $SPLUNK_HOME/etc/anonymizer directory. These default files can be overridden by specifying arguments to the scrub command. The arguments exactly correspond to the settings in the splunk anonymize CLI command. For details, issue the splunk help anonymize command. You can add your own versions of the configuration files to the default location. Alternatively, you can specify an application where you maintain your own copy of the dictionary and configuration files. To specify the application, use the namespace=<string> argument, where <string> is the name of the application that corresponds to the name that appears in the path $SPLUNK_HOME/etc/apps/<app>/anonymizer. If the $SPLUNK_HOME/etc/apps/<app>/anonymizer directory does not exist, the Splunk software looks for the files in the $SPLUNK_HOME/etc/slave-apps/<app>/anonymizer directory. The scrub command anonymizes all attributes, except those that start with underscore ( _ ) except _raw ) or start with date_. Additionally, the following attributes are not anonymized: eventtype , linecount , punct , sourcetype , timeendpos , timestartpos. The scrub command adheres to the default maxresultrows limit of 50000 results. This setting is documented in the limits.conf file in the [searchresults] stanza. See limits.conf in the Admin Manual .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "scrub", "section_heading": "Usage", "section_id": "ef2b31c6_1b99_47fb_a86c_d2097d62af3a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/scrub", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:00.518791+00:00", "version": "10.2"}}
{"id": "71ae37729bd8495b", "content": "1. Anonymize the current search results using the default files. 2. Anonymize the current search results using the specified private-terms file. This search uses the abc_private-terms file that is located in the $SPLUNK_HOME/etc/anonymizer directory.", "code_examples": [{"language": "spl", "code": "... | scrub"}, {"language": "spl", "code": "... | scrub private-file=abc_private-terms"}], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "scrub", "section_heading": "Examples", "section_id": "id_07722d7f_5b66_4d66_a9ad_de6d570d6a17--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/scrub", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:00.518796+00:00", "version": "10.2"}}
{"id": "99379f5a7f7283f5", "content": "A streaming command operates on each event as the event is returned by a search. A distributable streaming command runs on the indexer or the search head, depending on where in the search the command is invoked. Any distributable streaming command that comes after a non-streaming command in the search is processed on the search head. A centralized streaming command applies a transformation to each event returned by a search. Unlike distributable streaming commands, a centralized streaming command only works on the search head.", "code_examples": [], "tables": [{"headers": ["Command", "Notes"], "rows": [["addinfo", "Distributable streaming"], ["addtotals", "Distributable streaming. Atransformingcommand when used to calculate column totals (not row totals)."], ["arules", "Some of the work is distributable streaming running on the indexer or the search head. The rest of the work is centralized streaming running on the search head."], ["autoregress", "Centralized streaming."], ["bin", "Streaming if specified with thespanargument. Otherwise a dataset processing command."], ["bucketdir", "Distributable streaming by default, but centralized streaming if thelocalsetting specified for the command in the commands.conf file is set to true."], ["cluster", "Streaming in some modes."], ["convert", "Distributable streaming."], ["dedup", "Distributable streaming in a prededup phase. Centralized streaming after the individual indexers perform their own dedup and the results are returned to the search head from each indexer.Using thesortbyargument or specifyingkeepevents=truemakes thededupcommand a dataset processing command."], ["eval", "Distributable streaming."], ["extract", "Distributable streaming."], ["fieldformat", "Distributable streaming."], ["fields", "Distributable streaming."], ["fillnull", "Distributable streaming when afield-listis specified. Adataset processingcommand when nofield-listis specified."], ["head", "Centralized streaming."], ["highlight", "Distributable streaming."], ["iconify", "Distributable streaming."], ["iplocation", "Distributable streaming."], ["join", "Centralized streaming, if there is a defined set of fields to join to. Adataset processingcommand when nofield-listis specified."], ["lookup", "Distributable streaming when specified withlocal=false, which is the default. Anorchestratingcommand whenlocal=true."], ["makemv", "Distributable streaming."], ["multikv", "Distributable streaming."], ["mvexpand", "Distributable streaming."], ["nomv", "Distributable streaming."], ["rangemap", "Distributable streaming."], ["regex", "Distributable streaming."], ["reltime", "Distributable streaming."], ["rename", "Distributable streaming."], ["replace", "Distributable streaming."], ["rex", "Distributable streaming."], ["search", "Distributable streaming if used further down the search pipeline. Ageneratingcommand when it is the first command in the search."], ["spath", "Distributable streaming."], ["strcat", "Distributable streaming."], ["streamstats", "Centralized streaming."], ["tags", "Distributable streaming."], ["transaction", "Centralized streaming."], ["typer", "Distributable streaming."], ["where", "Distributable streaming."], ["untable", "Distributable streaming."], ["xmlkv", "Distributable streaming."], ["xmlunescape", "Distributable streaming by default, but centralized streaming if thelocalsetting specified for the command in the commands.conf file is set to true."], ["xpath", "Distributable streaming."], ["xyseries", "Distributable streaming if the argumentgrouped=falseis specified, which is the default. Otherwise atransformingcommand."]]}], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "Command types", "section_heading": "Streaming commands", "section_id": "id_3cf75549_04ad_471d_9cc7_15a934ab16f6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/command-types", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:18.229815+00:00", "version": "10.2"}}
{"id": "99e3468896acf97f", "content": "A generating command either returns information or generates results. Some generating commands can return information from an index, a data model, a lookup, or a CSV file without any transformations to the information. Other generating commands generate results, usually for testing purposes.", "code_examples": [], "tables": [{"headers": ["Command", "Notes"], "rows": [["datamodel", "Report-generating"], ["dbinspect", "Report-generating."], ["eventcount", "Report-generating."], ["from", "Can be either report-generating or event-generating depending on the search or knowledge object that is referenced by the command."], ["gentimes", "Event-generating."], ["history", "Report-generating."], ["inputcsv", "Event-generating (centralized)."], ["Inputlookup", "Event-generating (centralized) whenappend=false, which is the default."], ["loadjob", "Event-generating (centralized)."], ["makeresults", "Report-generating."], ["metadata", "Report-generating. Although metadata fetches data from all peers, any command run after it runs only on the search head."], ["metasearch", "Event-generating."], ["mstats", "Report-generating, except whenappend=trueis specified."], ["multisearch", "Event-generating."], ["pivot", "Report-generating."], ["rest", ""], ["search", "Event-generating (distributable) when the first command in the search, which is the default. Astreaming(distributable) command if used later in the search pipeline."], ["searchtxn", "Event-generating."], ["set", "Event-generating."], ["tstats", "Report-generating (distributable), except whenprestats=true. Whenprestats=true, thetstatscommand is event-generating."], ["typeaheadNo Content found for http://docs.splunk.com/Documentation/Splunk/10.0.0/SearchReference/Typeahead", "Event-generating."], ["walklexNo Content found for http://docs.splunk.com/Documentation/Splunk/10.0.0/SearchReference/Walklex", "Event-generating."]]}], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "Command types", "section_heading": "Generating commands", "section_id": "b050774d_9272_4319_97e4_4ab16ca6851b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/command-types", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:18.229839+00:00", "version": "10.2"}}
{"id": "ab2312b4dd5e9abd", "content": "A transforming command orders the results into a data table. The command \"transforms\" the specified cell values for each event into numerical values for statistical purposes. Note: In earlier versions of Splunk software, transforming commands were called reporting commands.", "code_examples": [], "tables": [{"headers": ["Command", "Notes"], "rows": [["addtotals", "Transforming when used to calculate column totals (not row totals). A distributablestreamingcommand when used to calculate row totals, which is the default."], ["anomalydetection", ""], ["append", ""], ["associate", ""], ["chart", ""], ["cofilter", ""], ["contingency", ""], ["history", ""], ["makecontinuous", ""], ["mvcombine", ""], ["rare", ""], ["stats", ""], ["table", ""], ["timechart", ""], ["top", ""], ["xyseries", "Transforming ifgrouped=true. Astreaming(distributable) command whengrouped=false, which is the default setting."]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "Command types", "section_heading": "Transforming commands", "section_id": "id_6523974f_d5aa_4270_9bd3_ca7585b8c1e3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/command-types", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:18.229855+00:00", "version": "10.2"}}
{"id": "7982b9c67af84534", "content": "Orchestrating commands control some aspect of how a search is processed. They do not directly affect the final result set of the search. For example, you might apply an orchestrating command to a search to enable or disable a search optimization that helps the overall search complete faster.", "code_examples": [], "tables": [{"headers": ["Command", "Notes"], "rows": [["localop", ""], ["lookup", "Only becomes an orchestrating command whenlocal=true. This forces thelookupcommand to run on the search head and not on any remote peers. Astreaming(distributable) command whenlocal=false, which is the default setting."], ["noop", ""], ["redistribute", ""], ["require", ""]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "Command types", "section_heading": "Orchestrating commands", "section_id": "f9767084_a3f5_4a6d_b5d9_4945edb1e173--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/command-types", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:18.229864+00:00", "version": "10.2"}}
{"id": "4bd6531692c228ca", "content": "A dataset processing command is a command that requires the entire dataset before the command can run. Some of these commands fit into other command types in specific situations or when specific arguments are used.", "code_examples": [], "tables": [{"headers": ["Command", "Notes"], "rows": [["anomalousvalue", "Some modes"], ["anomalydetection", "Some modes"], ["append", "Some modes"], ["appendcols", ""], ["appendpipe", ""], ["bin", "Some modes. Astreamingcommand if thespanargument is specified."], ["cluster", "Some modes"], ["concurrency", ""], ["datamodel", ""], ["dedup", "Using thesortbyargument or specifyingkeepevents=truemakes thededupcommand a dataset processing command. Otherwise,dedupis a distributable streaming command in a prededup phase. Centralized streaming after the individual indexers perform their own dedup and the results are returned to the search head from each indexer."], ["eventstats", ""], ["fieldsummary", ""], ["fillnull", "When nofield-listis specified, a dataset processing command. If afield-listis specifiedfillnullis adistributable streamingcommand."], ["from", "Some modes"], ["join", "Some modes. Acentralized streamingcommand when there is a defined set of fields to join to."], ["map", ""], ["outlier", ""], ["reverse", ""], ["sort", ""], ["tail", ""], ["transaction", "Some modes"], ["union", "Some modes"]]}], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "Command types", "section_heading": "Dataset processing commands", "section_id": "id_2e9101fb_8b20_4e1d_b1e6_75c4ccc48589--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/quick-reference/command-types", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Quick Reference", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:18.229880+00:00", "version": "10.2"}}
{"id": "fb7c58875f43677c", "content": "Converts JSON-formatted objects into multivalue fields. If you give the fromjson command a single field name that points to proper JSON objects, fromjson returns keys as fields and key values as field values.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "fromjson", "section_heading": "Description", "section_id": "id_3941a5ee_a340_4fe1_8b8f_a1374e50c29d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fromjson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:34.269193+00:00", "version": "10.2"}}
{"id": "028b911de8006d3b", "content": "Required syntax is in bold. | fromjson<string> [ prefix=<string>] Optional arguments prefix Syntax: prefix=<string> Description: Prepends a string to the fields that fromjson extracts from a JSON-formatted object. For example, including prefix=my_ in the search adds my_ to the beginning of field names in the results. Default: none", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "fromjson", "section_heading": "Syntax", "section_id": "a38b7966_d44b_4776_b377_2f1384428047--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fromjson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:34.269201+00:00", "version": "10.2"}}
{"id": "01a30091298c2754", "content": "The fromjson command is a streaming command , which means that it turns JSON-formatted objects into fields as each JSON object is received. See Types of commands .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "fromjson", "section_heading": "Usage", "section_id": "e824ac84_aeb9_4cc1_b052_93a2d442910e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fromjson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:34.269205+00:00", "version": "10.2"}}
{"id": "c0ccaec3fe567f9a", "content": "1. Expand a JSON object to create new fields Use the fromjson command to expand a JSON-formatted object and return the values in the search result. This example creates two new fields called name and age , and outputs the corresponding values in the search results. The results look like this. 2. Prepend the name of extracted fields You can use the optional argument prefix to prepend a string to fields extracted from a JSON-formatted object. This example creates two new fields called json_name and json_age. The results look something like this. 3. Expand nested JSON objects When you use fromjson to expand JSON-formatted objects into multivalue fields, you can retain the formatting of JSON objects by nesting them within the main object. In the following example, the object called json_obj with the key-value pair \"school\" and \"city\", is nested within another JSON object called object. The results look something like this.", "code_examples": [{"language": "spl", "code": "| makeresults |evalobject=json_object(\"name\",\"Albert\",\"age\", 63) | fromjson object"}, {"language": "spl", "code": "| makeresults |evalobject=json_object(\"name\",\"Albert\",\"age\", 63) | fromjson object prefix=my_"}, {"language": "spl", "code": "| makeresults |evalobject=json_object(\"age\", 19,\"name\",\"Sally\",\"new\",false(),\"classes\", json_array(\"math\",\"history\",\"science\"),\"another_json_object\", json_object(\"school\",\"city\"),\"null\", null)| fromjson object"}], "tables": [{"headers": ["_time", "age", "name", "object"], "rows": [["2020-11-09 17:01:22", "63", "Albert", "{\"name\":\"Albert\", \"age\":63}"]]}, {"headers": ["_time", "my_age", "my_name", "object"], "rows": [["2020-11-09 17:01:22", "63", "Albert", "{\"name\":\"Albert\", \"age\":63}"]]}, {"headers": ["_time", "age", "another_json_obj", "classes", "name", "new", "object"], "rows": [["2020-11-09 17:01:22", "19", "{\"school\":\"city\"}", "mathhistoryscience", "Sally", "false", "{\"age\":19,\"name\":\"Sally\",\"new\":false,\"classes\":[\"math\",\"history\",\"science\"],\"another_json_object\":{\"school\":\"city\"},\"null\":null}"]]}], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "fromjson", "section_heading": "Examples", "section_id": "ba544af7_979f_40cb_8e8b_79baa923c833--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fromjson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:34.269212+00:00", "version": "10.2"}}
{"id": "c8cbf5bf0da91e51", "content": "Commands tojson Evaluation functions JSON functions", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "fromjson", "section_heading": "See also", "section_id": "id_722d68e9_2b91_41fd_9511_e9e33b8aaf05--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/fromjson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:34.269216+00:00", "version": "10.2"}}
{"id": "bca06653d7d17ffb", "content": "When an event is processed by Splunk software, its timestamp is saved as the default field _time. This timestamp, which is the time when the event occurred, is saved in UNIX time notation. Searching with relative time modifiers, earliest or latest , finds every event with a timestamp beginning, ending, or between the specified timestamps. For example, when you search for earliest=@d , the search finds every event with a _time value since midnight. This example uses @d , which is a date format variable. See Date and time format variables. Time modifiers and the Time Range Picker When you use a time modifier in the SPL syntax, that time overrides the time specified in the Time Range Picker. For example, suppose your search uses yesterday in the Time Range Picker. You add the time modifier earliest=-2d to your search syntax. The search uses the time specified in the time modifier and ignores the time in the Time Range Picker. Because the search does not specify the latest time modifier, the default value now is used for latest. For more information, see Specify time modifiers in your search in the Search Manual. Time ranges and subsearches Time ranges selected from the Time Range Picker apply to the base search and to subsearches. However, time ranges specified directly in the base search do not apply to subsearches. Likewise, a time range specified directly in a subsearch applies only to that subsearch. The time range does not apply to the base search or any other subsearch. For example, if the Time Range Picker is set to Last 7 days and a subsearch contains earliest=2d@d , then the earliest time modifier applies only to the subsearch and Last 7 days applies to the base search. Searching based on index time You also have the option of searching for events based on when they were indexed. The UNIX time is saved in the _indextime field. Similar to earliest and latest for the _time field, you can use the relative time modifiers _index_earliest and _index_latest to search for events based on _indextime. For example, if you wanted to search for events indexed in the previous hour, use: _index_earliest=-h@h _index_latest=@h. Note: When using index-time based modifiers such as _index_earliest and _index_latest , your search must also have an event-time window which will retrieve the events. In other words, chunks of events might be ruled out based on the non index-time window as well as the index-time window. To be certain of retrieving every event based on index-time, you must run your search using All Time .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 6, "metadata": {"title": "Time modifiers", "section_heading": "Searching the _time field", "section_id": "d78b30d2_1789_45e5_9faf_b583bf9f6d25--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/time-format-variables-and-modifiers/time-modifiers", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Time Format Variables and Modifiers", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:52.317283+00:00", "version": "10.2"}}
{"id": "716261cae3df8248", "content": "Use the earliest and latest modifiers to specify custom and relative time ranges. You can specify an exact time such as earliest=\"10/5/2016:20:00:00\" , or a relative time such as earliest=-h or latest=@w6. When specifying relative time, you can use the now modifier to refer to the current time. For more information about customizing your search window, see Specify real-time time range windows in your search in the Search Manual .", "code_examples": [], "tables": [{"headers": ["Modifier", "Syntax", "Description"], "rows": [["earliest", "earliest=[+|-]<time_integer><time_unit>@<time_unit>", "Specify the earliest _time for the time range of your search.Useearliest=1to specify the UNIX epoch time 1, which is UTC January 1, 1970 at 12:00:01 AM.Useearliest=0to specify the earliest event in your data."], ["_index_earliest", "_index_earliest=[+|-]<time_integer><time_unit>@<time_unit>", "Specify the earliest _indextime for the time range of your search."], ["_index_latest", "_index_latest=[+|-]<time_integer><time_unit>@<time_unit>", "Specify the latest _indextime for the time range of your search."], ["latest", "latest=[+|-]<time_integer><time_unit>@<time_unit>", "Specify the latest time for the _time range of your search."], ["now", "now()ornow", "Refers to the current time. If set to earliest, now() is the start of the search."], ["time", "time()", "In real-time searches, time() is the current machine time."]]}], "chunk_index": 1, "total_chunks": 6, "metadata": {"title": "Time modifiers", "section_heading": "List of time modifiers", "section_id": "a5555775_9cf7_45d8_8e7c_8a9ceb52eefa--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/time-format-variables-and-modifiers/time-modifiers", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Time Format Variables and Modifiers", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:52.317315+00:00", "version": "10.2"}}
{"id": "8713b9365bb311fd", "content": "You can define the relative time in your search with a string of characters that indicate time amount (integer and unit). You can also specify a \"snap to\" time unit, which is specified with the @ symbol followed by a time unit. The syntax for using time modifiers is [+|-]<time_integer><time_unit>@<time_unit> The steps to specify a relative time modifier are: Indicate the time offset from the current time. Define the time amount, which is a number and a unit. Specify a \"snap to\" time unit. The time unit indicates the nearest or latest time to which your time amount rounds down. When a relative time modifier is processed, the offset is processed first, followed by the snap-to time. For example, if the relative time modifier is -2h@h the offset -2h is processed first. Then the snap-to time @h is processed. Indicate the time offset Begin your time offset with a plus (+) or minus (-) to indicate the offset from the current time. Define the time amount Define your time amount with a number and a time unit. The supported time units are listed in the following table: For example, to start your search an hour ago, use either of the following time modifiers. earliest=-h or earliest=-60m When specifying single time amounts, the number one is implied. An 's' is the same as '1s', 'm' is the same as '1m', 'h' is the same as '1h', and so forth. Note: Subsecond time units such as ms can be used in metrics searches only when they are searching over metrics indexes that are enabled for millisecond timestamp resolution. For more information about enabling metrics indexes to index metric data points with millisecond timestamp precision: For Splunk Cloud Platform, see Manage Splunk Cloud Platform indexes in the Splunk Cloud Platform Admin Manual. For Splunk Enterprise, see Create custom indexes in Managing indexers and clusters of indexers. Specify a snap to time unit You can specify a snap to time unit. The time unit indicates the nearest or latest time to which your time amount rounds down. Separate the time amount from the \"snap to\" time unit with an \"@\" character. You can use any of time units listed previously. For example: @w, @week, and @w0 for Sunday @month for the beginning of the month @q, @qtr, or @quarter for the beginning of the most recent quarter (Jan 1, Apr 1, Jul 1, or Oct 1). You can specify a day of the week: w0 (Sunday), w1, w2, w3, w4, w5 and w6 (Saturday). For Sunday, you can specify w0 or w7. You can also specify offsets from the snap-to-time or \"chain\" together the time modifiers for more specific relative time definitions. For example, @d-2h snaps to the beginning of today (12 AM or midnight), and then applies the time offset of -2h, This results in a time of 10 PM yesterday. The Splunk platform always applies the offset before it applies the snap. In other words, the left-hand side of the @ symbol is applied before the right-hand side. When snapping to the nearest or latest time, Splunk software always snaps backwards or rounds down to the latest time not after the specified time. For example, if it is 11:59:00 and you \"snap to\" hours, you will snap to 11:00 not 12:00. If you do not specify a time offset before the \"snap to\" amount, Splunk software interprets the time as \"current time snapped to\" the specified amount. For example, if it is currently 11:59 PM on Friday and you use @w6 to \"snap to Saturday\", the resulting time is the previous Saturday at 12:01 A.M.", "code_examples": [], "tables": [{"headers": ["Time unit", "Valid unit abbreviations"], "rows": [["subseconds", "microseconds (us), milliseconds (ms), centiseconds (cs), or deciseconds (ds)"], ["second", "s, sec, secs, second, seconds"], ["minute", "m, min, mins, minute, minutes"], ["hour", "h, hr, hrs, hour, hours"], ["day", "d, day, days"], ["week", "w, week, weeks"], ["month", "mon, month, months"], ["quarter", "q, qtr, qtrs, quarter, quarters"], ["year", "y, yr, yrs, year, years"]]}], "chunk_index": 2, "total_chunks": 6, "metadata": {"title": "Time modifiers", "section_heading": "How to specify relative time modifiers", "section_id": "f6b51910_831b_4b80_9728_343ed65e2b80--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/time-format-variables-and-modifiers/time-modifiers", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Time Format Variables and Modifiers", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:52.317344+00:00", "version": "10.2"}}
{"id": "a688b205fb1f3ec6", "content": "1. Run a search over all time If you want to search events from the start of UNIX time, use earliest=1. When earliest=1 and latest=now() are used, the search runs over all time. Specifying latest=now() does not return future events. To return future events, specify latest=<a_big_number>. Future events are events that contain timestamps later than the current time now(). 2. Search the events from the beginning of the current week 3. Search the events from the last full business week 4. Search with an exact date as a boundary With a boundary such as from November 15 at 8 PM to November 22 at 8 PM, use the timeformat %m/%d/%Y:%H:%M:%S. 5. Specify multiple time windows using a fixed date time format You can specify multiple time windows using the timeformat %m/%d/%Y:%H:%M:%S. For example to find events from 5-6 PM or 7-8 PM on specific dates, use the following syntax. 6. Specify multiple time windows using a relative time format You can specify multiple time windows using the time modifiers and snap-to with a relative time. For example to find events for the last 24 hours but omit the events from Midnight to 1:00 A.M., use the following syntax:", "code_examples": [{"language": "spl", "code": "...earliest=1 latest=now()"}, {"language": "spl", "code": "...earliest=@w0"}, {"language": "spl", "code": "...earliest=-5d@w1 latest=@w6"}, {"language": "spl", "code": "...earliest=\"11/15/2022:20:00:00\"latest=\"11/22/2022:20:00:00\""}, {"language": "spl", "code": "...(earliest=\"9/23/2022:17:00:00\"latest=\"9/23/2022:18:00:00\")  OR  (earliest=\"9/23/2022:19:00:00\"latest=\"9/23/2022:20:00:00\")"}, {"language": "spl", "code": "...((earliest=-24h latest<@d) OR (earliest>=@d+1h))"}], "tables": [], "chunk_index": 3, "total_chunks": 6, "metadata": {"title": "Time modifiers", "section_heading": "Examples", "section_id": "id_70506079_1962_46da_9484_407e99736d77--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/time-format-variables-and-modifiers/time-modifiers", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Time Format Variables and Modifiers", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:52.317365+00:00", "version": "10.2"}}
{"id": "c8eb2d1a6a6ec961", "content": "CAUTION: The following search time modifiers are still valid, but might be removed and their function no longer supported in a future release.", "code_examples": [], "tables": [{"headers": ["Modifier", "Syntax", "Description"], "rows": [["daysago", "daysago=<int>", "Search events within the lastintegernumber of days."], ["enddaysago", "enddaysago=<int>", "Set an end time for an integer number of days before Now."], ["endhoursago", "endhoursago=<int>", "Set an end time for an integer number of hours before Now."], ["endminutesago", "endminutesago=<int>", "Set an end time for an integer number of minutes before Now."], ["endmonthsago", "endmonthsago=<int>", "Set an end time for an integer number of months before Now."], ["endtime", "endtime=<string>", "Search for events before the specified time (exclusive of the specified time). Usetimeformatto specify how the timestamp is formatted."], ["endtimeu", "endtimeu=<int>", "Search for events before the specific UNIX time."], ["hoursago", "hoursago=<int>", "Search events within the lastintegernumber of hours."], ["minutesago", "minutesago=<int>", "Search events within the lastintegernumber of minutes."], ["monthsago", "monthsago=<int>", "Search events within the lastintegernumber of months."], ["searchtimespandays", "searchtimespandays=<int>", "Search within a specified range of days, expressed as an integer."], ["searchtimespanhours", "searchtimespanhours=<int>", "Search within a specified range of hours, expressed as an integer."], ["searchtimespanminutes", "searchtimespanminutes=<int>", "Search within a specified range of minutes, expressed as an integer."], ["searchtimespanmonths", "searchtimespanmonths=<int>", "Search within a specified range of months, expressed as an integer."], ["startdaysago", "startdaysago=<int>", "Search the specified number of days before the present time."], ["starthoursago", "starthoursago=<int>", "Search the specified number of hours before the present time."], ["startminutesago", "startminutesago=<int>", "Search the specified number of minutes before the present time."], ["startmonthsago", "startmonthsago=<int>", "Search the specified number of months before the present time."], ["starttime", "starttime=<timestamp>", "Search from the specified date and time to the present, inclusive of the specified time."], ["starttimeu", "starttimeu=<int>", "Search for events starting from the specific UNIX time."], ["timeformat", "timeformat=<string>", "Set the timeformat for thestarttimeandendtimemodifiers. By default:timeformat=%m/%d/%Y:%H:%M:%S"]]}], "chunk_index": 4, "total_chunks": 6, "metadata": {"title": "Time modifiers", "section_heading": "Other time modifiers", "section_id": "id_342f7d36_a0ff_4238_8463_8c6f05cb50b8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/time-format-variables-and-modifiers/time-modifiers", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Time Format Variables and Modifiers", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:52.317400+00:00", "version": "10.2"}}
{"id": "2f5a94903716c4c3", "content": "Functions Date and time functions used with evaluation commands Time functions used with statistical and charting commands Related information Specify time modifiers in your search in the Search Manual", "code_examples": [], "tables": [], "chunk_index": 5, "total_chunks": 6, "metadata": {"title": "Time modifiers", "section_heading": "See also", "section_id": "e38c568a_cc44_4828_bde4_ce293b5ab8fe--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/time-format-variables-and-modifiers/time-modifiers", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Time Format Variables and Modifiers", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:21:52.317415+00:00", "version": "10.2"}}
{"id": "89f0b3608f10b9bc", "content": "", "code_examples": [], "tables": [{"headers": ["Variable", "Description"], "rows": [["%c", "The date and time in the current locale's format as defined by the server's operating system. For example,Thu Jul 18 09:30:00 2019for US English on Linux."], ["%+", "The date and time with time zone in the current locale's format as defined by the server's operating system. For example,Thu Jul 18 09:30:00 PDT 2019for US English on Linux."]]}], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Date and time format variables", "section_heading": "Date and time variables", "section_id": "id_4c6e7191_9d2c_46f2_8fa7_f71261b90f3a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/time-format-variables-and-modifiers/date-and-time-format-variables", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Time Format Variables and Modifiers", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:22:09.110038+00:00", "version": "10.2"}}
{"id": "1bb6637997b86995", "content": "Note: To parse timestamps with GMT and an offset in data that you upload using Add Data , such as Fri Apr 29 2022 23:45:22 GMT-0700, you might need to use %:Z to capture both the timestamp and the offset.", "code_examples": [], "tables": [{"headers": ["Variable", "Description"], "rows": [["%Ez", "Splunk-specific, timezone in minutes."], ["%f", "Microseconds as a decimal number."], ["%H", "Hour (24-hour clock) as a decimal number. Hours are represented by the values 00 to 23. Leading zeros are accepted but not required."], ["%I", "Uppercase \"i\". Hour (12-hour clock) with the hours represented by the values 01 to 12. Leading zeros are accepted but not required. Use withÂ %p to specify AM or PM for the 12-hour clock."], ["%k", "LikeÂ %H, the hour (24-hour clock) as a decimal number. Leading zeros are replaced by a space, for example 0 to 23."], ["%M", "Minute as a decimal number. Minutes are represented by the values 00 to 59. Leading zeros are accepted but not required."], ["%N", "The number of subsecond digits. The default isÂ %9N. You can specifyÂ %3N = milliseconds,Â %6N = microseconds,Â %9N = nanoseconds."], ["%p", "AM or PM. Use withÂ %I to specify the 12-hour clock for AM or PM. Do not use withÂ %H."], ["%Q", "The subsecond component of a UTC timestamp. The default is milliseconds,Â %3Q. Some valid values are:%3Q = milliseconds, with values of 000-999%6Q = microseconds, with values of 000000-999999%9Q = nanoseconds, with values of 000000000-999999999"], ["%S", "Second as a decimal number, for example 00 to 59."], ["%s", "The UNIX Epoch Time timestamp, or the number of seconds since the Epoch: 1970-01-01 00:00:00 +0000 (UTC). For example the UNIX epoch time1484993700is equal toTue Jan 21 10:15:00 2020."], ["%T", "The time in 24-hour notation (%H:%M:%S). For example 23:59:59."], ["%X", "The time in the format for the current locale. For US English the format for 9:30 AM is9:30:00."], ["%Z", "The timezone abbreviation.  For exampleESTfor US Eastern Standard Time."], ["%z", "The timezone offset from UTC, in hour and minute: +hhmm or -hhmm. For example, for 5 hours before UTC the values is-0500which is US Eastern Standard Time.Examples:UseÂ %z to specify hour and minute, for example -0500UseÂ %:z to specify hour and minute separated by a colon, for example -05:00UseÂ %::z to specify hour minute and second separated with colons, for example -05:00:00UseÂ %:::z to  specify hour only, for example -05"], ["%%", "A literal \"%\" character."]]}], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Date and time format variables", "section_heading": "Time variables", "section_id": "fff6b6bb_a409_48a6_8f43_170e7963f1b7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/time-format-variables-and-modifiers/date-and-time-format-variables", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Time Format Variables and Modifiers", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:22:09.110055+00:00", "version": "10.2"}}
{"id": "e467e09abd935388", "content": "Specifying days and weeks Specifying months Specifying year", "code_examples": [], "tables": [{"headers": ["Variable", "Description"], "rows": [["%F", "Equivalent toÂ %Y-%m-%d (the ISO 8601 date format)."], ["%x", "The date in the format of the current locale. For example, 7/13/2019 for US English."]]}, {"headers": ["Variable", "Description"], "rows": [["%A", "Full weekday name. (Sunday, ..., Saturday)"], ["%a", "Abbreviated weekday name. (Sun, ... ,Sat)"], ["%d", "Day of the month as a decimal number, includes a leading zero. (01 to 31)"], ["%e", "LikeÂ %d, the day of the month as a decimal number, but a leading zero is replaced by a space. (1 to 31)"], ["%j", "Day of year as a decimal number, includes a leading zero. (001 to 366)"], ["%V (orÂ %U)", "Week of the year. TheÂ %V variable starts the count at 1, which is the most common start number.  TheÂ %U variable starts the count at 0."], ["%w", "Weekday as a decimal number. (0 = Sunday, ...,  6 = Saturday)"]]}, {"headers": ["Variable", "Description"], "rows": [["%b", "Abbreviated month name. (Jan, Feb, etc.)"], ["%B", "Full month name. (January, February, etc.)"], ["%m", "Month as a decimal number. (01 to 12). Leading zeros are accepted but not required."]]}, {"headers": ["Variable", "Description"], "rows": [["%C", "The century as a 2-digit decimal number."], ["%g", "The ISO 8601 date format for year as a 2-digit decimal number, without the century. (00 to 99). For example, 25."], ["%G", "The ISO 8601 date format for year with the century as a 4-digit decimal number that corresponds to the ISO week number (seeÂ %V). For example, 2025.Â %G uses the year that is associated with the ISO week number; if the ISO week number belongs to the previous or next year,Â %G specifies that year."], ["%y", "Year as a decimal number, without the century. (00 to 99). Leading zeros are accepted but not required."], ["%Y", "Year as a decimal number with the century. For example, 2025."]]}], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Date and time format variables", "section_heading": "Date variables", "section_id": "id_9ec59767_3adc_4334_ade3_5916937a963f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/time-format-variables-and-modifiers/date-and-time-format-variables", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Time Format Variables and Modifiers", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:22:09.110071+00:00", "version": "10.2"}}
{"id": "6a778ac40766a87f", "content": "The date format strings in the following examples include the T character as a delimiter, as defined by the ISO 8601 standard. Converting UNIX timestamps into dates The following table shows the results of several date format variables, using the strftime function. These examples show the results when you use the strftime function with the date Tue Apr 29 2025 23:45:22 GMT-0700 (Pacific Daylight Time). Converting UNIX timestamps into dates and times The following table shows the results of several date time format variables, using the strftime function. These examples show the results when you use the strftime function with the date Tue Apr 29 2025 23:45:22 GMT-0700 (Pacific Daylight Time). Converting timestamps into UNIX The following table shows the results of using several date time format variables to convert timestamps into UNIX time using the strptime function. For example, this search returns the UNIX time 1671126322.000000. The following table shows the results of searches that use time variables: Converting timestamp results to the year of the week format The following search generates timestamp results using the gentimes command and converts the dates to year of the week in ISO date format using the strftime function and %G. The results look like this:", "code_examples": [{"language": "spl", "code": "... |evalmytime=strptime(\"2022-12-15T09:45:22\",\"%Y-%m-%dT%H:%M:%S\")"}, {"language": "spl", "code": "host=\"www1\"|evalWeekNo = strftime(_time,\"%V\")"}, {"language": "spl", "code": "... |evalmytime=strftime(_time,\"%Y-%m-%dT%H:%M:%S.%Q\")"}, {"language": "spl", "code": "... |evalstart=strptime(Sent,\"%H:%M:%S.%N\"), end=strptime(Received,\"%H:%M:%S.%N\") |evaldifference=end-start | table end, start, difference"}, {"language": "spl", "code": "|gentimes start=12/27/2024 increment=1d\n|eval_time = starttime, time_ts = starttime\n``` This extracts Year-Week format using strftime```\n|evalyear_week_Y_format= strftime( _time,\"%Y\")\n|evalfinal_week_YV_format = strftime( _time,\"%Y-%V\")\n``` This extracts Year-WeekinISO format usingÂ %G with strftime ```\n|evalyear_week_G_format = strftime( _time,\"%G\")\n|evalfinal_week_G_format = strftime( _time,\"%G-%V\")\n|table _time year_week_Y_format final_week_YV_format year_week_G_format final_week_G_format time_ts\n|sort 0 -  _time"}], "tables": [{"headers": ["Date format string", "Result"], "rows": [["%Y-%m-%d", "2025-04-29"], ["%y-%m-%d", "25-04-29"], ["%bÂ %d,Â %Y", "Apr 29, 2025"], ["%BÂ %d,Â %Y", "April 29, 2025"], ["%aÂ %bÂ %d,Â %Y", "Tue Apr 29, 2025"], ["%dÂ %b '%y =Â %Y-%m-%d", "29 Apr '25 = 2025-04-29"]]}, {"headers": ["Date and Time format string", "Result", "Description"], "rows": [["%Y-%m-%dT%H:%M:%S.%Q", "2025-04-29T23:45:22.000", "Displays the date, followed by the letter T to separate the date from the time. The time includes  milliseconds."], ["%Y-%m-%dTÂ %H:%M:%S.%Z", "2025-04-29T 23:45:22.PDT", "Displays the date followed by the letter T and a space to separate the date from the time. The time includes the letters that represent timezone abbreviation."], ["%Y-%m-%dTÂ %H:%M:%SÂ %Z%:z", "2025-04-29T 23:45:22 PDT -07:00", "Displays the date followed by the letter T and a space to separate the date from the time. The time includes the letters that represent timezone abbreviation and the hours and minutes offset from UTC."], ["%Y-%m-%dTÂ %H:%M:%S.%QZ", "2025-04-29T 23:45:22.000Z", "Displays the date, followed by the letter T and a space to separate the date from the time. The time includes milliseconds followed by the letter Z to denote Zulu."], ["%Y-%m-%dT%H:%M:%S.%QZ", "2025-04-29T23:45:22.000Z", "Displays the date, followed by the letter T to separate the date from the time. The time includes milliseconds followed by the letter Z to denote Zulu."], ["%Y-%m-%dT%H:%M:%S", "2025-04-29T23:45:22", "Displays the date, followed by the letter T to separate the date from the time."], ["%Y-%m-%dT%T", "2025-04-29T23:45:22", "Displays the date, followed by the letter T to separate the date from the time. The time is represented in 24-hour notation (%H:%M:%S)."], ["%m-%d-%YÂ %I:%M:%SÂ %p", "04-29-2025 11:45:22 PM", "Displays the date, followed by a space separate the date from the time. The time is shown in a 12 hour format followed by PM to indicate this time is in the evening."], ["%bÂ %d,Â %YÂ %I:%M:%SÂ %p", "Apr 29, 2025 11:45:22 PM", "Displays the date, using the abbreviated name for the month. A space separates the date from the time. The time is shown in a 12 hour format followed by PM to indicate this time is in the evening."], ["%m-%d-%YÂ %H:%M:%S.%Q", "04-29-2025 23:45:22.000", "Displays the date, followed by a space to separate the date from the time. The time includes  milliseconds."], ["%m-%d-%YÂ %H:%M:%S.%QÂ %z", "04-29-2025 23:45:22.000 -0700", "Displays the date, followed by a space to separate the date from the time. The time includes  milliseconds and the hours and minutes offset from UTC."], ["%d/%b/%Y:%H:%M:%S.%fÂ %z", "29/Apr/2025:23:45:22.000000 -0700", "Displays the date, using the abbreviation for the month, and is immediately followed by the time.  The time includes nanoseconds and the hours and minutes offset from UTC."]]}, {"headers": ["Timestamps", "Date and Time format string", "UNIX time"], "rows": [["2022-9-25T09:45:22.000", "%Y-%m-%dT%H:%M:%S.%Q", "1664124322.000000"], ["2022-12-15 09:45:22", "%Y-%m-%dÂ %H:%M:%S", "1671126322.000000"]]}, {"headers": ["Sample search", "Result"], "rows": [["host=\"www1\"|evalWeekNo = strftime(_time,\"%V\")", "Creates a field calledWeekNoand returns the values for the week numbers that correspond to the dates in the_timefield."], ["... |evalmytime=strftime(_time,\"%Y-%m-%dT%H:%M:%S.%Q\")", "Creates a field calledmytimeand returns the converted timestamp values in the_timefield. The values are stored in UNIX format and converted using the format specified, which is the ISO 8601 format. For example: 2021-04-13T14:00:15.000."], ["... |evalstart=strptime(Sent,\"%H:%M:%S.%N\"), end=strptime(Received,\"%H:%M:%S.%N\") |evaldifference=end-start | table end, start, difference", "Takes the values in the Sent and Received fields and converts them into a standard time using thestrptimefunction. Then calculates the difference between the start and end times. The results are displayed in a table.You can use theroundfunction to round the difference to a specific number of decimal places. For example...| eval difference=round(end-start, 2)."]]}, {"headers": ["_time", "year_week_Y_format", "final_week_YV_format", "year_week_G_format", "final_week_G_format", "time_ts"], "rows": [["2025-02-03", "2025", "2025-06", "2025", "2025-06", "1738569600"], ["2025-02-02", "2025", "2025-05", "2025", "2025-05", "1738483200"], ["2025-02-01", "2025", "2025-05", "2025", "2025-05", "1738396800"], ["2025-01-31", "2025", "2025-05", "2025", "2025-05", "1738310400"], ["2025-01-30", "2025", "2025-05", "2025", "2025-05", "1738224000"]]}], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Date and time format variables", "section_heading": "Examples", "section_id": "id_75272b84_c33b_4663_b548_4985c61af0c9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/time-format-variables-and-modifiers/date-and-time-format-variables", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Time Format Variables and Modifiers", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:22:09.110094+00:00", "version": "10.2"}}
{"id": "9303390e36ff9186", "content": "Searching in real time may be very expensive on the indexer. If you want to turn off real time search on an indexer, you can edit a [default] setting in that indexer's indexes.conf. Note: This setting cannot be overridden on an index-by-index basis, it applies to all indexes located on the indexer. A search head that connects to multiple indexers will still be able to get real-time search results from the indexers that do have it enabled. CAUTION: Consult with Support before changing the enableRealtimeSearch setting in the indexes.conf file. Setting enableRealtimeSearch=false might prevent the ITSI Rules Engine from working properly. See Real-time search requirements in the IT Service Intelligence Install and Upgrade Manual .", "code_examples": [{"language": "spl", "code": "[default]\nenableRealtimeSearch = <bool>"}], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "How to restrict usage of real-time search", "section_heading": "Disable real-time search in indexes.conf", "section_id": "e49f90e3_e525_48ed_bbc1_8e8e232f37d2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/how-to-restrict-usage-of-real-time-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:22:25.371265+00:00", "version": "10.2"}}
{"id": "f0eae8f008eeffa4", "content": "Real-time search is a capability that you can map to specific users or roles in Splunk Web from Manager > Access Controls. By default, the rtsearch capability is assigned to the Admin and Power roles and not the User role. A role without the rtsearch capability will not be able to run a real-time search on that search head, regardless what indexers that search head is connected to.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "How to restrict usage of real-time search", "section_heading": "Disable real-time search for a user or role", "section_id": "id_2e2f4d04_e7be_4c5a_b185_788876d99b2b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/how-to-restrict-usage-of-real-time-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:22:25.371273+00:00", "version": "10.2"}}
{"id": "578297c25db5a395", "content": "You can use the [search] stanza in limits.conf to change the maximum number of real-time searches that can run concurrently on your system. max_rt_search_multiplier A number by which the maximum number of historical searches is multiplied to determine the maximum number of concurrent real-time searches. Defaults to 1. Note: The maximum number of real-time searches is computed as: max_rt_searches = max_rt_search_multiplier x max_hist_searches realtime_buffer The maximum number of accessible events to keep for real-time searches from the UI. Must be >= 1. Defaults to 10000. The real-time buffer acts as a circular buffer once this limit is reached.", "code_examples": [{"language": "spl", "code": "[search]\nmax_rt_search_multiplier = <decimal number>\nrealtime_buffer = <int>"}], "tables": [], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "How to restrict usage of real-time search", "section_heading": "Set search limits on real-time searches", "section_id": "b1aec05f_4726_4864_b4d6_07a88964d6a5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/how-to-restrict-usage-of-real-time-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:22:25.371278+00:00", "version": "10.2"}}
{"id": "ef52e42b78c0cbdb", "content": "You can use the [realtime] stanza in the limits.conf file to change the default settings for indexer support of real-time searches. These options can be overridden for individual searches using the REST API endpoints. queue_size = <int> The size of queue for each real-time search. Must be > 0. Defaults to 10000. blocking =[0|1] Specifies whether the indexer should block if a queue is full. Defaults to false (0). max_blocking_secs = <int> The maximum time to block if the queue is full. This option is meaningless, if blocking = false. Means \"no limit\" if set to 0. Defaults to 60. indexfilter = [0|1] Specifies whether the indexer should pre-filter events for efficiency. Defaults to true (1).", "code_examples": [{"language": "spl", "code": "[realtime] \nqueue_size = <int>\nblocking = [0|1] \nmax_blocking_secs = <int>\nindexfilter = [0|1]"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "How to restrict usage of real-time search", "section_heading": "Set indexer limits for real-time search", "section_id": "id_8f670c3b_0e15_4810_afc5_48a7d525605d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/how-to-restrict-usage-of-real-time-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:22:25.371282+00:00", "version": "10.2"}}
{"id": "585b7684788c0cc8", "content": "About real-time searches and reports Real-time searches and reports in Splunk Web Real-time searches and reports in the CLI Expected performance and known limitations of real-time searches and reports", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "How to restrict usage of real-time search", "section_heading": "See also", "section_id": "id_34899f60_6312_4e96_a5cc_fdc0996c3a47--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/how-to-restrict-usage-of-real-time-search", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:22:25.371286+00:00", "version": "10.2"}}
{"id": "d5066b534e0f2fd0", "content": "About real-time searches and reports Real-time searches and reports in Splunk Web Expected performance and known limitations of real-time searches and reports How to restrict usage of real-time searches", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "Real-time searches and reports in the CLI", "section_heading": "See also", "section_id": "id_32aa121f_f896_4ed0_b081_0587e328bde4--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/real-time-searches-and-reports-in-the-cli", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:22:43.750944+00:00", "version": "10.2"}}
{"id": "8fbe062bd22517f1", "content": "Splunk software performance is expected to be acceptable as long as the indexers are not currently heavily loaded and do not have more than a few concurrent real-time searches. CAUTION: Real-time searches will have a significant impact on performance in high volume environments and network load when you have many concurrent real-time searches. When planning your real-time searches, you should consider how it will affect the performance of both: The search peer that must stream the live events. The search head that must process the aggregated stream of live events. The more work that is done on the search peer, the less that is required on the search head, and vice versa. The search peer is important to the overall system function, so you do not want to burden it with too much filtering of live events. However, if the search peer does not filter at all, the processing power and bandwidth required to send all the live events to the search head may prove costly, especially when you have multiple real-time searches running concurrently. In cases where the search head cannot keep up with the search peer, the queue on the index processor will cease to flag events for the search. However, the events will have a sequence number that you can use to tell when and how many events were omitted from search consideration. Concurrent real-time and historical searches You can run real-time and historical searches concurrently, within the limits of your hardware. There are no restrictions on separate searches for the same or different users. Concurrent real-time searches Running multiple real-time searches will negatively impact indexing capacity. The real-time search feature is optimized for real-time alerting on sparse, or rare-term, searches and sacrifices indexing capacity for improved latency and reliability. Indexed real-time searches The number of concurrent real-time searches can greatly affect indexing performance. To lessen the impact on the indexer, you can enable indexed real-time search. This will basically run the search like a historical search, but will also continually update it with new events as they appear on disk. Read more about how to enable indexed real-time search in About real-time searches and reports .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Expected performance and known limitations of real-time searches and reports", "section_heading": "Indexing throughput", "section_id": "id_578b92f8_0939_4400_a049_03875108d8db--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/expected-performance-and-known-limitations-of-real-time-searches-and-reports", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:00.360888+00:00", "version": "10.2"}}
{"id": "f944263c33b3e78c", "content": "Windowed real-time searches are more expensive than non-windowed. The operations required to manage and preview the window contents can result in a windowed real time search not keeping up with a high rate of indexing. If your windowed search does not display the expected number of events, try a non-windowed search. If you are interested only in event counts, try using \"timechart count\" in your search. See Specify time ranges for real-time searches .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Expected performance and known limitations of real-time searches and reports", "section_heading": "Real-time search windows", "section_id": "id_3063bd29_2aa8_4d86_8b4d_cc358e7ecaa5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/expected-performance-and-known-limitations-of-real-time-searches-and-reports", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:00.360896+00:00", "version": "10.2"}}
{"id": "7be3940064e27adf", "content": "About real-time searches and reports Real-time searches and reports in Splunk Web Real-time searches and reports in the CLI How to restrict usage of real-time searches", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Expected performance and known limitations of real-time searches and reports", "section_heading": "See also", "section_id": "e3933b5e_ec02_49bb_91ea_7f571cf9d069--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/expected-performance-and-known-limitations-of-real-time-searches-and-reports", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:00.360901+00:00", "version": "10.2"}}
{"id": "7a1485ad5049f366", "content": "You run a real-time search in exactly the same way you run historical searches. However, because you are searching a live and continuous stream of data, the timeline updates as the events stream in and you can only view the report in preview mode. Also, some search commands are more applicable to real-time searches than historical searches. For example, streamstats and rtorder were designed for use in real-time searches. To kick off a real-time search in Splunk Web, use the time range menu to select a preset real-time time range window , such as 30 seconds or 1 minute. You can also specify a sliding time range window to apply to your real-time search. If you have Apache web access data, run the following search to see web traffic events as they stream in. The raw events that are streamed from the input pipeline are not time-ordered. You can use the rtorder command to buffer the events from a real-time search and emit them in ascending time order. The following example keeps a buffer of the last 5 minutes of web traffic events, emitting events in ascending time order once they are more than 5 minutes old. Newly received events that are older than 5 minutes are discarded if an event after that time has already been emitted. Real-time search relies on a stream of events. Thus, you cannot run a real-time search with any other leading search command, such as | metadata which does not produce events or | inputcsv which just reads in a file. Also, if you try to send the search results to | outputcsv , the CSV file will not be written until the real-time search is Finalized.", "code_examples": [{"language": "spl", "code": "sourcetype=access_*"}, {"language": "spl", "code": "sourcetype=access_* | rtorder discard=t buffer_span=5m"}], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Real-time searches and reports in Splunk Web", "section_heading": "Real-time searches in Splunk Web", "section_id": "id_4aac7298_f401_4151_8071_ba0a41c2e836--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/real-time-searches-and-reports-in-splunk-web", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:16.877805+00:00", "version": "10.2"}}
{"id": "424824551a01c0ba", "content": "Run a report to preview the IP addresses that access the most web pages. In this case, the top command returns a table with three columns: clientip, count, and percent. As the data streams in, the table updates with new values. For each web traffic event, add a count field that represents the number of events seen so far (but do not include the current event in the count). You can also drilldown into real-time reports. However, real-time drilldown does not spawn another real-time search. Instead, it spawns a historic search, as you will drilldown into the events that have already been retrieved and indexed. For more information, see Use drilldown for dashboard interactivity in Dashboards and Visualizations .", "code_examples": [{"language": "spl", "code": "sourcetype=access_* | top clientip"}, {"language": "spl", "code": "sourcetype=access_* | streamstats count current=false"}], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Real-time searches and reports in Splunk Web", "section_heading": "Real-time reports in Splunk Web", "section_id": "a15e1f68_e06c_4416_9303_075450a41f68--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/real-time-searches-and-reports-in-splunk-web", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:16.877814+00:00", "version": "10.2"}}
{"id": "fd44be13dae39011", "content": "About real-time searches and reports Real-time searches and reports in the CLI Expected performance and known limitations of real-time searches and reports How to restrict usage of real-time searches", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Real-time searches and reports in Splunk Web", "section_heading": "See also", "section_id": "eb88fd7c_04e6_4b27_b8ce_8ec37cdf877a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/search-and-report-in-real-time/real-time-searches-and-reports-in-splunk-web", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Search and Report in Real Time", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:16.877819+00:00", "version": "10.2"}}
{"id": "0579d6f2956aca6b", "content": "Exporting data starts with running a search job to generate results. You can then export this search result data to a file. Run a search job using a POST to /services/search/jobs/. If you are using a custom time range, pass it in with the POST request. Get the search job ID (SID) for the search. The /jobs endpoint returns an XML response including the <sid> , or search job ID. You can also get the search job ID by viewing the job in the Search Job Inspector. in Splunk Web. Navigate to Activity > Jobs to open the Job Manager. Locate the search job that you just ran and click Inspect. The Search Job Inspector opens in a separate window. See View the properties of a search job. Use a GET request on the /results endpoint to export the search results to a file. Ensure that you do the following in the GET request: Identify your object endpoints. To see a list of currently available object endpoints for your user, within your app, navigate to https://localhost:8089/servicesNS/<user>/<app>/. For example: Identify the search job user and app. The following example defines <user> as admin and <app> as search. Identify an output format. Use the output_mode parameter to specify one of the following available output formats. Use lower case for the format name, as shown here. This example exports search results to a JSON file.", "code_examples": [{"language": "spl", "code": "curl -k -u admin:changeme \\\n     https://localhost:8089/services/search/jobs/ -d search=\"search sourcetype=access_* earliest=-7d\""}, {"language": "spl", "code": "<?xml version='1.0'encoding='UTF-8'?>\n<response>\n  <sid>1423855196.339</sid>\n</response>"}, {"language": "spl", "code": "https://localhost:8089/servicesNS/admin/search/saved/searches/"}, {"language": "spl", "code": "atom | csv | json | json_cols | json_rows | raw | xml"}, {"language": "spl", "code": "curl -u admin:changeme \\\n     -k https://localhost:8089/servicesNS/admin/search/search/jobs/1423855196.339/results/ \\\n     --get -d output_mode=json -d count=5"}], "tables": [], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "Export data using the Splunk REST API", "section_heading": "Export data", "section_id": "id_74f2d393_05da_4dc1_8ac2_0277f1db9259--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-data-using-the-splunk-rest-api", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:33.377354+00:00", "version": "10.2"}}
{"id": "691f8258ea9d2e08", "content": "For more details about the /jobs and /export endpoints, see the following information in the REST API Reference. search/jobs search/jobs/export See also Creating searches using the REST API in the REST API Tutorials .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "Export data using the Splunk REST API", "section_heading": "See also", "section_id": "id_0c421add_4134_4869_b9d4_7b29a9936fbf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-data-using-the-splunk-rest-api", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:33.377363+00:00", "version": "10.2"}}
{"id": "8426d0731013cbbe", "content": "If your search returns a large number of results, it is possible that not all of the results will be stored with the search job artifact. When you export search results, the export process is based on the search job artifact, not the results in the Search app. If the artifact does not contain the full set of results, a message appears at the bottom of the Export Results dialog box to tell you that the search will be rerun by the Splunk software before the results are exported. The search is rerun when the search head believes that it cannot retrieve all of the events from the job artifact. The search head determines when to rerun the search based on the following logic: If the search is not a report, and one of the following is true. The search is not done The search is using a remote timeline The search head believes that the search has not retained all of events", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "Export data using Splunk Web", "section_heading": "When exporting triggers your search to run again", "section_id": "d49b8602_492e_4071_85c3_08c7f6083609--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-data-using-splunk-web", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:51.534516+00:00", "version": "10.2"}}
{"id": "8e83d8d491254389", "content": "Note: This capability is not available to Splunk Cloud Platform users. When you export large amounts of data using the Export button, the session might timeout before the export is complete. Splunk Enterprise users who have a role with the edit_server capability can extend the session timeout limit. Click Settings > Server Settings > General Settings. In the Splunk Web section, increase the number in the Session timeout field. Click Save .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "Export data using Splunk Web", "section_heading": "Extend the session timeout when exporting large amounts of data", "section_id": "e35aba1e_6584_46cc_858a_f2ee9d4224e8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-data-using-splunk-web", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:51.534524+00:00", "version": "10.2"}}
{"id": "f542fca26e6766c6", "content": "You can forward the data that you export to third-party systems. For an brief overview, see Forward data to third party systems in this manual. For more details, see Forward data to third party systems in Forwarding Data .", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "Export data using Splunk Web", "section_heading": "Forward data to third-party systems", "section_id": "e5123a94_f464_4675_a0d4_76f746826c0d--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-data-using-splunk-web", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:51.534529+00:00", "version": "10.2"}}
{"id": "05a7eab6814c4139", "content": "You can schedule reports to run on a regular interval and send the results to project stakeholders by email. The emails can present the report results in tables in the email, and as CSV or PDF attachments. The emails can also include links to the report results in Splunk Enterprise. See Schedule Reports in the Reporting Manual .", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "Export data using Splunk Web", "section_heading": "Use reports to send results to stakeholders", "section_id": "id_9cffe630_ab9d_4a65_b11b_c6dbe866ea2c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-data-using-splunk-web", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T14:23:51.534532+00:00", "version": "10.2"}}
{"id": "5204f01e101b3bdc", "content": "The Command Line Interface (CLI) is easy to script, can handle automation, and can process volumes of data faster and more efficiently than Splunk Web. To access Splunk Enterprise through the CLI, you either need shell access to a Splunk Enterprise server, or permission to access the correct port on a remote Splunk server. Note: Splunk Cloud Platform users don't have shell access to the Splunk Cloud Platform deployment and therefore can't use the CLI to export data. The syntax for exporting data using the CLI is as follows: If -output is not specified, the default output depends on the type of search. For non-transforming searches, the default output format is rawdata. For transforming searches, the default output format is table. By default, you can export a maximum of 100 events. To increase this number, use the -maxout argument. For example, if you include -maxout 300000 you can export 300,000 events. Set -maxout to 0 to export an unlimited number of events. To learn more about the Splunk Enterprise CLI, read About the CLI in the Admin Manual. CLI output command example This CLI example takes events from the _internal index that occur within the time range specified by the search string and outputs 200,000 of them in raw data format to the file test123.dmp .", "code_examples": [{"language": "spl", "code": "splunk search [eventdata] -preview 0 -maxout 0 -output [rawdata | table | csv | raw | auto | json] > [myfilename.log] ..."}, {"language": "spl", "code": "splunk search\"index=_internal earliest=09/14/2024:23:59:00 latest=09/16/2024:01:00:00 \"-output rawdata -maxout 200000 > c:/test123.dmp"}], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "Export data using the CLI", "section_heading": "", "section_id": "d15fe533e022846c38242b983238fc3d9--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-data-using-the-cli", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T14:24:07.934609+00:00", "version": "10.2"}}
{"id": "b7f352df3f90acce", "content": "The Splunk SDK for Python lets you write Python applications that can interact with Splunk deployments. Export searches using the Python SDK can be run in historical mode and real-time mode. They start right away, and stream results instantly, letting you integrate them into your Python application. Perform an export search using the Python SDK. This script has been made cross-compatible with Python 2 and Python 3 using python-future. 1. Set the parameters of what you wish to search. The following example sets the parameters as an export search of splunklib in the last hour. Note that sys.path.insert adds lib to the path so that the app calls the version of splunklib installed with this app, which you should store in the /lib directory of the app, as detailed in The directory structure of a Splunk App in Splunk developer docs. 2. Change or acquire these values, as necessary. 3. Run a normal-mode search. 4. Get the results and display them using the ResultsReader.", "code_examples": [{"language": "spl", "code": "sys.path.insert(0, os.path.join(os.path.dirname(__file__),\"..\",\"lib\"))\nimport splunklib.client as client\nimport splunklib.results as results\nfrom __future__ import print_function"}, {"language": "spl", "code": "HOST =\"localhost\"PORT = 8089\nUSERNAME =\"admin\"PASSWORD =\"changeme\""}, {"language": "spl", "code": "service = client.connect(\n    host=HOST,\n    port=PORT, \n    username=USERNAME,\n    password=PASSWORD)\n\nrr = results.ResultsReader(service.jobs.export(\"search index=_internal earliest=-1h | head 5\"))"}, {"language": "spl", "code": "forresultinrr:ifisinstance(result, results.Message):# Diagnostic messages might be returned in the resultsprint('%s:Â %s'% (result.type, result.message))elifisinstance(result, dict):# Normal events are returned as dictsprint(result)\nassert rr.is_preview == False"}], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "Export data using the Splunk SDKs", "section_heading": "Use Python SDK to export data", "section_id": "eeab8a3f_61f3_40cc_81e6_7866044a520e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-data-using-the-splunk-sdks", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T14:24:25.581246+00:00", "version": "10.2"}}
{"id": "a9cd9be26257415d", "content": "The Java SDK is able to conduct and export searches while using Java. To perform an export search using the Java SDK, run the following example in the /splunk-sdk-java directory using the CLI: The Export application exports the \"main\" index to export.out , which is saved to the current working directory. If you want to run this application again, delete export.out before you try again. If you do not do this, you will get an error. Here is a different CLI example of the Java SDK. It shows how to include a search query and change the output format to JSON.", "code_examples": [{"language": "spl", "code": "java -jar dist/examples/export.jar main --username=\"admin\"--password=\"changeme\""}, {"language": "spl", "code": "java -jar dist/examples/export.jar main --search=\"search sourcetype=access_*\"json"}], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "Export data using the Splunk SDKs", "section_heading": "Use Java SDK to export data", "section_id": "ea3289ba_059f_4d09_bcd4_589434417646--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-data-using-the-splunk-sdks", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T14:24:25.581255+00:00", "version": "10.2"}}
{"id": "c5e3136e0e4a51e0", "content": "The Javascript Export endpoint can export Splunk data in the Javascript framework. Though the Splunk Javascript SDK does not currently support the Javascript Export endpoint, you can use a node javascript (.js) application request to export data. To perform an export search using the Javascript Export endpoint: 1. Load the request module. Request is designed to be the simplest way to make an http/https call. 2. Call get to issue a GET request. Enter the following parameters: strictSSL : When set to false, strictSSL tells the request to not validate the server certificate returned by your Splunk deployment, which by default is not a valid certificate. uri: Provide the uri of the Splunk host along with the path for the export endpoint. A JSON response is specified in the query string. qs: Set qs to supply the search parameter. By passing it this way, you do not have to URI encode the search string. 3. Call auth to use HTTP Basic Auth and pass your Splunk username and password. 4. Pipe the results to stdout .", "code_examples": [{"language": "spl", "code": "var request = require('request');"}, {"language": "spl", "code": "request.get(\n    {\n        strictSSL:false,\n        uri:'https://localhost:8089/servicesNS/admin/search/search/jobs/\n              export?output_mode=json',\n        qs: {\n            search:'search index=_internal'}\n    }\n)"}, {"language": "spl", "code": ".auth('admin','changeme',false)"}, {"language": "spl", "code": ".pipe(process.stdout);"}], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "Export data using the Splunk SDKs", "section_heading": "Use JavaScript Export to export data", "section_id": "id_43322b09_dd94_4334_a48e_9eedad83341e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/export-data-using-the-splunk-sdks", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T14:24:25.581260+00:00", "version": "10.2"}}
{"id": "3b02d8472a938a13", "content": "Example 1: Return results from specified search peers. Example 2: Search different indexes on distributed search peers \"foo\" or \"bar\".", "code_examples": [{"language": "spl", "code": "error (splunk_server=NYsplunk OR splunk_server=CAsplunk) NOT splunk_server=TXsplunk"}, {"language": "spl", "code": "(splunk_server=foo index=main 404 ip=10.0.0.0/16) OR (splunk_server=bar index=mail user=admin)"}], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "Search across one or more distributed search peers", "section_heading": "Examples", "section_id": "ed9a0101_c296_4e96_bca1_880d338d0cde--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/retrieve-events/search-across-one-or-more-distributed-search-peers", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Retrieve Events", "manual": "search-manual", "scraped_at": "2026-01-23T14:24:42.494779+00:00", "version": "10.2"}}
{"id": "a952bee4ce793419", "content": "You can run historical searches using the search command, and real-time searches using the rtsearch command. The following is a table of useful search-related CLI help objects. To see the full help information for each object, type into the CLI:", "code_examples": [{"language": "spl", "code": "./splunkhelp<object>"}], "tables": [{"headers": ["Object", "Description"], "rows": [["rtsearch", "Returns the parameters and syntax for real-time searches."], ["search", "Returns the parameters and syntax for historical searches."], ["search-commands", "Returns a list of search commands that you can use from the CLI."], ["search-fields", "Returns a list of default fields."], ["search-modifiers", "Returns a list of search and time-based modifiers that you can use to narrow your search."]]}], "chunk_index": 0, "total_chunks": 2, "metadata": {"title": "About searches in the CLI", "section_heading": "CLI help for search", "section_id": "id_966d0abc_d26a_4932_8884_de4a8d820bfc--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-in-the-cli/about-searches-in-the-cli", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search in the CLI", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:24:59.772612+00:00", "version": "10.2"}}
{"id": "f98c4f3bb4f5f907", "content": "Historical and real-time searches in the CLI work the same way as searches in Splunk Web, except that there is no timeline rendered with the search results and there is no default time range. Instead, the results are displayed as a raw events list or a table, depending on the type of search. For more information, read \"Type of searches\" in the Search Overview chapter of the Search Manual. The syntax for CLI searches is similar to the syntax for Splunk Web searches, except that you can pass parameters outside of the query to specify the time limit of the search, where to run the search, and how results are displayed. For more information about the CLI search options, see the next topic in this chapter, \"CLI search syntax\". For more information about how to search remote Splunk servers from your local server, see \"Access and use the CLI on a remote server\" in the Splunk Enterprise Admin Manual .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 2, "metadata": {"title": "About searches in the CLI", "section_heading": "Search in the CLI", "section_id": "fc01b1b9_1003_49a4_9046_244a1b8ae9bf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-in-the-cli/about-searches-in-the-cli", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search in the CLI", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:24:59.772619+00:00", "version": "10.2"}}
{"id": "30e703532a75e1b9", "content": "Un-escapes xml characters, including entity references such as &, <, and >, so that they return to their corresponding characters. For example, &amp; becomes &. The xmlunescape command is a streaming command. It is distributable streaming by default, but centralized streaming if the local setting specified for the command in the commands.conf file is set to true. See Command types .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "xmlunescape", "section_heading": "Description", "section_id": "id_5214721b_c2d6_4829_958b_8e9a9da499b0--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xmlunescape", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:25:14.825592+00:00", "version": "10.2"}}
{"id": "62bf82001eb50874", "content": "xmlunescape maxinputs=<int> Optional arguments maxinputs Syntax: maxinputs=<int> Description: The maximum number of inputs per invocation of the command. The xmlunescape command is invoked repeatedly in increments according to the maxinputs argument until the search is complete and all of the results have been displayed. Do not change the value of maxinputs unless you know what you are doing. Default: 50000", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "xmlunescape", "section_heading": "Syntax", "section_id": "fb0a1062_009b_4df5_bb51_299fb5ab6d3b--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xmlunescape", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:25:14.825600+00:00", "version": "10.2"}}
{"id": "77a08442bff180a0", "content": "Example 1: Un-escape all XML characters.", "code_examples": [{"language": "spl", "code": "... | xmlunescape"}], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "xmlunescape", "section_heading": "Examples", "section_id": "id_160daa0e_d133_4f40_b186_409d9817f932--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/xmlunescape", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:25:14.825605+00:00", "version": "10.2"}}
{"id": "cca9811b586a42e5", "content": "The dbxquery command is used with Splunk DB Connect. For information about this command, see Execute SQL statements and stored procedures with the dbxquery command in Deploy and Use Splunk DB Connect .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "d4fef494b8d8842a797308ee4d58449d2--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/dbxquery", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:25:31.379877+00:00", "version": "10.2"}}
{"id": "3745ae6745c61eff", "content": "Welcome to the Search Reference. See the left navigation panel for links to the built-in search commands. If you don't find a command in the list, that command might be part of a third-party app or add-on. For information about commands contributed by apps and add-ons, see the documentation on Splunkbase .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "d39c3259da0db4e5b81bc83539a43a5ec--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/3rd-party-custom-commands", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:25:49.068874+00:00", "version": "10.2"}}
{"id": "9663c05d07ebf546", "content": "The snowevent command is used with the Splunk Add-on for ServiceNow. For information about this command, see Use custom generating search commands for the Splunk Add-on for ServiceNow in Splunk Add-on for ServiceNow .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "d024e3b7e85ec46119ad3700b16f7b795--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/snowevent", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:26:05.677192+00:00", "version": "10.2"}}
{"id": "27efa3617799d502", "content": "Internal search commands refer to a set of commands that are designed to be used in specific situations, typically at the direction and with guidance from Splunk Support. These commands might be removed, or updated and reimplemented differently, in future versions. Consult your Splunk Administrator or Splunk Support before using any of these commands. collapse dump findkeywords makejson mcatalog noop prjob redistribute runshellscript", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "d5da72b3b9a79419d8080549ec5632496--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/about-internal-commands", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:26:22.112530+00:00", "version": "10.2"}}
{"id": "3bd6eed9e55ebbc6", "content": "Use the sendalert command to invoke a custom alert action. The command gathers the configuration for the alert action from the alert_actions.conf file and the saved search and custom parameters passed using the command arguments. Then the command performs token replacement. The command determines the alert action script and arguments to run, creates the alert action payload and executes the script, handing over the payload by using STDIN to the script process. When running the custom script, the sendalert command honors the maxtime setting from the alert_actions.conf file and terminates the process if the process runs longer than the configured threshold. By default the threshold is set to 5 minutes. See Write the script for a custom alert action for Splunk Cloud Platform or Splunk Enterprise in the Developer Guide on the Developer Portal. CAUTION: This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "sendalert", "section_heading": "Description", "section_id": "id_6e349c90_9b00_4b8b_b24e_57b6636dfe83--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sendalert", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:26:38.483049+00:00", "version": "10.2"}}
{"id": "616b2e38a7b1bcb3", "content": "sendalert <alert_action_name> [results_link=<url>] [results_path=<path>] [param.<name>=<\"value\">...] Required arguments alert_action_name Syntax: <alert_action_name> Description: The name of the alert action configured in the alert_actions.conf file Optional arguments results_link Syntax: results_link=<url> Description: Set the URL link to the search results. results_path Syntax: results_path=<path> Description: Set the location to the file containing the search results. param.<name> Syntax: param.<name>=<\"value\"> Description: The parameter name and value. You can use this name and value pair to specify a variety of things, such as a threshold value, a team name, or the text of a message.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "sendalert", "section_heading": "Syntax", "section_id": "id_564d4a19_770b_4a7b_8fa4_10722f6bb399--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sendalert", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:26:38.483060+00:00", "version": "10.2"}}
{"id": "6d02927aa7d981da", "content": "When you use the sendalert command in an ad hoc search , the command might be called multiple times if there are a large number of search results. This occurs because previewing the search results on the Statistics tab is enabled by default. If you are using an ad hoc search to test the sendalert command, testing turn off preview to avoid the command being called multiple times. When the sendalert command is included in a saved search, such as a scheduled report or a scheduled search, the command is called only one time. Capability required To use this command, you must have a role with the run_sendalert capability. See Define roles on the Splunk platform with capabilities. Search results format When the sendalert command is used in a search or in an alert action, the search results are stored in an archive file in the dispatch directory using the CSV format. The file name is results.csv.gz. The default format for the search results is SRS, a Splunk-specific binary format for the search results. The CSV format for the archive file is used so that scripts can process the results file. The default SRS format is not designed to be parsed by scripts. The archived search results format is controlled through the forceCsvResults setting. This setting is in the [default] stanza in the alert_actions.conf file.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "sendalert", "section_heading": "Usage", "section_id": "id_6b5302e2_cae1_4795_a274_6dd32a6ec3a5--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sendalert", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:26:38.483068+00:00", "version": "10.2"}}
{"id": "083fbda5c5aeb4b0", "content": "Example 1: Invoke an alert action without any arguments. The alert action script handles checking whether there are necessary parameters that are missing and report the error appropriately. Example 2: Trigger the hipchat custom alert action and pass in room and message as custom parameters. Example 3: Trigger the servicenow alert option.", "code_examples": [{"language": "spl", "code": "... | sendalert myaction"}, {"language": "spl", "code": "... | sendalert hipchat param.room=\"SecOps\"param.message=\"There is a security problem!\""}, {"language": "spl", "code": "... | sendalert servicenow param.severity=\"3\"param.assigned_to=\"DevOps\"param.short_description=\"Splunk Alert: this is a potential security issue\""}], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "sendalert", "section_heading": "Examples", "section_id": "bbbc9be1_a8cd_4c04_b1c1_1bcbd9d93cd7--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/sendalert", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:26:38.483074+00:00", "version": "10.2"}}
{"id": "def8e8ea7c140895", "content": "The snowincident command is used with the Splunk Add-on for ServiceNow. For information about this command, see Use custom generating search commands for the Splunk Add-on for ServiceNow in Splunk Add-on for ServiceNow .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "de47739db966d43ea8056fe2fdc499a33--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/snowincident", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:26:55.465725+00:00", "version": "10.2"}}
{"id": "e8d518cb349214b8", "content": "The snowincidentstream command is used with the Splunk Add-on for ServiceNow. For information about this command, see Use custom streaming search commands for the Splunk Add-on for ServiceNow in Splunk Add-on for ServiceNow .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "d3e0cfd5f5e434ff096a1b440f668caeb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/snowincidentstream", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:27:10.796818+00:00", "version": "10.2"}}
{"id": "99ad88a3d4abb8ad", "content": "Commands mcatalog mstats", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "msearch", "section_heading": "See also", "section_id": "id_86786029_2c68_45ce_a828_b9d2eb2c2531--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/msearch", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:27:26.238453+00:00", "version": "10.2"}}
{"id": "3744ab0b9ea6eed4", "content": "The awssnsalert command is used with the Splunk Add-on for AWS. For information about this command, see Use the awssnsalert search command in Splunk Add-on for AWS .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "dde3ec9e707d64c008df9ebccd1881dca--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/awssnsalert", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:27:41.640336+00:00", "version": "10.2"}}
{"id": "d1a2d74b3f53286a", "content": "The ingestpreview command previews ingest-time configuration settings without having to ingest or import data. The ingestpreview command previews ingest-time configuration settings without having to ingest or import data. The ingestpreview command takes incoming search results, generates mock ingestion events from those results, and supplies those mock events to the specified ingestion processor, which then outputs the processed events. This lets you quickly author ingest-time configurations without having to upload or index real data. For example, you can iterate or debug an INGEST_EVAL or REGEX transform, as well as troubleshoot configurations in props.conf and transforms.conf.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 3, "metadata": {"title": "ingestpreview", "section_heading": "Description", "section_id": "cb9e0f420-ba77-4fc7-83d7-56050de7784a--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/ingestpreview", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:27:56.037840+00:00", "version": "10.2"}}
{"id": "99a65e19d1a9a582", "content": "Syntax for using the ingestpreview command. The required syntax is in bold. ingestpreview [generate_helper_fields=<boolean>] [ingest_processor=<string>] [meta_mode=<string>] [props:<key>=<value>]... [show_inputs=<boolean>] [transforms:<key>=<value>]... Required arguments None. Optional arguments generate_helper_fields Syntax: generate_helper_fields=<boolean> Description: Generates the following three additional fields: Default: true ingest_processor Syntax: ingest_processor=<string> Description: The target ingest-time processor accepts one of the following values: Default: regexreplacement meta_mode Syntax: meta_mode=<string> Description: Controls how the ingestpreview command displays the resulting _meta key. The _meta key contains the map of indexed time field/value pairs. The command always generates a _meta field if it is present in the results. However, Splunk Web will not show this by default since it is a field that starts with an underscore ( _ ). You can set meta_mode to one of the following options: Default: unhide props Syntax: props:<key>=<value> Description: Supply one or more settings for props using this syntax. For example, to configure a statsd event using METRICS_PROTOCOL, specify props:METRICS_PROTOCOL=statsd. Note: If field values contain spaces or special characters, you can wrap the values in parentheses or double quotes. The command strips the outer set of these characters before processing the arguments. show_inputs Syntax: show_inputs=<boolean> Description: If set to true , the command generates INPUT.* fields for each input field with the original value before transformation. This is helpful for determining the difference between the input and output for a particular field. transforms Syntax: transforms:<key>=<value> Description: Supply one or more settings for transforms using this syntax. For example, to configure the REGEX setting in transforms.conf, specify transforms:REGEX=<your regex>. Note: If field values contain spaces or special characters, you can wrap the values in parentheses or double quotes. The command strips the outer set of these characters before processing the arguments.", "code_examples": [], "tables": [{"headers": ["Field", "Description"], "rows": [["TRANSFORMS.CONF", "Generates the exact settings you can copy and paste into transforms.conf. Settings might differ from those you supply to this search command because of character escaping rule discrepancies between the search language and configuration files."], ["PROPS.CONF", "Generates the exact settings you can copy and paste into props.conf. Settings might differ from those you supply to this search command because of character escaping rule discrepancies between the search language and configuration files."], ["WARNS.ERRS", "Displays any errors or warnings reported by the processor to help further troubleshoot settings."]]}, {"headers": ["Option", "Description"], "rows": [["regexreplacement", "Use for regex replacement."], ["metrics", "Use for statsd or collectd data."], ["metricschema", "Use for logs to metrics."]]}, {"headers": ["Option", "Description"], "rows": [["Unhide", "Creates an alias to the _meta field namedMETAso it is visible in Splunk Web. This is equivalent to using|eval META=_meta."], ["Expand", "Allows each indexed time field/value pair to become a separate field. Each field will be prefixed withMETA.."], ["All", "Performs bothexpandandunhidebehaviors."], ["None", "Doesn't performexpandorunhidebehaviors."]]}], "chunk_index": 1, "total_chunks": 3, "metadata": {"title": "ingestpreview", "section_heading": "Syntax", "section_id": "ceba8ff6e-903f-423f-976d-3fbd9ffbc819--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/ingestpreview", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:27:56.037854+00:00", "version": "10.2"}}
{"id": "7159392535f079cd", "content": "Examples for the ingestpreview command. 1. Create a meta field and set it to Hello World Run INGEST_EVAL that creates a meta field, myfield and sets it to Hello World. 2. Run a REGEX transform that changes myfield if _raw matches Run a REGEX transform that changes myfield if _raw matches. Note that double quotes are used on the REGEX parameter to deal with spaces. 3. Test dimension extraction (ipv4) for statsd data Uses ingest_processor=metrics to test dimension extraction (ipv4) for statsd data. 4. Build a metrics event out of sample data Uses ingest_processor=metricschema to build a metrics event out of sample data. This search first builds a raw event, then mimics the metadata from the field extractions, and then runs the ingestpreview command to display the mock metrics event.", "code_examples": [{"language": "spl", "code": "| makeresults count | fields - count | ingestpreview transforms:INGEST_EVAL=(myfield=\"Hello World\")"}, {"language": "spl", "code": "| makeresults count| fields - count |eval_raw=\"raw with open(parenthesis)close\"|evalmyfield=\"original_value\"| ingestpreview transforms:REGEX=\"with open\\(parenthesis\\)close\"transforms:WRITE_META=truetransforms:FORMAT=\"$0myfield::new_value\""}, {"language": "spl", "code": "| makeresults count| fields - count |eval_raw=\"cpu.idle.10.3.4.134:1.2342|g\"| ingestpreview ingest_processor=metrics props:METRICS_PROTOCOL=statsd props:NO_BINARY_CHECK=trueprops:SHOULD_LINEMERGE=falsetransforms:REGEX=((?<ipv4>\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3})) transforms:REMOVE_DIMS_FROM_METRIC_NAME=true|evalmetric_value=_value"}, {"language": "spl", "code": "| makeresults |eval_raw=\"2021-01-03T10:35:12-0800 dns_name=contrarian.local severity=informational http_status=200 response_ms=244\"| extract auto=f field_extraction |eval_meta=\"dns_name::contrarian.local severity::informational http_status::200 response_ms::244\"| ingestpreview ingest_processor=metricschema generate_helper_fields=truemeta_mode=all transforms:METRIC-SCHEMA-MEASURES=\"NUMS_EXCEPT http_status\""}], "tables": [], "chunk_index": 2, "total_chunks": 3, "metadata": {"title": "ingestpreview", "section_heading": "Examples", "section_id": "c85a9a3ae-b51a-47f3-aebb-842f16a4e79e--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/ingestpreview", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:27:56.037861+00:00", "version": "10.2"}}
{"id": "26ec4b86962c4672", "content": "The run command is an alias for the script command. See the script command for the syntax and examples. CAUTION: This command is considered risky because, if used incorrectly, it can pose a security risk or potentially lose data when it runs. As a result, this command triggers SPL safeguards. See SPL safeguards for risky commands in Securing the Splunk Platform .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "d05d4bcdd67014a228dcf8f955416bc92--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/run", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:28:14.009052+00:00", "version": "10.2"}}
{"id": "8c5e556ebdc06975", "content": "The snoweventstream command is used with the Splunk Add-on for ServiceNow. For information about this command, see Use custom streaming search commands for the Splunk Add-on for ServiceNow in Splunk Add-on for ServiceNow .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "da383a7c6fda74bdca8935b09e6a15c8f--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/snoweventstream", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:28:31.901376+00:00", "version": "10.2"}}
{"id": "10dd44c0264b5880", "content": "The entitymerge command is used with Splunk Enterprise Security. For information about this command, see Overwrite asset or identity data with entitymerge in Splunk Enterprise Security in Administer Splunk Enterprise Security .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "d51c4dc8672c34287a93e66f7f4e96e20--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/entitymerge", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:28:50.453618+00:00", "version": "10.2"}}
{"id": "22f1a6c8048633b5", "content": "The ctable , or counttable , command is an alias for the contingency command. See the contingency command for the syntax and examples.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "d56a42ac6e3b841ce8361681d59403ace--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/ctable", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:29:07.282195+00:00", "version": "10.2"}}
{"id": "0de1f7c03674f194", "content": "The inputintelligence command is used with Splunk Enterprise Security. For information about this command, see Use the inputintelligence command to use generic intelligence in Splunk Enterprise Security in Administer Splunk Enterprise Security .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "d509a22a1a9414559841a972cc8c53f11--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/inputintelligence", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:29:24.959929+00:00", "version": "10.2"}}
{"id": "d5f37f973b89497b", "content": "The datamodelsimple command is used with the Splunk Common Information Model Add-on. For information about this command, see Use the datamodelsimple command in the Common Information Model Add-on Manual .", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "dc2f2efd4269242e6bc0089b68a2a64cb--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-commands/datamodelsimple", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:29:41.581288+00:00", "version": "10.2"}}
{"id": "5c1aabd11f146d45", "content": "Splunk software can forward data to third-party systems as follows: Through a plain TCP socket Packaged in a standard syslog To forward data to third-party systems, you configure heavy forwarders by editing the outputs.conf , props.conf and transforms.conf files. This export method is similar to routing your data to other Splunk deployments. You can filter the data by host, source, or source type. See Forward data to third party systems in the Forwarding Data manual.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 1, "metadata": {"title": "", "section_heading": "", "section_id": "db3bac686fbf94de5b0011c66afd73cd6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/search-manual/10.2/export-search-results/forward-data-to-third-party-systems", "breadcrumb": "Splunk Enterprise > Search > Search Manual > Export Search Results", "manual": "search-manual", "scraped_at": "2026-01-23T14:29:58.034117+00:00", "version": "10.2"}}
{"id": "e0c3778d51a8bbe4", "content": "By default when you run a search from the CLI, the search is uses All Time as the time range. You can specify time ranges using one of the CLI search parameters, such as earliest_time , index_earliest , or latest_time. The first 100 events are returned when you run a historical search using the CLI. Use the maxout search parameter to specify the number of events to return.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 5, "metadata": {"title": "Syntax for searches in the CLI", "section_heading": "Search defaults", "section_id": "id_18f9b466_d503_44f8_938a_f678675d47e8--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-in-the-cli/syntax-for-searches-in-the-cli", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search in the CLI", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:15.016823+00:00", "version": "10.2"}}
{"id": "bcfd341990e8108a", "content": "Search objects are enclosed in single quotes (' ') and can be keywords, expressions, or a series of search commands. On Windows OS use double quotes (\" \") to enclose your search object. For more information about searching, see Start searching in the Search Tutorial. For a brief description of every search command, see the Command quick reference in the Search Reference. For a quick reference for Splunk concepts, features, search commands, and functions, see the Quick Reference Guide in the Search Reference. Search objects can include not only keywords and search commands but also fields and modifiers to specify the events you want to retrieve and the results you want to generate. For more information about fields, see Use fields to search in the Search Tutorial. For more information about default fields and how to use them, see Use default and internal fields in the Knowledge Manager Manual. For more information about time modifiers, see Time modifiers for search in the Search Reference .", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 5, "metadata": {"title": "Syntax for searches in the CLI", "section_heading": "Search objects", "section_id": "id_95e97faf_ad41_48d2_babc_0d487fc0ea37--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-in-the-cli/syntax-for-searches-in-the-cli", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search in the CLI", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:15.016831+00:00", "version": "10.2"}}
{"id": "cd549ea36a8c245f", "content": "Search parameters are options that control the way the search is run or the way the search results are displayed. All of these parameters are optional. Parameters that take Boolean values support 0, false, f, no as negatives and 1, true, t, yes as positives. Specify these search parameters at the end of your search, after you have specified all of the commands and command arguments. See Example 4 .", "code_examples": [], "tables": [{"headers": ["Parameter", "Values", "Defaults", "Description"], "rows": [["app", "<app_name>", "search", "Specify the name of the app in which to run your search."], ["batch", "<bool>", "F", "Indicates how to handle updates in preview mode."], ["detach", "<bool>", "F", "Triggers an asynchronous search and displays the job ID and TTL for the search."], ["earliest_time", "<time-modifier>", "âˆ’", "The relative time modifier for the start time of the search. This is optional forsearchand required forrtsearch."], ["header", "<bool>", "T", "Indicates whether to display a header in the table output mode."], ["index_earliest", "<time-modifier>", "", "The start time of the search. This can be expressed as an epoch or relative time modifier and uses the same syntax as the \"earliest\" and \"latest\" time modifiers for search language. This is optional for bothsearchandrtsearch."], ["index_latest", "<time-modifier>", "", "The end time of the search. This can be expressed as an epoch or relative time modifier and uses the same syntax as the \"earliest\" and \"latest\" time modifiers for search language. This is optional for bothsearchandrtsearch."], ["latest_time", "<time-modifier>", "âˆ’", "The relative time modifier for the end time of search. Forsearch, if this is not specified, it defaults to the end of the time (or the time of the last event in the data), so that any \"future\" events are also included.Forrtsearch, this is a required parameterand the real-time search will not run if it's not specified."], ["max_time", "<number>", "0", "The length of time in seconds that a search job runs before it is finalized. A value of 0 means that there is no time limit."], ["maxout", "<number>", "search, 100rtsearch, 0", "The maximum number of events to return or send tostdoutwhen exporting events. A value of 0 means that it will output an unlimited number of events."], ["output", "rawdata, table, csv, auto, json", "Depends on the type of search:Usesrawdatafor non-transforming searches.Usestablefor transforming searches.", "Indicates how to display the job."], ["preview", "<bool>", "T", "Indicates that reporting searches should be previewed (displayed as results are calculated)."], ["timeout", "<number>", "0", "The length of time in seconds that a search job is allowed to live after running. A value of 0 means that the job is canceled immediately after it is run."], ["uri", "[http|https]://name_of_server:management_port", "", "Specify the server name and management port.name_of_servercan be the fully-resolved domain name or the IP address of the Splunk server.The default uri value is themgmtHostPortvalue that you defined in the Splunk server'sweb.conf.For more information, seeAccess and use the CLI on a remote Splunk Serverin theAdmin manual."], ["wrap", "<bool>", "T", "Indicates whether to line wrap for individual lines that are longer than the terminal width."]]}], "chunk_index": 2, "total_chunks": 5, "metadata": {"title": "Syntax for searches in the CLI", "section_heading": "Search parameters", "section_id": "id_5bec2c90_4f97_40ee_9675_f6f61d7b68bf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-in-the-cli/syntax-for-searches-in-the-cli", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search in the CLI", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:15.016844+00:00", "version": "10.2"}}
{"id": "3d97836b8337732f", "content": "You can see more examples in the CLI help information. 1. Retrieve events from yesterday that match root sessions 2. Retrieve events that match web access errors and detach the search 3. Run a windowed real-time search See more examples of Real-time searches and reports in the CLI in the Admin Manual. 4. Return a list of unique hostnames There are two recommended ways that you can do this. This first is with the stats command: Alternatively, since you are only interested in the host field, you can use the metadata command: Here, the -preview flag is optional and used to view the results as it is returned. In contrast, the table command, unlike the fields command, generally requires all inputs before it can emit any non-preview output. In this case, you would need to use the preview flag to be able to view the results of the search. 5. Return yesterday's internal events", "code_examples": [{"language": "spl", "code": "./splunk search\"session root daysago=1\""}, {"language": "spl", "code": "./splunk search'eventtype=webaccess error'-detachtrue"}, {"language": "spl", "code": "./splunk rtsearch'index=_internal'-earliest_time'rt-30s'-latest_time'rt+30s'"}, {"language": "spl", "code": "./splunk search'index=* | stats count by host | fields - count'-previewtrue"}, {"language": "spl", "code": "./splunk search'| metadata type=hosts | fields host'-previewtrue"}, {"language": "spl", "code": "./splunk search'index=_internal'-index_earliest -1d@d -index_latest @d"}], "tables": [], "chunk_index": 3, "total_chunks": 5, "metadata": {"title": "Syntax for searches in the CLI", "section_heading": "Examples", "section_id": "id_521ae635_3527_4554_87c2_51e6e67dfad3--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-in-the-cli/syntax-for-searches-in-the-cli", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search in the CLI", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:15.016851+00:00", "version": "10.2"}}
{"id": "90bd1dfdcecbf85c", "content": "About CLI searches Export data using the CLI in the Search Manual", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 5, "metadata": {"title": "Syntax for searches in the CLI", "section_heading": "See also", "section_id": "c746fa2d_98af_4510_9041_1129eede0824--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/search-in-the-cli/syntax-for-searches-in-the-cli", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Search in the CLI", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:15.016855+00:00", "version": "10.2"}}
{"id": "7e5ea7c21696d0a2", "content": "Use the prjob command for parallel reduce search processing of an SPL search in a distributed search environment. The prjob command analyzes the specified SPL search and attempts to reduce the search runtime by automatically placing a redistribute command in front of the first non-streaming SPL command like stats or transaction in the search. It provides the same functionality as the redistribute command, but with a simpler syntax. Similar to the redistribute command, use the prjob command to automatically speed up high cardinality searches that aggregate a large number of search results.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 4, "metadata": {"title": "prjob", "section_heading": "Description", "section_id": "id_47fcafa6_a22c_4e47_b6fb_01113284b159--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/prjob", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:31.913177+00:00", "version": "10.2"}}
{"id": "7268b1f1e59fd9d1", "content": "prjob [<subsearch>] or prjob [num_of_reducers=<int>] [subsearch] Required arguments subsearch Syntax: [<subsearch>] Description: Specifies the search string that the prjob command attempts to process in parallel. Optional arguments num_of_reducers Syntax: [num_of_reducers=<int>] Description: Specifies the number of eligible indexers from the indexer pool that may function as intermediate reducers. For example: When a search is run on 10 indexers and the configuration is set to use 60% of the indexer pool (with a maximum value of 5), it implies that only five indexers may be used as intermediate reducers. If the value of num_of_reducers is set to greater than 5, only five reducers are available due to the limit. If the value of of num_of_reducers is set to less than 5, the number of reducers used shrinks from the maximum limit of 5. The value for num_of_reducers is controlled by two groups of settings: reducers : maxReducersPerPhase + winningRate The number of intermediate reducers is determined by the value set for reducers. If no value is set for reducers , the search uses the values set for maxReducersPerPhase and winningRate to determine the number of intermediate reducers. For example: In a scenario where Splunk is configured so that the value of num_of_reducers is set to 50 percent of the indexer pool and the maxReducersPerPhase value is set to four indexers, a parallel reduce search that runs on six search peers will be assigned to run on three intermediate reducers. Similarly, a parallel reduce search that runs on four search peers, will be assigned to run on two intermediate reducers. However, searches that runs on ten search peers would be limited to the maximum of four intermediate reducers.", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 4, "metadata": {"title": "prjob", "section_heading": "Syntax", "section_id": "e9ebbfb7_eea2_48ac_9ef2_03ea188ab3db--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/prjob", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:31.913185+00:00", "version": "10.2"}}
{"id": "05cdb68f0adecf4e", "content": "Use the prjob command instead of the redistribute command when you want to run a parallel reduce job without determining where to insert the redistribute command or managing the by-clause field. The prjob command may be used only as the first command of a search. Additionally, you must include the entire search within the prjob command. To use the prjob command, set the phased_execution_mode to multithreaded or auto and set enabled to true in the [search_optimization::pr_job_extractor] stanza of the limits.conf configuration file. The prjob command does not support real time or verbose mode searches. Real time or verbose mode searches with the prjob command may run, but the redistribute operation will be ignored. Also, you may not use the prjob and the redistribute command within the same search. The prjob command supports the same commands as the redistribute command. For more information, see redistribute. The prjob command only reduces the search runtime of an SPL search that contains at least one of the following non-streaming commands: â€¦\" stats tstats streamstats eventstats sistats sichart sitimechart transaction (only on a single field)", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 4, "metadata": {"title": "prjob", "section_heading": "Usage", "section_id": "id_4fb1f23d_1f2c_49af_81fd_523f74a6e471--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/prjob", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:31.913190+00:00", "version": "10.2"}}
{"id": "ea1ae1aa5d136b94", "content": "Example 1: Using the prjob command in a search automatically places the redistribute command before the first non-streaming SPL command in the search. This speeds up a stats search that aggregates a large number of results. The stats count by host portion of the search is processed on the intermediate reducers and the search head aggregates the results. Therefore, the following search: is transformed to: Example 2: Speeds up a search that includes eventstats and uses sitimechart to perform the statistical calculations for a timechart operation. The intermediate reducers process eventstats , where , and sitimechart operations. The search head runs the timechart command to turn the reduced sitimechart statistics into sorted, visualization-ready results. Example 3: Speeds up a search that uses tstats to generate events. The tstats command must be placed at the start of the subsearch, and uses prestats=t to work with the timechart command. The sitimechart command is processed on the intermediate reducers and the timechart command is processed on the search head. Example 4: The eventstats and where commands are processed in parallel on the reducers, while the sort command and any other following commands are processed on the search head. This happens because the sort command is a non-streaming command that is not supported by the prjob command. Note: The prjob command does not have an impact on this search.", "code_examples": [{"language": "spl", "code": "| prjob [search index=myindex | stats count by host]"}, {"language": "spl", "code": "search index=myindex | redistribute | stats count by host"}, {"language": "spl", "code": "| prjob [search index=myindex | eventstats count by user,source|wherecount>10 | sitimechart max(count) bysource| timechart max(count) bysource]"}, {"language": "spl", "code": "| prjob [search index=myindex | tstats prestats=t count by _time span=1d | sitimechart span=1d count | timechart span=1d count]"}, {"language": "spl", "code": "| prjob [ search index=myindex | eventstats count by user,source|wherecount >10  | sort 0 -num(count) | ...]"}], "tables": [], "chunk_index": 3, "total_chunks": 4, "metadata": {"title": "prjob", "section_heading": "Examples", "section_id": "ad6b757d_1301_4e65_97c0_db7dbb140714--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/prjob", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:31.913195+00:00", "version": "10.2"}}
{"id": "1dbbaec66d1e780f", "content": "Creates a JSON object from the specified set of fields in the search results, and places the JSON object into a new field.", "code_examples": [], "tables": [], "chunk_index": 0, "total_chunks": 7, "metadata": {"title": "makejson", "section_heading": "Description", "section_id": "d4b41c6d_dc9c_48f3_a0d2_e7d7b5eb6273--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/makejson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:49.710171+00:00", "version": "10.2"}}
{"id": "07175d6d0e427759", "content": "makejson <wc-field-list> output=<string>", "code_examples": [], "tables": [], "chunk_index": 1, "total_chunks": 7, "metadata": {"title": "makejson", "section_heading": "Syntax", "section_id": "id_85d1603a_2c52_47d2_8846_45ee79e01dbf--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/makejson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:49.710178+00:00", "version": "10.2"}}
{"id": "2b5e52db2885110d", "content": "output Syntax: output=<string> Description: The name to use for the output field where the JSON object is placed.", "code_examples": [], "tables": [], "chunk_index": 2, "total_chunks": 7, "metadata": {"title": "makejson", "section_heading": "Required arguments", "section_id": "id_61d21c93_8ab3_4aca_87ac_918ad0e0fb26--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/makejson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:49.710182+00:00", "version": "10.2"}}
{"id": "41c61f49e21339ad", "content": "wc-field-list Syntax: <field>(,<field>) ... Description: Comma-delimited list of fields to use to generate a JSON object. You can use a wild card character in the field names. Default: All fields are included in the JSON object if a list is not specified.", "code_examples": [], "tables": [], "chunk_index": 3, "total_chunks": 7, "metadata": {"title": "makejson", "section_heading": "Optional arguments", "section_id": "b2a49b0f_a13e_4fd0_bf2f_a893ae7938e6--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/makejson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:49.710186+00:00", "version": "10.2"}}
{"id": "11ae5897eb179198", "content": "You cannot use the table or fields command to specify the field order in the JSON object that gets created.", "code_examples": [], "tables": [], "chunk_index": 4, "total_chunks": 7, "metadata": {"title": "makejson", "section_heading": "Usage", "section_id": "a2dce881_a507_47f5_8207_5d8bd3e3839c--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/makejson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:49.710189+00:00", "version": "10.2"}}
{"id": "1da8fad30a569fac", "content": "1. Create a JSON object using all of the available fields The following search create a JSON object in a field called \"data\" taking in values from all available fields. The makeresults command creates five search results that contain a timestamp. The eval command creates two fields in each search result. One field is named owner and contains the value vladimir. The other field is named error that takes a random number and uses the modulo mathematical operator ( % ) to divide the random number by 3. The makejson command creates a JSON object based on the values in the fields in each search result. The results look something like this: 2. Create a JSON object from a specific set of fields Consider the following data: The makejson command is used to create a JSON object in a field called \"data\" using the values from only the _time and owner fields. The error field is not included in the JSON object. The results look something like this: 3. Create a JSON object using a wildcard list of fields Create a JSON object in a field called \"json-object\" using the values from the _time field and fields that end in _owner. The results look something like this: 4. Use with schema-bound lookups You can use the makejson command with schema-bound lookups to store a JSON object in the description field for later processing. Suppose that a Splunk application comes with a KVStore collection called example_ioc_indicators , with the fields key and description. For long term supportability purposes you do not want to modify the collection, but simply want to utilize a custom lookup within a framework, such as Splunk Enterprise Security (ES) Threat Framework. Let's start with the first part of the search: This search produces a result that looks something like this: You would then add the outputlookup command to send the search results to the lookup: To use this custom lookup within a framework, you would specify this in a search:", "code_examples": [{"language": "spl", "code": "| makeresults count=5 |evalowner=\"vladimir\", error=random()%3 | makejson output=data"}, {"language": "spl", "code": "| makeresults count=7 |evalowner=\"claudia\", error=random()%5 | makejson _time, owner output=data"}, {"language": "spl", "code": "| makeresults count=5 |evalproduct_owner=\"wei\", system_owner=\"vanya\", error=random()%5 | makejson _time, *_owner output=\"json-object\""}, {"language": "spl", "code": "| makeresults count=1 \n|evalthreat=\"maliciousdomain.example\", threat_expiry=\"2020-01-01 21:13:37 UTC\", threat_name=\"Sample threat\", threat_campaign=\"Sample threat\", threat_confidence=\"100\"| makejson threat_expiry, threat_name, threat_campaign, threat_confidence output=description \n| table threat, description"}, {"language": "spl", "code": "... | outputlookup append=t example_ioc_indicators"}, {"language": "spl", "code": "...| lookup example_ioc_indicators OUTPUT description AS match_context | spath input=match_context"}], "tables": [{"headers": ["_time", "owner", "error", "data"], "rows": [["2020-03-10 21:45:14", "vladimir", "1", "{\"owner\": \"vladimir\", \"error\": 1, \"_time\": 1583901914}"], ["2020-03-10 21:45:14", "vladimir", "0", "{\"owner\": \"vladimir\", \"error\": 0, \"_time\": 1583901914}"], ["2020-03-10 21:45:14", "vladimir", "0", "{\"owner\": \"vladimir\", \"error\": 0, \"_time\": 1583901914}"], ["2020-03-10 21:45:14", "vladimir", "2", "{\"owner\": \"vladimir\", \"error\": 2, \"_time\": 1583901914}"], ["2020-03-10 21:45:14", "vladimir", "1", "{\"owner\": \"vladimir\", \"error\": 1, \"_time\": 1583901914}"]]}, {"headers": ["_time", "owner", "error_code"], "rows": [["2020-03-10 21:45:14", "claudia", "1"], ["2020-03-10 20:45:17", "alex", "4"], ["2020-03-10 06:48:11", "wei", "2"], ["2020-03-09 21:15:35", "david", "3"], ["2020-03-09 16:22:10", "maria", "4"], ["2020-03-08 23:32:56", "vanya", "1"], ["2020-03-07 14:05:14", "claudia", "2"]]}, {"headers": ["data"], "rows": [["{\"owner\": \"claudia\", \"_time\": 1583876714}"], ["{\"owner\": \"alex\", \"_time\": 1583873117}"], ["{\"owner\": \"wei\", \"_time\": 1583822891}"], ["{\"owner\": \"david\", \"_time\": 1583788535}"], ["{\"owner\": \"maria\", \"_time\": 1583770930}"], ["{\"owner\": \"vanya\", \"_time\": 1583710376}"], ["{\"owner\": \"claudia\", \"_time\": 1583589914}"]]}, {"headers": ["_time", "product_owner", "system_owner", "error", "json-object"], "rows": [["2020-03-10 22:23:24", "wei", "vanya", "3", "{\"product_owner\": \"wei\", \"system_owner\": \"vanya\", \"_time\": 1583904204}"], ["2020-03-10 22:23:24", "wei", "vanya", "2", "{\"product_owner\": \"wei\", \"system_owner\": \"vanya\", \"_time\": 1583904204}"], ["2020-03-10 22:23:24", "wei", "vanya", "1", "{\"product_owner\": \"wei\", \"system_owner\": \"vanya\", \"_time\": 1583904204}"], ["2020-03-10 22:23:24", "wei", "vanya", "3", "{\"product_owner\": \"wei\", \"system_owner\": \"vanya\", \"_time\": 1583904204}"], ["2020-03-10 22:23:24", "wei", "vanya", "2", "{\"product_owner\": \"wei\", \"system_owner\": \"vanya\", \"_time\": 1583904204}"]]}, {"headers": ["threat", "description"], "rows": [["maliciousdomain.example", "{\"threat_name\": \"Sample threat\", \"threat_confidence\": 100, \"threat_expiry\": \"2020-01-01 21:13:37 UTC\", \"threat_campaign\": \"Sample threat\"}"]]}], "chunk_index": 5, "total_chunks": 7, "metadata": {"title": "makejson", "section_heading": "Examples", "section_id": "id_6c0a2719_ec58_4ad2_822d_6789aabc4994--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/makejson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:49.710208+00:00", "version": "10.2"}}
{"id": "5c52bdefd1750959", "content": "Related commands spath", "code_examples": [], "tables": [], "chunk_index": 6, "total_chunks": 7, "metadata": {"title": "makejson", "section_heading": "See also", "section_id": "id_5b936360_2042_48f8_b141_d3ff7d981d40--en", "url": "https://help.splunk.com/en/splunk-enterprise/search/spl-search-reference/10.2/internal-commands/makejson", "breadcrumb": "Splunk Enterprise > Search > SPL Search Reference > Internal Commands", "manual": "spl-search-reference", "scraped_at": "2026-01-23T14:30:49.710212+00:00", "version": "10.2"}}
